<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.3.0',
    sidebar: {"position":"right","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Fidelty.">
<meta property="og:type" content="website">
<meta property="og:title" content="AutozLand">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="AutozLand">
<meta property="og:description" content="Fidelty.">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AutozLand">
<meta name="twitter:description" content="Fidelty.">






  <link rel="canonical" href="http://yoursite.com/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>AutozLand</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AutozLand</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Autoz's Learning Blogs</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives<span class="badge">12</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags<span class="badge">3</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories<span class="badge">2</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />About</a>
  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Developing-Data-Products/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/07/Developing-Data-Products/" itemprop="url">
                  Developing Data Products
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 14:17:19 / Modified: 14:20:31" itemprop="dateCreated datePublished" datetime="2018-08-07T14:17:19+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">20k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">18 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="shiny-tutorial">Shiny (<a href="http://rstudio.github.io/shiny/tutorial/" target="_blank" rel="noopener">tutorial</a>)</h2>
<ul>
<li>Shiny = platform for creating interactive R program embedded on web pages (made by RStudio)</li>
<li>knowledge of these helpful: HTML (web page structure), css (style), JavaScript (interactivity)</li>
<li><strong>OpenCPU</strong> = project created by Jerom Ooms providing API for creating more complex R/web apps</li>
<li><code>install.packages(&quot;shiny&quot;); library(shiny)</code> = install/load shiny package</li>
<li>capabilities
<ul>
<li>upload or download files</li>
<li>tabbed main panels</li>
<li>editable data tables</li>
<li>dynamic UI</li>
<li>user defined inputs/outputs</li>
<li>submit button to control when calculations/tasks are processed</li>
</ul></li>
</ul>
<h3 id="structure-of-shiny-app">Structure of Shiny App</h3>
<ul>
<li><strong>two</strong> scripts (in one directory) make up a Shiny project
<ul>
<li><code>ui.R</code> - controls appearance/all style elements
<ul>
<li>alternatively, a <code>www</code> directory with an <code>index.html</code> file enclosed can be used instead of <code>ui.R</code>
<ul>
<li><em><strong>Note</strong>: output is rendered into HTML elements based on matching their id attribute to an output slot and by specifying the requisite CSS class for the element (in this case either <code>shiny-text-output</code>, <code>shiny-plot-output</code>, or <code>shiny-html-output</code>) </em></li>
<li>it is possible to create highly customized user-interfaces using user-defined HTML/CSS/JavaScript</li>
</ul></li>
</ul></li>
<li><code>server.R</code> - controls functions</li>
</ul></li>
<li><code>runApp()</code> executes the Shiny application
<ul>
<li><code>runApp(display.mode = 'showcase')</code> = displays the code from <code>ui.R</code> and <code>server.R</code> and highlights what is being executed depending on the inputs</li>
</ul></li>
<li><em><strong>Note</strong>: <code>&quot;,&quot;</code> must be included <strong>ONLY INBETWEEN</strong> objects/functions on the same level </em></li>
</ul>
<h3 id="ui.r">ui.R</h3>
<ul>
<li><code>library(shiny)</code> = first line, loads the shiny package</li>
<li><code>shinyUI()</code> = shiny UI wrapper, contains sub-methods to create panels/parts/viewable object</li>
<li><code>pageWithSideBar()</code> = creates page with main/side bar division</li>
<li><code>headerPanel(&quot;title&quot;)</code> = specifies header of the page</li>
<li><code>sideBarPanel()</code> = specifies parameters/objects in the side bar (on the <strong>left</strong>)</li>
<li><code>mainPanel()</code> = specifies parameters/objects in the main panel (on the <strong>right</strong>)</li>
<li>for better control over style, use <code>shinyUI(fluidpage())</code> (<a href="http://shiny.rstudio.com/articles/layout-guide.html" target="_blank" rel="noopener">tutorial</a>) &lt;– produces responsive web pages
<ul>
<li><code>fluidRow()</code> = creates row of content with width 12 that can be subdivided into columns
<ul>
<li><code>column(4, ...)</code> = creates a column of width 4 within the fluid row</li>
<li><code>style = &quot;CSS&quot;</code> = can be used as the last element of the column to specify additional style</li>
</ul></li>
</ul></li>
<li><code>absolutePanel(top=0, left=0, right=0)</code> = used to produce floating panels on top of the page (<a href="http://shiny.rstudio.com/reference/shiny/latest/absolutePanel.html" target="_blank" rel="noopener">documentation</a>)
<ul>
<li><code>fixed = TRUE</code> = panel will not scroll with page, which means the panel will always stay in the same position as you scroll through the page</li>
<li><code>draggable = TRUE</code> = make panel movable by the user</li>
<li><code>top = 40</code> / <code>bottom = 50</code> = position from the top/bottom edge of the browser window
<ul>
<li><code>top = 0, bottom = 0</code> = creates panel that spans the entire vertical length of window</li>
</ul></li>
<li><code>left = 40</code> / <code>right = 50</code> = position from the left/right edge of the browser window
<ul>
<li><code>top = 0, bottom = 0</code> = creates panel that spans the entire horizontal length of window</li>
</ul></li>
<li><code>height = 30</code> / <code>width = 40</code> = specifies the height/width of the panel</li>
<li><code>style = &quot;opacity:0.92; z-index = 100&quot;</code> = makes panel transparent and ensures the panel is always the top-most element</li>
</ul></li>
<li><strong>content objects/functions</strong>
<ul>
<li><em><strong>Note</strong>: more HTML tags can be found <a href="http://shiny.rstudio.com/articles/tag-glossary.html" target="_blank" rel="noopener">here</a> </em></li>
<li><em><strong>Note</strong>: most of the content objects (h1, p, code, etc) can use <strong>both</strong> double and single quotes to specify values, just be careful to be consistent </em></li>
<li><code>h1/2/3/4('heading')</code> = creates heading for the panel</li>
<li><code>p('pargraph')</code> = creates regular text/paragraph</li>
<li><code>code('code')</code> = renders code format on the page</li>
<li><code>br()</code> = inserts line break</li>
<li><code>tags$hr()</code> = inserts horizontal line</li>
<li><code>tags$ol()</code>/ <code>tags$ul()</code> = initiates ordered/unordered list</li>
<li><code>div( ... , style = &quot;CSS Code&quot;)</code> / <code>span( ... , style = &quot;CSS Code&quot;)</code> = used to add additional style to particular parts of the app
<ul>
<li><code>div</code> should be used for a section/block, <code>span</code> should be used for a specific part/inline</li>
</ul></li>
<li><code>withMathJax()</code> = add this element to allow Shiny to process LaTeX
<ul>
<li>inline LaTex must be wrapped like this: <code>\\(LaTeX\\)</code></li>
<li>block equations are still wrapped by: <code>$$LaTeX$$</code></li>
</ul></li>
</ul></li>
<li><strong>inputs</strong>
<ul>
<li><code>textInput(inputId = &quot;id&quot;, label = &quot;textLabel&quot;)</code> = creates a plain text input field
<ul>
<li><code>inputId</code> = field identifier</li>
<li><code>label</code> = text that appear above/before a field</li>
</ul></li>
<li><code>numericInput('HTMLlabel', 'printedLabel', value = 0, min = 0, max = 10, step = 1)</code> = create a number input field with incrementer (up/down arrows)
<ul>
<li><code>'HTMLLabel'</code> = name given to the field, not printed, and can be called</li>
<li><code>'printedLabel'</code> = text that shows up above the input box explaining the field</li>
<li><code>value</code> = default numeric value that the field should take; 0 is an example</li>
<li><code>min</code> = minimum value that can be set in the field (if a smaller value is manually entered, then the value becomes the minimum specified once user clicks away from the field)</li>
<li><code>max</code> = max value that can be set in the field</li>
<li><code>step</code> = increments for the up/down arrows</li>
<li>more arguments can be found in <code>?numericInput</code></li>
</ul></li>
<li><code>checkboxGroupInput(&quot;id2&quot;, &quot;Checkbox&quot;,choices = c(&quot;Value 1&quot; = &quot;1&quot;, ...), selected = &quot;1&quot;, inline = TRUE)</code> = creates a series of checkboxes
<ul>
<li><code>&quot;id2&quot;, &quot;Checkbox&quot;</code> = field identifier/label</li>
<li><code>choices</code> = list of checkboxes and their labels
<ul>
<li>format = <code>&quot;checkboxName&quot; = &quot;fieldIdentifier&quot;</code></li>
<li><em><strong>Note</strong>: <code>fieldIdentifier</code> should generally be different from checkbox to checkbox, so we can properly identify the responses </em></li>
</ul></li>
<li><code>selected</code> = specifies the checkboxes that should be selected by default; uses <code>fieldIndentifier</code> values</li>
<li><code>inline</code> = whether the options should be displayed inline</li>
</ul></li>
<li><code>dateInput(&quot;fieldID&quot;, &quot;fieldLabel&quot;)</code> = creates a selectable date field (dropdown calendar/date picker automatically generated)
<ul>
<li><code>&quot;fieldID&quot;</code> = field identifier</li>
<li><code>&quot;fieldLabel&quot;</code> = text/name displayed above fields</li>
<li>more arguments can be found in <code>?dateInput</code></li>
</ul></li>
<li><code>submitButton(&quot;Submit&quot;)</code> = creates a submit button that updates the output/calculations only when the user submits the new inputs (default behavior = all changes update reactively/in real time)</li>
<li><code>actionButton(inputId = &quot;goButton&quot;, label = &quot;test&quot;)</code> = creates a button with the specified label and id
<ul>
<li>output can be specified for when the button is clicked</li>
</ul></li>
<li><code>sliderInput(&quot;id&quot;, &quot;label&quot;, value = 70, min = 62, max = 74, 0.05)</code> = creates a slider for input
<ul>
<li>arguments similar to <code>numericInput</code> and more information can be found <code>?sliderInput</code></li>
</ul></li>
</ul></li>
<li><strong>outputs</strong>
<ul>
<li><em><strong>Note</strong>: every variable called here must have a corresponding method corresponding method from the <code>output</code> element in <code>server.R</code> to render their value </em></li>
<li><code>textOutput(&quot;fieldId&quot;, inline = FALSE)</code> = prints the value of the variable/field in text format
<ul>
<li><code>inline = TRUE</code> = inserts the result inline with the HTML element</li>
<li><code>inline = FALSE</code> = inserts the result in block code format</li>
</ul></li>
<li><code>verbatimTextOutput(&quot;fieldId&quot;)</code> = prints out the value of the specified field defined in <code>server.R</code></li>
<li><code>plotOutput('fieldId')</code> = plots the output (‘sampleHist’ for example) created from <code>server.R</code> script</li>
<li><code>output$test &lt;- renderText({input$goButton}); isolate(paste(input$t1, input$2))})</code> = <code>isolate</code> action executes when the button is pressed
<ul>
<li><code>if (input$goButton == 1){  Conditional statements }</code> = create different behavior depending on the number of times the button is pressed</li>
</ul></li>
</ul></li>
</ul>
<h3 id="ui.r-example">ui.R Example</h3>
<ul>
<li>below is part of the ui.R code for a project on Shiny</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load shiny package</span></span><br><span class="line"><span class="keyword">library</span>(shiny)</span><br><span class="line"><span class="comment"># begin shiny UI</span></span><br><span class="line">shinyUI(navbarPage(<span class="string">"Shiny Project"</span>,</span><br><span class="line">	<span class="comment"># create first tab</span></span><br><span class="line">	tabPanel(<span class="string">"Documentation"</span>,</span><br><span class="line">		<span class="comment"># load MathJax library so LaTeX can be used for math equations</span></span><br><span class="line">        withMathJax(), h3(<span class="string">"Why is the Variance Estimator \\(S^2\\) divided by \\(n-1?\\)"</span>),</span><br><span class="line">        <span class="comment"># paragraph and bold text</span></span><br><span class="line">        p(<span class="string">"The "</span>, strong(<span class="string">"sample variance"</span>),<span class="string">" can be calculated in "</span>, strong(em(<span class="string">"two"</span>)),</span><br><span class="line">          <span class="string">" different ways:"</span>,</span><br><span class="line">          <span class="string">"$$S^2 \\mbox&#123;(unbiased)&#125; = \\frac&#123;\\sum_&#123;i=1&#125;^n (X_i - \\bar X)^2&#125;&#123;n-1&#125;</span></span><br><span class="line"><span class="string">          ~~~\\mbox&#123;and&#125;~~S^2\\mbox&#123;(biased)&#125;=\\frac&#123;\\sum_&#123;i=1&#125;^n (X_i-\\bar X)^2&#125;&#123;n&#125;$$"</span>,</span><br><span class="line">          <span class="string">"The unbiased calculation is most often used, as it provides a "</span>,</span><br><span class="line">          strong(em(<span class="string">"more accurate"</span>)), <span class="string">" estimate of population variance"</span>),</span><br><span class="line">        <span class="comment"># break used to space sections</span></span><br><span class="line">        br(), p(<span class="string">"To show this empirically, we simulated the following in the "</span>,</span><br><span class="line">                strong(<span class="string">"Simulation Experiment"</span>), <span class="string">" tab: "</span>), br(),</span><br><span class="line">        <span class="comment"># ordered list</span></span><br><span class="line">        tags$ol(</span><br><span class="line">            tags$li(<span class="string">"Create population by drawing observations from values 1 to 20."</span>),</span><br><span class="line">            tags$li(<span class="string">"Draw a number of samples of specified size from the population"</span>),</span><br><span class="line">            tags$li(<span class="string">"Plot difference between sample and true population variance"</span>),</span><br><span class="line">            tags$li(<span class="string">"Show the effects of sample size vs accuracy of variance estimated"</span>)</span><br><span class="line">        )),</span><br><span class="line">    <span class="comment"># second tab</span></span><br><span class="line">	tabPanel(<span class="string">"Simulation Experiment"</span>,</span><br><span class="line">        <span class="comment"># fluid row for space holders</span></span><br><span class="line">        fluidRow(</span><br><span class="line">        	<span class="comment"># fluid columns</span></span><br><span class="line">            column(<span class="number">4</span>, div(style = <span class="string">"height: 150px"</span>)),</span><br><span class="line">            column(<span class="number">4</span>, div(style = <span class="string">"height: 150px"</span>)),</span><br><span class="line">            column(<span class="number">4</span>, div(style = <span class="string">"height: 150px"</span>))),</span><br><span class="line">        <span class="comment"># main content</span></span><br><span class="line">        fluidRow(</span><br><span class="line">            column(<span class="number">12</span>,h4(<span class="string">"We start by generating a population of "</span>,</span><br><span class="line">                         span(textOutput(<span class="string">"population"</span>, inline = <span class="literal">TRUE</span>),</span><br><span class="line">                         style = <span class="string">"color: red; font-size: 20px"</span>),</span><br><span class="line">                         <span class="string">" observations from values 1 to 20:"</span>),</span><br><span class="line">                   tags$hr(),htmlOutput(<span class="string">"popHist"</span>),</span><br><span class="line">                   <span class="comment"># additional style</span></span><br><span class="line">                   style = <span class="string">"padding-left: 20px"</span></span><br><span class="line">            )</span><br><span class="line">        ),</span><br><span class="line">        <span class="comment"># absolute panel</span></span><br><span class="line">        absolutePanel(</span><br><span class="line">        	<span class="comment"># position attributes</span></span><br><span class="line">            top = <span class="number">50</span>, left = <span class="number">0</span>, right =<span class="number">0</span>,</span><br><span class="line">            fixed = <span class="literal">TRUE</span>,</span><br><span class="line">            <span class="comment"># panel with predefined background</span></span><br><span class="line">            wellPanel(</span><br><span class="line">                fluidRow(</span><br><span class="line">                	<span class="comment"># sliders</span></span><br><span class="line">                    column(<span class="number">4</span>, sliderInput(<span class="string">"population"</span>, <span class="string">"Size of Population:"</span>,</span><br><span class="line">                                          min = <span class="number">100</span>, max = <span class="number">500</span>, value = <span class="number">250</span>),</span><br><span class="line">                           p(strong(<span class="string">"Population Variance: "</span>),</span><br><span class="line">                           textOutput(<span class="string">"popVar"</span>, inline = <span class="literal">TRUE</span>))),</span><br><span class="line">                    column(<span class="number">4</span>, sliderInput(<span class="string">"numSample"</span>, <span class="string">"Number of Samples:"</span>,</span><br><span class="line">                                          min = <span class="number">100</span>, max = <span class="number">500</span>, value = <span class="number">300</span>),</span><br><span class="line">                           p(strong(<span class="string">"Sample Variance (biased): "</span>),</span><br><span class="line">                           textOutput(<span class="string">"biaVar"</span>, inline = <span class="literal">TRUE</span>))),</span><br><span class="line">                    column(<span class="number">4</span>, sliderInput(<span class="string">"sampleSize"</span>, <span class="string">"Size of Samples:"</span>,</span><br><span class="line">                                          min = <span class="number">2</span>, max = <span class="number">15</span>, value = <span class="number">10</span>),</span><br><span class="line">                           p(strong(<span class="string">"Sample Variance (unbiased): "</span>),</span><br><span class="line">                           textOutput(<span class="string">"unbiaVar"</span>, inline = <span class="literal">TRUE</span>)))),</span><br><span class="line">                style = <span class="string">"opacity: 0.92; z-index: 100;"</span></span><br><span class="line">            ))</span><br><span class="line">	)</span><br><span class="line">))</span><br></pre></td></tr></table></figure>
<h3 id="server.r">server.R</h3>
<ul>
<li>preamble/code to set up environment (executed only <strong><em>once</em></strong>)
<ul>
<li>start with <code>library()</code> calls to load packages/data</li>
<li>define/initiate variables and relevant default values
<ul>
<li><code>&lt;&lt;-</code> operator should be used to assign values to variables in the parent environment</li>
<li><code>x &lt;&lt;- x + 1</code> will define x to be the sum of 1 and the value of x (defined in the parent environment/working environment)</li>
</ul></li>
<li>any other code that you would like to only run once</li>
</ul></li>
<li><code>shinyServer()</code> = initiates the server function
<ul>
<li><code>function(input, output){}</code> = defines a function that performs actions on the inputs user makes and produces an output object</li>
<li><strong>non-reactive</strong> statements/code will be executed <strong><em>once for each page refresh/submit</em></strong></li>
<li><strong>reactive</strong> functions/code are <strong><em>run repeatedly</em></strong> as values are updated (i.e. render)
<ul>
<li><em><strong>Note</strong>: Shiny only runs what is needed for reactive statements, in other words, the rest of the code is left alone </em></li>
<li><code>reactive(function)</code> = can be used to wrap functions/expressions to create reactive expressions
<ul>
<li><code>renderText({x()})</code> = returns value of x, “()” must be included (syntax)</li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>reactive function example</strong> <figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># start shinyServer</span></span><br><span class="line">shinyServer(</span><br><span class="line">    <span class="comment"># specify input/output function</span></span><br><span class="line">    <span class="keyword">function</span>(input, output) &#123;</span><br><span class="line">        <span class="comment"># set x as a reactive function that adds 100 to input1</span></span><br><span class="line">        x &lt;- reactive(&#123;as.numeric(input$text1)+<span class="number">100</span>&#125;)</span><br><span class="line">        <span class="comment"># set value of x to output object text1</span></span><br><span class="line">        output$text1 &lt;- renderText(&#123;x()                          &#125;)</span><br><span class="line">        <span class="comment"># set value of x plus value of input object text2 to output object text1</span></span><br><span class="line">        output$text2 &lt;- renderText(&#123;x() + as.numeric(input$text2)&#125;)</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p></li>
<li><strong>functions/output objects in <code>shinyServer()</code></strong>
<ul>
<li><code>output$oid1 &lt;- renderPrint({input$id1})</code> = stores the user input value in field <code>id1</code> and stores the rendered, printed text in the <code>oid1</code> variable of the <code>output</code> object
<ul>
<li><code>renderPrint({expression})</code> = reactive function to render the specified expression</li>
<li><code>{}</code> is used to ensure the value is an expression</li>
<li><code>oid1</code> = variable in the output object that stores the result from the subsequent command</li>
</ul></li>
<li><code>output$sampleHist &lt;- renderPlot({code})</code> = stores plot generated by code into <code>sampleHist</code> variable
<ul>
<li><code>renderPlot({code})</code> = renders a plot generated by the enclosed R code</li>
</ul></li>
<li><code>output$sampleGVisPlot &lt;- renderGvis({code})</code> = renders Google Visualization object</li>
</ul></li>
</ul>
<h3 id="server.r-example">server.R Example</h3>
<ul>
<li>below is part of the server.R code for a project on Shiny that uses googleVis</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load libraries</span></span><br><span class="line"><span class="keyword">library</span>(shiny)</span><br><span class="line"><span class="keyword">require</span>(googleVis)</span><br><span class="line"><span class="comment"># begin shiny server</span></span><br><span class="line">shinyServer(<span class="keyword">function</span>(input, output) &#123;</span><br><span class="line">	<span class="comment"># define reactive parameters</span></span><br><span class="line">    pop&lt;- reactive(&#123;sample(<span class="number">1</span>:<span class="number">20</span>, input$population, replace = <span class="literal">TRUE</span>)&#125;)</span><br><span class="line">    bootstrapSample&lt;-reactive(&#123;sample(pop(),input$sampleSize*input$numSample,</span><br><span class="line">    	replace = <span class="literal">TRUE</span>)&#125;)</span><br><span class="line">    popVar&lt;- reactive(&#123;round(var(pop()),<span class="number">2</span>)&#125;)</span><br><span class="line">    <span class="comment"># print text through reactive funtion</span></span><br><span class="line">    output$biaVar &lt;- renderText(&#123;</span><br><span class="line">        sample&lt;- as.data.frame(matrix(bootstrapSample(), nrow = input$numSample,</span><br><span class="line">        	ncol =input$sampleSize))</span><br><span class="line">        <span class="keyword">return</span>(round(mean(rowSums((sample-rowMeans(sample))^<span class="number">2</span>)/input$sampleSize), <span class="number">2</span>))</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment"># google visualization histogram</span></span><br><span class="line">    output$popHist &lt;- renderGvis(&#123;</span><br><span class="line">        popHist &lt;- gvisHistogram(data.frame(pop()), options = list(</span><br><span class="line">            height = <span class="string">"300px"</span>,</span><br><span class="line">            legend = <span class="string">"&#123;position: 'none'&#125;"</span>, title = <span class="string">"Population Distribution"</span>,</span><br><span class="line">            subtitle = <span class="string">"samples randomly drawn (with replacement) from values 1 to 20"</span>,</span><br><span class="line">            histogram = <span class="string">"&#123; hideBucketItems: true, bucketSize: 2 &#125;"</span>,</span><br><span class="line">            hAxis = <span class="string">"&#123; title: 'Values', maxAlternation: 1, showTextEvery: 1&#125;"</span>,</span><br><span class="line">            vAxis = <span class="string">"&#123; title: 'Frequency'&#125;"</span></span><br><span class="line">        ))</span><br><span class="line">        <span class="keyword">return</span>(popHist)</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="distributing-shiny-application">Distributing Shiny Application</h3>
<ul>
<li>running code locally = running local server and browser routing through local host</li>
<li>quickest way = send application directory</li>
<li>possible to create R package and create a wrapper that calls <code>runApp</code> (requires R knowledge)</li>
<li>another option = run shiny server (<a href="http://www.rstudio.com/shiny/server/" target="_blank" rel="noopener">link</a>)</li>
</ul>
<h3 id="debugging">Debugging</h3>
<ul>
<li><code>runApp(display.mode = 'showcase')</code> = highlights execution while running a shiny application</li>
<li><code>cat</code> = can be used to display output to stdout/R console</li>
<li><code>browser()</code> = interrupts execution (<a href="http://shiny.rstudio.com/articles/debugging.html" target="_blank" rel="noopener">tutorial</a>)</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="manipulate-package"><code>manipulate</code> Package</h2>
<ul>
<li><code>manipulate</code> = package/function can be leveraged to create quick interactive graphics by allowing the user to vary the different variables to a model/calculation</li>
<li>creates sliders/checkbox/picker for the user (<a href="http://www.rstudio.com/ide/docs/advanced/manipulate" target="_blank" rel="noopener">documentation</a>)</li>
</ul>
<h3 id="example">Example</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data and manipulate package</span></span><br><span class="line"><span class="keyword">library</span>(UsingR)</span><br><span class="line"><span class="keyword">library</span>(manipulate)</span><br><span class="line"><span class="comment"># plotting function</span></span><br><span class="line">myHist &lt;- <span class="keyword">function</span>(mu)&#123;</span><br><span class="line">	<span class="comment"># histogram</span></span><br><span class="line">  	hist(galton$child,col=<span class="string">"blue"</span>,breaks=<span class="number">100</span>)</span><br><span class="line">  	<span class="comment"># vertical line to highlight the mean</span></span><br><span class="line">  	lines(c(mu, mu), c(<span class="number">0</span>, <span class="number">150</span>),col=<span class="string">"red"</span>,lwd=<span class="number">5</span>)</span><br><span class="line">  	<span class="comment"># calculate mean squared error</span></span><br><span class="line">  	mse &lt;- mean((galton$child - mu)^<span class="number">2</span>)</span><br><span class="line">  	<span class="comment"># updates the mean value as the mean is changed by the user</span></span><br><span class="line">  	text(<span class="number">63</span>, <span class="number">150</span>, paste(<span class="string">"mu = "</span>, mu))</span><br><span class="line">  	<span class="comment"># updates the mean squared error value as the mean is changed by the user</span></span><br><span class="line">  	text(<span class="number">63</span>, <span class="number">140</span>, paste(<span class="string">"MSE = "</span>, round(mse, <span class="number">2</span>)))</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># creates a slider to vary the mean for the histogram</span></span><br><span class="line">manipulate(myHist(mu), mu = slider(<span class="number">62</span>, <span class="number">74</span>, step = <span class="number">0.5</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library(grid); library(png)</span><br><span class="line">grid.raster(readPNG(&quot;figures/5.png&quot;))</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="rcharts">rCharts</h2>
<ul>
<li>rCharts = simple way of creating interactive JavaScript visualization using R
<ul>
<li>more in-depth knowledge of D3 will be needed to create more complex tools</li>
<li>written by Ramnath Vaidyanathan</li>
<li>uses formula interface to specify plots (like the <code>lattice</code> plotting system)</li>
<li>displays interactive tool tips when hovering over data points on the plots</li>
</ul></li>
<li><strong>installation</strong>
<ul>
<li><code>devtools</code> must be installed first (<code>install.packages(&quot;devtools&quot;)</code>)</li>
<li><code>require(devtools); install_github('rCharts', 'ramnathv')</code> installs the rCharts package from GitHub</li>
</ul></li>
<li><strong>plot types</strong>
<ul>
<li><em><strong>Note</strong>: each of the following JS library has different default styles as well as a multitude of capabilities; the following list is just what was demonstrated and more documentation can be found in the corresponding links </em></li>
<li>[<em>Polychart</em> <a href="https://github.com/Polychart/polychart2" target="_blank" rel="noopener">js library</a>] <code>rPlot</code> = paneled scatter plots</li>
<li>[<em>Morris</em> <a href="https://github.com/oesmith/morris.js" target="_blank" rel="noopener">js library</a>] <code>mPlot</code> = time series plot (similar to stock price charts)</li>
<li>[<em>NVD3</em> <a href="http://nvd3.org/" target="_blank" rel="noopener">js library</a>] <code>nPlot</code> = stacked/grouped bar charts</li>
<li>[<em>xCharts</em> <a href="https://github.com/tenXer/xcharts/" target="_blank" rel="noopener">js library</a>] = shaded line graphs</li>
<li>[<em>HighChart</em> <a href="http://www.highcharts.com/" target="_blank" rel="noopener">js library</a>] = stacked (different styles) scatter/line charts</li>
<li>[<em>LeafLet</em> <a href="http://leafletjs.com/" target="_blank" rel="noopener">js library</a>] = interactive maps</li>
<li>[<em>Rickshaw</em> <a href="http://ramnathv.github.io/rCharts/" target="_blank" rel="noopener">js library</a>] = stacked area plots/time series</li>
</ul></li>
<li>rChart objects have various attributes/functions that you can use when saved <code>n1 &lt;- nplot(...)</code>
<ul>
<li><code>n1$</code> + TAB in R Console brings up list of all functions contained in the object</li>
<li><code>n1$html()</code> = prints out the HTML for the plot</li>
<li><code>n1$save(&quot;filename.html&quot;)</code> = saves result to a file named “filename.html”</li>
<li><code>n1$print()</code> = print out the JavaScript</li>
<li><code>n1$show(&quot;inline&quot;, include_assets = TRUE, cdn = F)</code> = embed HTML/JS code directly with in Rmd file (for HTML output)
<ul>
<li><code>n1$publish('plotname', host = 'gist'/'rpubs')</code> = publishes the plot under the specified <code>plotname</code> as a <code>gist</code> or to <code>rpubs</code></li>
</ul></li>
</ul></li>
<li>to use with <strong><em>slidify</em></strong>,
<ul>
<li>the following YAML (Yet Another Markup Language/YAML Ain’t Markup Language) must be added
<ul>
<li><code>yaml ext_widgets : {rCharts: [&quot;libraries/nvd3&quot;]}</code></li>
</ul></li>
<li><code>cat('&lt;iframe src=&quot;map3.html&quot; width=100%, height=600&gt;&lt;/iframe&gt;')</code> to embed a map or chart form a saved file (saved with: <code>map3$save('map3.html', cdn = TRUE)</code>)</li>
</ul></li>
</ul>
<h3 id="example-1">Example</h3>
<figure class="highlight plain"><figcaption><span>results </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># load rCharts package</span><br><span class="line">require(rCharts); library(datasets); library(knitr)</span><br><span class="line"># create dataframe with HairEyeColor data</span><br><span class="line">haireye = as.data.frame(HairEyeColor)</span><br><span class="line"># create a nPlot object</span><br><span class="line">n1 &lt;- nPlot(Freq ~ Hair, group = &apos;Eye&apos;, type = &apos;multiBarChart&apos;,</span><br><span class="line">			data = subset(haireye, Sex == &apos;Male&apos;))</span><br><span class="line"># save the nPlot object to a html page</span><br><span class="line">n1$show(&quot;inline&quot;, include_assets = TRUE, cdn = F)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/4.png&quot;))</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="ggvis-package"><code>ggvis</code> package</h2>
<ul>
<li><code>ggvis</code> is a data visualization package for R that lets you:
<ul>
<li>declaratively describe data graphics with a syntax similar in spirit to ggplot2</li>
<li>create rich interactive graphics that you can play with locally in Rstudio or in your browser</li>
<li>leverage <code>shiny</code>’s infrastructure to publish interactive graphics usable from any browser (either within your company or to the world).</li>
</ul></li>
<li>the goal is to combine the best of R and the best of the web
<ul>
<li>data manipulation and transformation are done in R</li>
<li>graphics are rendered in a web browser, using <a href="https://github.com/trifacta/vega/" target="_blank" rel="noopener">Vega</a></li>
<li>for RStudio users, ggvis graphics display in a viewer panel, which is possible because RStudio is a web browser</li>
</ul></li>
<li>can use the <strong>pipe operator</strong>, <code>%&gt;%</code>, to chain graphing functions
<ul>
<li><code>set_options(renderer = &quot;canvas&quot;)</code> = can be used to control what renderer the graphics is produced with</li>
<li><strong><em>example</em></strong>: <code>mtcars %&gt;% ggvis(~mpg, ~wt, fill = ~ as.factor(am)) %&gt;% layer_points() %&gt;% layer_smooths()</code></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for pdf version</span></span><br><span class="line"><span class="keyword">library</span>(ggvis)</span><br><span class="line">mtcars %&gt;% ggvis(~mpg, ~wt, fill = ~ as.factor(am)) %&gt;% layer_points() %&gt;% layer_smooths()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/6.png&quot;))</span><br></pre></td></tr></table></figure>
<!-- <figure class="highlight plain"><figcaption><span>echo</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"># for HTML version</span><br><span class="line">library(ggvis)</span><br><span class="line">mtcars %&gt;% ggvis(~mpg, ~wt, fill = ~ as.factor(am)) %&gt;% layer_points() %&gt;% layer_smooths() %&gt;% set_options(renderer = &quot;canvas&quot;)</span><br><span class="line">``` --&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$\pagebreak$</span><br><span class="line"></span><br><span class="line">## GoogleVis API</span><br><span class="line">* GoogleVis allows R to create interactive HTML graphics (Google Charts)</span><br><span class="line">* **chart types** (format = gvis+ChartType)</span><br><span class="line">	- Motion charts:  `gvisMotionChart`</span><br><span class="line">	- Interactive maps: `gvisGeoChart`</span><br><span class="line">	- Interactive tables: `gvisTable`</span><br><span class="line">	- Line charts: `gvisLineChart`</span><br><span class="line">	- Bar charts: `gvisColumnChart`</span><br><span class="line">	- Tree maps: `gvisTreeMap`</span><br><span class="line">	- more charts can be found [here](http://cran.r-project.org/web/packages/googleVis/googleVis.pdf)</span><br><span class="line">* configuration options and default values for arguments for each of the plot types can be found [here](https://developers.google.com/chart/interactive/docs/gallery/geochart)</span><br><span class="line">* `print(chart, &quot;chart&quot;)` = prints the JavaScript for creating the interactive plot so it can be embedded in slidify/HTML document</span><br><span class="line">	- `print(chart)` = prints HTML + JavaScript directly</span><br><span class="line">* alternatively, to print the charts on a HTML page, you can use `op &lt;- options(gvis.plot.tag=&apos;chart&apos;)`</span><br><span class="line">  - this sets the googleVis options first to change the behaviour of `plot.gvis`, so that ***only the chart component*** of the HTML file is written into the output file</span><br><span class="line">  - `plot(chart)` can then be called to print the plots to HTML</span><br><span class="line">* `gvisMerge(chart1, chart2, horizontal = TRUE, tableOptions = &quot;bgcolor = \&quot;#CCCCCC\&quot; cellspacing = 10)` = combines the two plots into one horizontally (1 x 2 panel)</span><br><span class="line">	- ***Note**: `gvisMerge()` can only combine **TWO** plots at a time *</span><br><span class="line">	- `horizontal = FALSE` = combines plots vertically (TRUE for horizontal combination)</span><br><span class="line">	- `tableOptions = ...` = used to specify attributes of the combined plot</span><br><span class="line">* `demo(googleVis)` = demos how each of the plot works</span><br><span class="line">* **resources**</span><br><span class="line">	- [vignette](http://cran.r-project.org/web/packages/googleVis/vignettes/googleVis.pdf)</span><br><span class="line">	- [documentation](http://cran.r-project.org/web/packages/googleVis/googleVis.pdf)</span><br><span class="line">	- [plot gallery](https://developers.google.com/chart/interactive/docs/gallery)</span><br><span class="line">	- [FAQ](https://developers.google.com/chart/interactive/faq)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$\pagebreak$</span><br><span class="line"></span><br><span class="line">### Example (line chart)</span><br><span class="line"></span><br><span class="line">```&#123;r results=&quot;asis&quot;, message=FALSE&#125;</span><br><span class="line"># load googleVis package</span><br><span class="line">library(googleVis)</span><br><span class="line"># set gvis.plot options to only return the chart</span><br><span class="line">op &lt;- options(gvis.plot.tag=&apos;chart&apos;)</span><br><span class="line"># create initial data with x variable as &quot;label&quot; and y variable as &quot;var1/var2&quot;</span><br><span class="line">df &lt;- data.frame(label=c(&quot;US&quot;, &quot;GB&quot;, &quot;BR&quot;), val1=c(1,3,4), val2=c(23,12,32))</span><br><span class="line"># set up a gvisLineChart with x and y</span><br><span class="line">Line &lt;- gvisLineChart(df, xvar=&quot;label&quot;, yvar=c(&quot;val1&quot;,&quot;val2&quot;),</span><br><span class="line">		# set options for the graph (list) - title and location of legend</span><br><span class="line">        options=list(title=&quot;Hello World&quot;, legend=&quot;bottom&quot;,</span><br><span class="line">        		# set title text style</span><br><span class="line">                titleTextStyle=&quot;&#123;color:&apos;red&apos;, fontSize:18&#125;&quot;,</span><br><span class="line">                # set vertical gridlines</span><br><span class="line">                vAxis=&quot;&#123;gridlines:&#123;color:&apos;red&apos;, count:3&#125;&#125;&quot;,</span><br><span class="line">                # set horizontal axis title and style</span><br><span class="line">                hAxis=&quot;&#123;title:&apos;My Label&apos;, titleTextStyle:&#123;color:&apos;blue&apos;&#125;&#125;&quot;,</span><br><span class="line">                # set plotting style of the data</span><br><span class="line">                series=&quot;[&#123;color:&apos;green&apos;, targetAxisIndex: 0&#125;,</span><br><span class="line">                         &#123;color: &apos;blue&apos;,targetAxisIndex:1&#125;]&quot;,</span><br><span class="line">                # set vertical axis labels and formats</span><br><span class="line">                vAxes=&quot;[&#123;title:&apos;Value 1 (%)&apos;, format:&apos;##,######%&apos;&#125;,</span><br><span class="line">                                  &#123;title:&apos;Value 2 (\U00A3)&apos;&#125;]&quot;,</span><br><span class="line">                # set line plot to be smoothed and set width and height of the plot</span><br><span class="line">                curveType=&quot;function&quot;, width=500, height=300</span><br><span class="line">                ))</span><br><span class="line"># print the chart in JavaScript</span><br><span class="line">plot(Line)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/1.png&quot;))</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h3 id="example-merging-graphs">Example (merging graphs)</h3>
<figure class="highlight plain"><figcaption><span>results </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">G &lt;- gvisGeoChart(Exports, &quot;Country&quot;, &quot;Profit&quot;,options=list(width=200, height=100))</span><br><span class="line">T1 &lt;- gvisTable(Exports,options=list(width=200, height=270))</span><br><span class="line">M &lt;- gvisMotionChart(Fruits, &quot;Fruit&quot;, &quot;Year&quot;, options=list(width=400, height=370))</span><br><span class="line">GT &lt;- gvisMerge(G,T1, horizontal=FALSE)</span><br><span class="line">GTM &lt;- gvisMerge(GT, M, horizontal=TRUE,tableOptions=&quot;bgcolor=\&quot;#CCCCCC\&quot; cellspacing=10&quot;)</span><br><span class="line">plot(GTM)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/2.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: the motion chart only displays when it is hosted on a server or a trusted Macromedia source, see <a href="http://cran.r-project.org/web/packages/googleVis/vignettes/googleVis.pdf">googlVis vignette</a> for more details </em></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="shinyapps.io-link">ShinyApps.io (<a href="https://www.shinyapps.io">link</a>)</h2>
<ul>
<li>platform created by RStudio to share Shiny apps on the web</li>
<li>log in through GitHub/Google, and set up access in R
<ol type="1">
<li>Make sure you have <code>devtools</code> installed in R (<code>install.packages(&quot;devtools&quot;)</code>)</li>
<li>enter <code>devtools::install_github('rstudio/shinyapps')</code>, which installs the <code>shinyapps</code> package from GitHub</li>
<li>follow the instructions to authenticate your shiny apps account in R through the generated token</li>
<li>publish your app through <code>deployApp()</code> command</li>
</ol></li>
<li>the apps your deploy will be hosted on ShinyApps.io under your account</li>
</ul>
<h2 id="plot.ly-link">plot.ly (<a href="https://plot.ly/">link</a>)</h2>
<ul>
<li>platform share and edit plots modularly on the web (<a href="https://plot.ly/feed/">examples</a>)
<ul>
<li>every part of the plot can be customized and modified</li>
<li>graphs can be converted from one language to another</li>
</ul></li>
<li>you can choose to log in through Facebook/Twitter/Google/GitHub
<ol type="1">
<li>make sure you have <code>devtools</code> installed in R</li>
<li>enter <code>devtools::install_github(&quot;ropensci/plotly&quot;)</code>, which installs <code>plotly</code> package from GitHub</li>
<li>go to <a href="https://plot.ly/r/getting-started/" class="uri">https://plot.ly/r/getting-started/</a> and follow the instructions</li>
<li>enter <code>library(plotly); set_credentials_file(&quot;&lt;username&gt;&quot;, &quot;&lt;token&gt;&quot;)</code> with the appropriate username and token filled in</li>
<li>use <code>plotly()</code> methods to upload plots to your account</li>
<li>modify any part of the plot as you like once uploaded</li>
<li>share the plot</li>
</ol></li>
</ul>
<h3 id="example-2">Example</h3>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># load packages</span><br><span class="line">library(plotly); library(ggplot2)</span><br><span class="line"># make sure your plot.ly credentials are set correctly using the following command</span><br><span class="line">#   set_credentials_file(username=&lt;FILL IN&gt;, api_key=&lt;FILL IN&gt;)</span><br><span class="line"></span><br><span class="line"># load data</span><br><span class="line">load(&quot;courseraData.rda&quot;)</span><br><span class="line"># bar plot using ggplot2</span><br><span class="line">g &lt;- ggplot(myData, aes(y = enrollment, x = class, fill = as.factor(offering)))</span><br><span class="line">g &lt;- g + geom_bar(stat = &quot;identity&quot;)</span><br><span class="line">g</span><br><span class="line"># initiate plotly object</span><br><span class="line">py &lt;- plotly()</span><br><span class="line"># interface with plot.ly and ggplot2 to upload the plot to plot.ly under your credentials</span><br><span class="line">out &lt;- py$ggplotly(g)</span><br><span class="line"># typing this in R console will return the url of the generated plot</span><br><span class="line">out$response$url</span><br></pre></td></tr></table></figure>
<ul>
<li>the above is the response URL for the plot created on plot.ly
<ul>
<li><em><strong>Note</strong>: this particular URL corresponds to the plot on my personal account </em></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="structure-of-a-data-analysis-report">Structure of a Data Analysis Report</h2>
<ul>
<li>can be blog post or formal report for analysis of given dataset</li>
<li>components
<ul>
<li><em><strong>prompt</strong> file</em> = analysis being performed (this file is not mandatory/available in all cases)</li>
<li><em><strong>data</strong> folder</em> = data to be analyzed + report files
<ul>
<li><em><strong>Note</strong>: always keep track of data provenance (for reproducibility)</em></li>
</ul></li>
<li><em><strong>code</strong> folder</em>= rawcode vs finalcode
<ul>
<li><em>rawcode</em> = analysis performed (<code>.Rmd</code> file)
<ul>
<li>all exploratory data analysis</li>
<li>not for sharing with others</li>
</ul></li>
<li><em>finalcode</em> = only analysis to be shared with other people/summarizations (<code>.Rmd</code> file)
<ul>
<li>not necessarily final but pertinent to analysis discussed in final report</li>
</ul></li>
<li><em><strong>figures</strong> folder</em> = final plots that have all appropriate formatting for distribution/presentation</li>
<li><em><strong>writing</strong> folder</em> = final analysis report and final figure caption
<ul>
<li>report should have the following sections: title, introduction, methods, results, conclusions, references</li>
<li>final figure captions should correspond with the figures created</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="slidify">Slidify</h2>
<ul>
<li>create data-centric presentations created by Ramnath Vaidyanathan</li>
<li>amalgamation of knitr, Markdown, JavaScript libraries for HTML5 presentations</li>
<li>easily extendable/customizable</li>
<li>allows embedded code chunks and mathematical formulas (MathJax JS library) to be rendered correctly</li>
<li><p>final products are HTML files, which can be viewed with any web browser and shared easily</p></li>
<li><strong>installation</strong>
<ol type="1">
<li>make sure you have <code>devtools</code> package installed in R</li>
<li>enter <code>install_github('slidify', 'ramnathv'); install_github('slidifyLibraries', 'ramnathv')</code> to install the slidify packages</li>
<li>load slidify package with <code>library(slidify)</code></li>
<li>set the working directory to the project you are working on with <code>setwd(&quot;~/project&quot;)</code></li>
</ol></li>
<li><code>author(&quot;title&quot;)</code> = sets up initial files for a new slidify project (performs the following things)
<ol type="1">
<li><code>title</code> (or any name you typed) directory is created inside the current working directory</li>
<li><code>assets</code> subdirectory and a file named <code>index.Rmd</code> are created inside <code>title</code> directory</li>
<li><code>assets</code> subdirectory is populated with the following empty folders:
<ul>
<li><code>css</code></li>
<li><code>img</code></li>
<li><code>js</code></li>
<li><code>layouts</code></li>
<li><em><strong>Note</strong>: any custom CSS/images/JavaScript you want to use should be put into the above folders correspondingly </em></li>
</ul></li>
<li><code>index.Rmd</code> R Markdown file will open up in RStudio</li>
</ol></li>
<li><code>slidify(&quot;index.Rmd&quot;)</code> = processes the R Markdown file into a HTML page and imports all necessary libraries</li>
<li><code>library(knitr); browseURL(&quot;index.html&quot;)</code> = opens up the built-in web browser in R Studio and displays the slidify presentation
<ul>
<li><em><strong>Note</strong>: this is only necessary the first time; you can refresh the page to reflect any changes after saving the HTML file </em></li>
</ul></li>
</ul>
<h3 id="yaml-yaml-aint-markup-languageyet-another-markup-language">YAML (YAML Ain’t Markup Language/Yet Another Markup Language)</h3>
<ul>
<li>used to specify options for the R Markdown/slidify at the beginning of the file</li>
<li>format: <code>field : value # comment</code>
<ul>
<li><code>title</code> = title of document</li>
<li><code>subtitle</code> = subtitle of document</li>
<li><code>author</code> = author of document</li>
<li><code>job</code> = occupation of author (can be left blank)</li>
<li><code>framework</code> = controls formatting, usually the name of a library is used (i.e. <code>io2012</code>)
<ul>
<li><a href="https://code.google.com/p/io-2012-slides/">io2012</a></li>
<li><a href="https://code.google.com/p/html5slides/">html5slides</a></li>
<li><a href="http://imakewebthings.com/deck.js/">deck.js</a></li>
<li><a href="http://paulrouget.com/dzslides/">dzslides</a></li>
<li><a href="https://github.com/adamzap/landslide">landslide</a></li>
<li><a href="http://www.w3.org/Talks/Tools/Slidy2/Overview.html#">Slidy</a></li>
</ul></li>
<li><code>highlighter</code> = controls effects for presentation (i.e <code>highlight.js</code>)</li>
<li><code>hitheme</code> = specifies theme of code (i.e. <code>tomorrow</code>)</li>
<li><code>widgets</code> = loads additional libraries to display LaTeX math equations(<code>mathjax</code>), quiz-styles components (quiz), and additional style (<code>bootstrap</code> = Twitter-created style)
<ul>
<li>for math expressions, the code should be enclosed in <code>$expresion$</code> for inline expressions, and <code>$$expression$$</code> for block equations</li>
</ul></li>
<li><code>mode = selfcontained/standalone/draft</code> = depending whether the presentation will be given with Internet access or not
<ul>
<li><code>standalone</code> = all the JavaScript libraries will be save locally so that the presentation can be executed without Internet access</li>
<li><code>selfcontained</code> = load all JavaScript library at time of presentation</li>
</ul></li>
<li><code>logo</code> = displays a logo in title slide</li>
<li><code>url</code> = specify path to assets/other folders that are used in the presentation
<ul>
<li><em><strong>Note</strong>: <code>../</code> signifies the parent directory </em></li>
</ul></li>
</ul></li>
<li><strong>example</strong></li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="string">title</span>       <span class="string">:</span> <span class="string">Slidify</span></span><br><span class="line"><span class="string">subtitle</span>    <span class="string">:</span> <span class="string">Data</span> <span class="string">meets</span> <span class="string">presentation</span></span><br><span class="line"><span class="string">author</span>      <span class="string">:</span> <span class="string">Jeffrey</span> <span class="string">Leek,</span> <span class="string">Assistant</span> <span class="string">Professor</span> <span class="string">of</span> <span class="string">Biostatistics</span></span><br><span class="line"><span class="string">job</span>         <span class="string">:</span> <span class="string">Johns</span> <span class="string">Hopkins</span> <span class="string">Bloomberg</span> <span class="string">School</span> <span class="string">of</span> <span class="string">Public</span> <span class="string">Health</span></span><br><span class="line"><span class="string">logo</span>        <span class="string">:</span> <span class="string">bloomberg_shield.png</span></span><br><span class="line"><span class="string">framework</span>   <span class="string">:</span> <span class="string">io2012</span>        <span class="comment"># &#123;io2012, html5slides, shower, dzslides, ...&#125;</span></span><br><span class="line"><span class="string">highlighter</span> <span class="string">:</span> <span class="string">highlight.js</span>  <span class="comment"># &#123;highlight.js, prettify, highlight&#125;</span></span><br><span class="line"><span class="string">hitheme</span>     <span class="string">:</span> <span class="string">tomorrow</span>      <span class="comment">#</span></span><br><span class="line"><span class="attr">url:</span></span><br><span class="line"><span class="attr">  lib:</span> <span class="string">../../libraries</span></span><br><span class="line"><span class="attr">  assets:</span> <span class="string">../../assets</span></span><br><span class="line"><span class="string">widgets</span>     <span class="string">:</span> <span class="string">[mathjax]</span>            <span class="comment"># &#123;mathjax, quiz, bootstrap&#125;</span></span><br><span class="line"><span class="string">mode</span>        <span class="string">:</span> <span class="string">selfcontained</span> <span class="comment"># &#123;standalone, draft&#125;</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure>
<h3 id="slides">Slides</h3>
<ul>
<li><code>##</code> = signifies the title of the slide <span class="math inline">\(\rightarrow\)</span> equivalent of <code>h1</code> element in HTML</li>
<li><code>---</code> = marks the end of a slide</li>
<li><code>.class #id</code> = assigns <code>class</code> and <code>id</code> attributes (CSS) to the slide and can be used to customize the style of the page</li>
<li><em><strong>Note</strong>: make sure to leave space between each component of the slidify document (title, code, text, etc) to avoid errors </em></li>
<li>advanced HTML can be added directly to the <code>index.Rmd</code> file and most of the time it should function correctly</li>
<li>interactive element (quiz questions, rCharts, shiny apps) can be embedded into slidify documents (<a href="http://slidify.github.io/dcmeetup/demos/interactive/">demos</a>)
<ul>
<li>quiz elements
<ul>
<li><code>--- &amp;radio</code> before slide content for multiple choice (make sure quiz is included in widgets)</li>
<li><code>##</code> = signifies title of questions</li>
<li>the question can be type in plain text format</li>
<li>the multiple choice options are listed by number (<code>1. a</code>, <code>2. b</code>, etc.)
<ul>
<li>wrap the correct answer in underscores (<code>2. _b_</code>)</li>
</ul></li>
<li><code>*** .hint</code> = denotes the hint that will be displayed when the user clicks on <strong><em>Show Hint</em></strong> button</li>
<li><code>*** .explanation</code> = denotes the explanation that will be displayed when the user clicks on <strong><em>Show Answer</em></strong> button</li>
<li>a page like the one below will be generated when processed with <code>slidify</code></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">--- &amp;radio</span><br><span class="line"></span><br><span class="line"><span class="section">## Question 1</span></span><br><span class="line"></span><br><span class="line">What is 1 + 1?</span><br><span class="line"></span><br><span class="line"><span class="bullet">1. </span>1</span><br><span class="line"><span class="bullet">2. </span><span class="emphasis">_2_</span></span><br><span class="line"><span class="bullet">3. </span>3</span><br><span class="line"><span class="bullet">4. </span>4</span><br><span class="line"></span><br><span class="line"><span class="emphasis">***</span> .hint</span><br><span class="line">This is a hint</span><br><span class="line"></span><br><span class="line"><span class="emphasis">***</span> .explanation</span><br><span class="line">This is an explanation</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/3.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>knit HTML</code></strong> button can be used to generate previews for the presentation as well</li>
</ul>
<h3 id="publishing">Publishing</h3>
<ul>
<li>first, you will need to create a new repository on GitHub</li>
<li><code>publish_github(&quot;user&quot;, &quot;repo&quot;)</code> can be used to publish the slidify document on to your on-line repo</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="rstudio-presentation">RStudio Presentation</h2>
<ul>
<li>presentation authoring tool within the RStudio IDE (<a href="http://www.rstudio.com/ide/docs/presentations/overview">tutorial</a>)</li>
<li>output = html5 presentation</li>
<li><code>.Rpres</code> file <span class="math inline">\(\rightarrow\)</span> converted to <code>.md</code> file <span class="math inline">\(\rightarrow\)</span> <code>.html</code> file</li>
<li>uses R Markdown format as slidify/knitr
<ul>
<li><strong>mathjax</strong> JS library is loaded by default</li>
</ul></li>
<li>RStudio format/runs the code when the document is saved</li>
</ul>
<h3 id="creating-presentation">Creating Presentation</h3>
<ul>
<li>file <span class="math inline">\(\rightarrow\)</span> New File <span class="math inline">\(\rightarrow\)</span> R Presentation (<code>alt-f</code> + <code>f</code> + <code>p</code>)</li>
<li><code>class: classname</code> = specify slide-specific control from CSS</li>
<li><code>css: file.css</code> = can be used to import an external CSS file
<ul>
<li>alternatively, a css file that has the same name as the presentation will be automatically loaded</li>
</ul></li>
<li>knowledge of CSS/HTML/JavaScript useful to customize presentation more granularly
<ul>
<li><em><strong>Note</strong>: though the end HTML file can be edited directly, it should be used as a last resort as it defeats the purpose of reproducible presentations </em></li>
</ul></li>
<li>clicking on <strong>Preview</strong> button brings up <strong><em>Presentation</em></strong> viewer in RStudio
<ul>
<li><em>navigation controls</em> (left and right arrows) are located in right bottom corner</li>
<li>the <em>Notepad</em> icon on the menu bar above displays the section of code that corresponds with the current slide in the main window</li>
<li>the <em>More</em> button has four options
<ul>
<li>“Clear Knitr Cache” = clears cache for the generated presentation previews</li>
<li>“View in Browser” = creates temporary HTML file and opens in default web browser (does not create a local file)</li>
<li>“Save as Web Page” = creates a copy of the presentation as a web page</li>
<li>“Publish to RPubs” = publishes presentation on RPubs</li>
</ul></li>
<li>the <em>Refresh</em> button refreshes the page</li>
<li>the <em>Zoom</em> button opens a new window to display the presentation</li>
</ul></li>
<li><strong>transitions between slides</strong>
<ul>
<li>just after the beginning of each slide, the <code>transition</code> property (similar to YAML) can be specified to control the transition between the previous and current slides</li>
<li><code>transition: linear</code> = creates 2D linear transition (html5) between slides</li>
<li><code>transition: rotate</code> = creates 3D rotating transition (html5) between slides</li>
<li>more transition options are found <a href="https://support.rstudio.com/hc/en-us/articles/200714013-Slide-Transitions-and-Navigation">here</a></li>
</ul></li>
<li><strong>hierarchical organization</strong>
<ul>
<li>attribute <code>type</code> can be added to specify the appearance of the slide (“slide type”)</li>
<li><code>type: section</code> and <code>type: sub-section</code> = distinct background and font colors, slightly larger heading text, appear at a different indent level within the slide navigation menu</li>
<li><code>type: prompt</code> and <code>type: alert</code> = distinct background color to communicate to viewers that the slide has different intent</li>
</ul></li>
<li><strong>columns</strong>
<ul>
<li>simply place <code>***</code> in between two sections of content on a slide to separate it into two columns</li>
<li><code>left: 70%</code> can be used to specify the proportions of each column</li>
<li><code>right: 30%</code> works similarly</li>
</ul></li>
<li><strong>change slide font (<a href="https://support.rstudio.com/hc/en-us/articles/200532307-Customizing-Fonts-and-Appearance">guide</a>)</strong>
<ul>
<li><code>font-family: fontname</code> = changes the font of slide (specified in the same way as HTML)</li>
<li><code>font-import: http://fonts.googleapis.com/css?family=Risque</code> = imports font
<ul>
<li><em><strong>Note</strong>: fonts must be present on the system for presentation (or have Internet), or default fonts will be used </em></li>
</ul></li>
<li><em><strong>Note</strong>: CSS selectors for class and IDs must be preceded by <code>.reveal</code> to work (<code>.reveal section del</code> applies to any text enclosed by <code>~~text~~</code>) </em></li>
</ul></li>
</ul>
<h2 id="slidify-vs-rstudio-presenter">Slidify vs RStudio Presenter</h2>
<ul>
<li><strong>Slidify</strong>
<ul>
<li>flexible control from the <code>.Rmd</code> file</li>
<li>under constant development</li>
<li>large user base, more likely to get answer on <em>StackOverflow</em></li>
<li>lots of styles and options by default</li>
<li>steeper learning curve</li>
<li>more command-line oriented</li>
</ul></li>
<li><strong>R Studio Presenter</strong>
<ul>
<li>embedded in R Studio</li>
<li>more GUI oriented</li>
<li>very easy to get started</li>
<li>smaller set of easy styles and options</li>
<li>default styles look nice</li>
<li>as flexible as Slidify with CSS/HTML knowledge</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="r-package">R Package</h2>
<ul>
<li>R packages = collection of functions/data objects, extends base functionality of R
<ul>
<li>organized to provide consistency</li>
<li>written by people all over the world</li>
</ul></li>
<li>primarily available from CRAN and Bioconductor
<ul>
<li>installed with <code>install.packages()</code></li>
</ul></li>
<li>also available on GitHub/BitBucket/etc
<ul>
<li>installed using <code>devtools::install_github()</code></li>
</ul></li>
<li><strong>documentation/vignettes</strong> = forces author to provide detailed explanation for the arguments/results of their functions and objects
<ul>
<li>allows for well defined Application Programming Interface (API) to tell users what and how to use the functions</li>
<li>much of the implementation details can be hidden from the user so that updates can be made without interfering with use cases</li>
</ul></li>
<li>if package is available on CRAN then it must hold standards of reliability and robustness</li>
<li><p>fairly easily maintained with proper documentation</p></li>
<li><strong>package development process</strong>
<ul>
<li>write code in R script</li>
<li>desire to make code available to others</li>
<li>incorporate R script file into R package structure</li>
<li>write documentation for user functions</li>
<li>include examples/demos/datasets/tutorials</li>
<li>package the contents</li>
<li>submit to CRAN/Bioconductor</li>
<li>push source code repository to GitHub/other code sharing site
<ul>
<li>people find problems with code and expect the author to fix it</li>
<li>alternatively, people might fix the problem and show the author the changes</li>
</ul></li>
<li>incorporate changes and release a new version</li>
</ul></li>
</ul>
<h3 id="r-package-components">R Package Components</h3>
<ul>
<li><strong><em>directory</em></strong> with name of R package = created as first step</li>
<li><strong><em>DESCRIPTION file</em></strong> = metadata/information about package</li>
<li><strong><em>R code</em></strong> = code should be in <code>R/</code> sub-directory</li>
<li><strong><em>Documentation</em></strong> = file should be in <code>man/</code> sub-directory</li>
<li><strong><em>NAMESPACE</em></strong> = optional but common and best practice</li>
<li>full requirements documented in <a href="http://cran.r-project.org/doc/manuals/r-release/R-exts.html">Writing R Extensions</a></li>
</ul>
<h3 id="description-file">DESCRIPTION file</h3>
<ul>
<li><strong><em>Package</em></strong> = name of package (e.g. <code>library(name)</code> to load the package)</li>
<li><strong><em>Title</em></strong> = full name of package</li>
<li><strong><em>description</em></strong> = longer description of package in one or two sentences</li>
<li><strong><em>Version</em></strong> = version number (usually <code>M.m-p</code> format, “majorNumber.minorNumber-patchLevel”)</li>
<li><strong><em>Author</em></strong> = Name of the original author(s)</li>
<li><strong><em>Maintainer</em></strong> = name + email of person (maintainer) who fixes problems</li>
<li><strong><em>License</em></strong> = license for the source code, describes the term that the source code is released under
<ul>
<li>common licenses include GNU/BSD/MIT</li>
<li>typically a standard open source license is used</li>
</ul></li>
<li>optional fields
<ul>
<li><strong><em>Depends</em></strong> = R packages that your package depends on</li>
<li><strong><em>Suggests</em></strong> = optional R packages that users may want to have installed</li>
<li><strong><em>Date</em></strong> = release date in YYYY-MM-DD format</li>
<li><strong><em>URL</em></strong> = package home page/link to repository</li>
<li><strong><em>Other</em></strong> = fields can be added (generally ignored by R)</li>
</ul></li>
<li><strong><em>example: <code>gpclib</code></em></strong>
<ul>
<li><em>Package</em>: gpclib</li>
<li><em>Title</em>: General Polygon Clipping Library for R</li>
<li><em>Description</em>: General polygon clipping routines for R based on Alan Murta’s C library</li>
<li><em>Version</em>: 1.5-5</li>
<li><em>Author</em>: Roger D. Peng <a href="mailto:rpeng@jhsph.edu">rpeng@jhsph.edu</a> with contributions from Duncan Murdoch and Barry Rowlingson; GPC library by Alan Murta</li>
<li><em>Maintainer</em>: Roger D. Peng <a href="mailto:rpeng@jhsph.edu">rpeng@jhsph.edu</a></li>
<li><em>License</em>: file LICENSE</li>
<li><em>Depends</em>: R (&gt;= 2.14.0), methods</li>
<li><em>Imports</em>: graphics</li>
<li><em>Date</em>: 2013-04-01</li>
<li><em>URL</em>: http://www.cs.man.ac.uk/~toby/gpc/, http://github.com/rdpeng/gpclib</li>
</ul></li>
</ul>
<h3 id="r-code">R Code</h3>
<ul>
<li>copy R code to <code>R/</code> directory</li>
<li>can be any number of files</li>
<li>separate out files to logical groups (read/fit models)</li>
<li>all code should be included here and not anywhere else in the package</li>
</ul>
<h3 id="namespace-file">NAMESPACE file</h3>
<ul>
<li>effectively an API for the package</li>
<li>indicates which functions are <em>exported</em> <span class="math inline">\(\rightarrow\)</span> public functions that users have access to and can use
<ul>
<li>functions not exported cannot be called directly by the user</li>
<li>hides the implementation details from the users (clean package interface)</li>
</ul></li>
<li>lists all dependencies on other packages/indicate what functions you <em>imported</em> from other packages
<ul>
<li>allows for your package to use other packages without making them visible to the user</li>
<li>importing a function loads the package but <em>does not</em> attach it to the search list</li>
</ul></li>
<li><strong>key directives</strong>
<ul>
<li><code>export(&quot;\&lt;function&gt;&quot;)</code> = export a function</li>
<li><code>import(&quot;\&lt;package&gt;&quot;)</code> = import a package</li>
<li><code>importFrom(&quot;\&lt;package&gt;&quot;, &quot;\&lt;function&gt;&quot;)</code> = import specific function from a package</li>
<li><code>exportClasses(&quot;\&lt;class&gt;&quot;)</code> = indicate the new types of S4 (4<sup>th</sup> version of <em>S</em>) classes created with the package (objects of the specified class can be created)</li>
<li><code>exportMethods(&quot;\&lt;generic&gt;&quot;)</code> = methods that can operate on the new class objects</li>
<li><em><strong>Note</strong>: though they look like R functions, the above directives are not functions that users can use freely </em></li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># read.polyfile/write.polyfile are functions available to user</span></span><br><span class="line">export(<span class="string">"read.polyfile"</span>, <span class="string">"write.polyfile"</span>)</span><br><span class="line"><span class="comment"># import plot function from graphics package</span></span><br><span class="line">importFrom(graphics, plot)</span><br><span class="line"><span class="comment"># gpc.poly/gpc.poly.nohole classes can be created by the user</span></span><br><span class="line">exportClasses(<span class="string">"gpc.poly"</span>, <span class="string">"gpc.poly.nohole"</span>)</span><br><span class="line"><span class="comment"># the listed methods can be applied to the gpc.poly/gpc.poly.nohole classes</span></span><br><span class="line">exportMethods(<span class="string">"show"</span>, <span class="string">"get.bbox"</span>, <span class="string">"plot"</span>, <span class="string">"intersect”, "</span>union”, <span class="string">"setdiff"</span>,</span><br><span class="line">              <span class="string">"["</span>, <span class="string">"append.poly"</span>, <span class="string">"scale.poly"</span>, <span class="string">"area.poly"</span>, <span class="string">"get.pts"</span>,</span><br><span class="line">              <span class="string">"coerce"</span>, <span class="string">"tristrip"</span>, <span class="string">"triangulate"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="documentation">Documentation</h3>
<ul>
<li>documentation files (<code>.Rd</code>) should be placed in the <code>man/</code> sub-directory</li>
<li>written in specific markup language</li>
<li>required for every exported function available to the user (serves to limit the number of exported functions)</li>
<li><p>concepts/package/datasets overview can also be documented</p></li>
<li><strong>components</strong>
<ul>
<li><code>\name{}</code> = name of function</li>
<li><code>\alias{}</code> = anything listed as alias will bring up the help file (<code>?line</code> is the same as <code>?residuals.tukeyline</code>)
<ul>
<li>multiple aliases possible</li>
</ul></li>
<li><code>\title{}</code> = full title of the function</li>
<li><code>\description{}</code> = full description of the purpose of function</li>
<li><code>\usage{}</code> = format/syntax of function</li>
<li><code>\arguments{}</code> = explanation of the arguments in the syntax of function</li>
<li><code>\details{}</code> = notes/details about limitation/features of the function</li>
<li><code>\value{}</code> = specifies what object is returned</li>
<li><code>\reference{}</code> = references for the function (paper/book from which the method is created)</li>
</ul></li>
<li><p><strong><em>example: <code>line</code> function</em></strong></p></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">\name&#123;line&#125;</span><br><span class="line">\alias&#123;line&#125;</span><br><span class="line">\alias&#123;residuals.tukeyline&#125;</span><br><span class="line">\title&#123;Robust Line Fitting&#125;</span><br><span class="line">\description&#123;</span><br><span class="line">  Fit a line robustly as recommended <span class="keyword">in</span> \emph&#123;Exploratory Data Analysis&#125;.</span><br><span class="line">&#125;</span><br><span class="line">\usage&#123;</span><br><span class="line">line(x, y)</span><br><span class="line">&#125;</span><br><span class="line">\arguments&#123;</span><br><span class="line">  \item&#123;x, y&#125;&#123;the arguments can be any way of specifying x-y pairs.  See</span><br><span class="line">    \code&#123;\link&#123;xy.coords&#125;&#125;.&#125;</span><br><span class="line">&#125;</span><br><span class="line">\details&#123;</span><br><span class="line">  Cases with missing values are omitted.</span><br><span class="line"></span><br><span class="line">  Long vectors are not supported.</span><br><span class="line">&#125;</span><br><span class="line">\value&#123;</span><br><span class="line">  An object of class \code&#123;<span class="string">"tukeyline"</span>&#125;.</span><br><span class="line"></span><br><span class="line">  Methods are available <span class="keyword">for</span> the generic functions \code&#123;coef&#125;,</span><br><span class="line">  \code&#123;residuals&#125;, \code&#123;fitted&#125;, and \code&#123;print&#125;.</span><br><span class="line">&#125;</span><br><span class="line">\references&#123;</span><br><span class="line">  Tukey, J. W. (<span class="number">1977</span>).</span><br><span class="line">  \emph&#123;Exploratory Data Analysis&#125;,</span><br><span class="line">  Reading Massachusetts: Addison-Wesley.</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="buildingchecking-package">Building/Checking Package</h3>
<ul>
<li><strong>R CMD (Command) Build</strong> = command-line program that creates a package archive file (format = <code>.tar.gz</code>)</li>
<li><strong>R CMD Check</strong> = command-line program that runs a battery of tests on the package to ensure structure is consistent/all components are present/export and import are appropriately specified</li>
<li>R CMD Build/R CMD check can be run from the command-line using terminal/command-shell applications</li>
<li>alternatively, they can be run from R using the <code>system()</code> function
<ul>
<li><code>system(&quot;R CMD build newpackage&quot;)</code></li>
<li><code>system(&quot;R CMD check newpackage&quot;)</code></li>
</ul></li>
<li>the package must pass <strong><em>all</em></strong> tests to be put on CRAN
<ul>
<li>documentation exists for all exported function</li>
<li>code can be loaded without any major coding problems/errors</li>
<li>contains license</li>
<li>ensure examples in documentation can be executed</li>
<li>check documentation of arguments matches argument in the code</li>
</ul></li>
<li><code>package.skeleton()</code> function in the <code>utils</code> package = creates a “skeleton” R package
<ul>
<li>automatically creates directory structure (<code>R/</code>, <code>man/</code>), DESCRIPTION file, NAMESPACE file, documentation files</li>
<li>if there are any visible/stored functions in the current workspace, their code will be written as R code files in the <code>R/</code> directory</li>
<li>documentation stubs are created in <code>man/</code> directory</li>
<li>the rest of the content can then be modified and added</li>
</ul></li>
<li>alternatively, you can click on the menu on the top right hand corner of RStudio: <em>Project</em> <span class="math inline">\(\rightarrow\)</span> <em>New Project</em> <span class="math inline">\(\rightarrow\)</span> <em>New Directory</em> <span class="math inline">\(\rightarrow\)</span> <em>R Package</em> <span class="math inline">\(\rightarrow\)</span> fill in package names and details <span class="math inline">\(\rightarrow\)</span> automatically generate structure/skeleton of a new R package</li>
</ul>
<h3 id="checklist-for-creating-package">Checklist for Creating Package</h3>
<ul>
<li>create a new directory with <code>R/</code> and <code>man/</code> sub-directories (or just use <code>package.skeleton()</code>)</li>
<li>write a DESCRIPTION file</li>
<li>copy R code into the <code>R/</code> sub-directory</li>
<li>write documentation files in <code>man/</code> sub-directory</li>
<li>write a NAMESPACE file with exports/imports</li>
<li>build and check</li>
</ul>
<h3 id="example-topten-function">Example: <code>topten</code> function</h3>
<ul>
<li>when creating a package, generate skeleton by clicking on <em>Project</em> <span class="math inline">\(\rightarrow\)</span> <em>New Project</em> <span class="math inline">\(\rightarrow\)</span> <em>New Directory</em> <span class="math inline">\(\rightarrow\)</span> <em>R Package</em> <span class="math inline">\(\rightarrow\)</span> fill in package names and details</li>
<li>write the code first in a <code>.R</code> script and add documentation directly to the script
<ul>
<li><strong>Roxygen2</strong> package will be leveraged to extract and format the documentation from R script automatically</li>
</ul></li>
<li>Roxygen2 syntax
<ul>
<li><code>#'</code> = denotes the beginning of documentation
<ul>
<li>R will automatically add <code>#'</code> on the subsequent lines as you type or complete sections</li>
</ul></li>
<li>title should be on the first line (relatively concise, a few words)
<ul>
<li>press <strong>ENTER</strong> after you are finished and R will automatically insert an empty line and move the cursor to the next section</li>
</ul></li>
<li>description/summary should begin on the third line (one/two sentences)
<ul>
<li>press <strong>ENTER</strong> after you are finished and R will automatically insert an empty line and move the cursor to the next section</li>
</ul></li>
<li><code>@param x definition</code> = format of the documentation for the arguments
<ul>
<li><code>x</code> = argument name (formatted in code format when processed to differentiate from definition)</li>
<li><code>definiton</code> = explanation of the what x represents</li>
</ul></li>
<li><code>@author</code> = author of the function</li>
<li><code>@details</code> = detailed description of the function and its purpose</li>
<li><code>@seealso</code> = links to relevant functions used in creating the current function that may be of interest to the user</li>
<li><code>@import package function</code> = imports specific function from specified package</li>
<li><code>@export</code> = denotes that this function is exported for public use</li>
<li><code>@return</code> = specifies what is returned by the method</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#' Building a Model with Top Ten Features</span></span><br><span class="line"><span class="comment">#'</span></span><br><span class="line"><span class="comment">#' This function develops a prediction algorithm based on the top 10 features</span></span><br><span class="line"><span class="comment">#' in 'x' that are most predictive of 'y'.</span></span><br><span class="line"><span class="comment">#'</span></span><br><span class="line"><span class="comment">#' @param x a n x p matrix of n observations and p predictors</span></span><br><span class="line"><span class="comment">#' @param y a vector of length n representing the response</span></span><br><span class="line"><span class="comment">#' @return a 'lm' object representing the linear model with the top 10 predictors</span></span><br><span class="line"><span class="comment">#' @author Roger Peng</span></span><br><span class="line"><span class="comment">#' @details</span></span><br><span class="line"><span class="comment">#' This function runs a univariate regression of y on each predictor in x and</span></span><br><span class="line"><span class="comment">#' calculates the p-value indicating the significance of the association. The</span></span><br><span class="line"><span class="comment">#' final set of 10 predictors is the taken from the 10 smallest p-values.</span></span><br><span class="line"><span class="comment">#' @seealso \code&#123;lm&#125;</span></span><br><span class="line"><span class="comment">#' @import stats</span></span><br><span class="line"><span class="comment">#' @export</span></span><br><span class="line"></span><br><span class="line">topten &lt;- <span class="keyword">function</span>(x, y) &#123;</span><br><span class="line">        p &lt;- ncol(x)</span><br><span class="line">        <span class="keyword">if</span>(p &lt; <span class="number">10</span>)</span><br><span class="line">                <span class="keyword">stop</span>(<span class="string">"there are less than 10 predictors"</span>)</span><br><span class="line">        pvalues &lt;- numeric(p)</span><br><span class="line">        <span class="keyword">for</span>(i <span class="keyword">in</span> seq_len(p)) &#123;</span><br><span class="line">                fit &lt;- lm(y ~ x[, i])</span><br><span class="line">                summ &lt;- summary(fit)</span><br><span class="line">                pvalues[i] &lt;- summ$coefficients[<span class="number">2</span>, <span class="number">4</span>]</span><br><span class="line">        &#125;</span><br><span class="line">        ord &lt;- order(pvalues)</span><br><span class="line">        x10 &lt;- x[, ord]</span><br><span class="line">        fit &lt;- lm(y ~ x10)</span><br><span class="line">        coef(fit)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#' Prediction with Top Ten Features</span></span><br><span class="line"><span class="comment">#'</span></span><br><span class="line"><span class="comment">#' This function takes a set coefficients produced by the \code&#123;topten&#125;</span></span><br><span class="line"><span class="comment">#' function and makes a prediction for each of the values provided in the</span></span><br><span class="line"><span class="comment">#' input 'X' matrix.</span></span><br><span class="line"><span class="comment">#'</span></span><br><span class="line"><span class="comment">#' @param X a n x 10 matrix containing n observations</span></span><br><span class="line"><span class="comment">#' @param b a vector of coefficients obtained from the \code&#123;topten&#125; function</span></span><br><span class="line"><span class="comment">#' @return a numeric vector containing the predicted values</span></span><br><span class="line"></span><br><span class="line">predict10 &lt;- <span class="keyword">function</span>(X, b) &#123;</span><br><span class="line">        X &lt;- cbind(<span class="number">1</span>, X)</span><br><span class="line">        drop(X %*% b)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="r-classes-and-methods">R Classes and Methods</h2>
<ul>
<li>represents new data types (i.e. list) and its own structure/functions that R doesn’t support yet</li>
<li>R classes and method <span class="math inline">\(\rightarrow\)</span> system for object oriented programming (OOP)</li>
<li>R is interactive and supports OOP where as a lot of the other common OOP languages (C++, Java, Python, Perl) are generally not interactive</li>
<li>John Chambers wrote most of the code support classes/methods (documented in <em>Programming with Data</em>)</li>
<li>goal is to allow <em>user</em> (leverage system capabilities) to become <em>programmers</em> (extend system capabilities)</li>
<li><p>OOB structure in R is structured differently than most of the other languages</p></li>
<li><strong>two style of classes and methods</strong>
<ul>
<li>S3 (version three of the <em>S</em> language)
<ul>
<li>informal, “old-style”, kludgey (functional but not elegant)</li>
</ul></li>
<li>S4 (version four of the <em>S</em> language)
<ul>
<li>rigorous standard, more formal, “new-style”</li>
<li>code for implementing S4 classes and methods found in <code>methods</code> package</li>
</ul></li>
<li>two systems living side by side
<ul>
<li>each is independent but S4 are encouraged for new projects</li>
</ul></li>
</ul></li>
</ul>
<h3 id="objected-oriented-programming-in-r">Objected Oriented Programming in R</h3>
<ul>
<li><strong><em>class</em></strong> = description of some thing/object (new data type/idea)
<ul>
<li>can be defined using <code>setClass()</code> function in <code>methods</code> package</li>
<li>all objects in R have a class, which can be determined through the <code>class()</code> function
<ul>
<li><code>numeric</code> = number data, can be vectors as well (series of numbers)</li>
<li><code>logical</code> = TRUE, FALSE, NA
<ul>
<li><em><strong>Note</strong>: NA is by default of logical class, however you can have numeric/character NA’s as results of operations </em></li>
</ul></li>
<li><code>character</code> = string of characters</li>
<li><code>lm</code> = linear model class, output from a linear model</li>
</ul></li>
</ul></li>
<li><strong><em>object</em></strong> = instances of a class
<ul>
<li>can be created using <code>new()</code></li>
</ul></li>
<li><strong><em>method</em></strong> = function that operates on certain class of objects
<ul>
<li>also can be thought of as an implementation of a <em>generic function</em> for an object of particular class</li>
<li>can write new method for existing generic functions or create new generic function and associated methods</li>
<li><code>getS3method(&lt;genericFunction&gt;, &lt;class&gt;)</code> = returns code for S3 method for a given class
<ul>
<li>some S3 methods can be called directly (i.e. <code>mean.default</code>)</li>
<li>but you should <strong>never</strong> call them, always use the generic</li>
</ul></li>
<li><code>getMethod(&lt;genericFunction&gt;, &lt;signature/class&gt;)</code> = returns code for S4 method for a given class
<ul>
<li><em>signature</em> = character vector indicating class of objects accepted by the method</li>
<li>S4 methods can not be called at all</li>
</ul></li>
</ul></li>
<li><strong><em>generic function</em></strong> = R function that dispatches methods to perform a certain task (i.e. <code>plot, mean, predict</code>)
<ul>
<li>performs different calculations depending on context</li>
<li><em><strong>Note</strong>: generic functions themselves don’t perform any computation; typing the function name by itself (i.e. <code>plot</code>) will return the content of the function </em></li>
<li>S3 and S4 functions look different but are similar conceptually</li>
<li><code>methods(&quot;mean&quot;)</code> = returns methods associated with S3 generic function</li>
<li><code>showMethods(&quot;show&quot;)</code> = returns methods associated with S4 generic function
<ul>
<li><em><strong>Note</strong>: <code>show</code> is equivalent of <code>print</code>, but generally not called directly as objects are auto-printed </em></li>
</ul></li>
<li>first argument = object of particular class</li>
<li><em><strong>process</strong></em>:
<ol type="1">
<li>generic function checks class of object</li>
<li>if appropriate method for class exists, call that method on object (process complete)</li>
<li>if no method exists for class, search for default method</li>
<li>if default method exists, default method is called on object (process complete)</li>
<li>if no default method exists, error thrown (process complete)</li>
</ol></li>
<li>for classes like <code>data.frame</code> where each column can be of different class, the function uses the methods correspondingly
<ul>
<li>plotting <code>as.ts(x)</code> and x are completed different
<ul>
<li><code>as.ts()</code> = converts object to time series</li>
</ul></li>
</ul></li>
</ul></li>
<li><p><em><strong>Note</strong>: <code>?Classes</code>, <code>?Methods</code>, <code>?setClass</code>, <code>?setMethod</code>, and <code>?setGeneric</code> contains very helpful documentation </em></p></li>
<li><p><strong><em>example</em></strong></p></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># S3 method: mean</span><br><span class="line">mean</span><br><span class="line"># associated methods</span><br><span class="line">methods(&quot;mean&quot;)</span><br><span class="line"># code for mean (first 10 lines)</span><br><span class="line"># note: no specific funcyion got numeric class, so default is used</span><br><span class="line">head(getS3method(&quot;mean&quot;, &quot;default&quot;), 10)</span><br><span class="line"># S4 method: show</span><br><span class="line">show</span><br><span class="line"># associated methods</span><br><span class="line">showMethods(&quot;show&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="creating-a-new-classmethods">Creating a New Class/Methods</h3>
<ul>
<li>reason for creating new classes/data type (not necessarily unknown to the world, but just unknown to R)
<ul>
<li>powerful way to extend the functionality of R</li>
<li>represent new types of data (e.g. gene expression, space-time, hierarchical, sparse matrices)</li>
<li>new concepts/ideas that haven’t been thought of yet (e.g. a fitted point process model, mixed-effects model, a sparse matrix)</li>
<li>abstract/hide implementation details from the user</li>
</ul></li>
<li><strong>classes</strong> = define new data types</li>
<li><p><strong>methods</strong> = extend <em>generic functions</em> to specify the behavior of generic functions on new classes</p></li>
<li><code>setClass()</code> = function to create new class
<ul>
<li>at minimum, name of class needs to be specified</li>
<li><em>slots</em> or attributes can also be specified
<ul>
<li>a class is effectively a list, so slots are elements of that list</li>
</ul></li>
</ul></li>
<li><code>setMethod()</code> = define methods for class
<ul>
<li><code>@</code> is used to access the slots/attributes of the class</li>
</ul></li>
<li><code>showClass()</code> = displays definition/information about class</li>
<li>when drafting new class, new methods for <code>print</code>  <code>show</code>, <code>summary</code>, and <code>plot</code> should be written</li>
<li><em><strong>Note</strong>: creating classes are not something to be done on the console and are much better suited for a script </em></li>
<li><strong><em>example</em></strong>
<ul>
<li>create <code>ploygon</code> class with set of (x, y) coordinates with <code>setClass()</code></li>
<li>define a new plot function by extending existing <code>plot</code> function with <code>setMethod()</code></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># load methods library</span><br><span class="line">library(methods)</span><br><span class="line"># create polygon class with x and y coordinates as slots</span><br><span class="line">setClass(&quot;polygon&quot;, representation(x = &quot;numeric&quot;, y = &quot;numeric&quot;))</span><br><span class="line"># create plot method for ploygon class (ploygon = signature in this case)</span><br><span class="line">setMethod(&quot;plot&quot;, &quot;polygon&quot;,</span><br><span class="line">	# create function</span><br><span class="line">	function(x, y, ...) &#123;</span><br><span class="line">		# plot the x and y coordinates</span><br><span class="line">		plot(x@x, x@y, type = &quot;n&quot;, ...)</span><br><span class="line">		# plots lines between all (x, y) pairs</span><br><span class="line">		# x@x[1] is added at the end because we need</span><br><span class="line">		# to connect the last point of polygon to the first</span><br><span class="line">		xp &lt;- c(x@x, x@x[1])</span><br><span class="line">		yp &lt;- c(x@y, x@y[1])</span><br><span class="line">		lines(xp, yp)</span><br><span class="line">	  &#125;)</span><br><span class="line"># print polygon method</span><br><span class="line">showMethods(&quot;plot&quot;)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="yhat-link">Yhat (<a href="http://cloud.yhathq.com/">link</a>)</h2>
<ul>
<li>develop back-ends to algorithms to be hosted on-line for other people to access</li>
<li>others can create APIs (front-ends) to leverage the algorithm/model</li>
<li>before uploading, create an account and set up API key</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Create dataset of PM and O3 for all US taking year 2013 (annual</span></span><br><span class="line"><span class="comment">## data from EPA)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## This uses data from</span></span><br><span class="line"><span class="comment">## http://aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Read in the 2013 Annual Data</span></span><br><span class="line">d &lt;- read.csv(<span class="string">"annual_all_2013.csv"</span>, nrow = <span class="number">68210</span>)</span><br><span class="line"><span class="comment"># subset data to just variables we are interested in</span></span><br><span class="line">sub &lt;- subset(d, Parameter.Name %<span class="keyword">in</span>% c(<span class="string">"PM2.5 - Local Conditions"</span>, <span class="string">"Ozone"</span>)</span><br><span class="line">              &amp; Pullutant.Standard %<span class="keyword">in</span>% c(<span class="string">"Ozone 8-Hour 2008"</span>, <span class="string">"PM25 Annual 2006"</span>),</span><br><span class="line">              c(Longitude, Latitude, Parameter.Name, Arithmetic.Mean))</span><br><span class="line"><span class="comment"># calculate the average pollution for each location</span></span><br><span class="line">pollavg &lt;- aggregate(sub[, <span class="string">"Arithmetic.Mean"</span>],</span><br><span class="line">                     sub[, c(<span class="string">"Longitude"</span>, <span class="string">"Latitude"</span>, <span class="string">"Parameter.Name"</span>)],</span><br><span class="line">                     mean, na.rm = <span class="literal">TRUE</span>)</span><br><span class="line"><span class="comment"># refactors the Name parameter to drop all other levels</span></span><br><span class="line">pollavg$Parameter.Name &lt;- factor(pollavg$Parameter.Name, labels = c(<span class="string">"ozone"</span>, <span class="string">"pm25"</span>))</span><br><span class="line"><span class="comment"># renaming the last column from "x" (automatically generated) to "level"</span></span><br><span class="line">names(pollavg)[<span class="number">4</span>] &lt;- <span class="string">"level"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove unneeded objects</span></span><br><span class="line">rm(d, sub)</span><br><span class="line"><span class="comment"># extract out just the location information for convenience</span></span><br><span class="line">monitors &lt;- data.matrix(pollavg[, c(<span class="string">"Longitude"</span>, <span class="string">"Latitude"</span>)])</span><br><span class="line"><span class="comment"># load fields package which allows us to calculate distances on earth</span></span><br><span class="line"><span class="keyword">library</span>(fields)</span><br><span class="line"><span class="comment"># build function to calculate the distances for the given set of coordinates</span></span><br><span class="line"><span class="comment"># input = lon (longitude), lat (latitude), radius (radius in miles for finding monitors)</span></span><br><span class="line">pollutant &lt;- <span class="keyword">function</span>(df) &#123;</span><br><span class="line">		<span class="comment"># extract longitude/lagitude</span></span><br><span class="line">        x &lt;- data.matrix(df[, c(<span class="string">"lon"</span>, <span class="string">"lat"</span>)])</span><br><span class="line">        <span class="comment"># extract radius</span></span><br><span class="line">        r &lt;- df$radius</span><br><span class="line">        <span class="comment"># calculate distances between all monitors and input coordinates</span></span><br><span class="line">        d &lt;- rdist.earth(monitors, x)</span><br><span class="line">        <span class="comment"># locations for find which distance is less than the input radius</span></span><br><span class="line">        use &lt;- lapply(seq_len(ncol(d)), <span class="keyword">function</span>(i) &#123;</span><br><span class="line">                which(d[, i] &lt; r[i])</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment"># calculate levels of ozone and pm2.5 at each selected locations</span></span><br><span class="line">        levels &lt;- sapply(use, <span class="keyword">function</span>(idx) &#123;</span><br><span class="line">                with(pollavg[idx, ], tapply(level, Parameter.Name, mean))</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="comment"># convert to data.frame and transpose</span></span><br><span class="line">        dlevel &lt;- as.data.frame(t(levels))</span><br><span class="line">        <span class="comment"># return the input data frame and the calculated levels</span></span><br><span class="line">        data.frame(df, dlevel)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="deploying-the-model">Deploying the Model</h3>
<ul>
<li>once the functions are complete, three more functions should be written in order to upload to yhat,
<ul>
<li><code>model.require(){}</code> = defines dependencies on other packages
<ul>
<li>if there are no dependencies, this does not need to be defined</li>
</ul></li>
<li><code>model.transform(){}</code> = needed if the data needs to be transformed in anyway before feeding into the model</li>
<li><code>model.predict(){}</code> = performs the prediction</li>
</ul></li>
<li>store the following information as a vector named <code>yhat.config</code>
<ul>
<li><code>username = &quot;&lt;user@email.com&gt;&quot;</code> = user name for yhat website</li>
<li><code>apikey = &quot;&lt;generatedKey&gt;&quot;</code> = unique API key generated when you open an account with yhat</li>
<li><code>env=&quot;http://sandbox.yhathq.com/&quot;</code> = software environment (always going to be this link)</li>
</ul></li>
<li><code>yhat.deploy(&quot;name&quot;)</code> = uploads the model to yhat servers with provided credentials under the specified name
<ul>
<li>returns a data frame with status/model information</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Send to yhat</span></span><br><span class="line"><span class="keyword">library</span>(yhatr)</span><br><span class="line"></span><br><span class="line">model.require &lt;- <span class="keyword">function</span>() &#123;</span><br><span class="line">        <span class="keyword">library</span>(fields)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model.transform &lt;- <span class="keyword">function</span>(df) &#123;</span><br><span class="line">        df</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model.predict &lt;- <span class="keyword">function</span>(df) &#123;</span><br><span class="line">        pollutant(df)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">yhat.config  &lt;- c(</span><br><span class="line">        username=<span class="string">"email@gmail.com"</span>,</span><br><span class="line">        apikey=<span class="string">"90d2a80bb532cabb2387aa51ac4553cc"</span>,</span><br><span class="line">        env=<span class="string">"http://sandbox.yhathq.com/"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">yhat.deploy(<span class="string">"pollutant"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="accessing-the-model">Accessing the Model</h3>
<ul>
<li>once uploaded, the model can be accessed directly on the yhat website
<ul>
<li>click on name of the model after logging in or go to “http://cloud.yhathq.com/model/name” where name is the name you uploaded the model under</li>
<li>enter the inputs in JSON format (associative arrays): <code>{ &quot;variable&quot; : &quot;value&quot;}</code>
<ul>
<li><em>example</em>: <code>{ &quot;lon&quot; : -76.61, &quot;lat&quot;: 39.28, &quot;radius&quot;: 50 }</code></li>
</ul></li>
<li>click on <strong>Send Data to Model</strong></li>
<li>results return in the <em>Model Response</em> section</li>
</ul></li>
<li>the model can also be accessed from R directly through the <code>yhat.predict</code> function
<ul>
<li>store the data you want to predict on in a data frame with the correct variable names</li>
<li>set up the configuration information through <code>yhat.config</code> (see above section)</li>
<li><code>yhat.predict(&quot;name&quot;, df)</code> = returns the result by feeding the input data to the model hosted on yhat under your credentials</li>
<li>can be applied to multiple rows of data at the same time</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(yhatr)</span><br><span class="line">yhat.config  &lt;- c(</span><br><span class="line">        username=<span class="string">"email@gmail.com"</span>,</span><br><span class="line">        apikey=<span class="string">"90d2a80bb532cabb2387aa51ac4553cc"</span>,</span><br><span class="line">        env=<span class="string">"http://sandbox.yhathq.com/"</span></span><br><span class="line">)</span><br><span class="line">df &lt;- data.frame(lon = c(-<span class="number">76.6167</span>, -<span class="number">118.25</span>), lat = c(<span class="number">39.2833</span>, <span class="number">34.05</span>),</span><br><span class="line">                 radius = <span class="number">20</span>)</span><br><span class="line">yhat.predict(<span class="string">"pollutant"</span>, df)</span><br></pre></td></tr></table></figure>
<ul>
<li>the model can also directly from command line interfaces (CLI) such as cmd on Windows and terminal on Mac</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST -H <span class="string">"Content-Type: application/json"</span> \</span><br><span class="line">    --user email@gmail.com:90d2a80bb532cabb2387aa51ac4553cc \</span><br><span class="line">    --data <span class="string">'&#123; "lon" : -76.61, "lat": 39.28, "radius": 50 &#125;'</span> \</span><br><span class="line">    http://cloud.yhathq.com/rdpeng@gmail.com/models/pollutant/</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em>additional example</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load library</span></span><br><span class="line"><span class="keyword">library</span>(yhatr)</span><br><span class="line"><span class="comment"># yhat functions</span></span><br><span class="line">model.require &lt;- <span class="keyword">function</span>() &#123;&#125;</span><br><span class="line">model.transform &lt;- <span class="keyword">function</span>(df) &#123;</span><br><span class="line">        transform(df, Wind = as.numeric(as.character(Wind)),</span><br><span class="line">                  Temp = as.integer(as.character(Temp)))</span><br><span class="line">&#125;</span><br><span class="line">model.predict &lt;- <span class="keyword">function</span>(df) &#123;</span><br><span class="line">        result &lt;- data.frame(Ozone = predict(fit, newdata = df))</span><br><span class="line">        cl &lt;- data.frame(clWind = class(df$Wind), clTemp = class(df$Temp))</span><br><span class="line">        data.frame(result, Temp = as.character(df$Temp),</span><br><span class="line">                   Wind = as.character(df$Wind), cl)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">fit &lt;- lm(Ozone ~ Wind + Temp, data = airquality)</span><br><span class="line"><span class="comment"># configuration</span></span><br><span class="line">yhat.config  &lt;- c(</span><br><span class="line">        username=<span class="string">"email@gmail.com"</span>,</span><br><span class="line">        apikey=<span class="string">"90d2a80bb532cabb2387aa51ac4553cc"</span>,</span><br><span class="line">        env=<span class="string">"http://sandbox.yhathq.com/"</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># deploy to yhat</span></span><br><span class="line">yhat.deploy(<span class="string">"ozone"</span>)</span><br><span class="line"><span class="comment"># predict using uploaded model</span></span><br><span class="line">yhat.predict(<span class="string">"ozone"</span>, data.frame(Wind = <span class="number">9.7</span>, Temp = <span class="number">67</span>))</span><br></pre></td></tr></table></figure>
-->
          
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Practical-Machine-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/07/Practical-Machine-Learning/" itemprop="url">
                  Practical Machine Learning
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 14:16:57 / Modified: 14:20:48" itemprop="dateCreated datePublished" datetime="2018-08-07T14:16:57+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">105k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">1:35</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="prediction">Prediction</h2>
<ul>
<li><strong>process for prediction</strong> = population <span class="math inline">\(\rightarrow\)</span> probability and sampling to pick set of data <span class="math inline">\(\rightarrow\)</span> split into training and test set <span class="math inline">\(\rightarrow\)</span> build prediction function <span class="math inline">\(\rightarrow\)</span> predict for new data <span class="math inline">\(\rightarrow\)</span> evaluate
<ul>
<li><em><strong>Note</strong>: choosing the right dataset and knowing what the specific question is are paramount to the success of the prediction algorithm (GoogleFlu failed to predict accurately when people’s search habits changed) </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">library(png);library(grid)</span><br><span class="line">library(doMC); registerDoMC(cores = 4)</span><br><span class="line">grid.raster(readPNG(&quot;figures/1.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>components of predictor</strong> = question <span class="math inline">\(\rightarrow\)</span> input data <span class="math inline">\(\rightarrow\)</span> features <em>(extracting variables/characteristics)</em> <span class="math inline">\(\rightarrow\)</span> algorithm <span class="math inline">\(\rightarrow\)</span> parameters <em>(estimate)</em> <span class="math inline">\(\rightarrow\)</span> evaluation</li>
<li><strong>relative order of importance</strong> = question (concrete/specific) <strong>&gt;</strong> data (relevant) <strong>&gt;</strong> features (properly extract) <strong>&gt;</strong> algorithms</li>
<li><strong>data selection</strong>
<ul>
<li><em><strong>Note</strong>: “garbage in = garbage out” <span class="math inline">\(\rightarrow\)</span> having the correct/relevant data will decide whether the model is successful </em></li>
<li>data for what you are trying to predict is most helpful</li>
<li>more data <span class="math inline">\(\rightarrow\)</span> better models (usually)</li>
</ul></li>
<li><strong>feature selection</strong>
<ul>
<li>good features <span class="math inline">\(\rightarrow\)</span> lead to data compression, retain relevant information, created based on expert domain knowledge</li>
<li>common mistakes <span class="math inline">\(\rightarrow\)</span> automated feature selection (can yield good results but likely to behave inconsistently with slightly different data), not understanding/dealing with skewed data/outliers, throwing away information unnecessarily</li>
</ul></li>
<li><strong>algorithm selection</strong>
<ul>
<li>matter less than one would expect</li>
<li>getting a sensible approach/algorithm will be the basis for a successful prediction</li>
<li>more complex algorithms can yield incremental improvements</li>
<li>ideally <em>interpretable</em> (simple to explain), accurate, scalable/fast (may leverage parallel computation)</li>
</ul></li>
<li>prediction is effectively about <strong><em>trade-offs</em></strong>
<ul>
<li>find the correct balance between interpretability vs accuracy vs speed vs simplicity vs scalability</li>
<li><em>interpretability</em> is especially important in conveying how features are used to predict outcome</li>
<li>scalability is important because for an algorithm to be of practical use, it needs to be implementable on large datasets without incurring large costs (computational complexity/time)</li>
</ul></li>
</ul>
<h3 id="in-sample-vs-out-of-sample-errors">In Sample vs Out of Sample Errors</h3>
<ul>
<li><strong>in sample error</strong> = error resulted from applying your prediction algorithm to the dataset you built it with
<ul>
<li>also known as <em>resubstitution error</em></li>
<li>often optimistic (less than on a new sample) as the model may be tuned to error of the sample</li>
</ul></li>
<li><strong>out of sample error</strong> = error resulted from applying your prediction algorithm to a new data set
<ul>
<li>also known as <em>generalization error</em></li>
<li>out of sample error most important as it better evaluates how the model should perform</li>
</ul></li>
<li>in sample error <strong>&lt;</strong> out of sample error
<ul>
<li>reason is <strong><em>over-fitting</em></strong>: model too adapted/optimized for the initial dataset
<ul>
<li>data have two parts: <em>signal</em> vs <em>noise</em></li>
<li>goal of predictor (should be simple/robust) = find signal</li>
<li>it is possible to design an accurate in-sample predictor, but it captures both signal and noise</li>
<li>predictor won’t perform as well on new sample</li>
</ul></li>
<li>often times it is better to give up a little accuracy for more robustness when predicting on new data</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">library(kernlab); data(spam); set.seed(333)</span><br><span class="line"># picking a small subset (10 values) from spam data set</span><br><span class="line">smallSpam &lt;- spam[sample(dim(spam)[1],size=10),]</span><br><span class="line"># label spam = 2 and ham = 1</span><br><span class="line">spamLabel &lt;- (smallSpam$type==&quot;spam&quot;)*1 + 1</span><br><span class="line"># plot the capitalAve values for the dataset with colors differentiated by spam/ham (2 vs 1)</span><br><span class="line">plot(smallSpam$capitalAve,col=spamLabel)</span><br><span class="line"># first rule (over-fitting to capture all variation)</span><br><span class="line">rule1 &lt;- function(x)&#123;</span><br><span class="line">	prediction &lt;- rep(NA,length(x))</span><br><span class="line">	prediction[x &gt; 2.7] &lt;- &quot;spam&quot;</span><br><span class="line">	prediction[x &lt; 2.40] &lt;- &quot;nonspam&quot;</span><br><span class="line">	prediction[(x &gt;= 2.40 &amp; x &lt;= 2.45)] &lt;- &quot;spam&quot;</span><br><span class="line">	prediction[(x &gt; 2.45 &amp; x &lt;= 2.70)] &lt;- &quot;nonspam&quot;</span><br><span class="line">	return(prediction)</span><br><span class="line">&#125;</span><br><span class="line"># tabulate results of prediction algorithm 1 (in sample error -&gt; no error in this case)</span><br><span class="line">table(rule1(smallSpam$capitalAve),smallSpam$type)</span><br><span class="line"># second rule (simple, setting a threshold)</span><br><span class="line">rule2 &lt;- function(x)&#123;</span><br><span class="line">	prediction &lt;- rep(NA,length(x))</span><br><span class="line">	prediction[x &gt; 2.8] &lt;- &quot;spam&quot;</span><br><span class="line">	prediction[x &lt;= 2.8] &lt;- &quot;nonspam&quot;</span><br><span class="line">	return(prediction)</span><br><span class="line">&#125;</span><br><span class="line"># tabulate results of prediction algorithm 2(in sample error -&gt; 10% in this case)</span><br><span class="line">table(rule2(smallSpam$capitalAve),smallSpam$type)</span><br><span class="line"># tabulate out of sample error for algorithm 1</span><br><span class="line">table(rule1(spam$capitalAve),spam$type)</span><br><span class="line"># tabulate out of sample error for algorithm 2</span><br><span class="line">table(rule2(spam$capitalAve),spam$type)</span><br><span class="line"># accuracy and total correct for algorithm 1 and 2</span><br><span class="line">rbind(&quot;Rule 1&quot; = c(Accuracy = mean(rule1(spam$capitalAve)==spam$type),</span><br><span class="line">	&quot;Total Correct&quot; = sum(rule1(spam$capitalAve)==spam$type)),</span><br><span class="line">	&quot;Rule 2&quot; = c(Accuracy = mean(rule2(spam$capitalAve)==spam$type),</span><br><span class="line">	&quot;Total Correct&quot; = sum(rule2(spam$capitalAve)==spam$type)))</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="prediction-study-design">Prediction Study Design</h2>
<ul>
<li><strong>procedures</strong>
<ol type="1">
<li>define error rate (type I/type II)</li>
<li>split data into:
<ul>
<li>training, testing, validation (optional)</li>
</ul></li>
<li>pick features from the training set
<ul>
<li>use cross-validation</li>
</ul></li>
<li>pick prediction function (model) on the training set
<ul>
<li>use cross-validation</li>
</ul></li>
<li>if no validation set
<ul>
<li>apply <strong>1 time</strong> to test set</li>
</ul></li>
<li>if there is a validation set
<ul>
<li>apply to test set and refine</li>
<li>apply <strong>1 time</strong> to validation</li>
</ul></li>
</ol>
<ul>
<li><em><strong>Note</strong>: it’s important to hold out an untouched sample to accurately estimate the out of sample error rate </em></li>
</ul></li>
<li>benchmarks (i.e. set all variables = 0) can help pinpoint/test the model to see what is wrong with the model</li>
<li>avoid small sample sizes
<ul>
<li>consider binary outcomes (i.e. coin flip)</li>
<li>for n = 1, the probability of perfect classification (100% accuracy) is 50%</li>
<li>for n = 10, the probability of perfect classification (100% accuracy) is 0.1%</li>
<li>so it’s important to have bigger samples so that when you do get a high accuracy, it may actually be a significant result and not just by chance</li>
</ul></li>
<li><strong><em>example: Netflix rating prediction competition</em></strong>
<ul>
<li>split data between training and “held-out”
<ul>
<li>held-out included probe, quiz and test sets</li>
<li>probe is used to test the predictor built from the training dataset</li>
<li>quiz is used to realistically evaluate out of sample error rates</li>
<li>test is used to finally evaluate the validity of algorithm</li>
</ul></li>
<li>important to not tune model to quiz set specifically</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/2.png&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="sample-division-guidelines-for-prediction-study-design">Sample Division Guidelines for Prediction Study Design</h3>
<ul>
<li>for large sample sizes
<ul>
<li>60% training</li>
<li>20% test</li>
<li>20% validation</li>
</ul></li>
<li>for medium sample sizes
<ul>
<li>60% training</li>
<li>40% test</li>
<li>no validation set to refine model (to ensure test set is of sufficient size)</li>
</ul></li>
<li>for small sample sizes
<ul>
<li>carefully consider if there are enough sample to build a prediction algorithm</li>
<li>no test/validation sets</li>
<li>perform cross validation</li>
<li>report caveat of small sample size and highlight the fact that the prediction algorithm has never been tested for out of sample error</li>
</ul></li>
<li>there should always be a test/validation set that is held away and should <strong><em>NOT</em></strong> be looked at when building model
<ul>
<li>when complete, apply the model to the held-out set only one time</li>
</ul></li>
<li><strong><em>randomly sample</em></strong> training and test sets
<ul>
<li>for data collected over time, build training set in chunks of times</li>
</ul></li>
<li>datasets must reflect structure of problem
<ul>
<li>if prediction evolves with time, split train/test sets in time chunks (known as <em>backtesting</em> in finance)</li>
</ul></li>
<li>subsets of data should reflect as much diversity as possible</li>
</ul>
<h3 id="picking-the-right-data">Picking the Right Data</h3>
<ul>
<li>use like data to predict like</li>
<li>to predict a variable/process X, use the data that’s as closely related to X as possible</li>
<li>weighting the data/variables by understanding and intuition can help to improve accuracy of prediction</li>
<li>data properties matter <span class="math inline">\(\rightarrow\)</span> knowing how the data connects to what you are trying to measure</li>
<li>predicting on unrelated data is the most common mistake
<ul>
<li>if unrelated data must be used, be careful about interpreting the model as to why it works/doesn’t work</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="types-of-errors">Types of Errors</h2>
<ul>
<li>when discussing the outcome decided on by the algorithm, <strong>Positive</strong> = identified and <strong>negative</strong> = rejected
<ul>
<li><strong>True positive</strong> = correctly identified (predicted true when true)</li>
<li><strong>False positive</strong> = incorrectly identified (predicted true when false)</li>
<li><strong>True negative</strong> = correctly rejected (predicted false when false)</li>
<li><strong>False negative</strong> = incorrectly rejected (predicted false when true)</li>
</ul></li>
<li><em><strong>example: medical testing</strong></em>
<ul>
<li><em>True positive</em> = Sick people correctly diagnosed as sick</li>
<li><em>False positive</em> = Healthy people incorrectly identified as sick</li>
<li><em>True negative</em> = Healthy people correctly identified as healthy</li>
<li><em>False negative</em> = Sick people incorrectly identified as healthy</li>
</ul></li>
</ul>
<h3 id="notable-measurements-for-error-binary-variables">Notable Measurements for Error – Binary Variables</h3>
<ul>
<li><strong>accuracy</strong> = weights false positives/negatives equally</li>
<li><strong>concordance</strong> = for multi-class cases, <span class="math display">\[\kappa = \frac{accuracy - P(e)}{1 - P(e)}\]</span> where <span class="math display">\[P(e) = \frac{TP+FP}{total} \times \frac{TP+FN}{total} + \frac{TN+FN}{total} \times \frac{FP+TN}{total}\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/3.png&quot;))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/4.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em>example</em></strong>
<ul>
<li>given that a disease has 0.1% prevalence in the population, we want to know what’s probability of a person having the disease given the test result is positive? the test kit for the disease is 99% sensitive (most positives = disease) and 99% specific (most negatives = no disease)</li>
<li>what about 10% prevalence?</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/5.png&quot;))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/6.png&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="notable-measurements-for-error-continuous-variables">Notable Measurements for Error – Continuous Variables</h3>
<ul>
<li><strong>Mean squared error (MSE)</strong> = <span class="math display">\[\frac{1}{n} \sum_{i=1}^n (Prediction_i - Truth_i)^2\]</span></li>
<li><strong>Root mean squared error (RMSE)</strong> - <span class="math display">\[\sqrt{\frac{1}{n} \sum_{i=1}^n(Prediction_i - Truth_i)^2}\]</span>
<ul>
<li>in the same units as variable</li>
<li>most commonly used error measure for continuous data</li>
<li>is not an effective measures when there are outliers
<ul>
<li>one large value may significantly raise the RMSE</li>
</ul></li>
</ul></li>
<li><strong>median absolute deviation</strong> = <span class="math display">\[median(|Prediction_i - Truth_i|)\]</span></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="receiver-operating-characteristic-curves">Receiver Operating Characteristic Curves</h2>
<ul>
<li>are commonly used techniques to measure the quality of a prediction algorithm.</li>
<li>predictions for binary classification often are quantitative (i.e. probability, scale of 1 to 10)
<ul>
<li>different cutoffs/threshold of classification (&gt; 0.8 <span class="math inline">\(\rightarrow\)</span> one outcome) yield different results/predictions</li>
<li><strong>Receiver Operating Characteristic</strong> curves are generated to compare the different outcomes</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/7.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>ROC Curves
<ul>
<li><strong><em>x-axis</em></strong> = 1 - specificity (or, probability of false positive)</li>
<li><strong><em>y-axis</em></strong> = sensitivity (or, probability of true positive)</li>
<li><strong><em>points plotted</em></strong> = cutoff/combination</li>
<li><strong><em>areas under curve</em></strong> = quantifies whether the prediction model is viable or not
<ul>
<li>higher area <span class="math inline">\(\rightarrow\)</span> better predictor</li>
<li>area = 0.5 <span class="math inline">\(\rightarrow\)</span> effectively random guessing (diagonal line in the ROC curve)</li>
<li>area = 1 <span class="math inline">\(\rightarrow\)</span> perfect classifier</li>
<li>area = 0.8 <span class="math inline">\(\rightarrow\)</span> considered good for a prediction algorithm</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/8.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em>example</em></strong>
<ul>
<li>each point on the graph corresponds with a specificity and sensitivity</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/9.png&quot;))</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="cross-validation">Cross Validation</h2>
<ul>
<li><strong>procedures</strong>
<ol type="1">
<li>split training set into sub-training/test sets</li>
<li>build model on sub-training set</li>
<li>evaluate on sub-test set</li>
<li>repeat and average estimated errors</li>
</ol></li>
<li><strong>result</strong>
<ul>
<li>we are able to fit/test various different models with different variables included to the find the best one on the cross-validated test sets</li>
<li>we are able to test out different types of prediction algorithms to use and pick the best performing one</li>
<li>we are able to choose the parameters in prediction function and estimate their values</li>
<li><em><strong>Note</strong>: original test set completely untouched, so when final prediction algorithm is applied, the result will be an unbiased measurement of the <strong>out of sample accuracy</strong> of the model </em></li>
</ul></li>
<li><strong>approaches</strong>
<ul>
<li>random subsampling</li>
<li>K-fold</li>
<li>leave one out</li>
</ul></li>
<li><strong>considerations</strong>
<ul>
<li>for time series data data must be used in “chunks”
<ul>
<li>one time period might depending all time periods previously (should not take random samples)</li>
</ul></li>
<li>if you cross-validate to pick predictors, the out of sample error rate may not be the most accurate and thus the errors should still be measured on independent data</li>
</ul></li>
</ul>
<h3 id="random-subsampling">Random Subsampling</h3>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/10.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>a randomly sampled test set is subsetted out from the original training set</li>
<li>the predictor is built on the remaining training data and applied to the test set</li>
<li>the above are <strong><em>three</em></strong> random subsamplings from the same training set</li>
<li><strong><em>considerations</em></strong>
<ul>
<li>must be done <em>without replacement</em></li>
<li>random sampling with replacement = <em>bootstrap</em>
<ul>
<li>underestimates of the error, since the if we get one right and the sample appears more than once we’ll get the other right</li>
<li>can be corrected with the (<a href="http://www.jstor.org/discover/10.2307/2965703?uid=2&amp;uid=4&amp;sid=21103054448997" target="_blank" rel="noopener">0.632 Bootstrap</a>), but it is complicated</li>
</ul></li>
</ul></li>
</ul>
<h3 id="k-fold">K-Fold</h3>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/11.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>break training set into <span class="math inline">\(K\)</span> subsets (above is a 3-fold cross validation)</li>
<li>build the model/predictor on the remaining training data in each subset and applied to the test subset</li>
<li>rebuild the data <span class="math inline">\(K\)</span> times with the training and test subsets and average the findings</li>
<li><strong><em>considerations</em></strong>
<ul>
<li>larger k = less bias, more variance</li>
<li>smaller k = more bias, less variance</li>
</ul></li>
</ul>
<h3 id="leave-one-out">Leave One Out</h3>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/12.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>leave out exactly one sample and build predictor on the rest of training data</li>
<li>predict value for the left out sample</li>
<li>repeat for each sample</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="caret-package-tutorial"><code>caret</code> Package (<a href="http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf" target="_blank" rel="noopener">tutorial</a>)</h2>
<ul>
<li><strong>core functionality</strong>
<ul>
<li>preprocessing/cleaning data <span class="math inline">\(\rightarrow\)</span> <code>preProcess()</code></li>
<li>cross validation/data splitting <span class="math inline">\(\rightarrow\)</span> <code>createDataPartition()</code>, <code>createResample()</code>, <code>createTimeSlices()</code></li>
<li>train algorithms on training data and apply to test sets <span class="math inline">\(\rightarrow\)</span> <code>train()</code>, <code>predict()</code></li>
<li>model comparison (evaluate the accuracy of model on new data) <span class="math inline">\(\rightarrow\)</span> <code>confusionMatrix()</code></li>
</ul></li>
<li>machine learning algorithms in <code>caret</code> package
<ul>
<li>linear discriminant analysis</li>
<li>regression</li>
<li>naive Bayes</li>
<li>support vector machines</li>
<li>classification and regression trees</li>
<li>random forests</li>
<li>boosting</li>
<li>many others</li>
</ul></li>
<li><code>caret</code> provides uniform framework to build/predict using different models
<ul>
<li>create objects of different classes for different algorithms, and <code>caret</code> package allows algorithms to be run the same way through <code>predict()</code> function</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/13.png&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="data-slicing">Data Slicing</h3>
<ul>
<li><code>createDataPartition(y=data$var, times=1, p=0.75, list=FALSE)</code> <span class="math inline">\(\rightarrow\)</span> creates data partitions using given variable
<ul>
<li><code>y=data$var</code> = specifies what outcome/variable to split the data on</li>
<li><code>times=1</code> = specifies number of partitions to create (number of data splitting performed)</li>
<li><code>p=0.75</code> = percent of data that will be for training the model</li>
<li><code>list=FALSE</code> = returns a matrix of indices corresponding to <strong><code>p</code>%</strong> of the data (training set)
<ul>
<li><em><strong>Note</strong>: matrix is easier to subset the data with, so <code>list = FALSE</code> is generally what is used </em></li>
<li><code>list=TRUE</code> = returns a list of indices corresponding to <strong><code>p</code>%</strong> of the data (training set)</li>
</ul></li>
<li>the function effectively returns a list of indexes of the training set which can then be leveraged to subset the data
<ul>
<li><code>training&lt;-data[inTrain, ]</code> = subsets the data to training set only</li>
<li><code>testing&lt;-data[-inTrain, ]</code> = the rest of the data set can then be stored as the test set</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># load packages and data</span><br><span class="line">library(caret)</span><br><span class="line"># create training set indexes with 75% of data</span><br><span class="line">inTrain &lt;- createDataPartition(y=spam$type,p=0.75, list=FALSE)</span><br><span class="line"># subset spam data to training</span><br><span class="line">training &lt;- spam[inTrain,]</span><br><span class="line"># subset spam data (the rest) to test</span><br><span class="line">testing &lt;- spam[-inTrain,]</span><br><span class="line"># dimension of original and training dataset</span><br><span class="line">rbind(&quot;original dataset&quot; = dim(spam),&quot;training set&quot; = dim(training))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>createFolds(y=data$var, k=10, list=TRUE, returnTrain=TRUE)</code> = slices the data in to <span class="math inline">\(k\)</span> folds for cross validation and returns <span class="math inline">\(k\)</span> lists of indices
<ul>
<li><code>y=data$var</code> = specifies what outcome/variable to split the data on</li>
<li><code>k=10</code> = specifies number of folds to create (See <strong><em><a href="#k-folds">K Fold Cross Validation</a></em></strong>)
<ul>
<li>each training set has approximately has <span class="math inline">\(\frac{k-1}{k} \%\)</span> of the data (in this case 90%)</li>
<li>each training set has approximately has <span class="math inline">\(\frac{1}{k} \%\)</span> of the data (in this case 10%)</li>
</ul></li>
<li><code>list=TRUE</code> = returns <code>k</code> list of indices that corresponds to the cross-validated sets
<ul>
<li><em><strong>Note</strong>: the returned list conveniently splits the data into <code>k</code> datasets/vectors of indices, so <code>list=TRUE</code> is generally what is used </em></li>
<li>when the returned object is a list (called <code>folds</code> in the case), you can use <code>folds[[1]][1:10]</code> to access different elements from that list</li>
<li><code>list=FALSE</code> = returns a vector indicating which of the <code>k</code> folds each data point belongs to (i.e. 1 - 10 is assigned for each of the data points in this case)
<ul>
<li><em><strong>Note</strong>: these group values corresponds to test sets for each cross validation, which means everything else besides the marked points should be used for training </em></li>
</ul></li>
</ul></li>
<li>[only works when <code>list=T</code>] <code>returnTrain=TRUE</code> = returns the indices of the training sets
<ul>
<li>[default value when unspecified]<code>returnTrain=FALSE</code> = returns indices of the test sets</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># create 10 folds for cross validation and return the training set indices</span><br><span class="line">folds &lt;- createFolds(y=spam$type,k=10,list=TRUE,returnTrain=TRUE)</span><br><span class="line"># structure of the training set indices</span><br><span class="line">str(folds)</span><br><span class="line"># return the test set indices instead</span><br><span class="line"># note: returnTrain = FALSE is unnecessary as it is the default behavior</span><br><span class="line">folds.test &lt;- createFolds(y=spam$type,k=10,list=TRUE,returnTrain=FALSE)</span><br><span class="line">str(folds.test)</span><br><span class="line"># return first 10 elements of the first training set</span><br><span class="line">folds[[1]][1:10]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>createResample(y=data$var, times=10, list=TRUE)</code> = create 10 resamplings from the given data with replacement
<ul>
<li><code>list=TRUE</code> = returns list of n vectors that contain indices of the sample
<ul>
<li><em><strong>Note</strong>: each of the vectors is of length of the data, and contains indices </em></li>
</ul></li>
<li><code>times=10</code> = number of samples to create</li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create 10 resamples</span><br><span class="line">resamples &lt;- createResample(y=spam$type,times=10,list=TRUE)</span><br><span class="line"># structure of the resamples (note some samples are repeated)</span><br><span class="line">str(resamples)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>createTimeSlices(y=data, initialWindow=20, horizon=10)</code> = creates training sets with specified window length and the corresponding test sets
<ul>
<li><code>initialWindow=20</code> = number of consecutive values in each time slice/training set (i.e. values 1 - 20)</li>
<li><code>horizon=10</code> = number of consecutive values in each predict/test set (i.e. values 21 - 30)</li>
<li><code>fixedWindow=FALSE</code> = training sets always start at the first observation
<ul>
<li>this means that the first training set would be 1 - 20, the second will be 1 - 21, third 1 - 22, etc.</li>
<li>but the test sets are still like before (21 - 30, 22 - 31, etc.)</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># create time series data</span><br><span class="line">tme &lt;- 1:1000</span><br><span class="line"># create time slices</span><br><span class="line">folds &lt;- createTimeSlices(y=tme,initialWindow=20,horizon=10)</span><br><span class="line"># name of lists</span><br><span class="line">names(folds)</span><br><span class="line"># first training set</span><br><span class="line">folds$train[[1]]</span><br><span class="line"># first test set</span><br><span class="line">folds$test[[1]]</span><br></pre></td></tr></table></figure>
<h3 id="training-options-tutorial">Training Options (<a href="http://topepo.github.io/caret/training.html" target="_blank" rel="noopener">tutorial</a>)</h3>
<ul>
<li><code>train(y ~ x, data=df, method=&quot;glm&quot;)</code> = function to apply the machine learning algorithm to construct model from training data</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># returns the arguments of the default train function</span><br><span class="line">args(train.default)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>train</code> function has a large set of parameters, below are the default options
<ul>
<li><code>method=&quot;rf&quot;</code> = default algorithm is random forest for training a given data set; <code>caret</code> contains a large number of algorithms
<ul>
<li><code>names(getModelInfo())</code> = returns all the options for <code>method</code> argument</li>
<li>list of models and their information can be found <a href="http://topepo.github.io/caret/modelList.html" target="_blank" rel="noopener">here</a></li>
</ul></li>
<li><code>preProcess=NULL</code> = set preprocess options (see <strong><em><a href="#preprocessing-tutorial">Preprocessing</a></em></strong>)</li>
<li><code>weights=NULL</code> = can be used to add weights to observations, useful for unbalanced distribution (a lot more of one type than another)</li>
<li><code>metric=ifelse(is.factor(y), &quot;Accuracy&quot;, &quot;RMSE&quot;)</code> = default metric for algorithm is <em>Accuracy</em> for factor variables, and <em>RMSE</em>, or root mean squared error, for continuous variables
<ul>
<li><em>Kappa</em> = measure of concordance (see <strong><em><a href="#notable-measurements-for-error-binary-variables">Notable Measurements for Error – Binary Variables</a></em></strong>)</li>
<li><em>RSquared</em> can also be used here as a metric, which represents R<sup>2</sup> from regression models (only useful for linear models)</li>
</ul></li>
<li><code>maximize=ifelse(metric==&quot;RMSE&quot;, FALSE, TRUE)</code> = the algorithm should maximize <em>accuracy</em> and minimize <em>RMSE</em></li>
<li><code>trControl=trainControl()</code> = training controls for the model, more details below</li>
<li><code>tuneGrid=NULL</code></li>
<li><code>tuneLength=3</code></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># returns the default arguments for the trainControl object</span><br><span class="line">args(trainControl)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>trainControl</code> creates an object that sets many options for how the model will be applied to the training data
<ul>
<li><em><strong>Note</strong>: the default values are listed below but you can use them to set the parameters to your discretion </em></li>
<li><code>method=&quot;boot&quot;</code> =
<ul>
<li><code>&quot;boot&quot;</code> = bootstrapping (drawing with replacement)</li>
<li><code>&quot;boot632&quot;</code> = bootstrapping with adjustment</li>
<li><code>&quot;cv&quot;</code> = cross validation</li>
<li><code>&quot;repeatedcv&quot;</code> = repeated cross validation</li>
<li><code>&quot;LOOCV&quot;</code> = leave one out cross validation</li>
</ul></li>
<li><code>number=ifelse(grepl(&quot;cv&quot;, method),10, 25)</code> = number of subsamples to take
<ul>
<li><code>number=10</code> = default for any kind of cross validation</li>
<li><code>number=25</code> = default for bootstrapping</li>
<li><em><strong>Note</strong>: <code>number</code> should be increased when fine-tuning model with large number of parameter </em></li>
</ul></li>
<li><code>repeats=ifelse(grepl(&quot;cv&quot;, method), 1, number)</code> = numbers of times to repeat the subsampling
<ul>
<li><code>repeats=1</code> = default for any cross validation method</li>
<li><code>repeats=25</code> = default for bootstrapping</li>
</ul></li>
<li><code>p=0.75</code> = default percentage of data to create training sets</li>
<li><code>initialWindow=NULL, horizon=1, fixedWindow=TRUE</code> = parameters for time series data</li>
<li><code>verboseIter=FALSE</code> = print the training logs</li>
<li><code>returnData=TRUE</code>, returnResamp = “final”,</li>
<li><code>savePredictions=FALSE</code> = save the predictions for each resample</li>
<li><code>classProbs=FALSE</code> = return classification probabilities along with the predictions</li>
<li><code>summaryFunction=defaultSummary</code> = default summary of the model,</li>
<li><code>preProcOptions=list(thresh = 0.95, ICAcomp = 3, k = 5)</code> = specifies preprocessing options for the model</li>
<li><code>predictionBounds=rep(FALSE, 2)</code> = specify the range of the predicted value
<ul>
<li>for numeric predictions, <code>predictionBounds=c(10, NA)</code> would mean that any value lower than 10 would be treated as 10 and no upper bounds</li>
</ul></li>
<li><code>seeds=NA</code> = set the seed for the operation
<ul>
<li><em><strong>Note</strong>: setting this is important when you want to reproduce the same results when the <code>train</code> function is run </em></li>
</ul></li>
<li><code>allowParallel=TRUE</code> = sets for parallel processing/computations</li>
</ul></li>
</ul>
<h3 id="plotting-predictors-tutorial">Plotting Predictors (<a href="http://caret.r-forge.r-project.org/visualizations.html" target="_blank" rel="noopener">tutorial</a>)</h3>
<ul>
<li>it is important to only plot the data in the training set
<ul>
<li>using the test data may lead to over-fitting (model should not be adjusted to test set)</li>
</ul></li>
<li>goal of producing these exploratory plots = look for potential outliers, skewness, imbalances in outcome/predictors, and explainable groups of points/patterns</li>
<li><code>featurePlot(x=predictors, y=outcome, plot=&quot;pairs&quot;)</code> = short cut to plot the relationships between the predictors and outcomes</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># load relevant libraries</span><br><span class="line">library(ISLR); library(ggplot2);</span><br><span class="line"># load wage data</span><br><span class="line">data(Wage)</span><br><span class="line"># create training and test sets</span><br><span class="line">inTrain &lt;- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)</span><br><span class="line">training &lt;- Wage[inTrain,]</span><br><span class="line">testing &lt;- Wage[-inTrain,]</span><br><span class="line"># plot relationships between the predictors and outcome</span><br><span class="line">featurePlot(x=training[,c(&quot;age&quot;,&quot;education&quot;,&quot;jobclass&quot;)], y = training$wage,plot=&quot;pairs&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>qplot(age, wage, color=eduction, data=training)</code> = can be used to separate data by a factor variable (by coloring the points differently)
<ul>
<li><code>geom_smooth(method = &quot;lm&quot;)</code> = adds a regression line to the plots</li>
<li><code>geom=c(&quot;boxplot&quot;, &quot;jitter&quot;)</code> = specifies what kind of plot to produce, in this case both the boxplot and the point cloud</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># qplot plus linear regression lines</span><br><span class="line">qplot(age,wage,colour=education,data=training)+geom_smooth(method=&apos;lm&apos;,formula=y~x)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cut2(variable, g=3)</code> = creates a new factor variable by cutting the specified variable into n groups (3 in this case) based on percentiles
<ul>
<li><em><strong>Note</strong>: <code>cut2</code> function is part of the <code>Hmisc</code> package, so <code>library(Hmisc)</code> must be run first </em></li>
<li>this variable can then be used to tabulate/plot the data</li>
</ul></li>
<li><code>grid.arrange(p1, p2, ncol=2)</code> = <code>ggplot2</code> function the print multiple graphs on the same plot
<ul>
<li><em><strong>Note</strong>: <code>grid.arrange</code> function is part of the <code>gridExtra</code> package, so <code>library(gridExtra)</code> must be run first </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># load Hmisc and gridExtra packages</span><br><span class="line">library(Hmisc);library(gridExtra);</span><br><span class="line"># cute the wage variable</span><br><span class="line">cutWage &lt;- cut2(training$wage,g=3)</span><br><span class="line"># plot the boxplot</span><br><span class="line">p1 &lt;- qplot(cutWage,age, data=training,fill=cutWage,</span><br><span class="line">      geom=c(&quot;boxplot&quot;))</span><br><span class="line"># plot boxplot and point clusters</span><br><span class="line">p2 &lt;- qplot(cutWage,age, data=training,fill=cutWage,</span><br><span class="line">      geom=c(&quot;boxplot&quot;,&quot;jitter&quot;))</span><br><span class="line"># plot the two graphs side by side</span><br><span class="line">grid.arrange(p1,p2,ncol=2)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>table(cutVariable, data$var2)</code> = tabulates the cut factor variable vs another variable in the dataset (ie; builds a contingency table using cross-classifying factors)</li>
<li><code>prop.table(table, margin=1)</code> = converts a table to a proportion table
<ul>
<li><code>margin=1</code> = calculate the proportions based on the rows</li>
<li><code>margin=2</code> = calculate the proportions based on the columns</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># tabulate the cutWage and jobclass variables</span><br><span class="line">t &lt;- table(cutWage,training$jobclass)</span><br><span class="line"># print table</span><br><span class="line">t</span><br><span class="line"># convert to proportion table based on the rows</span><br><span class="line">prop.table(t,1)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>qplot(var1, color=var2, data=training, geom=&quot;density&quot;)</code> = produces density plot for the given numeric and factor variables
<ul>
<li>effectively smoothed out histograms</li>
<li>provides for easy overlaying of groups of data
<ul>
<li>break different variables up by group and see how outcomes change between groups</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># produce density plot</span><br><span class="line">qplot(wage,colour=education,data=training,geom=&quot;density&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="preprocessing-tutorial">Preprocessing (<a href="http://caret.r-forge.r-project.org/preprocess.html" target="_blank" rel="noopener">tutorial</a>)</h3>
<ul>
<li>some predictors may have strange distributions (i.e. skewed) and may need to be transformed to be more useful for prediction algorithm
<ul>
<li>particularly true for model based algorithms <span class="math inline">\(\rightarrow\)</span> naive Bayes, linear discriminate analysis, linear regression</li>
</ul></li>
<li><strong>centering</strong> = subtracting the observations of a particular variable by its mean</li>
<li><strong>scaling</strong> = dividing the observations of a particular variable by its standard deviation</li>
<li><strong>normalizing</strong> = centering and scaling the variable <span class="math inline">\(\rightarrow\)</span> effectively converting each observation to the number of standard deviations away from the mean
<ul>
<li>the distribution of the normalized variable will have a mean of 0 and standard deviation of 1</li>
<li><em><strong>Note</strong>: normalizing data can help remove bias and high variability, but may not be applicable in all cases </em></li>
</ul></li>
<li><em><strong>Note</strong>: if a predictor/variable is standardized when training the model, the same transformations must be performed on the <code>test</code> set with the mean and standard deviation of the <code>train</code> variables </em>
<ul>
<li>this means that the mean and standard deviation of the normalized test variable will <strong><em>NOT</em></strong> be 0 and 1, respectively, but will be close</li>
<li>transformations must likely be imperfect but test/train sets must be processed the same way</li>
</ul></li>
<li><code>train(y~x, data=training, preProcess=c(&quot;center&quot;, &quot;scale&quot;))</code> = preprocessing can be directly specified in the <code>train</code> function
<ul>
<li><code>preProcess=c(&quot;center&quot;, &quot;scale&quot;)</code> = normalize all predictors before constructing model</li>
</ul></li>
<li><code>preProcess(trainingData, method=c(&quot;center&quot;, &quot;scale&quot;)</code> = function in the <code>caret</code> to standardize data
<ul>
<li>you can store the result of the <code>preProcess</code> function as an object and apply it to the <code>train</code> and <code>test</code> sets using the <code>predict</code> function</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># load spam data</span><br><span class="line">data(spam)</span><br><span class="line"># create train and test sets</span><br><span class="line">inTrain &lt;- createDataPartition(y=spam$type,p=0.75, list=FALSE)</span><br><span class="line">training &lt;- spam[inTrain,]</span><br><span class="line">testing &lt;- spam[-inTrain,]</span><br><span class="line"># create preProcess object for all predictors (&quot;-58&quot; because 58th = outcome)</span><br><span class="line">preObj &lt;- preProcess(training[,-58],method=c(&quot;center&quot;,&quot;scale&quot;))</span><br><span class="line"># normalize training set</span><br><span class="line">trainCapAveS &lt;- predict(preObj,training[,-58])$capitalAve</span><br><span class="line"># normalize test set using training parameters</span><br><span class="line">testCapAveS &lt;- predict(preObj,testing[,-58])$capitalAve</span><br><span class="line"># compare results for capitalAve variable</span><br><span class="line">rbind(train = c(mean = mean(trainCapAveS), std = sd(trainCapAveS)),</span><br><span class="line">	test = c(mean(testCapAveS), sd(testCapAveS)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>preprocess(data, method=&quot;BoxCox&quot;)</code> = applies BoxCox transformations to continuous data to help normalize the variables through maximum likelihood
<ul>
<li><em><strong>Note</strong>: note it assumes continuous values and DOES NOT deal with repeated values </em></li>
<li><code>qqnorm(processedVar)</code> = can be used to produce the Q-Q plot which compares the theoretical quantiles with the sample quantiles to see the normality of the data</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># set up BoxCox transforms</span><br><span class="line">preObj &lt;- preProcess(training[,-58],method=c(&quot;BoxCox&quot;))</span><br><span class="line"># perform preprocessing on training data</span><br><span class="line">trainCapAveS &lt;- predict(preObj,training[,-58])$capitalAve</span><br><span class="line"># plot histogram and QQ Plot</span><br><span class="line"># Note: the transformation definitely helped to</span><br><span class="line"># normalize the data but it does not produce perfect result</span><br><span class="line">par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>preProcess(data, method=&quot;knnImpute&quot;)</code> = impute/estimate the missing data using <strong>k nearest neighbors (knn)</strong> imputation
<ul>
<li><code>knnImpute</code> = takes the k nearest neighbors from the missing value and averages the value to impute the missing observations</li>
<li><em><strong>Note</strong>: most prediction algorithms are not build to handle missing data </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Make some values NA</span><br><span class="line">training$capAve &lt;- training$capitalAve</span><br><span class="line">selectNA &lt;- rbinom(dim(training)[1],size=1,prob=0.05)==1</span><br><span class="line">training$capAve[selectNA] &lt;- NA</span><br><span class="line"># Impute and standardize</span><br><span class="line">preObj &lt;- preProcess(training[,-58],method=&quot;knnImpute&quot;)</span><br><span class="line">capAve &lt;- predict(preObj,training[,-58])$capAve</span><br><span class="line"># Standardize true values</span><br><span class="line">capAveTruth &lt;- training$capitalAve</span><br><span class="line">capAveTruth &lt;- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)</span><br><span class="line"># compute differences between imputed values and true values</span><br><span class="line">quantile(capAve - capAveTruth)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="covariate-creationfeature-extraction">Covariate Creation/Feature Extraction</h2>
<ul>
<li><strong>[level 1]</strong>: construct covariate (usable metric, feature) from raw data depends heavily on application
<ul>
<li>ideally we want to summarize data without too much information loss</li>
<li>examples
<ul>
<li><em>text files</em>: frequency of words, frequency of phrases (<a href="https://books.google.com/ngrams" target="_blank" rel="noopener">Google ngrams</a>), frequency of capital letters</li>
<li><em>images</em>: edges, corners, blobs, ridges (<a href="http://en.wikipedia.org/wiki/Feature_detection_(computer_vision)" target="_blank" rel="noopener">computer vision feature detection</a>)</li>
<li><em>webpages</em>: number and type of images, position of elements, colors, videos (<a href="http://en.wikipedia.org/wiki/A/B_testing" target="_blank" rel="noopener">A/B Testing</a>)</li>
<li><em>people</em>: height, weight, hair color, sex, country of origin</li>
</ul></li>
<li>generally, more knowledge and understanding you have of the system/data, the easier it will be to extract the summarizing features
<ul>
<li>when in doubt, more features is always safer <span class="math inline">\(\rightarrow\)</span> lose less information and the features can be filtered during model construction</li>
</ul></li>
<li>this process can be automated (i.e. PCA) but generally have to be very careful, as one very useful feature in the training data set may not have as much effect on the test data set</li>
<li><em><strong>Note</strong>: science is the key here, Google “feature extraction for [data type]” for more guidance </em>
<ul>
<li>the goal is always to find the salient characteristics that are likely to be different from observation to observation</li>
</ul></li>
</ul></li>
<li><strong>[level 2]</strong>: construct new covariates from extracted covariate
<ul>
<li>generally transformations of features you extract from raw data</li>
<li>used more for methods like regression and support vector machines (SVM), whose accuracy depend more on the distribution of input variables</li>
<li>models like classification trees don’t require as many complex covariates</li>
<li>best approach is through exploratory analysis (tables/plots)</li>
<li>should only be performed on the train dataset</li>
<li>new covariates should be added to data frames under recognizable names so they can be used later</li>
<li><code>preProcess()</code> can be leveraged to handle creating new covariates</li>
<li><em><strong>Note</strong>: always be careful about over-fitting </em></li>
</ul></li>
</ul>
<h3 id="creating-dummy-variables">Creating Dummy Variables</h3>
<ul>
<li>convert factor variables to indicator/dummy variable <span class="math inline">\(\rightarrow\)</span> qualitative become quantitative</li>
<li><code>dummyVars(outcome~var, data=training)</code> = creates a dummy variable object that can be used through <code>predict</code> function to create dummy variables
<ul>
<li><code>predict(dummyObj, newdata=training)</code> = creates appropriate columns to represent the factor variable with appropriate 0s and 1s
<ul>
<li>2 factor variable <span class="math inline">\(\rightarrow\)</span> two columns which have 0 or 1 depending on the outcome</li>
<li>3 factor variable <span class="math inline">\(\rightarrow\)</span> three columns which have 0, 0, and 1 representing the outcome</li>
<li><em><strong>Note</strong>: only one of the columns can have values of 1 for each observation </em></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># setting up data</span><br><span class="line">inTrain &lt;- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)</span><br><span class="line">training &lt;- Wage[inTrain,]; testing &lt;- Wage[-inTrain,]</span><br><span class="line"># create a dummy variable object</span><br><span class="line">dummies &lt;- dummyVars(wage ~ jobclass,data=training)</span><br><span class="line"># create the dummy variable columns</span><br><span class="line">head(predict(dummies,newdata=training))</span><br></pre></td></tr></table></figure>
<h3 id="removing-zero-covariates">Removing Zero Covariates</h3>
<ul>
<li>some variables have no variability at all (i.e. variable indicating if an email contained letters)</li>
<li>these variables are not useful when we want to construct a prediction model</li>
<li><code>nearZeroVar(training, saveMetrics=TRUE)</code> = returns list of variables in training data set with information on frequency ratios, percent uniques, whether or not it has zero variance
<ul>
<li><code>freqRatio</code> = ratio of frequencies for the most common value over second most common value</li>
<li><code>percentUnique</code> = percentage of unique data points out of total number of data points</li>
<li><code>zeroVar</code> = TRUE/FALSE indicating whether the predictor has only one distinct value</li>
<li><code>nzv</code> = TRUE/FALSE indicating whether the predictor is a near zero variance predictor</li>
<li><em><strong>Note</strong>: when <code>nzv</code> = TRUE, those variables should be thrown out </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># print nearZeroVar table</span><br><span class="line">nearZeroVar(training,saveMetrics=TRUE)</span><br></pre></td></tr></table></figure>
<h3 id="creating-splines-polynomial-functions">Creating Splines (Polynomial Functions)</h3>
<ul>
<li>when you want to fit curves through the data, basis functions can be leveraged</li>
<li>[<code>splines</code> package] <code>bs(data$var, df=3)</code> = creates 3 new columns corresponding to the var, var<sup>2</sup>, and var<sup>3</sup> terms</li>
<li><code>ns()</code> and <code>poly()</code> can also be used to generate polynomials</li>
<li><code>gam()</code> function can also be used and it allows for smoothing of multiple variables with different values for each variable</li>
<li><em><strong>Note</strong>: the same polynomial operations must be performed on the test sets using the <code>predict</code> function </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># load splines package</span><br><span class="line">library(splines)</span><br><span class="line"># create polynomial function</span><br><span class="line">bsBasis &lt;- bs(training$age,df=3)</span><br><span class="line"># fit the outcome on the three polynomial terms</span><br><span class="line">lm1 &lt;- lm(wage ~ bsBasis,data=training)</span><br><span class="line"># plot all age vs wage data</span><br><span class="line">plot(training$age,training$wage,pch=19,cex=0.5)</span><br><span class="line"># plot the fitted polynomial function</span><br><span class="line">points(training$age,predict(lm1,newdata=training),col=&quot;red&quot;,pch=19,cex=0.5)</span><br><span class="line"># predict on test values</span><br><span class="line">head(predict(bsBasis,age=testing$age))</span><br></pre></td></tr></table></figure>
<h3 id="multicore-parallel-processing">Multicore Parallel Processing</h3>
<ul>
<li>many of the algorithms in the <code>caret</code> package are computationally intensive</li>
<li>since most of the modern machines have multiple cores on their CPUs, it is often wise to enable <strong><em>multicore parallel processing</em></strong> to expedite the computations</li>
<li><code>doMC</code> package is recommended to be used for <code>caret</code> computations (<a href="http://topepo.github.io/caret/parallel.html" target="_blank" rel="noopener">reference</a>)
<ul>
<li><code>doMC::registerDoMC(cores=4)</code> = registers 4 cores for R to utilize</li>
<li>the number of cores you should specify depends on the CPU on your computer (system information usually contains the number of cores)
<ul>
<li>it’s also possible to find the number of cores by directly searching for your CPU model number on the Internet</li>
</ul></li>
<li><em><strong>Note</strong>: once registered, you should see in your task manager/activity monitor that 4 “R Session” appear when you run your code </em></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="preprocessing-with-principal-component-analysis-pca">Preprocessing with Principal Component Analysis (PCA)</h2>
<ul>
<li>constructing a prediction model may not require every predictor</li>
<li>ideally we want to capture the <strong><em>most variation</em></strong> with the <strong><em>least</em></strong> amount of variables
<ul>
<li>weighted combination of predictors may improve fit</li>
<li>combination needs to capture the <em>most information</em></li>
</ul></li>
<li>PCA is suited to do this and will help reduce number of predictors as well as reduce noise (due to averaging)
<ul>
<li>statistical goal = find new set of multivariate variables that are <em>uncorrelated</em> and explain as much variance as possible</li>
<li>data compression goal = find the best matrix created with fewer variables that explains the original data</li>
<li>PCA is most useful for linear-type models (GLM, LDA)</li>
<li>generally more difficult to interpret the predictors (complex weighted sums of variables)</li>
<li><em><strong>Note</strong>: outliers are can be detrimental to PCA as they may represent a lot of variation in data </em>
<ul>
<li>exploratory analysis (plots/tables) should be used to identify problems with the predictors</li>
<li>transformations with log/BoxCox may be helpful</li>
</ul></li>
</ul></li>
</ul>
<h3 id="prcomp-function"><code>prcomp</code> Function</h3>
<ul>
<li><code>pr&lt;-prcomp(data)</code> = performs PCA on all variables and returns a <code>prcomp</code> object that contains information about standard deviations and rotations
<ul>
<li><code>pr$rotations</code> = returns eigenvectors for the linear combinations of all variables (coefficients that variables are multiplied by to come up with the principal components) <span class="math inline">\(\rightarrow\)</span> how the principal components are created</li>
<li>often times, it is useful to take the <code>log</code> transformation of the variables and adding 1 before performing PCA
<ul>
<li>helps to reduce skewness or strange distribution in data</li>
<li>log(0) = - infinity, so we add 1 to account for zero values</li>
<li>makes data more Gaussian</li>
</ul></li>
<li><code>plot(pr)</code> = plots the percent variation explained by the first 10 principal components (PC)
<ul>
<li>can be used to find the PCs that represent the most variation</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># load  spam data</span><br><span class="line">data(spam)</span><br><span class="line"># perform PCA on dataset</span><br><span class="line">prComp &lt;- prcomp(log10(spam[,-58]+1))</span><br><span class="line"># print out the eigenvector/rotations first 5 rows and PCs</span><br><span class="line">head(prComp$rotation[, 1:5], 5)</span><br><span class="line"># create new variable that marks spam as 2 and nospam as 1</span><br><span class="line">typeColor &lt;- ((spam$type==&quot;spam&quot;)*1 + 1)</span><br><span class="line"># plot the first two principal components</span><br><span class="line">plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab=&quot;PC1&quot;,ylab=&quot;PC2&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="caret-package"><code>caret</code> Package</h3>
<ul>
<li><code>pp&lt;-preProcess(log10(training[,-58]+1),method=&quot;pca&quot;,pcaComp=2,thresh=0.8))</code> = perform PCA with <code>preProcess</code> function and returns the number of principal components that can capture the majority of the variation
<ul>
<li>creates a <code>preProcess</code> object that can be applied using <code>predict</code> function</li>
<li><code>pcaComp=2</code> = specifies the number of principal components to compute (2 in this case)</li>
<li><code>thresh=0.8</code> = threshold for variation captured by principal components
<ul>
<li><code>thresh=0.95</code> = default value, which returns the number of principal components that are needed to capture 95% of the variation in data</li>
</ul></li>
</ul></li>
<li><code>predict(pp, training)</code> = computes new variables for the PCs (2 in this case) for the training data set
<ul>
<li>the results from <code>predict</code> can then be used as data for the prediction model</li>
<li><em><strong>Note</strong>: the same PCA must be performed on the test set </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># create train and test sets</span><br><span class="line">inTrain &lt;- createDataPartition(y=spam$type,p=0.75, list=FALSE)</span><br><span class="line">training &lt;- spam[inTrain,]</span><br><span class="line">testing &lt;- spam[-inTrain,]</span><br><span class="line"># create preprocess object</span><br><span class="line">preProc &lt;- preProcess(log10(training[,-58]+1),method=&quot;pca&quot;,pcaComp=2)</span><br><span class="line"># calculate PCs for training data</span><br><span class="line">trainPC &lt;- predict(preProc,log10(training[,-58]+1))</span><br><span class="line"># run model on outcome and principle components</span><br><span class="line">modelFit &lt;- train(training$type ~ .,method=&quot;glm&quot;,data=trainPC)</span><br><span class="line"># calculate PCs for test data</span><br><span class="line">testPC &lt;- predict(preProc,log10(testing[,-58]+1))</span><br><span class="line"># compare results</span><br><span class="line">confusionMatrix(testing$type,predict(modelFit,testPC))</span><br></pre></td></tr></table></figure>
<ul>
<li>alternatively, PCA can be directly performed with the <code>train</code> method
<ul>
<li><code>train(outcome ~ ., method=&quot;glm&quot;, preProcess=&quot;pca&quot;, data=training)</code> = performs PCA first on the training set and then runs the specified model
<ul>
<li>effectively the same procedures as above (<code>preProcess</code> <span class="math inline">\(\rightarrow\)</span> <code>predict</code>)</li>
</ul></li>
</ul></li>
<li><em><strong>Note</strong>: in both cases, the PCs were able to achieve 90+% accuracy </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># construct model</span><br><span class="line">modelFit &lt;- train(training$type ~ .,method=&quot;glm&quot;,preProcess=&quot;pca&quot;,data=training)</span><br><span class="line"># print results of model</span><br><span class="line">confusionMatrix(testing$type,predict(modelFit,testing))</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="predicting-with-regression">Predicting with Regression</h2>
<ul>
<li><strong>prediction with regression</strong> = fitting regression model (line) to data <span class="math inline">\(\rightarrow\)</span> multiplying each variable by coefficients to predict outcome</li>
<li>useful when the relationship between the variables can be modeled as linear</li>
<li>the model is easy to implement and the coefficients are easy to interpret</li>
<li>if the relationships are non-linear, the regression model may produce poor results/accuracy
<ul>
<li><em><strong>Note</strong>: linear regressions are generally used in combination with other models </em></li>
</ul></li>
<li><strong>model</strong> <span class="math display">\[Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots + \beta_p X_{pi} + e_i \]</span>
<ul>
<li>where <span class="math inline">\(\beta_0\)</span> is the intercept (when all variables are 0)</li>
<li><span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span> are the coefficients</li>
<li><span class="math inline">\(X_{1i}, X_{2i}, \ldots, X_{pi}\)</span> are the variables/covariates</li>
<li><span class="math inline">\(e_i\)</span> is the error</li>
<li><span class="math inline">\(Y_i\)</span> is the outcome</li>
</ul></li>
<li><strong>prediction</strong> <span class="math display">\[\hat Y_i = \hat \beta_0 + \hat \beta_1 X_{1i} + \hat \beta_2 X_{2i} + \ldots + \hat \beta_p X_{pi}\]</span>
<ul>
<li>where <span class="math inline">\(\hat \beta_0\)</span> is the estimated intercept (when all variables are 0)</li>
<li><span class="math inline">\(\hat \beta_1, \hat \beta_2, \ldots, \hat \beta_p\)</span> are the estimated coefficients</li>
<li><span class="math inline">\(X_{1i}, X_{2i}, \ldots, X_{pi}\)</span> are the variables/covariates</li>
<li><span class="math inline">\(\hat Y_i\)</span> is the <strong><em>predicted outcome</em></strong></li>
</ul></li>
</ul>
<h3 id="r-commands-and-examples">R Commands and Examples</h3>
<ul>
<li><code>lm&lt;-lm(y ~ x, data=train)</code> = runs a linear model of outcome y on predictor x <span class="math inline">\(\rightarrow\)</span> univariate regression
<ul>
<li><code>summary(lm)</code> = returns summary of the linear regression model, which will include coefficients, standard errors, <span class="math inline">\(t\)</span> statistics, and p values</li>
<li><code>lm(y ~ x1+x2+x3, data=train)</code> = run linear model of outcome y on predictors x1, x2, and x3</li>
<li><code>lm(y ~ ., data=train</code> = run linear model of outcome y on all predictors</li>
</ul></li>
<li><code>predict(lm, newdata=df)</code> = use the constructed linear model to predict outcomes (<span class="math inline">\(\hat Y_i\)</span>) for the new values
<ul>
<li><code>newdata</code> data frame must have the same variables (factors must have the same levels) as the training data</li>
<li><code>newdata=test</code> = predict outcomes for the test set based on linear regression model from the training</li>
<li><em><strong>Note</strong>: the regression line will not be a perfect fit on the test set since it was constructed on the training set </em></li>
</ul></li>
<li>RSME can be calculated to measure the accuracy of the linear model
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(RSME_{test}\)</span>, which estimates the out-of-sample error, is almost always <strong>GREATER</strong> than <span class="math inline">\(RSME_{train}\)</span> </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">data(faithful)</span><br><span class="line"># create train and test sets</span><br><span class="line">inTrain &lt;- createDataPartition(y=faithful$waiting, p=0.5, list=FALSE)</span><br><span class="line">trainFaith &lt;- faithful[inTrain,]; testFaith &lt;- faithful[-inTrain,]</span><br><span class="line"># build linear model</span><br><span class="line">lm1 &lt;- lm(eruptions ~ waiting,data=trainFaith)</span><br><span class="line"># print summary of linear model</span><br><span class="line">summary(lm1)</span><br><span class="line"># predict eruptions for new waiting time</span><br><span class="line">newdata &lt;- data.frame(waiting=80)</span><br><span class="line">predict(lm1,newdata)</span><br><span class="line"># create 1 x 2 panel plot</span><br><span class="line">par(mfrow=c(1,2))</span><br><span class="line"># plot train data with the regression line</span><br><span class="line">plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col=&quot;blue&quot;,xlab=&quot;Waiting&quot;,</span><br><span class="line">	ylab=&quot;Duration&quot;, main = &quot;Train&quot;)</span><br><span class="line">lines(trainFaith$waiting,predict(lm1),lwd=3)</span><br><span class="line"># plot test data with the regression line</span><br><span class="line">plot(testFaith$waiting,testFaith$eruptions,pch=19,col=&quot;blue&quot;,xlab=&quot;Waiting&quot;,</span><br><span class="line">	ylab=&quot;Duration&quot;, main = &quot;Test&quot;)</span><br><span class="line">lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=3)</span><br><span class="line"># Calculate RMSE on training and test sets</span><br><span class="line">c(trainRMSE = sqrt(sum((lm1$fitted-trainFaith$eruptions)^2)),</span><br><span class="line">	testRMSE = sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2)))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>pi&lt;-predict(lm, newdata=test, interval=&quot;prediction&quot;)</code> = returns 3 columns for <code>fit</code> (predicted value, same as before), <code>lwr</code> (lower bound of prediction interval), and <code>upr</code> (upper bound of prediction interval)
<ul>
<li><code>matlines(x, pi, type=&quot;l&quot;)</code> = plots three lines, one for the linear fit and two for upper/lower prediction interval bounds</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># calculate prediction interval</span><br><span class="line">pred1 &lt;- predict(lm1,newdata=testFaith,interval=&quot;prediction&quot;)</span><br><span class="line"># plot data points (eruptions, waiting)</span><br><span class="line">plot(testFaith$waiting,testFaith$eruptions,pch=19,col=&quot;blue&quot;)</span><br><span class="line"># plot fit line and prediction interval</span><br><span class="line">matlines(testFaith$waiting,pred1,type=&quot;l&quot;,,col=c(1,2,2),lty = c(1,1,1), lwd=3)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>lm &lt;- train(y ~ x, method=&quot;lm&quot;, data=train)</code> = run linear model on the training data <span class="math inline">\(\rightarrow\)</span> identical to <code>lm</code> function
<ul>
<li><code>summary(lm$finalModel)</code> = returns summary of the linear regression model, which will include coefficients, standard errors, <span class="math inline">\(t\)</span> statistics, and p values <span class="math inline">\(\rightarrow\)</span> identical to <code>summary(lm)</code> for a <code>lm</code> object</li>
<li><code>train(y ~ ., method=&quot;lm&quot;, data=train)</code> = run linear model on all predictors in training data
<ul>
<li>multiple predictors (dummy/indicator variables) are created for factor variables</li>
</ul></li>
<li><code>plot(lm$finalModel)</code> = construct 4 diagnostic plots for evaluating the model
<ul>
<li><em><strong>Note</strong>: more information on these plots can be found at <code>?plot.lm</code> </em></li>
<li><strong><em>Residuals vs Fitted</em></strong></li>
<li><strong><em>Normal Q-Q</em></strong></li>
<li><strong><em>Scale-Location</em></strong></li>
<li><strong><em>Residuals vs Leverage</em></strong></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.align </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># create train and test sets</span><br><span class="line">inTrain &lt;- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)</span><br><span class="line">training &lt;- Wage[inTrain,]; testing &lt;- Wage[-inTrain,]</span><br><span class="line"># fit linear model for age jobclass and education</span><br><span class="line">modFit&lt;- train(wage ~ age + jobclass + education,method = &quot;lm&quot;,data=training)</span><br><span class="line"># store final model</span><br><span class="line">finMod &lt;- modFit$finalModel</span><br><span class="line"># set up 2 x 2 panel plot</span><br><span class="line">par(mfrow = c(2, 2))</span><br><span class="line"># construct diagnostic plots for model</span><br><span class="line">plot(finMod,pch=19,cex=0.5,col=&quot;#00000010&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>plotting residuals by fitted values and coloring with a variable not used in the model helps spot a trend in that variable.</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot fitted values by residuals </span><br><span class="line">qplot(finMod$fitted, finMod$residuals, color=race, data=training)</span><br></pre></td></tr></table></figure>
<ul>
<li>plotting residuals by index (ie; row numbers) can be helpful in showing missing variables
<ul>
<li><code>plot(finMod$residuals)</code> = plot the residuals against index (row number)</li>
<li>if there’s a trend/pattern in the residuals, it is highly likely that another variable (such as age/time) should be included.
<ul>
<li>residuals should not have relationship to index</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot residual by index</span><br><span class="line">plot(finMod$residuals,pch=19,cex=0.5)</span><br></pre></td></tr></table></figure>
<ul>
<li>here the residuals increase linearly with the index, and the highest residuals are concentrated in the higher indexes, so there must be a missing variable</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="prediction-with-trees">Prediction with Trees</h2>
<ul>
<li><strong>prediction with trees</strong> = iteratively split variables into groups (effectively constructing decision trees) <span class="math inline">\(\rightarrow\)</span> produces nonlinear model
<ul>
<li>the classification tree uses interactions between variables <span class="math inline">\(\rightarrow\)</span> the ultimate groups/leafs may depend on many variables</li>
</ul></li>
<li>the result (tree) is easy to interpret, and generally performs better predictions than regression models when the relationships are <strong><em>non-linear</em></strong></li>
<li>transformations less important <span class="math inline">\(\rightarrow\)</span> monotone transformations (order unchanged, such as <span class="math inline">\(\log\)</span>) will produce same splits</li>
<li>trees can be used for regression problems as well and use RMSE as <em>measure of impurity</em></li>
<li>however, without proper cross-validation, the model can be <strong><em>over-fitted</em></strong> (especially with large number of variables) and results may be variable from one run to the next
<ul>
<li>it is also harder to estimate the uncertainty of the model</li>
</ul></li>
<li><code>party</code>, <code>rpart</code>, <code>tree</code> packages can all build trees</li>
</ul>
<h3 id="process">Process</h3>
<ol type="1">
<li>start with all variables in one group</li>
<li>find the variable that best splits the outcomes into two groups</li>
<li>divide data into two groups (<em>leaves</em>) based on the split performed (<em>node</em>)</li>
<li>within each split, find variables to split the groups again</li>
<li>continue this process until all groups are sufficiently small/homogeneous/“pure”</li>
</ol>
<h3 id="measures-of-impurity-reference">Measures of Impurity (<a href="http://en.wikipedia.org/wiki/Decision_tree_learning" target="_blank" rel="noopener">Reference</a>)</h3>
<p><span class="math display">\[\hat{p}_{mk} = \frac{\sum_{i}^m \mathbb{1}(y_i = k)}{N_m}\]</span></p>
<ul>
<li><span class="math inline">\(\hat p_mk\)</span> is the probability of the objects in group <span class="math inline">\(m\)</span> to take on the classification <span class="math inline">\(k\)</span></li>
<li><p><span class="math inline">\(N_m\)</span> is the size of the group</p></li>
<li><strong>Misclassification Error</strong> <span class="math display">\[ 1 - \hat{p}_{m~k(m)}\]</span> where <span class="math inline">\(k(m)\)</span> is the most common classification/group
<ul>
<li>0 = perfect purity</li>
<li>0.5 = no purity
<ul>
<li><em><strong>Note</strong>: it is not 1 here because when <span class="math inline">\(\hat{p}_{m~k(m)} &lt; 0.5\)</span> or there’s not predominant classification for the objects, it means the group should be further subdivided until there’s a majority </em></li>
</ul></li>
</ul></li>
<li><strong>Gini Index</strong><span class="math display">\[ \sum_{k \neq k&#39;} \hat{p}_{mk} \times \hat{p}_{mk&#39;} = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}) = 1 - \sum_{k=1}^K p_{mk}^2\]</span>
<ul>
<li>0 = perfect purity</li>
<li>0.5 = no purity</li>
</ul></li>
<li><strong>Deviance</strong> <span class="math display">\[ -\sum_{k=1}^K \hat{p}_{mk} \log_e\hat{p}_{mk} \]</span>
<ul>
<li>0 = perfect purity</li>
<li>1 = no purity</li>
</ul></li>
<li><strong>Information Gai</strong> <span class="math display">\[ -\sum_{k=1}^K \hat{p}_{mk} \log_2\hat{p}_{mk} \]</span>
<ul>
<li>0 = perfect purity</li>
<li>1 = no purity</li>
</ul></li>
<li><p><strong><em>example</em></strong></p></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># set margin and seed</span><br><span class="line">par(mar=c(1,1,1,1), mfrow = c(1, 2)); set.seed(1234);</span><br><span class="line"># simulate data</span><br><span class="line">x = rep(1:4,each=4); y = rep(1:4,4)</span><br><span class="line"># plot first scenario</span><br><span class="line">plot(x,y,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,cex=3,col=c(rep(&quot;blue&quot;,15),rep(&quot;red&quot;,1)),pch=19)</span><br><span class="line"># plot second scenario</span><br><span class="line">plot(x,y,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,cex=3,col=c(rep(&quot;blue&quot;,8),rep(&quot;red&quot;,8)),pch=19)</span><br></pre></td></tr></table></figure>
<ul>
<li>left graph
<ul>
<li><strong>Misclassification:</strong> <span class="math inline">\(\frac{1}{16} = 0.06\)</span></li>
<li><strong>Gini:</strong> <span class="math inline">\(1 - [(\frac{1}{16})^2 + (\frac{15}{16})^2] = 0.12\)</span></li>
<li><strong>Information:</strong><span class="math inline">\(-[\frac{1}{16} \times \log_2 (\frac{1}{16})+ \frac{15}{16} \times \log_2(\frac{1}{16})] = 0.34\)</span></li>
</ul></li>
<li>right graph
<ul>
<li><strong>Misclassification:</strong> <span class="math inline">\(\frac{8}{16} = 0.5\)</span></li>
<li><strong>Gini:</strong> <span class="math inline">\(1 - [(\frac{8}{16})^2 + (\frac{8}{16})^2] = 0.5\)</span></li>
<li><strong>Information:</strong><span class="math inline">\(-[\frac{8}{16} \times \log_2 (\frac{8}{16})+ \frac{8}{16} \times \log_2(\frac{8}{16})] = 1\)</span></li>
</ul></li>
</ul>
<h3 id="constructing-trees-with-caret-package">Constructing Trees with <code>caret</code> Package</h3>
<ul>
<li><code>tree&lt;-train(y ~ ., data=train, method=&quot;rpart&quot;)</code> = constructs trees based on the outcome and predictors
<ul>
<li>produces an <code>rpart</code> object, which can be used to <code>predict</code> new/test values</li>
<li><code>print(tree$finalModel)</code> = returns text summary of all nodes/splits in the tree constructed</li>
</ul></li>
<li><code>plot(tree$finalModel, uniform=TRUE)</code> = plots the classification tree with all nodes/splits
<ul>
<li>[<code>rattle</code> package] <code>fancyRpartPlot(tree$finalModel)</code> = produces more readable, better formatted classification tree diagrams</li>
<li>each split will have the condition/node in bold and the splits/leafs on the left and right sides following the “yes” or “no” indicators
<ul>
<li>“yes” <span class="math inline">\(\rightarrow\)</span> go left</li>
<li>“no” <span class="math inline">\(\rightarrow\)</span> go right</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># load iris data set</span><br><span class="line">data(iris)</span><br><span class="line"># create test/train data sets</span><br><span class="line">inTrain &lt;- createDataPartition(y=iris$Species,p=0.7, list=FALSE)</span><br><span class="line">training &lt;- iris[inTrain,]</span><br><span class="line">testing &lt;- iris[-inTrain,]</span><br><span class="line"># fit classification tree as a model</span><br><span class="line">modFit &lt;- train(Species ~ .,method=&quot;rpart&quot;,data=training)</span><br><span class="line"># print the classification tree</span><br><span class="line">print(modFit$finalModel)</span><br><span class="line"># plot the classification tree</span><br><span class="line">rattle::fancyRpartPlot(modFit$finalModel)</span><br><span class="line"># predict on test values</span><br><span class="line">predict(modFit,newdata=testing)</span><br></pre></td></tr></table></figure>
<h2 id="bagging">Bagging</h2>
<ul>
<li><strong>bagging</strong> = bootstrap aggregating
<ul>
<li>resample training data set (with replacement) and recalculate predictions</li>
<li>average the predictions together or majority vote</li>
<li>more information can be found <a href="http://stat.ethz.ch/education/semesters/FS_2008/CompStat/sk-ch8.pdf" target="_blank" rel="noopener">here</a></li>
</ul></li>
<li>averaging multiple complex models have <strong><em>similar bias</em></strong> as each of the models on its own, and <strong><em>reduced variance</em></strong> because of the average</li>
<li><p>most useful for non-linear models</p></li>
<li><strong><em>example</em></strong>
<ul>
<li><code>loess(y ~ x, data=train, span=0.2)</code> = fits a smooth curve to data
<ul>
<li><code>span=0.2</code> = controls how smooth the curve should be</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">library(ElemStatLearn); data(ozone,package=&quot;ElemStatLearn&quot;)</span><br><span class="line"># reorder rows based on ozone variable</span><br><span class="line">ozone &lt;- ozone[order(ozone$ozone),]</span><br><span class="line"># create empty matrix</span><br><span class="line">ll &lt;- matrix(NA,nrow=10,ncol=155)</span><br><span class="line"># iterate 10 times</span><br><span class="line">for(i in 1:10)&#123;</span><br><span class="line">	# create sample from data with replacement</span><br><span class="line">	ss &lt;- sample(1:dim(ozone)[1],replace=T)</span><br><span class="line">	# draw sample from the dataa and reorder rows based on ozone</span><br><span class="line">	ozone0 &lt;- ozone[ss,]; ozone0 &lt;- ozone0[order(ozone0$ozone),]</span><br><span class="line">	# fit loess function through data (similar to spline)</span><br><span class="line">	loess0 &lt;- loess(temperature ~ ozone,data=ozone0,span=0.2)</span><br><span class="line">	# prediction from loess curve for the same values each time</span><br><span class="line">	ll[i,] &lt;- predict(loess0,newdata=data.frame(ozone=1:155))</span><br><span class="line">&#125;</span><br><span class="line"># plot the data points</span><br><span class="line">plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)</span><br><span class="line"># plot each prediction model</span><br><span class="line">for(i in 1:10)&#123;lines(1:155,ll[i,],col=&quot;grey&quot;,lwd=2)&#125;</span><br><span class="line"># plot the average in red</span><br><span class="line">lines(1:155,apply(ll,2,mean),col=&quot;red&quot;,lwd=2)</span><br></pre></td></tr></table></figure>
<h3 id="bagging-algorithms">Bagging Algorithms</h3>
<ul>
<li>in the <code>caret</code> package, there are three options for the <code>train</code> function to perform bagging
<ul>
<li><code>bagEarth</code> - Bagged MARS (<a href="http://www.inside-r.org/packages/cran/caret/docs/bagEarth" target="_blank" rel="noopener">documentation</a>)</li>
<li><code>treebag</code> - Bagged CART (<a href="http://www.inside-r.org/packages/cran/ipred/docs/bagging" target="_blank" rel="noopener">documentation</a>)</li>
<li><code>bagFDA</code> - Bagged Flexible Discriminant Analysis (<a href="http://www.inside-r.org/packages/cran/caret/docs/bagFDA" target="_blank" rel="noopener">documentation</a>)</li>
</ul></li>
<li>alternatively, custom <code>bag</code> functions can be constructed (<a href="http://www.inside-r.org/packages/cran/caret/docs/nbBag" target="_blank" rel="noopener">documentation</a>)
<ul>
<li><code>bag(predictors, outcome, B=10, bagControl(fit, predict, aggregate))</code> = define and execute custom bagging algorithm
<ul>
<li><code>B=10</code> = iterations/resampling to perform</li>
<li><code>bagControl()</code> = controls for how the bagging should be executed
<ul>
<li><code>fit=ctreeBag$fit</code> = the model ran on each resampling of data</li>
<li><code>predict=ctreeBag$predict</code> = how predictions should be calculated from each model</li>
<li><code>aggregate=ctreeBag$aggregate</code> = how the prediction models should be combined/averaged</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># load relevant package and data</span><br><span class="line">library(party); data(ozone,package=&quot;ElemStatLearn&quot;)</span><br><span class="line"># reorder rows based on ozone variable</span><br><span class="line">ozone &lt;- ozone[order(ozone$ozone),]</span><br><span class="line"># extract predictors</span><br><span class="line">predictors &lt;- data.frame(ozone=ozone$ozone)</span><br><span class="line"># extract outcome</span><br><span class="line">temperature &lt;- ozone$temperature</span><br><span class="line"># run bagging algorithm</span><br><span class="line">treebag &lt;- bag(predictors, temperature, B = 10,</span><br><span class="line">				# custom bagging function</span><br><span class="line">                bagControl = bagControl(fit = ctreeBag$fit,</span><br><span class="line">                                        predict = ctreeBag$pred,</span><br><span class="line">                                        aggregate = ctreeBag$aggregate))</span><br><span class="line"># plot data points</span><br><span class="line">plot(ozone$ozone,temperature,col=&apos;lightgrey&apos;,pch=19)</span><br><span class="line"># plot the first fit</span><br><span class="line">points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col=&quot;red&quot;)</span><br><span class="line"># plot the aggregated predictions</span><br><span class="line">points(ozone$ozone,predict(treebag,predictors),pch=19,col=&quot;blue&quot;)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="random-forest">Random Forest</h2>
<ul>
<li><strong>random forest</strong> = extension of bagging on classification/regression trees
<ul>
<li>one of the most used/accurate algorithms along with boosting</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/14.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>process</strong>
<ul>
<li>bootstrap samples from training data (with replacement)</li>
<li>split and bootstrap variables</li>
<li>grow trees (repeat split/bootstrap) and vote/average final trees</li>
</ul></li>
<li><strong>drawbacks</strong>
<ul>
<li>algorithm can be slow (process large number of trees)</li>
<li>hard to interpret (large numbers of splits and nodes)</li>
<li>over-fitting (difficult to know which tree is causing over-fitting)</li>
<li><em><strong>Note</strong>: it is extremely important to use cross validation when running random forest algorithms </em></li>
</ul></li>
</ul>
<h3 id="r-commands-and-examples-1">R Commands and Examples</h3>
<ul>
<li><code>rf&lt;-train(outcome ~ ., data=train, method=&quot;rf&quot;, prox=TRUE, ntree=500)</code> = runs random forest algorithm on the training data against all predictors
<ul>
<li><em><strong>Note</strong>: random forest algorithm automatically bootstrap by default, but it is still important to have train/test/validation split to verify the accuracy of the model </em></li>
<li><code>prox=TRUE</code> = the proximity measures between observations should be calculated (used in functions such as <code>classCenter()</code> to find center of groups)
<ul>
<li><code>rf$finalModel$prox</code> = returns matrix of proximities</li>
</ul></li>
<li><code>ntree=500</code> = specify number of trees that should be constructed</li>
<li><code>do.trace=TRUE</code> = prints logs as the trees are being built <span class="math inline">\(\rightarrow\)</span> useful by indicating progress to user</li>
<li><em><strong>Note</strong>: <code>randomForest()</code> function can be used to perform random forest algorithm (syntax is the same as <code>train</code>) and is much faster </em></li>
</ul></li>
<li><code>getTree(rf$finalModel, k=2)</code> = return specific tree from random forest model</li>
<li><code>classCenters(predictors, outcome, proximity, nNbr)</code> = return computes the cluster centers using the <code>nNbr</code> nearest neighbors of the observations
<ul>
<li><code>prox = rf$finalModel$prox</code> = proximity matrix from the random forest model</li>
<li><code>nNbr</code> = number of nearest neighbors that should be used to compute cluster centers</li>
</ul></li>
<li><code>predict(rf, test)</code> = apply the random forest model to test data set
<ul>
<li><code>confusionMatrix(predictions, actualOutcome)</code> = tabulates the predictions of the model against the truths
<ul>
<li><em><strong>Note</strong>: this is generally done for the validation data set using the model built from training </em></li>
</ul></li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">data(iris)</span><br><span class="line"># create train/test data sets</span><br><span class="line">inTrain &lt;- createDataPartition(y=iris$Species,p=0.7, list=FALSE)</span><br><span class="line">training &lt;- iris[inTrain,]</span><br><span class="line">testing &lt;- iris[-inTrain,]</span><br><span class="line"># apply random forest</span><br><span class="line">modFit &lt;- train(Species~ .,data=training,method=&quot;rf&quot;,prox=TRUE)</span><br><span class="line"># return the second tree (first 6 rows)</span><br><span class="line">head(getTree(modFit$finalModel,k=2))</span><br><span class="line"># compute cluster centers</span><br><span class="line">irisP &lt;- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)</span><br><span class="line"># convert irisP to data frame and add Species column</span><br><span class="line">irisP &lt;- as.data.frame(irisP); irisP$Species &lt;- rownames(irisP)</span><br><span class="line"># plot data points</span><br><span class="line">p &lt;- qplot(Petal.Width, Petal.Length, col=Species,data=training)</span><br><span class="line"># add the cluster centers</span><br><span class="line">p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)</span><br><span class="line"># predict outcome for test data set using the random forest model</span><br><span class="line">pred &lt;- predict(modFit,testing)</span><br><span class="line"># logic value for whether or not the rf algorithm predicted correctly</span><br><span class="line">testing$predRight &lt;- pred==testing$Species</span><br><span class="line"># tabulate results</span><br><span class="line">table(pred,testing$Species)</span><br><span class="line"># plot data points with the incorrect classification highlighted</span><br><span class="line">qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main=&quot;newdata Predictions&quot;)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="boosting">Boosting</h2>
<ul>
<li><strong>boosting</strong> = one of the most widely used and accurate prediction models, along with random forest</li>
<li>boosting can be done with any set of classifiers, and a well-known approach is <a href="http://en.wikipedia.org/wiki/Gradient_boosting" target="_blank" rel="noopener">gradient boosting</a></li>
<li><p>more detail tutorial can be found <a href="http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf" target="_blank" rel="noopener">here</a></p></li>
<li><strong>process</strong>: take a group of weak predictors <span class="math inline">\(\rightarrow\)</span> weight them and add them up <span class="math inline">\(\rightarrow\)</span> result in a stronger predictor
<ul>
<li>start with a set of classifiers <span class="math inline">\(h_1, \ldots, h_k\)</span>
<ul>
<li>examples: all possible trees, all possible regression models, all possible cutoffs (divide data into different parts)</li>
</ul></li>
<li>calculate a weighted sum of classifiers as the prediction value <span class="math display">\[f(x) = \sum_i \alpha_i h_i(x)\]</span> where <span class="math inline">\(\alpha_i\)</span> = coefficient/weight and <span class="math inline">\(h_i(x)\)</span> = value of classifier
<ul>
<li>goal = minimize error (on training set)</li>
<li>select one <span class="math inline">\(h\)</span> at each step (iterative)</li>
<li>calculate weights based on errors</li>
<li>up-weight missed classifications and select next <span class="math inline">\(h\)</span></li>
</ul></li>
</ul></li>
<li><p><strong><em>example</em></strong></p></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/15.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>we start with space with <strong>blue +</strong> and <strong>red -</strong> and the goal is to classify all the object correctly</li>
<li>only straight lines will be used for classification</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/16.png&quot;))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/17.png&quot;))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/18.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>from the above, we can see that a group of weak predictors (lines in this case), can be combined and weighed to become a much stronger predictor</li>
</ul>
<h3 id="r-commands-and-examples-2">R Commands and Examples</h3>
<ul>
<li><code>gbm &lt;- train(outcome ~ variables, method=&quot;gbm&quot;, data=train, verbose=F)</code> = run boosting model on the given data
<ul>
<li>options for <code>method</code> for boosting
<ul>
<li><a href="http://cran.r-project.org/web/packages/gbm/index.html" target="_blank" rel="noopener"><code>gbm</code></a> - boosting with trees</li>
<li><a href="http://cran.r-project.org/web/packages/mboost/index.html" target="_blank" rel="noopener"><code>mboost</code></a> - model based boosting</li>
<li><a href="http://cran.r-project.org/web/packages/ada/index.html" target="_blank" rel="noopener"><code>ada</code></a> - statistical boosting based on <a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1016218223" target="_blank" rel="noopener">additive logistic regression</a></li>
<li><a href="http://cran.r-project.org/web/packages/GAMBoost/index.html" target="_blank" rel="noopener"><code>gamBoost</code></a> for boosting generalized additive models</li>
<li><em><strong>Note</strong>: differences between packages include the choice of basic classification functions and combination rules </em></li>
</ul></li>
</ul></li>
<li><p><code>predict</code> function can be used to apply the model to test data, similar to the rest of the algorithms in <code>caret</code> package</p></li>
<li><p><strong><em>example</em></strong></p></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">data(Wage)</span><br><span class="line"># remove log wage variable (we are trying to predict wage)</span><br><span class="line">Wage &lt;- subset(Wage,select=-c(logwage))</span><br><span class="line"># create train/test data sets</span><br><span class="line">inTrain &lt;- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)</span><br><span class="line">training &lt;- Wage[inTrain,]; testing &lt;- Wage[-inTrain,]</span><br><span class="line"># run the gbm model</span><br><span class="line">modFit &lt;- train(wage ~ ., method=&quot;gbm&quot;,data=training,verbose=FALSE)</span><br><span class="line"># print model summary</span><br><span class="line">print(modFit)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="model-based-prediction">Model Based Prediction</h2>
<ul>
<li><strong>model based prediction</strong> = assumes the data follow a probabilistic model/distribution and use <em>Bayes’ theorem</em> to identify optimal classifiers/variables
<ul>
<li>can potentially take advantage of structure of the data</li>
<li>could help reduce computational complexity (reduce variables)</li>
<li>can be reasonably accurate on real problems</li>
</ul></li>
<li>this approach does make <em><strong>additional assumptions</strong></em> about the data, which can lead to model failure/reduced accuracy if they are too far off</li>
<li><strong><em>goal</em></strong> = build parameter-based model (based on probabilities) for conditional distribution <span class="math inline">\(P(Y = k~|~X = x)\)</span>, or the probability of the outcome <span class="math inline">\(Y\)</span> is equal to a particular value <span class="math inline">\(k\)</span> given a specific set of predictor variables <span class="math inline">\(x\)</span>
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(X\)</span> is the data for the model (observations for all predictor variables), which is also known as the <strong>design matrix</strong> </em></li>
</ul></li>
<li><strong>typical approach/process</strong>
<ol type="1">
<li>start with the quantity <span class="math inline">\(P(Y = k~|~X = x)\)</span></li>
<li>apply <em>Bayes’ Theorem</em> such that <span class="math display">\[ P(Y = k ~|~ X=x) = \frac{P(X=x~|~Y=k)P(Y=k)}{\sum_{\ell=1}^K P(X=x ~|~Y = \ell) P(Y=\ell)}\]</span> where the denominator is simply the sum of probabilities for the predictor variables are the set specified in <span class="math inline">\(x\)</span> for all outcomes of <span class="math inline">\(Y\)</span></li>
<li>assume the term <span class="math inline">\(P(X=x~|~Y=k)\)</span> in the numerator follows a parameter-based probability distribution, or <span class="math inline">\(f_k(x)\)</span>
<ul>
<li>common choice = <strong><em>Gaussian distribution</em></strong> <span class="math display">\[f_k(x) = \frac{1}{\sigma_k \sqrt{2 \pi}}e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}}\]</span></li>
</ul></li>
<li>assume the probability for the outcome <span class="math inline">\(Y\)</span> to take on value of <span class="math inline">\(k\)</span>, or <span class="math inline">\(P(Y=k)\)</span>, is determined from the data to be some known quantity <span class="math inline">\(\pi_k\)</span>
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(P(Y=k)\)</span> is known as the <a href="http://en.wikipedia.org/wiki/Prior_probability" target="_blank" rel="noopener"><strong>prior probability</strong></a> </em></li>
</ul></li>
<li>so the quantity <span class="math inline">\(P(Y = k~|~X = x)\)</span> can be rewritten as <span class="math display">\[P(Y = k ~|~ X=x) = \frac{f_k(x) \pi_k}{\sum_{\ell = 1}^K f_{\ell}(x) \pi_{\ell}}\]</span></li>
<li>estimate the parameters (<span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(\sigma_k^2\)</span>) for the function <span class="math inline">\(f_k(x)\)</span> from the data</li>
<li>calculate <span class="math inline">\(P(Y = k~|~X = x)\)</span> using the parameters</li>
<li>the outcome <span class="math inline">\(Y\)</span> is where the value of <span class="math inline">\(P(Y = k ~|~ X = x)\)</span> is the highest</li>
</ol></li>
<li>prediction models that leverage this approach
<ul>
<li><a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis" target="_blank" rel="noopener"><strong><em>linear discriminant analysis</em></strong></a> = assumes <span class="math inline">\(f_k(x)\)</span> is multivariate Gaussian distribution with <strong>same</strong> covariance for each predictor variables
<ul>
<li>effectively drawing lines through “covariate space”</li>
</ul></li>
<li><a href="http://en.wikipedia.org/wiki/Quadratic_classifier" target="_blank" rel="noopener"><strong><em>quadratic discriminant analysis</em></strong></a> = assumes <span class="math inline">\(f_k(x)\)</span> is multivariate Gaussian distribution with <strong>different</strong> covariance for predictor variables
<ul>
<li>effectively drawing curves through “covariate space”</li>
</ul></li>
<li><a href="http://www.stat.washington.edu/mclust/" target="_blank" rel="noopener"><strong><em>normal mixture modeling</em></strong></a> = assumes more complicated covariance matrix for the predictor variables</li>
<li><a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier" target="_blank" rel="noopener"><strong><em>naive Bayes</em></strong></a> = assumes independence between predictor variables/features for model building (covariance = 0)
<ul>
<li><em><strong>Note</strong>: this may be an incorrect assumption but it helps to reduce computational complexity and may still produce a useful result </em></li>
</ul></li>
</ul></li>
</ul>
<h3 id="linear-discriminant-analysis">Linear Discriminant Analysis</h3>
<ul>
<li>to compare the probability for outcome <span class="math inline">\(Y = k\)</span> versus probability for outcome <span class="math inline">\(Y = k\)</span>, we can look at the ratio of <span class="math display">\[\frac{P(Y=k~|~X=x)}{P(Y=j~|~X=x)}\]</span></li>
<li>take the <strong>log</strong> of the ratio and apply Bayes’ Theorem, we get <span class="math display">\[\log \frac{P(Y = k~|~X=x)}{P(Y = j~|~X=x)} = \log \frac{f_k(x)}{f_j(x)} + \log \frac{\pi_k}{\pi_j}\]</span> which is effectively the log ratio of probability density functions plus the log ratio of prior probabilities
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(\log\)</span> = <strong>monotone</strong> transformation, which means taking the <span class="math inline">\(\log\)</span> of a quantity does not affect implication of the ratio since th <span class="math inline">\(\log(ratio)\)</span> is <em>directly correlated</em> with ratio </em></li>
</ul></li>
<li>if we substitute <span class="math inline">\(f_k(x)\)</span> and <span class="math inline">\(f_l(x)\)</span> with Gaussian probability density functions <span class="math display">\[f(x) = \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span> so the ratio can be simplified to <span class="math display">\[\log \frac{P(Y = k~|~X=x)}{P(Y = j~|~X=x)} = \log \frac{\pi_k}{\pi_j} - \frac{1}{2}(\mu_k + \mu_j)^T \Sigma^{-1}(\mu_k + \mu_j) + x^T \Sigma^{-1} (\mu_k - \mu_j)\]</span> where <span class="math inline">\(\Sigma^{-1}\)</span> = covariance matrix for the predictor variables, <span class="math inline">\(x^T\)</span> = set of predictor variables, and <span class="math inline">\(\mu_k\)</span> / <span class="math inline">\(\mu_j\)</span> = mean of <span class="math inline">\(k\)</span>, <span class="math inline">\(j\)</span> respectively</li>
<li>as annotated above, the log-ratio is effectively an equation of a line for a set of predictor variables <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> the first two terms are constants and the last term is in the form of <span class="math inline">\(X \beta\)</span>
<ul>
<li><em><strong>Note</strong>: the lines are also known as <strong>decision boundaries</strong> </em></li>
</ul></li>
<li>therefore, we can classify values based on <strong><em>which side of the line</em></strong> the value is located (<span class="math inline">\(k\)</span> vs <span class="math inline">\(j\)</span>)</li>
<li><strong>discriminant functions</strong> are used to determine value of <span class="math inline">\(k\)</span>, the functions are in the form of <span class="math display">\[\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2}\mu_k \Sigma^{-1}\mu_k + log(\mu_k)\]</span>
<ul>
<li>plugging in the set of predictor variables, <span class="math inline">\(x^T\)</span>, into the discriminant function, we can find the value of <span class="math inline">\(k\)</span> that <strong><em>maximizes</em></strong> the function <span class="math inline">\(\delta_k(x)\)</span></li>
<li>the terms of the discriminant function can be estimated using maximum likelihood</li>
</ul></li>
<li><p>the predicted value for the outcome is therefore <span class="math inline">\(\hat{Y}(x) = argmax_k \delta_k(x)\)</span></p></li>
<li><strong><em>example</em></strong>
<ul>
<li>classify a group of values into 3 groups using 2 variables (<span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> coordinates)</li>
<li>3 lines are draw to split the data into 3 Gaussian distributions
<ul>
<li>each line splits the data into two groups <span class="math inline">\(\rightarrow\)</span> 1 vs 2, 2 vs 3, 1 vs 3</li>
<li>each side of the line represents a region where the probability of one group (1, 2, or 3) is the highest<br>
</li>
</ul></li>
<li>the result is represented in the the following graph</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/19.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>R Commands</strong>
<ul>
<li><code>lda&lt;-train(outcome ~ predictors, data=training, method=&quot;lda&quot;)</code> = constructs a linear discriminant analysis model on the predictors with the provided training data</li>
<li><code>predict(lda, test)</code> = applies the LDA model to test data and return the prediction results in data frame</li>
<li><strong><em>example: <code>caret</code> package</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">data(iris)</span><br><span class="line"># create training and test sets</span><br><span class="line">inTrain &lt;- createDataPartition(y=iris$Species,p=0.7, list=FALSE)</span><br><span class="line">training &lt;- iris[inTrain,]</span><br><span class="line">testing &lt;- iris[-inTrain,]</span><br><span class="line"># run the linear discriminant analysis on training data</span><br><span class="line">lda &lt;- train(Species ~ .,data=training,method=&quot;lda&quot;)</span><br><span class="line"># predict test outcomes using LDA model</span><br><span class="line">pred.lda &lt;- predict(lda,testing)</span><br><span class="line"># print results</span><br><span class="line">pred.lda</span><br></pre></td></tr></table></figure>
<h3 id="naive-bayes">Naive Bayes</h3>
<ul>
<li>for predictors <span class="math inline">\(X_1,\ldots,X_m\)</span>, we want to model <span class="math inline">\(P(Y = k ~|~ X_1,\ldots,X_m)\)</span></li>
<li>by applying <em>Bayes’ Theorem</em>, we get <span class="math display">\[P(Y = k ~|~ X_1,\ldots,X_m) = \frac{\pi_k P(X_1,\ldots,X_m~|~ Y=k)}{\sum_{\ell = 1}^K P(X_1,\ldots,X_m ~|~ Y=k) \pi_{\ell}}\]</span></li>
<li>since the denominator is just a sum (constant), we can rewrite the quantity as <span class="math display">\[P(Y = k ~|~ X_1,\ldots,X_m) \propto \pi_k P(X_1,\ldots,X_m~|~ Y=k)\]</span> or the probability is <strong>proportional to</strong> the numerator
<ul>
<li><em><strong>Note</strong>: maximizing the numerator is the same as maximizing the ratio </em></li>
</ul></li>
<li><span class="math inline">\(\pi_k P(X_1,\ldots,X_m~|~ Y=k)\)</span> can be rewritten as <span class="math display">\[\begin{aligned}
\pi_k P(X_1,\ldots,X_m~|~ Y=k) &amp; = \pi_k P(X_1 ~|~ Y = k)P(X_2,\ldots,X_m ~|~ X_1,Y=k) \\
&amp; = \pi_k P(X_1 ~|~ Y = k) P(X_2 ~|~ X_1, Y=k) P(X_3,\ldots,X_m ~|~ X_1,X_2, Y=k) \\
&amp; = \pi_k P(X_1 ~|~ Y = k) P(X_2 ~|~ X_1, Y=k)\ldots P(X_m~|~X_1\ldots,X_{m-1},Y=k) \\
\end{aligned}\]</span> where each variable has its own probability term that depends on all the terms before it
<ul>
<li>this is effectively indicating that each of the predictors may be dependent on other predictors</li>
</ul></li>
<li>however, if we make the assumption that all predictor variables are <strong>independent</strong> to each other, the quantity can be simplified to <span class="math display">\[ \pi_k P(X_1,\ldots,X_m~|~ Y=k) \approx \pi_k P(X_1 ~|~ Y = k) P(X_2 ~|~ Y = k)\ldots P(X_m ~|~,Y=k)\]</span> which is effectively the product of the prior probability for <span class="math inline">\(k\)</span> and the probability of variables <span class="math inline">\(X_1,\ldots,X_m\)</span> given that <span class="math inline">\(Y = k\)</span>
<ul>
<li><em><strong>Note</strong>: the assumption is naive in that it is unlikely the predictors are completely independent of each other, but this model still produces <strong>useful</strong> results particularly with <strong>large number of binary/categorical variables</strong> </em>
<ul>
<li>text and document classification usually require large quantities of binary and categorical features</li>
</ul></li>
</ul></li>
<li><strong>R Commands</strong>
<ul>
<li><code>nb &lt;- train(outcome ~ predictors, data=training, method=&quot;nb&quot;)</code> = constructs a naive Bayes model on the predictors with the provided training data</li>
<li><code>predict(nb, test)</code> = applies the naive Bayes model to test data and return the prediction results in data frame</li>
<li><strong><em>example: <code>caret</code> package</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># using the same data from iris, run naive Bayes on training data</span><br><span class="line">nb &lt;- train(Species ~ ., data=training,method=&quot;nb&quot;)</span><br><span class="line"># predict test outcomes using naive Bayes model</span><br><span class="line">pred.nb &lt;- predict(nb,testing)</span><br><span class="line"># print results</span><br><span class="line">pred.nb</span><br></pre></td></tr></table></figure>
<h3 id="compare-results-for-lda-and-naive-bayes">Compare Results for LDA and Naive Bayes</h3>
<ul>
<li>linear discriminant analysis and naive Bayes generally produce similar results for small data sets</li>
<li>for our example data from <code>iris</code> data set, we can compare the prediction the results from the two models</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># tabulate the prediction results from LDA and naive Bayes</span><br><span class="line">table(pred.lda,pred.nb)</span><br><span class="line"># create logical variable that returns TRUE for when predictions from the two models match</span><br><span class="line">equalPredictions &lt;- (pred.lda==pred.nb)</span><br><span class="line"># plot the comparison</span><br><span class="line">qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, only one data point, which is located inbetween the two classes is predicted differently by the two models</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="model-selection">Model Selection</h2>
<ul>
<li>the general behavior of the errors of the training and test sets are as follows
<ul>
<li>as the number of predictors used increases (or model complexity), the error for the prediction model on <em>training</em> set <strong>always decreases</strong></li>
<li>the error for the prediction model on <em>test</em> set <strong>decreases first and then increases</strong> as number of predictors used approaches the total number of predictors available</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/20.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>this is expected since as more predictors used, the model is more likely to <em>overfit</em> the training data</li>
<li><strong>goal</strong> in selecting models = <strong><em>avoid overfitting</em></strong> on training data and <strong><em>minimize error</em></strong> on test data</li>
<li><strong><em>approaches</em></strong>
<ul>
<li>split samples</li>
<li>decompose expected prediction error</li>
<li>hard thresholding for high-dimensional data</li>
<li>regularization for regression
<ul>
<li>ridge regression</li>
<li>lasso regression</li>
</ul></li>
</ul></li>
<li><strong><em>problems</em></strong>
<ul>
<li>time/computational complexity limitations</li>
<li>high dimensional</li>
</ul></li>
</ul>
<h3 id="example-training-vs-test-error-for-combination-of-predictors">Example: Training vs Test Error for Combination of Predictors</h3>
<ul>
<li><em><strong>Note</strong>: the code for this example comes from <a href="http://www.cbcb.umd.edu/~hcorrada/PracticalML/src/selection.R" target="_blank" rel="noopener">Hector Corrada Bravo’s Practical Machine Learning Course</a> </em></li>
<li>to demonstrate the behavior of training and test errors, the <code>prostate</code> dataset from <em>Elements of Statistical Learning</em> is used</li>
<li>all combinations of predictors are used to produce prediction models, and Residual Squared Error (RSS) is calculated for all models on both the training and test sets</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># load data and set seed</span><br><span class="line">data(prostate); set.seed(1)</span><br><span class="line"># define outcome y and predictors x</span><br><span class="line">covnames &lt;- names(prostate[-(9:10)])</span><br><span class="line">y &lt;- prostate$lpsa; x &lt;- prostate[,covnames]</span><br><span class="line"># create test set predictors and outcomes</span><br><span class="line">train.ind &lt;- sample(nrow(prostate), ceiling(nrow(prostate))/2)</span><br><span class="line">y.test &lt;- prostate$lpsa[-train.ind]; x.test &lt;- x[-train.ind,]</span><br><span class="line"># create training set predictors and outcomes</span><br><span class="line">y &lt;- prostate$lpsa[train.ind]; x &lt;- x[train.ind,]</span><br><span class="line"># p = number of predictors</span><br><span class="line">p &lt;- length(covnames)</span><br><span class="line"># initialize the list of residual sum squares</span><br><span class="line">rss &lt;- list()</span><br><span class="line"># loop through each combination of predictors and build models</span><br><span class="line">for (i in 1:p) &#123;</span><br><span class="line">    # compute matrix for p choose i predictors for i = 1...p (creates i x p matrix)</span><br><span class="line">    Index &lt;- combn(p,i)</span><br><span class="line">    # calculate residual sum squares of each combination of predictors</span><br><span class="line">    rss[[i]] &lt;- apply(Index, 2, function(is) &#123;</span><br><span class="line">    	# take each combination (or column of Index matrix) and create formula for regression</span><br><span class="line">        form &lt;- as.formula(paste(&quot;y~&quot;, paste(covnames[is], collapse=&quot;+&quot;), sep=&quot;&quot;))</span><br><span class="line">        # run linear regression with combination of predictors on training data</span><br><span class="line">        isfit &lt;- lm(form, data=x)</span><br><span class="line">        # predict outcome for all training data points</span><br><span class="line">        yhat &lt;- predict(isfit)</span><br><span class="line">        # calculate residual sum squares for predictions on training data</span><br><span class="line">        train.rss &lt;- sum((y - yhat)^2)</span><br><span class="line">        # predict outcome for all test data points</span><br><span class="line">        yhat &lt;- predict(isfit, newdata=x.test)</span><br><span class="line">        # calculate residual sum squares for predictions on test data</span><br><span class="line">        test.rss &lt;- sum((y.test - yhat)^2)</span><br><span class="line">        # store each pair of training and test residual sum squares as a list</span><br><span class="line">        c(train.rss, test.rss)</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br><span class="line"># set up plot with labels, title, and proper x and y limits</span><br><span class="line">plot(1:p, 1:p, type=&quot;n&quot;, ylim=range(unlist(rss)), xlim=c(0,p),</span><br><span class="line">	xlab=&quot;Number of Predictors&quot;, ylab=&quot;Residual Sum of Squares&quot;,</span><br><span class="line">	main=&quot;Prostate Cancer Data - Training vs Test RSS&quot;)</span><br><span class="line"># add data points for training and test residual sum squares</span><br><span class="line">for (i in 1:p) &#123;</span><br><span class="line">	# plot training residual sum squares in blue</span><br><span class="line">    points(rep(i, ncol(rss[[i]])), rss[[i]][1, ], col=&quot;blue&quot;, cex = 0.5)</span><br><span class="line">    # plot test residual sum squares in red</span><br><span class="line">    points(rep(i, ncol(rss[[i]])), rss[[i]][2, ], col=&quot;red&quot;, cex = 0.5)</span><br><span class="line">&#125;</span><br><span class="line"># find the minimum training RSS for each combination of predictors</span><br><span class="line">minrss &lt;- sapply(rss, function(x) min(x[1,]))</span><br><span class="line"># plot line through the minimum training RSS data points in blue</span><br><span class="line">lines((1:p), minrss, col=&quot;blue&quot;, lwd=1.7)</span><br><span class="line"># find the minimum test RSS for each combination of predictors</span><br><span class="line">minrss &lt;- sapply(rss, function(x) min(x[2,]))</span><br><span class="line"># plot line through the minimum test RSS data points in blue</span><br><span class="line">lines((1:p), minrss, col=&quot;red&quot;, lwd=1.7)</span><br><span class="line"># add legend</span><br><span class="line">legend(&quot;topright&quot;, c(&quot;Train&quot;, &quot;Test&quot;), col=c(&quot;blue&quot;, &quot;red&quot;), pch=1)</span><br></pre></td></tr></table></figure>
<ul>
<li>from the above, we can clearly that test RSS error approaches the minimum at around 3 predictors and increases slightly as more predictors are used</li>
</ul>
<h3 id="split-samples">Split Samples</h3>
<ul>
<li>the best method to pick predictors/model is to split the given data into different test sets</li>
<li><strong>process</strong>
<ol type="1">
<li>divide data into training/test/validation sets (60 - 20 - 20 split)</li>
<li>train all competing models on the training data</li>
<li>apply the models on validation data and choose the best performing model</li>
<li>re-split data into training/test/validation sets and repeat steps 1 to 3</li>
<li>apply the overall best performing model on test set to appropriately assess performance on new data</li>
</ol></li>
<li><strong>common problems</strong>
<ul>
<li>limited data = if not enough data is available, it may not be possible to produce a good model fit after splitting the data into 3 sets</li>
<li>computational complexity = modeling with all subsets of models can be extremely taxing in terms of computations, especially when a large number of predictors are available</li>
</ul></li>
</ul>
<h3 id="decompose-expected-prediction-error">Decompose Expected Prediction Error</h3>
<ul>
<li>the outcome <span class="math inline">\(Y_i\)</span> can be modeled by <span class="math display">\[Y_i = f(X_i) + \epsilon_i\]</span> where <span class="math inline">\(\epsilon_i\)</span> = error term</li>
<li>the <strong>expected prediction error</strong> is defined as <span class="math display">\[EPE(\lambda) = E\left[\left(Y - \hat{f}_{\lambda}(X)\right)^2\right]\]</span> where <span class="math inline">\(\lambda\)</span> = specific set of tuning parameters</li>
<li>estimates from the model constructed with training data can be denoted as <span class="math inline">\(\hat{f}_{\lambda}(x^*)\)</span> where <span class="math inline">\(X = x^*\)</span> is the new data point that we would like to predict at</li>
<li>the expected prediction error is as follows <span class="math display">\[\begin{aligned}
E\left[\left(Y - \hat{f}_{\lambda}(x^*)\right)^2\right] &amp; = \sigma^2 + \left(E[\hat{f}_{\lambda}(x^*)] - f(x^*)\right)^2 + E\left[\hat{f}_{\lambda}(x^*) - E[\hat{f}_{\lambda}(x^*)]\right]^2\\
&amp; = \mbox{Irreducible Error} + \mbox{Bias}^2 + \mbox{Variance}\\
\end{aligned} \]</span>
<ul>
<li><strong>goal of prediction model</strong> = minimize overall expected prediction error</li>
<li>irreducible error = noise inherent to the data collection process <span class="math inline">\(\rightarrow\)</span> cannot be reduced</li>
<li>bias/variance = can be traded in order to find optimal model (least error)</li>
</ul></li>
</ul>
<h3 id="hard-thresholding">Hard Thresholding</h3>
<ul>
<li>if there are more predictors than observations (high-dimensional data), linear regressions will only return coefficients for some of the variables because there’s not enough data to estimate the rest of the parameters
<ul>
<li>conceptually, this occurs because the design matrix that the model is based on cannot be inverted</li>
<li><em><strong>Note</strong>: ridge regression can help address this problem </em></li>
</ul></li>
<li><strong>hard thresholding</strong> can help estimate the coefficients/model by taking subsets of predictors and building models</li>
<li><strong>process</strong>
<ul>
<li>model the outcome as <span class="math display">\[Y_i = f(X_i) + \epsilon_i\]</span> where <span class="math inline">\(\epsilon_i\)</span> = error term</li>
<li>assume the prediction estimate has a linear form <span class="math display">\[\hat{f}_{\lambda}(x) = x&#39;\beta\]</span> where only <span class="math inline">\(\lambda\)</span> coefficients for the set of predictors <span class="math inline">\(x\)</span> are <strong><em>nonzero</em></strong></li>
<li>after setting the value of <span class="math inline">\(\lambda\)</span>, compute models using all combinations of <span class="math inline">\(\lambda\)</span> variables to find which variables’ coefficients should be set to be zero</li>
</ul></li>
<li><strong>problem</strong>
<ul>
<li>computationally intensive</li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>as we can see from the results below, some of the coefficients have values of <code>NA</code></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># load prostate data</span><br><span class="line">data(prostate)</span><br><span class="line"># create subset of observations with 10 variables</span><br><span class="line">small = prostate[1:5,]</span><br><span class="line"># print linear regression</span><br><span class="line">lm(lpsa ~ .,data =small)</span><br></pre></td></tr></table></figure>
<h3 id="regularized-regression-concept-resource">Regularized Regression Concept (<a href="http://www.cbcb.umd.edu/~hcorrada/PracticalML/pdf/lectures/selection.pdf" target="_blank" rel="noopener">Resource</a>)</h3>
<ul>
<li><strong>regularized regression</strong> = fit a regression model and adjust for the large coefficients in attempt to help with bias/variance trade-off or model selection
<ul>
<li>when running regressions unconstrained (without specifying any criteria for coefficients), the model may be susceptible to high variance (coefficients explode <span class="math inline">\(\rightarrow\)</span> very large values) if there are variables that are highly correlated</li>
<li>controlling/regularizing coefficients may slightly <strong><em>increase bias</em></strong> (lose a bit of prediction capability) but will <strong><em>reduce variance</em></strong> and improve the prediction error</li>
<li>however, this approach may be very demanding computationally and generally does not perform as well as random forest/boosting</li>
</ul></li>
<li><strong>Penalized Residual Sum of Squares (PRSS)</strong> is calculated by adding a penalty term to the prediction squared error <span class="math display">\[PRSS(\beta) = \sum_{j=1}^n (Y_j - \sum_{i=1}^m \beta_{1i} X_{ij})^2 + P(\lambda; \beta)\]</span>
<ul>
<li>penalty shrinks coefficients if their values become too large</li>
<li>penalty term is generally used to reduce complexity and variance for the model, while respecting the structure of the data/relationship</li>
</ul></li>
<li><strong><em>example: co-linear variables</em></strong>
<ul>
<li>given a linear model, <span class="math display">\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon\]</span> where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are nearly perfectly correlated (co-linear)</li>
<li>the model can then be approximated by this model by omitting <span class="math inline">\(X_2\)</span>, so the model becomes <span class="math display">\[Y = \beta_0 + (\beta_1 + \beta_2)X_1 + \epsilon\]</span></li>
<li>with the above model, we can get a good estimate of <span class="math inline">\(Y\)</span>
<ul>
<li>the estimate of <span class="math inline">\(Y\)</span> will be biased</li>
<li>but the variance of the prediction may be reduced</li>
</ul></li>
</ul></li>
</ul>
<h3 id="regularized-regression---ridge-regression">Regularized Regression - Ridge Regression</h3>
<ul>
<li>the penalized residual sum of squares (PRSS) takes the form of <span class="math display">\[PRSS(\beta_j) = \sum_{i=1}^N \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^p \beta_j^2\]</span></li>
<li>this is equivalent to solving the equation <span class="math display">\[PRSS(\beta_j) = \sum_{i=1}^N \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2\]</span> subject to constraint <span class="math inline">\(\sum_{j=1}^p \beta_j^2 \leq s\)</span> where <span class="math inline">\(s\)</span> is inversely proportional to <span class="math inline">\(\lambda\)</span>
<ul>
<li>if the coefficients <span class="math inline">\(\beta_j\)</span> are large in value, the term <span class="math inline">\(\sum_{j=1}^p \beta_j^2\)</span> will cause the overall PRSS value to increase, leading to worse models</li>
<li>the presence of the term thus requires some of the coefficients to be small</li>
</ul></li>
<li>inclusion of <span class="math inline">\(\lambda\)</span> makes the problem <em>non-singular</em> even if <span class="math inline">\(X^TX\)</span> is not invertible
<ul>
<li>this means that even in cases where there are more predictors than observations, the coefficients of the predictors can still be estimated</li>
</ul></li>
<li><span class="math inline">\(\lambda\)</span> = tuning parameter
<ul>
<li>controls size of coefficients or the amount of regularization</li>
<li>as <span class="math inline">\(\lambda \rightarrow 0\)</span>, the result approaches the least square solution</li>
<li>as <span class="math inline">\(\lambda \rightarrow \infty\)</span>, all of the coefficients receive large penalties and the conditional coefficients <span class="math inline">\(\hat{\beta}_{\lambda=\infty}^{ridge}\)</span> approaches zero collectively</li>
<li><span class="math inline">\(\lambda\)</span> should be carefully chosen through cross-validation/other techniques to find the optimal tradeoff of bias for variance</li>
<li><em><strong>Note</strong>: it is important realize that all coefficients (though they may be shrunk to very small values) will <strong>still be included</strong> in the model when applying ridge regression </em></li>
</ul></li>
<li><strong>R Commands</strong>
<ul>
<li>[<code>MASS</code> package] <code>ridge&lt;-lm.ridge(outcome ~ predictors, data=training, lambda=5)</code> = perform ridge regression with given outcome and predictors using the provided <span class="math inline">\(\lambda\)</span> value
<ul>
<li><em><strong>Note</strong>: the predictors are <strong>centered and scaled first</strong> before the regression is run </em></li>
<li><code>lambda=5</code> = tuning parameter</li>
<li><code>ridge$xm</code> = returns column/predictor mean from the data</li>
<li><code>ridge$scale</code> = returns the scaling performed on the predictors for the ridge regression
<ul>
<li><em><strong>Note</strong>: all the variables are divided by the biased standard deviation <span class="math inline">\(\sum (X_i - \bar X_i) / n\)</span> </em></li>
</ul></li>
<li><code>ridge$coef</code> = returns the conditional coefficients, <span class="math inline">\(\beta_j\)</span> from the ridge regression</li>
<li><code>ridge$ym</code> = return mean of outcome</li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>train(outcome ~ predictors, data=training, method=&quot;ridge&quot;, lambda=5)</code> = perform ridge regression with given outcome and predictors
<ul>
<li><code>preProcess=c(&quot;center&quot;, &quot;scale&quot;)</code> = centers and scales the predictors before the model is built
<ul>
<li><em><strong>Note</strong>: this is generally a good idea for building ridge regressions </em></li>
</ul></li>
<li><code>lambda=5</code> = tuning parameter</li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>train(outcome ~ predictors, data=training, method=&quot;foba&quot;, lambda=5, k=4)</code> = perform ridge regression with variable selection
<ul>
<li><code>lambda=5</code> = tuning parameter</li>
<li><code>k=4</code> = number of variables that should be retained
<ul>
<li>this means that <code>length(predictors)-k</code> variables will be eliminated</li>
</ul></li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>predict(model,test)</code> = use the model to predict on test set <span class="math inline">\(\rightarrow\)</span> similar to all other <code>caret</code> algorithms</li>
</ul></li>
<li><strong><em>example: ridge coefficient paths vs <span class="math inline">\(\lambda\)</span></em></strong>
<ul>
<li>using the same <code>prostate</code> dataset, we will run ridge regressions with different values of <span class="math inline">\(\lambda\)</span> and find the optimum <span class="math inline">\(\lambda\)</span> value that minimizes test RSS</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"># load MASS library for lm.ridge function</span><br><span class="line">library(MASS)</span><br><span class="line"># we will run the models for 10 different values from 0 to 50</span><br><span class="line">lambdas &lt;- seq(0,50,len=10)</span><br><span class="line"># create empty vectors for training and test residual sum squares</span><br><span class="line">M &lt;- length(lambdas)</span><br><span class="line">train.rss &lt;- rep(0,M); test.rss &lt;- rep(0,M)</span><br><span class="line"># create empty matrix for coefficients/betas</span><br><span class="line">betas &lt;- matrix(0,ncol(x),M)</span><br><span class="line"># run ridge regressions on the predictors for all values of lambda</span><br><span class="line">for(i in 1:M)&#123;</span><br><span class="line">	# create formula for the ridge regression (outcome ~ predictors)</span><br><span class="line">	Formula &lt;-as.formula(paste(&quot;y~&quot;,paste(covnames,collapse=&quot;+&quot;),sep=&quot;&quot;))</span><br><span class="line">	# run ridge regression with the ith lambda value</span><br><span class="line">	fit1 &lt;- lm.ridge(Formula,data=x,lambda=lambdas[i])</span><br><span class="line">	# store the coefficients of the ridge regression as a column in betas matrix</span><br><span class="line">	betas[,i] &lt;- fit1$coef</span><br><span class="line">	# center the training data based on column means</span><br><span class="line">	scaledX &lt;- sweep(as.matrix(x),2,fit1$xm)</span><br><span class="line">	# scale the training data based on standard deviations</span><br><span class="line">	scaledX &lt;- sweep(scaledX,2,fit1$scale,&quot;/&quot;)</span><br><span class="line">	# calculate predictions for training data using the model coefficients</span><br><span class="line">	yhat &lt;- scaledX%*%fit1$coef+fit1$ym</span><br><span class="line">	# calculate residual sum squares for training data</span><br><span class="line">	train.rss[i] &lt;- sum((y - yhat)^2)</span><br><span class="line">	# center the test data based on column means</span><br><span class="line">	scaledX &lt;- sweep(as.matrix(x.test),2,fit1$xm)</span><br><span class="line">	# scale the test data based on standard deviations</span><br><span class="line">	scaledX &lt;- sweep(scaledX,2,fit1$scale,&quot;/&quot;)</span><br><span class="line">	# calculate predictions for test data using the model coefficients</span><br><span class="line">	yhat &lt;- scaledX%*%fit1$coef+fit1$ym</span><br><span class="line">	# calculate residual sum squares for test data</span><br><span class="line">	test.rss[i] &lt;- sum((y.test - yhat)^2)</span><br><span class="line">&#125;</span><br><span class="line"># plot the line for test RSS vs lambda in red</span><br><span class="line">plot(lambdas,test.rss,type=&quot;l&quot;,col=&quot;red&quot;,lwd=2,ylab=&quot;RSS&quot;,ylim=range(train.rss,test.rss))</span><br><span class="line"># plot the line for training RSS vs lambda in blue</span><br><span class="line">lines(lambdas,train.rss,col=&quot;blue&quot;,lwd=2,lty=2)</span><br><span class="line"># find the best lambda that minimizes test RSS</span><br><span class="line">best.lambda &lt;- lambdas[which.min(test.rss)]</span><br><span class="line"># plot vertical line marking the best lambda</span><br><span class="line">abline(v=best.lambda)</span><br><span class="line"># add text label</span><br><span class="line">text(best.lambda+5, min(test.rss)-2, paste(&quot;best lambda=&quot;, round(best.lambda, 2)))</span><br><span class="line"># add legend to plot</span><br><span class="line">legend(30,30,c(&quot;Train&quot;,&quot;Test&quot;),col=c(&quot;blue&quot;,&quot;red&quot;),lty=c(2,1))</span><br><span class="line"># plot all coefficients vs values of lambda</span><br><span class="line">plot(lambdas,betas[1,],ylim=range(betas),type=&quot;n&quot;,ylab=&quot;Coefficients&quot;)</span><br><span class="line">for(i in 1:ncol(x))</span><br><span class="line">	lines(lambdas,betas[i,],type=&quot;b&quot;,lty=i,pch=as.character(i), cex = 0.5)</span><br><span class="line"># add horizontal line for reference</span><br><span class="line">abline(h=0)</span><br><span class="line"># add legend to plot</span><br><span class="line">legend(&quot;topright&quot;,covnames,pch=as.character(1:8), cex = 0.5)</span><br></pre></td></tr></table></figure>
<h3 id="regularized-regression---lasso-regression">Regularized Regression - LASSO Regression</h3>
<ul>
<li>LASSO (least absolute shrinkage and selection operator) was introduced by Tibshirani (Journal of the Royal Statistical Society 1996)</li>
<li>similar to <strong>ridge</strong>, with slightly different penalty term</li>
<li>the penalized residual sum of squares (PRSS) takes the form of <span class="math display">\[PRSS(\beta_j) = \sum_{i=1}^N \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^p |\beta_j|\]</span></li>
<li>this is equivalent to solving the equation <span class="math display">\[PRSS(\beta_j) = \sum_{i=1}^N \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2\]</span> subject to constraint <span class="math inline">\(\sum_{j=1}^p |\beta_j| \leq s\)</span> where <span class="math inline">\(s\)</span> is inversely proportional to <span class="math inline">\(\lambda\)</span></li>
<li><span class="math inline">\(\lambda\)</span> = tuning parameter
<ul>
<li>controls size of coefficients or the amount of regularization</li>
<li>large values of <span class="math inline">\(\lambda\)</span> will set some coefficient equal to zero
<ul>
<li><em><strong>Note</strong>: LASSO effectively performs model selection (choose subset of predictors) while shrinking other coefficients, where as Ridge only shrinks the coefficients </em></li>
</ul></li>
</ul></li>
<li><strong>R Commands</strong>
<ul>
<li>[<code>lars</code> package] <code>lasso&lt;-lars(as.matrix(x), y, type=&quot;lasso&quot;, trace=TRUE)</code> = perform lasso regression by adding predictors one at a time (or setting some variables to 0)
<ul>
<li><em><strong>Note</strong>: the predictors are <strong>centered and scaled first</strong> before the regression is run </em></li>
<li><code>as.matrix(x)</code> = the predictors must be in matrix/dataframe format</li>
<li><code>trace=TRUE</code> = prints progress of the lasso regression</li>
<li><code>lasso$lambda</code> = return the <span class="math inline">\(\lambda\)</span>s used for each step of the lasso regression</li>
<li><code>plot(lasso)</code> = prints plot that shows the progression of the coefficients as they are set to zero one by one</li>
<li><code>predit.lars(lasso, test)</code> = use the lasso model to predict on test data
<ul>
<li><em><strong>Note</strong>: more information/documentation can be found in <code>?predit.lars</code> </em></li>
</ul></li>
</ul></li>
<li>[<code>lars</code> package] <code>cv.lars(as.matrix(x), y, K=10, type=&quot;lasso&quot;, trace=TRUE)</code> = computes K-fold cross-validated mean squared prediction error for lasso regression
<ul>
<li>effectively the <code>lars</code> function is run <code>K</code> times with each of the folds to estimate the</li>
<li><code>K=10</code> = create 10-fold cross validation</li>
<li><code>trace=TRUE</code> = prints progress of the lasso regression</li>
</ul></li>
<li>[<code>enet</code> package] <code>lasso&lt;-enet(predictors, outcome, lambda = 0)</code> = perform elastic net regression on given predictors and outcome
<ul>
<li><code>lambda=0</code> = default value for <span class="math inline">\(\lambda\)</span>
<ul>
<li><em><strong>Note</strong>: lasso regression is a special case of elastic net regression, and forcing <code>lambda=0</code> tells the function to fit a lasso regression </em></li>
</ul></li>
<li><code>plot(lasso)</code> = prints plot that shows the progression of the coefficients as they are set to zero one by one</li>
<li><code>predict.ent(lasso, test)</code>= use the lasso model to predict on test data</li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>train(outcome ~ predictors, data=training, method=&quot;lasso&quot;)</code> = perform lasso regression with given outcome and predictors
<ul>
<li><em><strong>Note</strong>: outcome and predictors must be in the same dataframe </em></li>
<li><code>preProcess=c(&quot;center&quot;, &quot;scale&quot;)</code> = centers and scales the predictors before the model is built
<ul>
<li><em><strong>Note</strong>: this is generally a good idea for building lasso regressions </em></li>
</ul></li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>train(outcome~predictors,data=train,method=&quot;relaxo&quot;,lambda=5,phi=0.3)</code> = perform relaxed lasso regression on given predictors and outcome
<ul>
<li><code>lambda=5</code> = tuning parameter</li>
<li><code>phi=0.3</code> = relaxation parameter
<ul>
<li><code>phi=1</code> corresponds to the regular Lasso solutions</li>
<li><code>phi=0</code> computes the OLS estimates on the set of variables selected by the Lasso</li>
</ul></li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>predict(model,test)</code> = use the model to predict on test set <span class="math inline">\(\rightarrow\)</span> similar to all other <code>caret</code> algorithms</li>
</ul></li>
<li><strong><em>example: <code>lars</code> package</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># load lars package</span><br><span class="line">library(lars)</span><br><span class="line"># perform lasso regression</span><br><span class="line">lasso.fit &lt;- lars(as.matrix(x), y, type=&quot;lasso&quot;, trace=TRUE)</span><br><span class="line"># plot lasso regression model</span><br><span class="line">plot(lasso.fit, breaks=FALSE, cex = 0.75)</span><br><span class="line"># add legend</span><br><span class="line">legend(&quot;topleft&quot;, covnames, pch=8, lty=1:length(covnames),</span><br><span class="line">	col=1:length(covnames), cex = 0.6)</span><br><span class="line"># plots the cross validation curve</span><br><span class="line">lasso.cv &lt;- cv.lars(as.matrix(x), y, K=10, type=&quot;lasso&quot;, trace=TRUE)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="combining-predictors">Combining Predictors</h2>
<ul>
<li><strong>combining predictors</strong> = also known as <a href="http://en.wikipedia.org/wiki/Ensemble_learning" target="_blank" rel="noopener"><em>ensembling methods in learning</em></a>, combine classifiers by averaging/voting to improve accuracy (generally)
<ul>
<li>this reduces interpretability and increases computational complexity</li>
<li>boosting/bagging/random forest algorithms all use this idea, except all classifiers averaged are of the same type</li>
</ul></li>
<li>Netflix Competition was won by a team that blended together 107 machine learning algorithms, Heritage Health Prize was also won by a combination of algorithms
<ul>
<li><em><strong>Note</strong>: the winning algorithm for Netflix was not used because it was too computationally complex/intensive, so the trade-off between accuracy and scalability is very important </em></li>
</ul></li>
<li><strong>approach</strong>
<ul>
<li>combine similar classifiers using bagging/boosting/random forest</li>
<li>combine different classifiers using model stacking/ensembling
<ul>
<li>build an odd number of models (we need an odd number for the majority vote to avoid ties)</li>
<li>predict with each model</li>
<li>combine the predictions and predict the final outcome by majority vote</li>
</ul></li>
</ul></li>
<li><strong>process: simple model ensembling</strong>
<ol type="1">
<li><strong><em>build multiple models</em></strong> on the training data set</li>
<li>use the models to <strong><em>predict</em></strong> on the training/test set
<ul>
<li><em>predict on training</em> if the data was divided to only training/test sets</li>
<li><em>predict on test</em> if the data was divided into training/test/validation sets</li>
</ul></li>
<li>combine the prediction results from each model and the true results for the training/test set into a <strong><em>new data frame</em></strong>
<ul>
<li>one column for each model</li>
<li>add the true outcome from the training/test set as a separate column</li>
</ul></li>
<li>train the new data frame with a <strong><em>new model</em></strong> the true outcome as the outcome, and the predictions from various models as predictors</li>
<li>use the combined model fit to predict results on the training/test data
<ul>
<li>calculate the RMSE for all models, including combined fit, to evaluate the accuracy of the different models</li>
<li><em><strong>Note</strong>: the RMSE for the combined fit should generally be <strong>lower</strong> than the the rest of the models </em></li>
</ul></li>
<li>to predict on the final test/validation set, use all the initial models to predict on the data set first to <strong><em>recreate a prediction data frame</em></strong> for the test/validation set like in <strong><em>step 3</em></strong>
<ul>
<li>one column for each model, no truth column this time</li>
</ul></li>
<li><strong><em>apply the combined fit</em></strong> on the combined prediction data frame to get the final resultant predictions</li>
</ol></li>
</ul>
<h3 id="example---majority-vote">Example - Majority Vote</h3>
<ul>
<li>suppose we have 5 independent classifiers/models</li>
<li>each has 70% accuracy</li>
<li><strong>majority vote accuracy (mva)</strong> = probability of the majority of the models achieving 70% at the same time</li>
</ul>
<p><span class="math display">\[\begin{aligned}
\mbox{majority vote accuracy} &amp; = p(3~correct,~2~wrong) + p(4~correct,~1~wrong) \\
&amp;\qquad+ p(5~correct) \\
&amp; = {5 \choose 3} \times(0.7)^3(0.3)^2 + {5 \choose 4} \times(0.7)^4(0.3)^1 - {5 \choose 5} (0.7)^5 \\
&amp; = 10 \times(0.7)^3(0.3)^2 + 5 \times(0.7)^4(0.3)^2 - 1 \times (0.7)^5 \\
&amp; = 83.7% \\
\end{aligned}\]</span></p>
<ul>
<li>with 101 classifiers, the majority vote accuracy becomes 99.9%</li>
</ul>
<h3 id="example---model-ensembling">Example - Model Ensembling</h3>
<figure class="highlight plain"><figcaption><span>warning </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># set up data</span><br><span class="line">inBuild &lt;- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)</span><br><span class="line">validation &lt;- Wage[-inBuild,]; buildData &lt;- Wage[inBuild,]</span><br><span class="line">inTrain &lt;- createDataPartition(y=buildData$wage,p=0.7, list=FALSE)</span><br><span class="line">training &lt;- buildData[inTrain,]; testing &lt;- buildData[-inTrain,]</span><br><span class="line"># train the data using both glm and random forest models</span><br><span class="line">glm.fit &lt;- train(wage ~.,method=&quot;glm&quot;,data=training)</span><br><span class="line">rf.fit &lt;- train(wage ~.,method=&quot;rf&quot;,data=training,</span><br><span class="line">	trControl = trainControl(method=&quot;cv&quot;),number=3)</span><br><span class="line"># use the models to predict the results on the testing set</span><br><span class="line">glm.pred.test &lt;- predict(glm.fit,testing)</span><br><span class="line">rf.pred.test &lt;- predict(rf.fit,testing)</span><br><span class="line"># combine the prediction results and the true results into new data frame</span><br><span class="line">combinedTestData &lt;- data.frame(glm.pred=glm.pred.test,</span><br><span class="line">	rf.pred = rf.pred.test,wage=testing$wage)</span><br><span class="line"># run a Generalized Additive Model (gam) model on the combined test data</span><br><span class="line">comb.fit &lt;- train(wage ~.,method=&quot;gam&quot;,data=combinedTestData)</span><br><span class="line"># use the resultant model to predict on the test set</span><br><span class="line">comb.pred.test &lt;- predict(comb.fit, combinedTestData)</span><br><span class="line"># use the glm and rf models to predict results on the validation data set</span><br><span class="line">glm.pred.val &lt;- predict(glm.fit,validation)</span><br><span class="line">rf.pred.val &lt;- predict(rf.fit,validation)</span><br><span class="line"># combine the results into data frame for the comb.fit</span><br><span class="line">combinedValData &lt;- data.frame(glm.pred=glm.pred.val,rf.pred=glm.pred.val)</span><br><span class="line"># run the comb.fit on the combined validation data</span><br><span class="line">comb.pred.val &lt;- predict(comb.fit,combinedValData)</span><br><span class="line"># tabulate the results - test data set RMSE Errors</span><br><span class="line">rbind(test = c(glm = sqrt(sum((glm.pred.test-testing$wage)^2)),</span><br><span class="line">		rf = sqrt(sum((rf.pred.test-testing$wage)^2)),</span><br><span class="line">		combined = sqrt(sum((comb.pred.test-testing$wage)^2))),</span><br><span class="line">	# validation data set RMSE Errors</span><br><span class="line">	validation = c(sqrt(sum((glm.pred.val-validation$wage)^2)),</span><br><span class="line">		sqrt(sum((rf.pred.val-validation$wage)^2)),</span><br><span class="line">		sqrt(sum((comb.pred.val-validation$wage)^2))))</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="forecasting">Forecasting</h2>
<ul>
<li><strong>forecasting</strong> = typically used with time series, predict one or more observations into the future
<ul>
<li>data are dependent over time so subsampling/splitting data into training/test is more complicated and must be done very carefully</li>
</ul></li>
<li>specific patterns need to be considered for time series data (<em>time series decomposition</em>)
<ul>
<li><strong>trend</strong> = long term increase/decrease in data</li>
<li><strong>seasonal</strong> = patterns related to time of week/month/year/etc</li>
<li><strong>cyclic</strong> = patterns that rise/fall periodically</li>
</ul></li>
<li><em><strong>Note</strong>: issues that arise from time series are similar to those from spatial data </em>
<ul>
<li>dependency between nearby observations</li>
<li>location specific effects</li>
</ul></li>
<li>all standard predictions models can be used but requires more consideration</li>
<li><p><em><strong>Note</strong>: more detailed tutorial can be found in <a href="https://www.otexts.org/fpp/" target="_blank" rel="noopener">Rob Hyndman’s Forecasting: principles and practice</a> </em></p></li>
<li><strong>considerations for interpreting results</strong>
<ul>
<li>unrelated time series can often seem to be correlated with each other (<em>spurious correlations</em>)</li>
<li>geographic analysis may exhibit similar patterns due to population distribution/concentrations</li>
<li>extrapolations too far into future can be dangerous as they can produce in insensible results</li>
<li>dependencies over time (seasonal effects) should be examined and isolated from the trends</li>
</ul></li>
<li><strong>process</strong>
<ul>
<li>ensure the data is a time series data type</li>
<li>split data into training and test sets
<ul>
<li>both must have consecutive time points</li>
</ul></li>
<li>choose forecast approach (SMA - <code>ma</code> vs EMA - <code>ets</code>, see below) and apply corresponding functions to training data</li>
<li>apply constructed forecast model to test data using <code>forecast</code> function</li>
<li>evaluate accuracy of forecast using <code>accuracy</code> function</li>
</ul></li>
<li><strong>approaches</strong>
<ul>
<li><p><strong><em>simple moving averages</em></strong> = prediction will be made for a time point by averaging together values from a number of prior periods <span class="math display">\[ Y_{t}=\frac{1}{2*k+1}\sum_{j=-k}^k {y_{t+j}}\]</span></p></li>
<li><strong><em>exponential smoothing/exponential moving average</em></strong> = weight time points that are closer to point of prediction than those that are further away <span class="math display">\[\hat{y}_{t+1} = \alpha y_t + (1-\alpha)\hat{y}_{t-1}\]</span>
<ul>
<li><em><strong>Note</strong>: many different methods of exponential smoothing are available, more information can be found <a href="https://www.otexts.org/fpp/7/6" target="_blank" rel="noopener">here</a> </em></li>
</ul></li>
</ul></li>
</ul>
<h3 id="r-commands-and-examples-3">R Commands and Examples</h3>
<ul>
<li><code>quantmod</code> package can be used to pull trading/price information for publicly traded stocks
<ul>
<li><code>getSymbols(&quot;TICKER&quot;, src=&quot;google&quot;, from=date, to=date)</code> = gets the <strong><em>daily</em></strong> high/low/open/close price and volume information for the specified stock ticker
<ul>
<li>returns the data in a data frame under the stock ticker’s name</li>
<li><code>&quot;TICKER&quot;</code> = ticker of the stock you are attempting to pull information for</li>
<li><code>src=&quot;google&quot;</code> = get price/volume information from Google finance
<ul>
<li>default source of information is Yahoo Finance</li>
</ul></li>
<li><code>from</code> and <code>to</code> = from and to dates for the price/volume information
<ul>
<li>both arguments must be specified with <code>date</code> objects</li>
</ul></li>
<li><em><strong>Note</strong>: more information about how to use <code>getSymbols</code> can be found in the documentation <code>?getSymbols</code> </em></li>
</ul></li>
<li><code>to.monthly(GOOG)</code> = converts stock data to monthly time series from daily data
<ul>
<li>the function aggregates the open/close/high/low/volume information for each day into monthly data</li>
<li><code>GOOG</code> = data frame returned from <code>getSymbols</code> function</li>
<li><em><strong>Note</strong>: <code>?to.period</code> contains documentation for converting time series to OHLC (open high low close) series </em></li>
</ul></li>
<li><code>googOpen&lt;-Op(GOOG)</code> = returns the opening price from the stock data frame
<ul>
<li><code>Cl(), Hi(), Lo()</code> = returns the close, high and low price from the stock data frame</li>
</ul></li>
<li><code>ts(googOpen, frequency=12)</code> = convert data to a time series with <code>frequency</code> observations per time unit
<ul>
<li><code>frequency=12</code> = number of observations per unit time (12 in this case because there are 12 months in each year <span class="math inline">\(\rightarrow\)</span> converts data into <strong><em>yearly</em></strong> time series)</li>
</ul></li>
</ul></li>
<li><code>decompose(ts)</code> = decomposes time series into trend, seasonal, and irregular components by using moving averages
<ul>
<li><code>ts</code> = time series object</li>
</ul></li>
<li><code>window(ts, start=1, end=6)</code> = subsets the time series at the specified starting and ending points
<ul>
<li><code>start</code> and <code>end</code> arguments must correspond to the <strong>time unit</strong> rather than the <strong><em>index</em></strong>
<ul>
<li>for instance, if the <code>ts</code> is a yearly series (<code>frequency = 12</code>), <code>start</code>/<code>end</code> should correspond to the row numbers or year (each year has 12 observations corresponding to the months)</li>
<li><code>c(1, 7)</code> can be used to specify the element of a particular year (in this case, July of the first year/row)</li>
<li><em><strong>Note</strong>: you can use 9.5 or any decimal as values for <code>start</code>/<code>end</code>, and the closest element (June of the 9th year in this case) will be used </em></li>
<li><em><strong>Note</strong>: <code>end=9-0.01</code> can be used a short cut to specify “up to 9”, since <code>end = 9</code> will include the first element of the 9th row </em></li>
</ul></li>
</ul></li>
<li><code>forecast</code> package can be used for forecasting time series data
<ul>
<li><code>ma(ts, order=3)</code> = calculates the simple moving average for the order specified
<ul>
<li><code>order=3</code> = order of moving average smoother, effectively the number of values that should be used to calculate the moving average</li>
</ul></li>
<li><code>ets(train, model=&quot;MMM&quot;)</code> = runs exponential smoothing model on training data
<ul>
<li><code>model = &quot;MMM&quot;</code> = method used to create exponential smoothing
<ul>
<li><em><strong>Note</strong>: more information can be found at <code>?ets</code> and the corresponding model chart is <a href="https://www.otexts.org/fpp/7/6" target="_blank" rel="noopener">here</a> </em></li>
</ul></li>
</ul></li>
<li><code>forecast(ts)</code> = performs forecast on specified time series and returns 5 columns: forecast values, high/low 80 confidence intervals bounds, high/low 95 percent interval bounds
<ul>
<li><code>plot(forecast)</code> = plots the forecast object, which includes the training data, forecast values for test periods, as well as the 80 and 95 percent confidence interval regions</li>
</ul></li>
<li><code>accuracy(forecast, testData)</code> = returns the accuracy metrics (RMSE, etc.) for the forecast model</li>
</ul></li>
<li><code>quandl</code> package is also used for finance-related predictions</li>
<li><strong><em>example: decomposed time series</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># load quantmod package</span><br><span class="line">library(quantmod);</span><br><span class="line"># specify to and from dates</span><br><span class="line">from.dat &lt;- as.Date(&quot;01/01/00&quot;, format=&quot;%m/%d/%y&quot;)</span><br><span class="line">to.dat &lt;- as.Date(&quot;3/2/15&quot;, format=&quot;%m/%d/%y&quot;)</span><br><span class="line"># get data for AAPL from Google Finance for the specified dates</span><br><span class="line">getSymbols(&quot;AAPL&quot;, src=&quot;google&quot;, from = from.dat, to = to.dat)</span><br><span class="line"># convert the retrieved daily data to monthly data</span><br><span class="line">mAAPL &lt;- to.monthly(AAPL)</span><br><span class="line"># extract the closing price and convert it to yearly time series (12 observations per year)</span><br><span class="line">ts &lt;- ts(Cl(mAAPL), frequency = 12)</span><br><span class="line"># plot the decomposed parts of the time series</span><br><span class="line">plot(decompose(ts),xlab=&quot;Years&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em>example: forecast</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># load forecast library</span><br><span class="line">library(forecast)</span><br><span class="line"># find the number of rows (years)</span><br><span class="line">rows &lt;- ceiling(length(ts)/12)</span><br><span class="line"># use 90% of the data to create training set</span><br><span class="line">ts.train &lt;- window(ts, start = 1, end = floor(rows*.9)-0.01)</span><br><span class="line"># use the rest of data to create test set</span><br><span class="line">ts.test &lt;- window(ts, start = floor(rows*.9))</span><br><span class="line"># plot the training set</span><br><span class="line">plot(ts.train)</span><br><span class="line"># add the moving average in red</span><br><span class="line">lines(ma(ts.train,order=3),col=&quot;red&quot;)</span><br><span class="line"># compute the exponential smoothing average</span><br><span class="line">ets &lt;- ets(ts.train,model=&quot;MMM&quot;)</span><br><span class="line"># construct a forecasting model using the exponential smoothing function</span><br><span class="line">fcast &lt;- forecast(ets)</span><br><span class="line"># plot forecast and add actual data in red</span><br><span class="line">plot(fcast); lines(ts.test,col=&quot;red&quot;)</span><br><span class="line"># print the accuracy of the forecast model</span><br><span class="line">accuracy(fcast,ts.test)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="unsupervised-prediction">Unsupervised Prediction</h2>
<ul>
<li><strong>supervised classification</strong> = predicting outcome when we know what the different classifications are
<ul>
<li><em>example</em>: predicting the type of flower (setosa, versicolor, or virginica) based on sepal width/length</li>
</ul></li>
<li><strong>unsupervised classification</strong> = predicting outcome when we don’t know what the different classifications are
<ul>
<li><em>example</em>: splitting all data for sepal width/length into different groups (cluster similar data together)</li>
</ul></li>
<li><strong>process</strong>
<ul>
<li>provided that the labels for prediction/outcome are unknown, we first build clusters from observed data
<ul>
<li>creating clusters are not noiseless process, and thus may introduce higher variance/error for data</li>
<li><strong><em>K-means</em></strong> is an example of a clustering approach</li>
</ul></li>
<li>label the clusters
<ul>
<li>interpreting the clusters well (sensible vs non-sensible clusters) is incredibly challenging</li>
</ul></li>
<li>build prediction model with the clusters as the outcome
<ul>
<li>all algorithms can be applied here</li>
</ul></li>
<li>in new data set, we will predict the clusters labels</li>
</ul></li>
<li>unsupervised prediction is effectively a <strong><em>exploratory technique</em></strong>, so the resulting clusters should be carefully interpreted
<ul>
<li>clusters may be highly variable depending on the method through which the data is sample</li>
</ul></li>
<li>generally a good idea to create custom clustering algorithms for given data as it is <strong>crucial</strong> to define the process to identify clusters for interpretability and utility of the model</li>
<li>unsupervised prediction = basic approach to <a href="http://en.wikipedia.org/wiki/Recommender_system" target="_blank" rel="noopener">recommendation engines</a>, in which the tastes of the existing users are clustered and applied to new users</li>
</ul>
<h3 id="r-commands-and-examples-4">R Commands and Examples</h3>
<ul>
<li><code>kmeans(data, centers=3)</code> = can be used to perform clustering from the provided data
<ul>
<li><code>centers=3</code> = controls the number of clusters the algorithm should aim to divide the data into</li>
</ul></li>
<li><code>cl_predict</code> function in <code>clue</code> package provides similar functionality</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># load iris data</span><br><span class="line">data(iris)</span><br><span class="line"># create training and test sets</span><br><span class="line">inTrain &lt;- createDataPartition(y=iris$Species,p=0.7, list=FALSE)</span><br><span class="line">training &lt;- iris[inTrain,]; testing &lt;- iris[-inTrain,]</span><br><span class="line"># perform k-means clustering for the data without the Species information</span><br><span class="line"># Species = what the true clusters are</span><br><span class="line">kMeans1 &lt;- kmeans(subset(training,select=-c(Species)),centers=3)</span><br><span class="line"># add clusters as new variable to training set</span><br><span class="line">training$clusters &lt;- as.factor(kMeans1$cluster)</span><br><span class="line"># plot clusters vs Species classification</span><br><span class="line">p1 &lt;- qplot(Petal.Width,Petal.Length,colour=clusters,data=training) +</span><br><span class="line">	ggtitle(&quot;Clusters Classification&quot;)</span><br><span class="line">p2 &lt;- qplot(Petal.Width,Petal.Length,colour=Species,data=training) +</span><br><span class="line">	ggtitle(&quot;Species Classification (Truth)&quot;)</span><br><span class="line">grid.arrange(p1, p2, ncol = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see, there are three clear groups that emerge from the data
<ul>
<li>this is fairly close to the actual results from Species</li>
<li>we can compare the results from the clustering and Species classification by tabulating the values</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># tabulate the results from clustering and actual species</span><br><span class="line">table(kMeans1$cluster,training$Species)</span><br></pre></td></tr></table></figure>
<ul>
<li>with the clusters determined, the training data can be trained on all predictors with the clusters from k-means as outcome</li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># build classification trees using the k-means cluster</span><br><span class="line">clustering &lt;- train(clusters ~.,data=subset(training,select=-c(Species)),method=&quot;rpart&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>we can compare the prediction results on training set vs truth</li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># tabulate the prediction results on training set vs truth</span><br><span class="line">table(predict(clustering,training),training$Species)</span><br></pre></td></tr></table></figure>
<ul>
<li>similarly, we can compare the prediction results on test set vs truth</li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># tabulate the prediction results on test set vs truth</span><br><span class="line">table(predict(clustering,testing),testing$Species)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Regression-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/07/Regression-Models/" itemprop="url">
                  Regression Models
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 14:16:32 / Modified: 14:21:13" itemprop="dateCreated datePublished" datetime="2018-08-07T14:16:32+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">161k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">2:27</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="introduction-to-regression">Introduction to Regression</h2>
<ul>
<li>linear regression/linear models <span class="math inline">\(\rightarrow\)</span> go to procedure to analyze data</li>
<li><em>Francis Galton</em> invented the term and concepts of regression and correlation
<ul>
<li>he predicted child’s height from parents height</li>
</ul></li>
<li>questions that regression can help answer
<ul>
<li>prediction of one thing from another</li>
<li>find simple, interpretable, meaningful model to predict the data</li>
<li>quantify and investigate variations that are unexplained or unrelated to the predictor <span class="math inline">\(\rightarrow\)</span> <strong>residual variation</strong></li>
<li>quantify the effects of other factors may have on the outcome</li>
<li>assumptions to generalize findings beyond data we have <span class="math inline">\(\rightarrow\)</span> <strong>statistical inference</strong></li>
<li><strong>regression to the mean</strong> (see below)</li>
</ul></li>
</ul>
<h2 id="notation">Notation</h2>
<ul>
<li>regular letters (i.e. <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>) = generally used to denote <strong>observed</strong> variables</li>
<li>Greek letters (i.e. <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span>) = generally used to denote <strong>unknown</strong> variables that we are trying to estimate</li>
<li><span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> describes <span class="math inline">\(n\)</span> data points</li>
<li><span class="math inline">\(\bar X\)</span>, <span class="math inline">\(\bar Y\)</span> = observed means for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></li>
<li><span class="math inline">\(\hat \beta_0\)</span>, <span class="math inline">\(\hat \beta_1\)</span> = estimators for true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></li>
</ul>
<h3 id="empiricalsample-mean">Empirical/Sample Mean</h3>
<ul>
<li><strong>empirical mean</strong> is defined as <span class="math display">\[\bar X = \frac{1}{n}\sum_{i=1}^n X_i\]</span></li>
<li><strong>centering</strong> the random variable is defined as <span class="math display">\[\tilde X_i = X_i - \bar X\]</span>
<ul>
<li>mean of <span class="math inline">\(\tilde X_i\)</span> = 0</li>
</ul></li>
</ul>
<h3 id="empiricalsample-standard-deviation-variance">Empirical/Sample Standard Deviation &amp; Variance</h3>
<ul>
<li><strong>empirical variance</strong> is defined as <span class="math display">\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X)^2
= \frac{1}{n-1} \left( \sum_{i=1}^n X_i^2 - n \bar X ^ 2 \right)
\Leftarrow \mbox{shortcut for calculation}\]</span></li>
<li><strong>empirical standard deviation</strong> is defined as <span class="math inline">\(S = \sqrt{S^2}\)</span>
<ul>
<li>average squared distances between the observations and the mean</li>
<li>has same units as the data</li>
</ul></li>
<li><strong>scaling</strong> the random variables is defined as <span class="math inline">\(X_i / S\)</span>
<ul>
<li>standard deviation of <span class="math inline">\(X_i / S\)</span> = 1</li>
</ul></li>
</ul>
<h3 id="normalization">Normalization</h3>
<ul>
<li><strong>normalizing</strong> the data/random variable is defined <span class="math display">\[Z_i = \frac{X_i - \bar X}{s}\]</span>
<ul>
<li>empirical mean = 0, empirical standard deviation = 1</li>
<li>distribution centered around 0 and data have units = # of standard deviations away from the original mean
<ul>
<li><strong><em>example</em></strong>: <span class="math inline">\(Z_1 = 2\)</span> means that the data point is 2 standard deviations larger than the original mean</li>
</ul></li>
</ul></li>
<li>normalization makes non-comparable data <strong>comparable</strong></li>
</ul>
<h3 id="empirical-covariance-correlation">Empirical Covariance &amp; Correlation</h3>
<ul>
<li>Let <span class="math inline">\((X_i, Y_i)\)</span> = pairs of data</li>
<li><strong>empirical covariance</strong> is defined as <span class="math display">\[
Cov(X, Y) = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar X) (Y_i - \bar Y) = \frac{1}{n-1}\left( \sum_{i=1}^n X_i Y_i - n \bar X \bar Y\right)
\]</span>
<ul>
<li>has units of <span class="math inline">\(X \times\)</span> units of <span class="math inline">\(Y\)</span></li>
</ul></li>
<li><strong>correlation</strong> is defined as <span class="math display">\[Cor(X, Y) = \frac{Cov(X, Y)}{S_x S_y}\]</span> where <span class="math inline">\(S_x\)</span> and <span class="math inline">\(S_y\)</span> are the estimates of standard deviations for the <span class="math inline">\(X\)</span> observations and <span class="math inline">\(Y\)</span> observations, respectively
<ul>
<li>the value is effectively the covariance standardized into a unit-less quantity</li>
<li><span class="math inline">\(Cor(X, Y) = Cor(Y, X)\)</span></li>
<li><span class="math inline">\(-1 \leq Cor(X, Y) \leq 1\)</span></li>
<li><span class="math inline">\(Cor(X,Y) = 1\)</span> and <span class="math inline">\(Cor(X, Y) = -1\)</span> only when the <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> observations fall perfectly on a positive or negative sloped line, respectively</li>
<li><span class="math inline">\(Cor(X, Y)\)</span> measures the strength of the linear relationship between the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> data, with stronger relationships as <span class="math inline">\(Cor(X,Y)\)</span> heads towards -1 or 1</li>
<li><span class="math inline">\(Cor(X, Y) = 0\)</span> implies no linear relationship</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="daltons-data-and-least-squares">Dalton’s Data and Least Squares</h2>
<ul>
<li>collected data from 1885 in <code>UsingR</code> package</li>
<li>predicting children’s heights from parents’ height</li>
<li>observations from the marginal/individual parent/children distributions</li>
<li>looking only at the children’s dataset to find the best predictor
<ul>
<li>“middle” of children’s dataset <span class="math inline">\(\rightarrow\)</span> best predictor</li>
<li>“middle” <span class="math inline">\(\rightarrow\)</span> center of mass <span class="math inline">\(\rightarrow\)</span> mean of the dataset
<ul>
<li>Let <span class="math inline">\(Y_i\)</span> = height of child <span class="math inline">\(i\)</span> for <span class="math inline">\(i = 1, \ldots, n = 928\)</span>, the “middle” = <span class="math inline">\(\mu\)</span> such that <span class="math display">\[\sum_{i=1}^n (Y_i - \mu)^2\]</span></li>
<li><span class="math inline">\(\mu = \bar Y\)</span> for the above sum to be the smallest <span class="math inline">\(\rightarrow\)</span> <strong>least squares = empirical mean</strong></li>
</ul></li>
<li><em><strong>Note</strong>: <code>manipulate</code> function can help to show this </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># load necessary packages/install if needed</span><br><span class="line">library(ggplot2); library(UsingR); data(galton)</span><br><span class="line"># function to plot the histograms</span><br><span class="line">myHist &lt;- function(mu)&#123;</span><br><span class="line">	# calculate the mean squares</span><br><span class="line">    mse &lt;- mean((galton$child - mu)^2)</span><br><span class="line">    # plot histogram</span><br><span class="line">    g &lt;- ggplot(galton, aes(x = child)) + geom_histogram(fill = &quot;salmon&quot;,</span><br><span class="line">    	colour = &quot;black&quot;, binwidth=1)</span><br><span class="line">    # add vertical line marking the center value mu</span><br><span class="line">    g &lt;- g + geom_vline(xintercept = mu, size = 2)</span><br><span class="line">    g &lt;- g + ggtitle(paste(&quot;mu = &quot;, mu, &quot;, MSE = &quot;, round(mse, 2), sep = &quot;&quot;))</span><br><span class="line">    g</span><br><span class="line">&#125;</span><br><span class="line"># manipulate allows the user to change the variable mu to see how the mean squares changes</span><br><span class="line">#   library(manipulate); manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))]</span><br><span class="line"># plot the correct graph</span><br><span class="line">myHist(mean(galton$child))</span><br></pre></td></tr></table></figure>
<ul>
<li><p>in order to visualize the parent-child height relationship, a scatter plot can be used</p></li>
<li><p><em><strong>Note</strong>: because there are multiple data points for the same parent/child combination, a third dimension (size of point) should be used when constructing the scatter plot </em></p></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">library(dplyr)</span><br><span class="line"># constructs table for different combination of parent-child height</span><br><span class="line">freqData &lt;- as.data.frame(table(galton$child, galton$parent))</span><br><span class="line">names(freqData) &lt;- c(&quot;child (in)&quot;, &quot;parent (in)&quot;, &quot;freq&quot;)</span><br><span class="line"># convert to numeric values</span><br><span class="line">freqData$child &lt;- as.numeric(as.character(freqData$child))</span><br><span class="line">freqData$parent &lt;- as.numeric(as.character(freqData$parent))</span><br><span class="line"># filter to only meaningful combinations</span><br><span class="line">g &lt;- ggplot(filter(freqData, freq &gt; 0), aes(x = parent, y = child))</span><br><span class="line">g &lt;- g + scale_size(range = c(2, 20), guide = &quot;none&quot; )</span><br><span class="line"># plot grey circles slightly larger than data as base (achieve an outline effect)</span><br><span class="line">g &lt;- g + geom_point(colour=&quot;grey50&quot;, aes(size = freq+10, show_guide = FALSE))</span><br><span class="line"># plot the accurate data points</span><br><span class="line">g &lt;- g + geom_point(aes(colour=freq, size = freq))</span><br><span class="line"># change the color gradient from default to lightblue -&gt; $white</span><br><span class="line">g &lt;- g + scale_colour_gradient(low = &quot;lightblue&quot;, high=&quot;white&quot;)</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h3 id="derivation-for-least-squares-empirical-mean-finding-the-minimum">Derivation for Least Squares = Empirical Mean (Finding the Minimum)</h3>
<ul>
<li>Let <span class="math inline">\(X_i =\)</span> <strong>regressor</strong>/<strong>predictor</strong>, and <span class="math inline">\(Y_i =\)</span> <strong>outcome</strong>/<strong>result</strong> so we want to minimize the the squares: <span class="math display">\[\sum_{i=1}^n (Y_i - \mu)^2\]</span></li>
<li>Proof is as follows <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \mu)^2 &amp; = \sum_{i=1}^n (Y_i - \bar Y + \bar Y - \mu)^2 \Leftarrow \mbox{added}  \pm \bar Y \mbox{which is adding 0 to the original equation}\\
(expanding~the~terms)&amp; = \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 \sum_{i=1}^n (Y_i - \bar Y)  (\bar Y - \mu) + \sum_{i=1}^n (\bar Y - \mu)^2 \Leftarrow (Y_i - \bar Y), (\bar Y - \mu) \mbox{ are the terms}\\
(simplifying) &amp; = \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 (\bar Y - \mu) \sum_{i=1}^n (Y_i - \bar Y)  +\sum_{i=1}^n (\bar Y - \mu)^2 \Leftarrow (\bar Y - \mu) \mbox{ does not depend on } i \\
(simplifying) &amp; = \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 (\bar Y - \mu)  (\sum_{i=1}^n Y_i - n \bar Y) +\sum_{i=1}^n (\bar Y - \mu)^2 \Leftarrow \sum_{i=1}^n \bar Y \mbox{ is equivalent to }n \bar Y\\
(simplifying) &amp; = \sum_{i=1}^n (Y_i - \bar Y)^2 + \sum_{i=1}^n (\bar Y - \mu)^2 \Leftarrow \sum_{i=1}^n Y_i - n \bar Y = 0 \mbox{ since } \sum_{i=1}^n Y_i = n \bar Y  \\
\sum_{i=1}^n (Y_i - \mu)^2 &amp; \geq \sum_{i=1}^n (Y_i - \bar Y)^2  \Leftarrow \sum_{i=1}^n (\bar Y - \mu)^2 \mbox{ is always} \geq 0 \mbox{ so we can take it out to form the inequality}
\end{aligned}
\]</span>
<ul>
<li>because of the inequality above, to minimize the sum of the squares <span class="math inline">\(\sum_{i=1}^n (Y_i - \mu)^2\)</span>, <strong><span class="math inline">\(\bar Y\)</span> must be equal to <span class="math inline">\(\mu\)</span></strong></li>
</ul></li>
<li>An alternative approach to finding the minimum is taking the <strong><em>derivative</em></strong> with respect to <span class="math inline">\(\mu\)</span> <span class="math display">\[
\begin{aligned}
\frac{d(\sum_{i=1}^n (Y_i - \mu)^2)}{d\mu} &amp; = 0 \Leftarrow \mbox{setting this equal to 0 to find minimum}\\
-2\sum_{i=1}^n (Y_i - \mu) &amp; = 0 \Leftarrow \mbox{divide by -2 on both sides and move } \mu \mbox{ term over to the right}\\
\sum_{i=1}^n Y_i &amp; = \sum_{i=1}^n \mu \Leftarrow \mbox{for the two sums to be equal, all the terms must be equal}\\
Y_i &amp; = \mu \\
\end{aligned}
\]</span></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="regression-through-the-origin">Regression through the Origin</h2>
<ul>
<li>Let <span class="math inline">\(X_i =\)</span> parents’ heights (<strong>regressor</strong>) and <span class="math inline">\(Y_i =\)</span> children’s heights (<strong>outcome</strong>)</li>
<li>find a line with slope <span class="math inline">\(\beta\)</span> that passes through the origin at (0,0) <span class="math display">\[Y_i = X_i \beta\]</span> such that it minimizes <span class="math display">\[\sum_{i=1}^n (Y_i - X_i \beta)^2\]</span></li>
<li><em><strong>Note</strong>: it is generally a bad practice forcing the line through (0, 0) </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># install grid and png packages if not present</span><br><span class="line">library(png); library(grid); doMC::registerDoMC(cores = 4)</span><br><span class="line">grid.raster(readPNG(&quot;figures/1.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em>Centering Data/Gaussian Elimination</em></strong>
<ul>
<li><em><strong>Note</strong>: this is <strong>different</strong> from regression through the origin, because it is effectively moving the regression line </em></li>
<li>subtracting the means from the <span class="math inline">\(X_i\)</span>s and <span class="math inline">\(Y_i\)</span>s moves the origin (reorienting the axes) to the center of the data set so that a regression line can be constructed</li>
<li><em><strong>Note</strong>: the line constructed here has an <strong>equivalent slope</strong> as the result from linear regression with intercept </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/2.png&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="derivation-for-beta">Derivation for <span class="math inline">\(\beta\)</span></h3>
<ul>
<li><p>Let <span class="math inline">\(Y = \beta X\)</span>, and <span class="math inline">\(\hat \beta\)</span> = estimate of <span class="math inline">\(\beta\)</span>, the slope of the least square regression line <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - X_i \beta)^2 &amp; = \sum_{i=1}^n \left[ (Y_i - X_i \hat \beta) + (X_i \hat \beta - X_i \beta) \right]^2 \Leftarrow \mbox{added }  \pm X_i \hat \beta \mbox{ is effectively adding zero}\\
(expanding~the~terms)&amp; = \sum_{i=1}^n (Y_i - X_i \hat \beta)^2 + 2 \sum_{i=1}^n (Y_i - X_i \hat \beta)(X_i \hat \beta - X_i \beta) + \sum_{i=1}^n (X_i \hat \beta - X_i \beta)^2 \\
\sum_{i=1}^n (Y_i - X_i \beta)^2 &amp; \geq \sum_{i=1}^n (Y_i - X_i \hat \beta)^2 + 2 \sum_{i=1}^n (Y_i - X_i \hat \beta)(X_i \hat \beta - X_i \beta) \Leftarrow \sum_{i=1}^n (X_i \hat \beta - X_i \beta)^2 \mbox{ is always positive}\\
&amp; (ignoring~the~second~term~for~now,~for~\hat \beta ~to~be~the~minimizer~of~the~squares, \\
&amp; the~following~must~be~true)\\
\sum_{i=1}^n (Y_i - X_i \beta)^2  &amp; \geq \sum_{i=1}^n (Y_i - X_i \hat \beta)^2 \Leftarrow \mbox{every other } \beta \mbox{ value creates a least square criteria that is }\geq \hat \beta\\
(this~means)&amp; \Rightarrow 2 \sum_{i=1}^n (Y_i - X_i \hat \beta)(X_i \hat \beta - X_i \beta) = 0\\
(simplifying)&amp; \Rightarrow \sum_{i=1}^n (Y_i - X_i \hat \beta) X_i (\hat \beta - \beta) = 0 \Leftarrow (\hat \beta - \beta) \mbox{ does not depend on }i\\
(simplifying)&amp; \Rightarrow \sum_{i=1}^n (Y_i - X_i \hat \beta) X_i = 0 \\
(solving~for~\hat \beta) &amp; \Rightarrow \hat \beta = \frac{\sum_{i=1}^n Y_i X_i}{\sum_{i=1}^n X_i^2} = \beta\\
\end{aligned}
\]</span></p></li>
<li><strong><em>example</em></strong>
<ul>
<li>Let <span class="math inline">\(X_1, X_2, \ldots , X_n = 1\)</span> <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - X_i \beta)^2 = \sum_{i=1}^n (Y_i - \beta)^2 \\
\Rightarrow \hat \beta = \frac{\sum_{i=1}^n Y_i X_i}{\sum_{i=1}^n X_i^2} = \frac{\sum_{i=1}^n Y_i}{\sum_{i=1}^n 1} = \frac{\sum_{i=1}^n Y_i}{n} = \bar Y
\end{aligned}
\]</span></li>
<li><em><strong>Note</strong>: this is the result from our previous derivation for least squares = empirical mean </em></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="finding-the-best-fit-line-ordinary-least-squares">Finding the Best Fit Line (Ordinary Least Squares)</h2>
<ul>
<li>best fitted line for predictor, <span class="math inline">\(X\)</span>, and outcome, <span class="math inline">\(Y\)</span> is derived from the <strong>least squares</strong> <span class="math display">\[\sum_{i=1}^n \{Y_i - (\beta_0 + \beta_1 X_i)\}^2\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/3.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>each of the data point contributes equally to the error between the their locations and the regression line <span class="math inline">\(\rightarrow\)</span> goal of regression is to <strong><em>minimize</em></strong> this error</li>
</ul>
<h3 id="least-squares-model-fit">Least Squares Model Fit</h3>
<ul>
<li>model fit = <span class="math inline">\(Y = \beta_0 + \beta_1 X\)</span> through the data pairs <span class="math inline">\((X_i, Y_i)\)</span> where <span class="math inline">\(Y_i\)</span> as the outcome
<ul>
<li><em><strong>Note</strong>: this is the model that we use to guide our <strong>estimated</strong> best fit (see below) </em></li>
</ul></li>
<li>best fit line with estimated slope and intercept (<span class="math inline">\(X\)</span> as predictor, <span class="math inline">\(Y\)</span> as outcome) <span class="math inline">\(\rightarrow\)</span> <span class="math display">\[Y = \hat \beta_0 + \hat \beta_1 X\]</span> where <span class="math display">\[\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X\]</span>
<ul>
<li>[<strong><em>slope</em></strong>] <span class="math inline">\(\hat \beta_1\)</span> has the units of <span class="math inline">\(Y / X\)</span>
<ul>
<li><span class="math inline">\(Cor(Y, X)\)</span> = unit-less</li>
<li><span class="math inline">\(Sd(Y)\)</span> = has units of <span class="math inline">\(Y\)</span></li>
<li><span class="math inline">\(Sd(X)\)</span> = has units of <span class="math inline">\(X\)</span></li>
</ul></li>
<li>[<strong><em>intercept</em></strong>] <span class="math inline">\(\hat \beta_0\)</span> has the units of <span class="math inline">\(Y\)</span></li>
<li>the line passes through the point <span class="math inline">\((\bar X, \bar Y\)</span>)
<ul>
<li>this is evident from equation for <span class="math inline">\(\beta_0\)</span> (rearrange equation)</li>
</ul></li>
</ul></li>
<li>best fit line with <span class="math inline">\(X\)</span> as outcome and <span class="math inline">\(Y\)</span> as predictor has slope, <span class="math inline">\(\hat \beta_1 = Cor(Y, X) Sd(X)/ Sd(Y)\)</span>.</li>
<li>slope of best fit line = slope of best fit line through the origin for centered data <span class="math inline">\((X_i - \bar X, Y_i - \bar Y)\)</span></li>
<li>slope of best fit line for normalized the data, <span class="math inline">\(\{ \frac{X_i - \bar X}{Sd(X)}, \frac{Y_i - \bar Y}{Sd(Y)}\}\)</span> = <span class="math inline">\(Cor(Y, X)\)</span></li>
</ul>
<h3 id="derivation-for-beta_0-and-beta_1">Derivation for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></h3>
<ul>
<li>Let <span class="math inline">\(Y = \beta_0 + \beta_1 X\)</span>, and <span class="math inline">\(\hat \beta_0\)</span>/<span class="math inline">\(\hat \beta_1 =\)</span> estimates <span class="math inline">\(\beta_0\)</span>/<span class="math inline">\(\beta_1\)</span>, the intercept and slope of the least square regression line, respectively <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2 &amp; = \sum_{i=1}^n (Y_i^* - \beta_0)^2 ~~~where~ Y_i^* = Y_i - \beta_1 X_i\\
solution~for~\sum_{i=1}^n (Y_i^* - \beta_0)^2 &amp; \Rightarrow \hat \beta_0 = \frac{\sum_{i=1}^n Y_i^*}{n} = \frac{\sum_{i=1}^n Y_i - \beta_1 X_i}{n}\\
&amp; \Rightarrow \hat \beta_0 = \frac{\sum_{i=1}^n Y_i}{n} - \beta_1 \frac{\sum_{i=1}^n X_i}{n}\\
&amp; \Rightarrow \hat \beta_0 = \bar Y - \beta_1 \bar X\\
\Longrightarrow \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2 &amp; = \sum_{i=1}^n \left[Y_i - (\bar Y - \beta_1 \bar X) - \beta_1 X_i \right]^2\\
&amp; = \sum_{i=1}^n \left[(Y_i - \bar Y) - (X_i - \bar X)\beta_1 \right]^2\\
&amp; = \sum_{i=1}^n \left[\tilde Y_i - \tilde X_i \beta_1 \right]^2 ~~~ where~ \tilde Y_i = Y_i - \bar Y, \tilde X_i = X_i - \bar X\\
&amp; \Rightarrow \hat \beta_1 = \frac{\sum_{i=1}^n \tilde Y_i \tilde X_i}{\sum_{i=1}^n \tilde{X_i}^2} = \frac{(Y_i - \bar Y)(X_i - \bar X)}{\sum_{i=1}^n (X_i - \bar X)^2}\\
&amp; \Rightarrow \hat \beta_1 = \frac{(Y_i - \bar Y)(X_i - \bar X)/(n-1)}{\sum_{i=1}^n (X_i - \bar X)^2/(n-1)} = \frac{Cov(Y, X)}{Var(X)}\\
&amp; \Rightarrow \hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)}\\
&amp; \Rightarrow \hat \beta_0 = \bar Y - \hat \beta_1 \bar X\\
\end{aligned}
\]</span></li>
</ul>
<h3 id="examples-and-r-commands">Examples and R Commands</h3>
<ul>
<li><span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span> can be manually calculated through the above formulas</li>
<li><code>coef(lm(y ~ x)))</code> = R command to run the least square regression model on the data with <code>y</code> as the outcome, and <code>x</code> as the regressor
<ul>
<li><code>coef()</code> = returns the slope and intercept coefficients of the <code>lm</code> results</li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># outcome</span><br><span class="line">y &lt;- galton$child</span><br><span class="line"># regressor</span><br><span class="line">x &lt;- galton$parent</span><br><span class="line"># slope</span><br><span class="line">beta1 &lt;- cor(y, x) * sd(y) / sd(x)</span><br><span class="line"># intercept</span><br><span class="line">beta0 &lt;- mean(y) - beta1 * mean(x)</span><br><span class="line"># results are the same as using the lm command</span><br><span class="line">results &lt;- rbind(&quot;manual&quot; = c(beta0, beta1), &quot;lm(y ~ x)&quot; = coef(lm(y ~ x)))</span><br><span class="line"># set column names</span><br><span class="line">colnames(results) &lt;- c(&quot;intercept&quot;, &quot;slope&quot;)</span><br><span class="line"># print results</span><br><span class="line">results</span><br></pre></td></tr></table></figure>
<ul>
<li>slope of the best fit line = slope of best fit line through the origin for centered data</li>
<li><code>lm(y ~ x - 1)</code> = forces a regression line to go through the origin (0, 0)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># centering y</span><br><span class="line">yc &lt;- y - mean(y)</span><br><span class="line"># centering x</span><br><span class="line">xc &lt;- x - mean(x)</span><br><span class="line"># slope</span><br><span class="line">beta1 &lt;- sum(yc * xc) / sum(xc ^ 2)</span><br><span class="line"># results are the same as using the lm command</span><br><span class="line">results &lt;- rbind(&quot;centered data (manual)&quot; = beta1, &quot;lm(y ~ x)&quot; = coef(lm(y ~ x))[2],</span><br><span class="line">	&quot;lm(yc ~ xc - 1)&quot; = coef(lm(yc ~ xc - 1))[1])</span><br><span class="line"># set column names</span><br><span class="line">colnames(results) &lt;- c(&quot;slope&quot;)</span><br><span class="line"># print results</span><br><span class="line">results</span><br></pre></td></tr></table></figure>
<ul>
<li>slope of best fit line for normalized the data = <span class="math inline">\(Cor(Y, X)\)</span></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># normalize y</span><br><span class="line">yn &lt;- (y - mean(y))/sd(y)</span><br><span class="line"># normalize x</span><br><span class="line">xn &lt;- (x - mean(x))/sd(x)</span><br><span class="line"># compare correlations</span><br><span class="line">results &lt;- rbind(&quot;cor(y, x)&quot; = cor(y, x), &quot;cor(yn, xn)&quot; = cor(yn, xn),</span><br><span class="line">	&quot;slope&quot; = coef(lm(yn ~ xn))[2])</span><br><span class="line"># print results</span><br><span class="line">results</span><br></pre></td></tr></table></figure>
<ul>
<li><code>geom_smooth(method = &quot;lm&quot;, formula = y~x)</code> function in <code>ggplot2</code> = adds regression line and confidence interval to graph
<ul>
<li><code>formula = y~x</code> = default for the line (argument can be eliminated if <code>y~x</code> produces the line you want)</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># constructs table for different combination of parent-child height</span><br><span class="line">freqData &lt;- as.data.frame(table(galton$child, galton$parent))</span><br><span class="line">names(freqData) &lt;- c(&quot;child (in)&quot;, &quot;parent (in)&quot;, &quot;freq&quot;)</span><br><span class="line"># convert to numeric values</span><br><span class="line">freqData$child &lt;- as.numeric(as.character(freqData$child))</span><br><span class="line">freqData$parent &lt;- as.numeric(as.character(freqData$parent))</span><br><span class="line">g &lt;- ggplot(filter(freqData, freq &gt; 0), aes(x = parent, y = child))</span><br><span class="line">g &lt;- g + scale_size(range = c(2, 20), guide = &quot;none&quot; )</span><br><span class="line">g &lt;- g + geom_point(colour=&quot;grey50&quot;, aes(size = freq+10, show_guide = FALSE))</span><br><span class="line">g &lt;- g + geom_point(aes(colour=freq, size = freq))</span><br><span class="line">g &lt;- g + scale_colour_gradient(low = &quot;lightblue&quot;, high=&quot;white&quot;)</span><br><span class="line">g &lt;- g + geom_smooth(method=&quot;lm&quot;, formula=y~x)</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="regression-to-the-mean">Regression to the Mean</h2>
<ul>
<li>first investigated by Francis Galton in the paper <em>“Regression towards mediocrity in hereditary stature” The Journal of the Anthropological Institute of Great Britain and Ireland , Vol. 15, (1886)</em></li>
<li><strong>regression to the mean</strong> was invented by Fancis Galton to capture the following phenomena
<ul>
<li>children of tall parents tend to be tall, but not as tall as their parents</li>
<li>children of short parents tend to be short, but not as short as their parents</li>
<li>parents of very short children, tend to be short, but not as short as their child</li>
<li>parents of very tall children, tend to be tall, but not as tall as their children</li>
</ul></li>
<li>in thinking of the extremes, the following are true
<ul>
<li><span class="math inline">\(P(Y &lt; x | X = x)\)</span> gets bigger as <span class="math inline">\(x\)</span> heads to very large values
<ul>
<li>in other words, given that the value of X is already very large (extreme), the chance that the value of Y is as large or larger than that of X is small (unlikely)</li>
</ul></li>
<li>similarly, <span class="math inline">\(P(Y &gt; x | X = x)\)</span> gets bigger as <span class="math inline">\(x\)</span> heads to very small values
<ul>
<li>in other words, given that the value of X is already very small (extreme), the chance that the value of Y is as small or smaller than that of X is small (unlikely)</li>
</ul></li>
</ul></li>
<li>when constructing regression lines between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the line represents the intrinsic relationship (“mean”) between the variables, but does not capture the extremes (“noise”)
<ul>
<li>unless <span class="math inline">\(Cor(Y, X) = 1\)</span>, the regression line or the intrinsic part of the relationship between variables won’t capture all of the variation (some noise exists)</li>
</ul></li>
</ul>
<h3 id="daltons-investigation-on-regression-to-the-mean">Dalton’s Investigation on Regression to the Mean</h3>
<ul>
<li>both <span class="math inline">\(X\)</span>, child’s heights, and <span class="math inline">\(Y\)</span>, parent’s heights, are <strong>normalized</strong> so that they mean of 0 and variance of 1</li>
<li>regression lines must pass <span class="math inline">\((\bar X, \bar Y)\)</span> or <span class="math inline">\((0, 0)\)</span> in this case</li>
<li>slope of regression line = <span class="math inline">\(Cor(Y,X)\)</span> regardless of which variable is the outcome/regressor (because standard deviations of both variables = 1)
<ul>
<li><em><strong>Note</strong>: however, for both regression lines to be plotted on the same graph, the second line’s slope must be <span class="math inline">\(1/Cor(Y, X)\)</span> because the two relationships have flipped axes </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># load father.son data</span><br><span class="line">data(father.son)</span><br><span class="line"># normalize son&apos;s height</span><br><span class="line">y &lt;- (father.son$sheight - mean(father.son$sheight)) / sd(father.son$sheight)</span><br><span class="line"># normalize father&apos;s height</span><br><span class="line">x &lt;- (father.son$fheight - mean(father.son$fheight)) / sd(father.son$fheight)</span><br><span class="line"># calculate correlation</span><br><span class="line">rho &lt;- cor(x, y)</span><br><span class="line"># plot the relationship between the two</span><br><span class="line">g = ggplot(data.frame(x = x, y = y), aes(x = x, y = y))</span><br><span class="line">g = g + geom_point(size = 3, alpha = .2, colour = &quot;black&quot;)</span><br><span class="line">g = g + geom_point(size = 2, alpha = .2, colour = &quot;red&quot;)</span><br><span class="line">g = g + xlim(-4, 4) + ylim(-4, 4)</span><br><span class="line"># reference line for perfect correlation between</span><br><span class="line"># variables (data points will like on diagonal line)</span><br><span class="line">g = g + geom_abline(position = &quot;identity&quot;)</span><br><span class="line"># if there is no correlation between the two variables,</span><br><span class="line"># the data points will lie on horizontal/vertical lines</span><br><span class="line">g = g + geom_vline(xintercept = 0)</span><br><span class="line">g = g + geom_hline(yintercept = 0)</span><br><span class="line"># plot the actual correlation for both</span><br><span class="line">g = g + geom_abline(intercept = 0, slope = rho, size = 2)</span><br><span class="line">g = g + geom_abline(intercept = 0, slope = 1 / rho, size = 2)</span><br><span class="line"># add appropriate labels</span><br><span class="line">g = g + xlab(&quot;Father&apos;s height, normalized&quot;)</span><br><span class="line">g = g + ylab(&quot;Son&apos;s height, normalized&quot;)</span><br><span class="line">g = g + geom_text(x = 3.8, y = 1.6, label=&quot;x~y&quot;, angle = 25) +</span><br><span class="line">    geom_text(x = 3.2, y = 3.6, label=&quot;cor(y,x)=1&quot;, angle = 35) +</span><br><span class="line">    geom_text(x = 1.6, y = 3.8, label=&quot;y~x&quot;, angle = 60)</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="statistical-linear-regression-models">Statistical Linear Regression Models</h2>
<ul>
<li>goal is use statistics to draw inferences <span class="math inline">\(\rightarrow\)</span> generalize from data to population through models</li>
<li><strong>probabilistic model for linear regression</strong> <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_{i}\]</span> where <span class="math inline">\(\epsilon_{i}\)</span> represents the sampling errors and is assumed to be iid <span class="math inline">\(N(0, \sigma^2)\)</span></li>
<li>this model has the following properties
<ul>
<li><span class="math inline">\(E[Y_i ~|~ X_i = x_i] = E[\beta_0] + E[\beta_1 x_i] + E[\epsilon_{i}] = \mu_i = \beta_0 + \beta_1 x_i\)</span></li>
<li><span class="math inline">\(Var(Y_i ~|~ X_i = x_i) = Var(\beta_0 + \beta_1 x_i) + Var(\epsilon_{i}) = Var(\epsilon_{i}) = \sigma^2\)</span>
<ul>
<li><span class="math inline">\(\beta_0 + \beta_1 x_i\)</span> = line = constant/no variance</li>
</ul></li>
</ul></li>
<li>it can then be said to have <span class="math inline">\(Y_i\)</span> as independent <span class="math inline">\(N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\mu = \beta_0 + \beta_1 x_i\)</span> &lt;– likelihood equivalent model
<ul>
<li><strong>likelihood</strong> = given the outcome, what is the probability?
<ul>
<li>in this case, the likelihood is as follows <span class="math display">\[\mathcal{L}(\beta_0, \beta_1, \sigma) = \prod_{i=1}^{n} \left\{(2\pi \sigma^2)^{-1/2}\exp \left(- \frac{1}{2\sigma^2}(y_i - \mu_i)^2\right)\right\}\]</span> where <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_i\)</span></li>
<li>above is the <strong>probability density function </strong> of <span class="math inline">\(n\)</span> samples from the normal distribution <span class="math inline">\(\rightarrow\)</span> this is because the regression line is normally distributed due to <span class="math inline">\(\epsilon_{i}\)</span></li>
</ul></li>
<li><strong>maximum likelihood estimator (MLE)</strong> = most likely estimate of the population parameter/probability
<ul>
<li>in this case, the maximum likelihood = -2 minimum natural log (<span class="math inline">\(ln\)</span>, base <span class="math inline">\(e\)</span>) likelihood <span class="math display">\[-2 \log{\mathcal{L}(\beta_0, \beta_1, \sigma)} = n\log(2\pi \sigma^2) + \frac{1}{\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2\]</span>
<ul>
<li>since everything else is constant, minimizing this function would only depend on <span class="math inline">\(\sum_{i=1}^n (y_i - \mu_i)^2\)</span>, which from our previous derivations yields <span class="math inline">\(\hat \mu_i = \beta_0 + \beta_1 \hat x_i\)</span></li>
</ul></li>
<li><strong><em>maximum likelihood estimate</em></strong> = <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_i\)</span></li>
</ul></li>
</ul></li>
</ul>
<h3 id="interpreting-regression-coefficients">Interpreting Regression Coefficients</h3>
<ul>
<li>for the linear regression line <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_{i}\]</span> MLE for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are as follows <span class="math display">\[\hat \beta_1 = Cor(Y, X)\frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X\]</span></li>
<li><strong><span class="math inline">\(\beta_0\)</span></strong> = expected value of the outcome/response when the predictor is 0 <span class="math display">\[
E[Y | X = 0] =  \beta_0 + \beta_1 \times 0 = \beta_0
\]</span>
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(X=0\)</span> may not always be of interest as it may be impossible/outside of data range (i.e blood pressure, height etc.) </em></li>
<li>it may be useful to move the intercept at times <span class="math display">\[
\begin{aligned}
Y_i &amp;= \beta_0 + \beta_1 X_i + \epsilon_i\\
&amp;= \beta_0 + a \beta_1 + \beta_1 (X_i - a) + \epsilon_i \\
&amp;= \tilde \beta_0 + \beta_1 (X_i - a) + \epsilon_i ~~~where \tilde \beta_0 = \beta_0 + a \beta_1\\
\end{aligned}
\]</span></li>
<li><em><strong>Note</strong>: shifting <span class="math inline">\(X\)</span> values by value <span class="math inline">\(a\)</span> changes the intercept, but not the slope </em></li>
<li>often, <span class="math inline">\(a\)</span> is set to <span class="math inline">\(\bar X\)</span> so that the intercept is interpreted as the expected response at the average <span class="math inline">\(X\)</span> value</li>
</ul></li>
<li><strong><span class="math inline">\(\beta_1\)</span></strong> = expected change in outcome/response for a 1 unit change in the predictor<span class="math display">\[
E[Y ~|~ X = x+1] - E[Y ~|~ X = x] =
\beta_0 + \beta_1 (x + 1) - (\beta_0 + \beta_1 x ) = \beta_1
\]</span>
<ul>
<li>sometimes it is useful to change the units of <span class="math inline">\(X\)</span> <span class="math display">\[
\begin{aligned}
Y_i &amp; = \beta_0 + \beta_1 X_i + \epsilon_i \\
&amp; = \beta_0 + \frac{\beta_1}{a} (X_i a) + \epsilon_i \\
&amp; = \beta_0 + \tilde \beta_1 (X_i a) + \epsilon_i \\
\end{aligned}
\]</span></li>
<li>multiplication of <span class="math inline">\(X\)</span> by a factor <span class="math inline">\(a\)</span> results in dividing the coefficient by a factor of <span class="math inline">\(a\)</span></li>
<li><strong><em>example</em></strong>:
<ul>
<li><span class="math inline">\(X\)</span> = height in <span class="math inline">\(m\)</span></li>
<li><span class="math inline">\(Y\)</span> = weight in <span class="math inline">\(kg\)</span></li>
<li><span class="math inline">\(\beta_1\)</span> has units of <span class="math inline">\(kg/m\)</span></li>
<li>converting <span class="math inline">\(X\)</span> to <span class="math inline">\(cm\)</span> <span class="math inline">\(\Longrightarrow\)</span> multiplying <span class="math inline">\(X\)</span> by <span class="math inline">\(100 \frac{cm}{m}\)</span></li>
<li>this mean <span class="math inline">\(\beta_1\)</span> has to be divided by <span class="math inline">\(100 \frac{cm}{m}\)</span> for the correct units. <span class="math display">\[
X~m \times 100\frac{cm}{m} = (100~X) cm
~~\mbox{and}~~
\beta_1 \frac{kg}{m} \times \frac{1}{100}\frac{m}{cm} = \left(\frac{\beta_1}{100}\right)\frac{kg}{cm}
\]</span></li>
</ul></li>
</ul></li>
<li>95% confidence intervals for the coefficients can be constructed from the coefficients themselves and their standard errors (from <code>summary(lm)</code>)
<ul>
<li>use the resulting intervals to evaluate the significance of the results</li>
</ul></li>
</ul>
<h3 id="use-regression-coefficients-for-prediction">Use Regression Coefficients for Prediction</h3>
<ul>
<li>for observed values of the predictor, <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span>, the prediction of the outcome/response is as follows <span class="math display">\[\hat \mu_i = \hat Y_i = \hat \beta_0 + \hat \beta_1 X\]</span> where <span class="math inline">\(\mu_i\)</span> describes a point on the regression line</li>
</ul>
<h3 id="example-and-r-commands">Example and R Commands</h3>
<ul>
<li><code>diamond</code> dataset from <code>UsingR</code> package
<ul>
<li>diamond prices in Singapore Dollars, diamond weight in carats (standard measure of diamond mass, 0.2g)</li>
</ul></li>
<li><code>lm(price ~ I(carat - mean(carat)), data=diamond)</code> = mean centered linear regression
<ul>
<li><em><strong>Note</strong>: arithmetic operations must be enclosed in <code>I()</code> to work </em></li>
</ul></li>
<li><code>predict(fitModel, newdata=data.frame(carat=c(0, 1, 2)))</code> = returns predicted outcome from the given model (linear in our case) at the provided points within the <code>newdata</code> data frame
<ul>
<li>if <code>newdata</code> is unspecified (argument omitted), then <code>predict</code> function will return predicted values for all values of the predictor (x variable, carat in this case)
<ul>
<li><em><strong>Note</strong>: <code>newdata</code> has to be a dataframe, and the values you would like to predict (x variable, <code>carat</code> in this case) has to be specified, or the system won’t know what to do with the provided values </em></li>
</ul></li>
</ul></li>
<li><code>summary(fitModel)</code> = prints detailed summary of linear model</li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># standard linear regression for price vs carat</span><br><span class="line">fit &lt;- lm(price ~ carat, data = diamond)</span><br><span class="line"># intercept and slope</span><br><span class="line">coef(fit)</span><br><span class="line"># mean-centered regression</span><br><span class="line">fit2 &lt;- lm(price ~ I(carat - mean(carat)), data = diamond)</span><br><span class="line"># intercept and slope</span><br><span class="line">coef(fit2)</span><br><span class="line"># regression with more granular scale (1/10th carat)</span><br><span class="line">fit3 &lt;- lm(price ~ I(carat * 10), data = diamond)</span><br><span class="line"># intercept and slope</span><br><span class="line">coef(fit3)</span><br><span class="line"># predictions for 3 values</span><br><span class="line">newx &lt;- c(0.16, 0.27, 0.34)</span><br><span class="line"># manual calculations</span><br><span class="line">coef(fit)[1] + coef(fit)[2] * newx</span><br><span class="line"># prediction using the predict function</span><br><span class="line">predict(fit, newdata = data.frame(carat = newx))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>interpretation</strong>
<ul>
<li>we expect <code>r round(coef(fit)[2], 2)</code> (SIN) dollar increase in price for <strong><em>every carat increase</em></strong> in mass of diamond</li>
<li>or <code>r round(coef(fit3)[2], 2)</code> (SIN) dollar increase in price for <strong><em>every 1/10 carat</em></strong> increase in mass of diamond</li>
</ul></li>
<li><strong>prediction</strong>
<ul>
<li>for 0.16, 0.27, and 0.34 carats, we predict the prices to be <code>r round(as.numeric(predict(fit, newdata = data.frame(carat = newx))),2)</code> (SIN) dollars</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># plot the data points</span><br><span class="line">plot(diamond$carat, diamond$price, xlab = &quot;Mass (carats)&quot;, ylab = &quot;Price (SIN $)&quot;,</span><br><span class="line">     bg = &quot;lightblue&quot;, col = &quot;black&quot;, cex = 1.1, pch = 21,frame = FALSE)</span><br><span class="line"># plot linear regression line</span><br><span class="line">abline(fit, lwd = 2)</span><br><span class="line"># plot predictions for every value of carat (in red)</span><br><span class="line">points(diamond$carat, predict(fit), pch = 19, col = &quot;red&quot;)</span><br><span class="line"># add guidelines for predictions for 0.16, 0.27, and 0.34</span><br><span class="line">lines(c(0.16, 0.16, 0.12), c(200, coef(fit)[1] + coef(fit)[2] * 0.16,</span><br><span class="line">      coef(fit)[1] + coef(fit)[2] * 0.16))</span><br><span class="line">lines(c(0.27, 0.27, 0.12), c(200, coef(fit)[1] + coef(fit)[2] * 0.27,</span><br><span class="line">      coef(fit)[1] + coef(fit)[2] * 0.27))</span><br><span class="line">lines(c(0.34, 0.34, 0.12), c(200, coef(fit)[1] + coef(fit)[2] * 0.34,</span><br><span class="line">      coef(fit)[1] + coef(fit)[2] * 0.34))</span><br><span class="line"># add text labels</span><br><span class="line">text(newx+c(0.03, 0, 0), rep(250, 3), labels = newx, pos = 2)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h3 id="derivation-for-maximum-likelihood-estimator">Derivation for Maximum Likelihood Estimator</h3>
<ul>
<li><em><strong>Note</strong>: this derivation is for the maximum likelihood estimator of the mean, <span class="math inline">\(\mu\)</span>, of a normal distribution as it is the basis of the linear regression model </em></li>
<li><strong>linear regression model</strong> <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_{i}\]</span> follows a normal distribution because <span class="math inline">\(\epsilon_{i} \sim N(0, \sigma^2)\)</span></li>
<li>for the above model, <span class="math inline">\(E[Y_i] = \mu_i = \beta_0 + \beta_1 X_i\)</span> and <span class="math inline">\(Var(Y_i) = \sigma^2\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/4.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>the <strong>probability density function (pdf)</strong> for an outcome <span class="math inline">\(x\)</span> from the normal distribution is defined as <span class="math display">\[f(x~|~\mu, \sigma^2) = (2\pi \sigma^2)^{-1/2}\exp \left(- \frac{1}{2\sigma^2}(y_i - \mu_i)^2\right)\]</span></li>
<li>the corresponding pdf for <span class="math inline">\(n\)</span> iid normal random outcomes <span class="math inline">\(x_1, \ldots, x_n\)</span> is defined as <span class="math display">\[f(x_1, \ldots, x_n~|~\mu, \sigma^2) = \prod_{i=1}^n (2\pi \sigma^2)^{-1/2}\exp \left(- \frac{1}{2\sigma^2}(y_i - \mu_i)^2\right)\]</span> which is also known as the <strong>likelihood function</strong>, denoted in this case as <span class="math inline">\(\mathcal{L}(\mu, \sigma)\)</span></li>
<li>to find the <strong>maximum likelihood estimator (MLE)</strong> of the mean, <span class="math inline">\(\mu\)</span>, we take the derivative of the likelihood <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mu}\)</span></li>
<li>since derivatives of products are quite complex to compute, taking the <span class="math inline">\(\log\)</span> (base <span class="math inline">\(e\)</span>) makes the calculation much simpler
<ul>
<li><span class="math inline">\(\log\)</span> properties:
<ul>
<li><span class="math inline">\(\log(AB) = \log(A) + \log(B)\)</span></li>
<li><span class="math inline">\(\log(A^B) = B\log(A)\)</span></li>
</ul></li>
<li>because <span class="math inline">\(\log\)</span> is always increasing and <strong>monotonic</strong>, or preserves order, finding the maximum MLE <strong>=</strong> finding th maximum of <span class="math inline">\(\log\)</span> transformation of MLE</li>
</ul></li>
<li>-2 <span class="math inline">\(\log\)</span> of <strong>likelihood function</strong> <span class="math display">\[
\begin{aligned}
\log(\mathcal{L}(\mu, \sigma)) &amp; = \sum_{i=1}^n -\frac{1}{2}\log(2\pi \sigma^2) - \frac{1}{2\sigma^2}(y_i - \mu_i)^2 \Leftarrow \mbox{multiply -2 on both sides} \\
-2\log(\mathcal{L}(\mu, \sigma)) &amp; = \sum_{i=1}^n \log(2\pi \sigma^2) + \frac{1}{\sigma^2}(y_i - \mu_i)^2 \Leftarrow \sigma^2 \mbox{ does not depend on }i \\
-2\log(\mathcal{L}(\mu, \sigma)) &amp; = n\log(2\pi \sigma^2) + \frac{1}{\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2\\
\end{aligned}
\]</span></li>
<li>minimizing -2 <span class="math inline">\(\log\)</span> likelihood is <strong><em>computationally equivalent</em></strong> as maximizing <span class="math inline">\(\log\)</span> likelihood <span class="math display">\[
\begin{aligned}
\frac{\partial \log(\mathcal{L}(\mu, \sigma))}{\partial \mu} &amp; = \frac{1}{\sigma^2} \frac{\partial \sum_{i=1}^n (y_i - \mu_i)^2}{\partial \mu} = 0 \Leftarrow \mbox{setting this equal to 0 to find minimum}\\
\Rightarrow -\frac{2}{\sigma^2}\sum_{i=1}^n (y_i - \mu_i) &amp; = 0 \Leftarrow \mbox{divide by }-2/\sigma^2 \mbox{ on both sides}\\
\sum_{i=1}^n (y_i - \mu_i) &amp; = 0 \Leftarrow \mbox{move } \mu \mbox{ term over to the right}\\
\sum_{i=1}^n y_i &amp; = \sum_{i=1}^n \mu_i \Leftarrow \mbox{for the two sums to be equal, all the terms must be equal}\\
y_i &amp; = \mu_i \\
\end{aligned}
\]</span></li>
<li>in the case of our linear regression, <span class="math inline">\(\mu_i = \beta_0 + \beta_1 X_i\)</span> so <span class="math display">\[
\begin{aligned}
\frac{\partial \mathcal{L}(\mu, \sigma))}{\partial \mu} = \frac{\partial \mathcal{L}(\beta_0, \beta_1, \sigma))}{\partial \mu} \\
\mbox{MLE} \Rightarrow Y_i &amp; = \mu_i \\
\mu_i &amp;= \beta_0 + \beta_1 X_i\\
\end{aligned}
\]</span></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="residuals">Residuals</h2>
<ul>
<li>Residual, <span class="math inline">\(e_i\)</span> = difference between the observed and predicted outcome <span class="math display">\[e_i = Y_i - \hat Y_i\]</span>
<ul>
<li>Or, vertical distance between observed data point and regression line</li>
<li>Least squares minimizes <span class="math inline">\(\sum_{i=1}^n e_i^2\)</span></li>
</ul></li>
<li><span class="math inline">\(e_i\)</span> can be interpreted as estimates of the regression error, <span class="math inline">\(\epsilon_i\)</span></li>
<li><span class="math inline">\(e_i\)</span> can also be interpreted as the outcome (<span class="math inline">\(Y\)</span>) with the linear association of the predictor (<span class="math inline">\(X\)</span>) removed
<ul>
<li>or, “Y adjusted for X”</li>
</ul></li>
<li><span class="math inline">\(E[e_i] = 0\)</span> <span class="math inline">\(\rightarrow\)</span> this is because the mean of the residuals is expected to be 0 (assumed Gaussian distribution)
<ul>
<li>the Gaussian distribution assumption also implies that the error is <strong>NOT</strong> correlated with any predictors</li>
<li><code>mean(fitModel$residuals)</code> = returns mean of residuals <span class="math inline">\(\rightarrow\)</span> should equal to 0</li>
<li><code>cov(fit$residuals, predictors)</code> = returns the covariance (measures correlation) of residuals and predictors <span class="math inline">\(\rightarrow\)</span> should also equal to 0</li>
</ul></li>
<li><span class="math inline">\(\sum_{i=1}^n e_i = 0\)</span> (if an intercept is included) and <span class="math inline">\(\sum_{i=1}^n e_i X_i = 0\)</span> (if a regressor variable <span class="math inline">\(X_i\)</span> is included)</li>
<li>for standard linear regression model
<ul>
<li>positive residuals = above the line</li>
<li>negative residuals = below</li>
</ul></li>
<li>residuals/residual plots can highlight poor model fit</li>
</ul>
<h3 id="estimating-residual-variation">Estimating Residual Variation</h3>
<ul>
<li>residual variation measures how well the regression line fit the data points</li>
<li><strong>MLE of variance</strong>, <span class="math inline">\(\sigma^2\)</span>, of the linear model = <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n e_i^2\)</span> or the <strong><em>average squared residual/mean squared error</em></strong>
<ul>
<li>the square root of the estimate, <span class="math inline">\(\sigma\)</span>, = <strong>root mean squared error (RMSE)</strong></li>
</ul></li>
<li>however, a more common approach is to use <span class="math display">\[\hat \sigma^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2\]</span>
<ul>
<li><span class="math inline">\(n-2\)</span> is used instead of <span class="math inline">\(n\)</span> to make the estimator <strong><em>unbiased</em></strong> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(E[\hat \sigma^2] = \sigma^2\)</span></li>
<li><em><strong>Note</strong>: the -2 is accounting for the degrees of freedom for intercept and slope, which had to be estimated </em></li>
</ul></li>
<li><code>deviance(fitModel)</code> = calculates sum of the squared error/residual for the linear model/residual variation</li>
<li><code>summary(fitModel)$sigma</code> = returns the residual variation of a fit model or the <strong>unbiased RMSE</strong>
<ul>
<li><code>summary(fitModel)</code> = creates a list of different parameters of the fit model</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># get data</span><br><span class="line">y &lt;- diamond$price; x &lt;- diamond$carat; n &lt;- length(y)</span><br><span class="line"># linear fit</span><br><span class="line">fit &lt;- lm(y ~ x)</span><br><span class="line"># calculate residual variation through summary and manual</span><br><span class="line">rbind(&quot;from summary&quot; = summary(fit)$sigma, &quot;manual&quot; =sqrt(sum(resid(fit)^2) / (n - 2)))</span><br></pre></td></tr></table></figure>
<h3 id="total-variation-r2-and-derivations">Total Variation, <span class="math inline">\(R^2\)</span>, and Derivations</h3>
<ul>
<li><strong>total variation</strong> = <strong><em>residual variation</em></strong> (variation after removing predictor) + <strong><em>systematic/regression variation</em></strong> (variation explained by regression model) <span class="math display">\[\sum_{i=1}^n (Y_i - \bar Y)^2 = \sum_{i=1}^n (Y_i - \hat Y_i)^2 + \sum_{i=1}^n  (\hat Y_i - \bar Y)^2
\]</span></li>
<li><span class="math inline">\(R^2\)</span> = percent of total variability that is explained by the regression model <span class="math display">\[
\begin{aligned}
R^2 &amp; = \frac{\mbox{regression variation}}{\mbox{total variation}} = \frac{\sum_{i=1}^n  (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2} \\
&amp; = 1- \frac{\mbox{residual variation}}{\mbox{total variation}} = 1- \frac{\sum_{i=1}^n (Y_i - \hat Y_i)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}\\
\end{aligned}
\]</span></li>
<li><span class="math inline">\(0 \leq R^2 \leq 1\)</span></li>
<li><span class="math inline">\(R^2\)</span> = sample correlation squared
<ul>
<li><code>cor(outcome, predictor</code> = calculates the correlation between predictor and outcome <span class="math inline">\(\rightarrow\)</span> the same as calculating <span class="math inline">\(R^2\)</span></li>
</ul></li>
<li><span class="math inline">\(R^2\)</span> can be a <strong><em>misleading</em></strong> summary of model fit
<ul>
<li>deleting data <span class="math inline">\(\rightarrow\)</span> inflate <span class="math inline">\(R^2\)</span></li>
<li>adding terms to a regression model <span class="math inline">\(\rightarrow\)</span> always increases <span class="math inline">\(R^2\)</span></li>
<li><code>example(anscombe)</code> demonstrates the fallacy of <span class="math inline">\(R^2\)</span> through the following
<ul>
<li>basically same mean and variance of X and Y</li>
<li>identical correlations (hence same <span class="math inline">\(R^2\)</span>)</li>
<li>same linear regression relationship</li>
</ul></li>
</ul></li>
<li><p><strong>relationship between <span class="math inline">\(R^2\)</span> and <span class="math inline">\(r\)</span></strong> <span class="math display">\[
\begin{aligned}
\mbox{Correlation between X and Y} \Rightarrow r &amp; = Cor(Y, X)\\
R^2 &amp; = \frac{\sum_{i=1}^n  (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2} \\
&amp;recall \Rightarrow (\hat Y_i - \bar Y) = \hat \beta_1  (X_i - \bar X)\\
(substituting (\hat Y_i - \bar Y)) &amp; = \hat \beta_1^2  \frac{\sum_{i=1}^n(X_i - \bar X)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}\\
&amp;recall \Rightarrow \hat \beta_1 = Cor(Y, X)\frac{Sd(Y)}{Sd(X)}\\
(substituting \hat \beta_1) &amp; = Cor(Y, X)^2\frac{Var(Y)}{Var(X)} \times \frac{\sum_{i=1}^n(X_i - \bar X)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}\\
&amp; recall Var(Y) = \sum_{i=1}^n (Y_i - \bar Y)^2 and Var(X) = \sum_{i=1}^n (X_i - \bar X)^2\\
(simplifying) \Rightarrow R^2 &amp;= Cor(Y,X)^2\\
\mbox{Or } R^2 &amp; = r^2\\
\end{aligned}
\]</span></p></li>
<li><p><strong><em>total variation derivation</em></strong> <span class="math display">\[
\begin{aligned}
\mbox{First, we know that } \bar Y &amp; = \hat \beta_0 + \hat \beta_1 \bar X \\
(transforming) \Rightarrow \hat \beta_0 &amp; = \bar Y - \hat \beta_1 \bar X \\
\\
\mbox{We also know that } \hat Y_i &amp; = \hat \beta_0 + \hat \beta_1 X_i \\
\\
\mbox{Next, the residual } = (Y_i - \hat Y_i) &amp; = Y_i - \hat \beta_0 - \hat \beta_1 X_i \\
(substituting~\hat \beta_0) &amp; = Y_i - (\bar Y - \hat \beta_1 \bar X) - \hat \beta_1 X_i \\
(transforming) \Rightarrow (Y_i - \hat Y_i) &amp; = (Y_i - \bar Y) - \hat \beta_1 (X_i - \bar X) \\
\\
\mbox{Next, the regression difference } = (\hat Y_i - \bar Y) &amp; = \hat \beta_0 - \hat \beta_1 X_i -\bar Y \\
(substituting~\hat \beta_0) &amp; = \bar Y - \hat \beta_1 \bar X - \hat \beta_1 X_i - \bar Y \\
(transforming) \Rightarrow (\hat Y_i - \bar Y) &amp; = \hat \beta_1  (X_i - \bar X) \\
\\
\mbox{Total Variation} = \sum_{i=1}^n (Y_i - \bar Y)^2 &amp; = \sum_{i=1}^n (Y_i - \hat Y_i + \hat Y_i - \bar Y)^2 \Leftarrow (adding~\pm \hat Y_i) \\
(expanding) &amp; = \sum_{i=1}^n (Y_i - \hat Y_i)^2 + 2 \sum_{i=1}^n (Y_i - \hat Y_i)(\hat Y_i - \bar Y) + \sum_{i=1}^n (\hat Y_i - \bar Y)^2 \\
\\
\mbox{Looking at } \sum_{i=1}^n (Y_i - \hat Y_i)(\hat Y_i - \bar Y) &amp;  \\
(substituting~(Y_i - \hat Y_i),(\hat Y_i - \bar Y)) &amp;= \sum_{i=1}^n  \left[(Y_i - \bar Y) - \hat \beta_1 (X_i - \bar X))\right]\left[\hat \beta_1  (X_i - \bar X)\right]\\
(expanding) &amp; = \hat \beta_1 \sum_{i=1}^n (Y_i - \bar Y)(X_i - \bar X) -\hat\beta_1^2\sum_{i=1}^n (X_i - \bar X)^2\\
&amp; (substituting~Y_i, \bar Y) \Rightarrow (Y_i - \bar Y) = (\hat \beta_0 + \hat \beta_1 X_i) - (\hat \beta_0 + \hat \beta_1 \bar X) \\
&amp; (simplifying) \Rightarrow (Y_i - \bar Y) = \hat \beta_1 (X_i - \bar X) \\
(substituting~(Y_i - \bar Y)) &amp; = \hat \beta_1^2 \sum_{i=1}^n (X_i - \bar X)^2-\hat\beta_1^2\sum_{i=1}^n (X_i - \bar X)^2\\
\Rightarrow \sum_{i=1}^n (Y_i - \hat Y_i)(\hat Y_i - \bar Y) &amp;= 0 \\
\\
\mbox{Going back to} \sum_{i=1}^n (Y_i - \bar Y)^2 &amp;= \sum_{i=1}^n (Y_i - \hat Y_i)^2 + 2 \sum_{i=1}^n (Y_i - \hat Y_i)(\hat Y_i - \bar Y) + \sum_{i=1}^n (\hat Y_i - \bar Y)^2 \\
(since~second~term= 0) \Rightarrow \sum_{i=1}^n (Y_i - \bar Y)^2 &amp;= \sum_{i=1}^n (Y_i - \hat Y_i)^2 + \sum_{i=1}^n (\hat Y_i - \bar Y)^2\\
\end{aligned}
\]</span></p></li>
</ul>
<h3 id="example-and-r-commands-1">Example and R Commands</h3>
<ul>
<li><code>resid(fitModel)</code> or <code>fitModel$residuals</code> = extracts model residuals from the fit model (lm in our case) <span class="math inline">\(\rightarrow\)</span> list of residual values for every value of X</li>
<li><code>summary(fitModel)$r.squared</code> = return <span class="math inline">\(R^2\)</span> value of the regression model</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># load multiplot function</span><br><span class="line">source(&quot;multiplot.R&quot;)</span><br><span class="line"># get data</span><br><span class="line">y &lt;- diamond$price; x &lt;- diamond$carat; n &lt;- length(y)</span><br><span class="line"># linear regression</span><br><span class="line">fit &lt;- lm(y ~ x)</span><br><span class="line"># calculate residual</span><br><span class="line">e &lt;- resid(fit)</span><br><span class="line"># calculate predicted values</span><br><span class="line">yhat &lt;- predict(fit)</span><br><span class="line"># create 1 x 2 panel plot</span><br><span class="line">par(mfrow=c(1, 2))</span><br><span class="line"># plot residuals on regression line</span><br><span class="line">plot(x, y, xlab = &quot;Mass (carats)&quot;, ylab = &quot;Price (SIN $)&quot;, bg = &quot;lightblue&quot;,</span><br><span class="line">    col = &quot;black&quot;, cex = 2, pch = 21,frame = FALSE,main = &quot;Residual on Regression Line&quot;)</span><br><span class="line"># draw linear regression line</span><br><span class="line">abline(fit, lwd = 2)</span><br><span class="line"># draw red lines from data points to regression line</span><br><span class="line">for (i in 1 : n)&#123;lines(c(x[i], x[i]), c(y[i], yhat[i]), col = &quot;red&quot; , lwd = 2)&#125;</span><br><span class="line"># plot residual vs x</span><br><span class="line">plot(x, e, xlab = &quot;Mass (carats)&quot;, ylab = &quot;Residuals (SIN $)&quot;, bg = &quot;lightblue&quot;,</span><br><span class="line">    col = &quot;black&quot;, cex = 2, pch = 21,frame = FALSE,main = &quot;Residual vs X&quot;)</span><br><span class="line"># draw horizontal line</span><br><span class="line">abline(h = 0, lwd = 2)</span><br><span class="line"># draw red lines from residual to x axis</span><br><span class="line">for (i in 1 : n)&#123;lines(c(x[i], x[i]), c(e[i], 0), col = &quot;red&quot; , lwd = 2)&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>non-linear data/patterns can be more easily revealed through residual plots</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># create sin wave pattern</span><br><span class="line">x &lt;- runif(100, -3, 3); y &lt;- x + sin(x) + rnorm(100, sd = .2);</span><br><span class="line"># plot data + regression</span><br><span class="line">g &lt;- ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +</span><br><span class="line">	geom_smooth(method = &quot;lm&quot;, colour = &quot;black&quot;) +</span><br><span class="line">	geom_point(size = 3, colour = &quot;black&quot;, alpha = 0.4) +</span><br><span class="line">	geom_point(size = 2, colour = &quot;red&quot;, alpha = 0.4)+</span><br><span class="line"> ggtitle(&quot;Residual on Regression Line&quot;)</span><br><span class="line"># plot residuals</span><br><span class="line">f &lt;- ggplot(data.frame(x = x, y = resid(lm(y ~ x))), aes(x = x, y = y))+</span><br><span class="line">	geom_hline(yintercept = 0, size = 2)+</span><br><span class="line">	geom_point(size = 3, colour = &quot;black&quot;, alpha = 0.4)+</span><br><span class="line">	geom_point(size = 2, colour = &quot;red&quot;, alpha = 0.4)+</span><br><span class="line">	xlab(&quot;X&quot;) + ylab(&quot;Residual&quot;)+ ggtitle(&quot;Residual vs X&quot;)</span><br><span class="line">multiplot(g, f, cols = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>heteroskedasticity</strong> = heteroskedastic model’s variance is not constant and is a function of x</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># create heteroskedastic data</span><br><span class="line">x &lt;- runif(100, 0, 6); y &lt;- x + rnorm(100,  mean = 0, sd = .001 * x)</span><br><span class="line"># plot data + regression</span><br><span class="line">g &lt;- ggplot(data.frame(x = x, y = y), aes(x = x, y = y))+</span><br><span class="line"> geom_smooth(method = &quot;lm&quot;, colour = &quot;black&quot;)+</span><br><span class="line"> geom_point(size = 3, colour = &quot;black&quot;, alpha = 0.4)+</span><br><span class="line"> geom_point(size = 2, colour = &quot;red&quot;, alpha = 0.4) +</span><br><span class="line"> ggtitle(&quot;Residual on Regression Line&quot;)</span><br><span class="line"># plot residuals</span><br><span class="line">f &lt;- ggplot(data.frame(x = x, y = resid(lm(y ~ x))), aes(x = x, y = y))+</span><br><span class="line">           geom_hline(yintercept = 0, size = 2) +</span><br><span class="line">           geom_point(size = 3, colour = &quot;black&quot;, alpha = 0.4)+</span><br><span class="line">           geom_point(size = 2, colour = &quot;red&quot;, alpha = 0.4)+</span><br><span class="line">           xlab(&quot;X&quot;) + ylab(&quot;Residual&quot;) + ggtitle(&quot;Residual vs X&quot;)</span><br><span class="line"># combine two plots</span><br><span class="line">multiplot(g, f, cols = 2)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="inference-in-regression">Inference in Regression</h2>
<ul>
<li>statistics used for hypothesis tests and confidence intervals have the following attributes <span class="math display">\[\frac{\hat \theta - \theta}{\hat \sigma_{\hat \theta}} \sim N(0,1)\]</span>
<ul>
<li>it follows a finite sample Student’s <strong><em>T distribution</em></strong> and is <strong><em>normally distributed</em></strong> if the sample has IID components built in (i.e. <span class="math inline">\(\epsilon_i\)</span>)</li>
<li>used to test <span class="math inline">\(H_0 : \theta = \theta_0\)</span> vs. <span class="math inline">\(H_a : \theta &gt;, &lt;, \neq \theta_0\)</span>.</li>
<li>confidence interval for <span class="math inline">\(\theta\)</span> = <span class="math inline">\(\hat \theta \pm Q_{1-\alpha/2} \hat \sigma_{\hat \theta}\)</span>, where <span class="math inline">\(Q_{1-\alpha/2}\)</span> = relevant quantile from normal(for large samples)/T distribution(small samples, n-1 degrees of freedom)</li>
</ul></li>
</ul>
<h3 id="intervalstests-for-coefficients">Intervals/Tests for Coefficients</h3>
<ul>
<li><p>standard errors for coefficients <span class="math display">\[\begin{aligned}
Var(\hat \beta_1) &amp; = Var\left(\frac{\sum_{i=1}^n (Y_i - \bar Y)(X_i - \bar X)}{((X_i - \bar X)^2)}\right) \\
(expanding) &amp; = Var\left(\frac{\sum_{i=1}^n Y_i (X_i - \bar X) - \bar Y \sum_{i=1}^n (X_i - \bar X)}{((X_i - \bar X)^2)}\right) \\
&amp; Since~ \sum_{i=1}^n X_i - \bar X = 0 \\
(simplifying) &amp; = \frac{\sum_{i=1}^n Y_i (X_i - \bar X)}{(\sum_{i=1}^n (X_i - \bar X)^2)^2} \Leftarrow \mbox{denominator taken out of } Var\\
(Var(Y_i) = \sigma^2) &amp; = \frac{\sigma^2 \sum_{i=1}^n (X_i - \bar X)^2}{(\sum_{i=1}^n (X_i - \bar X)^2)^2} \\
\sigma_{\hat \beta_1}^2 = Var(\hat \beta_1) &amp;= \frac{\sigma^2 }{ \sum_{i=1}^n (X_i - \bar X)^2 }\\
\Rightarrow \sigma_{\hat \beta_1} &amp;= \frac{\sigma}{ \sum_{i=1}^n X_i - \bar X}  \\
\\
\mbox{by the same derivation} \Rightarrow &amp; \\
\sigma_{\hat \beta_0}^2 = Var(\hat \beta_0) &amp; = \left(\frac{1}{n} + \frac{\bar X^2}{\sum_{i=1}^n (X_i - \bar X)^2 }\right)\sigma^2 \\
\Rightarrow \sigma_{\hat \beta_0} &amp;= \sigma \sqrt{\frac{1}{n} + \frac{\bar X^2}{\sum_{i=1}^n (X_i - \bar X)^2 }}\\
\end{aligned}\]</span></p></li>
<li><span class="math inline">\(\sigma\)</span> is unknown but it’s estimate is as follows <span class="math display">\[\hat \sigma^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2\]</span></li>
<li>under IID Gaussian errors (assumed in linear regression with term <span class="math inline">\(\epsilon_0\)</span>), statistics for <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span> are as follows <span class="math display">\[\frac{\hat \beta_j - \beta_j}{\hat \sigma_{\hat \beta_j}} ~~~where~j = 0, 1\]</span>
<ul>
<li>these statistics follow <strong><span class="math inline">\(t\)</span> distribution</strong> with <strong><span class="math inline">\(n-2\)</span></strong> degrees of freedom for small <span class="math inline">\(n\)</span> and normal distribution for large <span class="math inline">\(n\)</span></li>
</ul></li>
<li><code>summary(fitModel)$coefficients</code> = returns table summarizing the estimate, standar error, t value and p value for the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>
<ul>
<li>the below example reproduces the table <code>summary(fitModel)$coefficients</code> produces</li>
</ul></li>
<li><em><strong>Note</strong>: the variability in the slope, or <span class="math inline">\(Var(\hat \beta_1)\)</span>, is the largest when the predictor values are spread into two cluster that are far apart from each other </em>
<ul>
<li>when modeling linear relationships, it is generally good practice to have many data points that evenly cover the entire range of data, increasing the denominator <span class="math inline">\(\sum_{i=1}^n (X_i - \bar X)^2\)</span></li>
<li>this is so that variance of slope is minimized and we can be more confident about the relationship</li>
</ul></li>
<li><p><strong><em>example</em></strong></p></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># getting data</span><br><span class="line">y &lt;- diamond$price; x &lt;- diamond$carat; n &lt;- length(y)</span><br><span class="line"># calculate beta1</span><br><span class="line">beta1 &lt;- cor(y, x) * sd(y) / sd(x)</span><br><span class="line"># calculate beta0</span><br><span class="line">beta0 &lt;- mean(y) - beta1 * mean(x)</span><br><span class="line"># Gaussian regression error</span><br><span class="line">e &lt;- y - beta0 - beta1 * x</span><br><span class="line"># unbiased estimate for variance</span><br><span class="line">sigma &lt;- sqrt(sum(e^2) / (n-2))</span><br><span class="line"># (X_i - X Bar)</span><br><span class="line">ssx &lt;- sum((x - mean(x))^2)</span><br><span class="line"># calculate standard errors</span><br><span class="line">seBeta0 &lt;- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma</span><br><span class="line">seBeta1 &lt;- sigma / sqrt(ssx)</span><br><span class="line"># testing for H0: beta0 = 0 and beta0 = 0</span><br><span class="line">tBeta0 &lt;- beta0 / seBeta0; tBeta1 &lt;- beta1 / seBeta1</span><br><span class="line"># calculating p-values for Ha: beta0 != 0 and beta0 != 0 (two sided)</span><br><span class="line">pBeta0 &lt;- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)</span><br><span class="line">pBeta1 &lt;- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)</span><br><span class="line"># store results into table</span><br><span class="line">coefTable &lt;- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))</span><br><span class="line">colnames(coefTable) &lt;- c(&quot;Estimate&quot;, &quot;Std. Error&quot;, &quot;t value&quot;, &quot;P(&gt;|t|)&quot;)</span><br><span class="line">rownames(coefTable) &lt;- c(&quot;(Intercept)&quot;, &quot;x&quot;)</span><br><span class="line"># print table</span><br><span class="line">coefTable</span><br><span class="line"># regression model and the generated table from lm (identical to above)</span><br><span class="line">fit &lt;- lm(y ~ x); summary(fit)$coefficients</span><br><span class="line"># store results in matrix</span><br><span class="line">sumCoef &lt;- summary(fit)$coefficients</span><br><span class="line"># print out confidence interval for beta0</span><br><span class="line">sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]</span><br><span class="line"># print out confidence interval for beta1 in 1/10 units</span><br><span class="line">(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>interpretation</strong>: With 95% confidence, we estimate that a 0.1 carat increase in diamond size results in a <code>r round((sumCoef[2,1] - qt(.975, df = fit$df) * sumCoef[2, 2]) / 10, 1)</code> to <code>r round((sumCoef[2,1] + qt(.975, df = fit$df) * sumCoef[2, 2]) / 10, 1)</code> increase in price in (Singapore) dollars.</li>
</ul>
<h3 id="prediction-interval">Prediction Interval</h3>
<ul>
<li>estimated prediction, <span class="math inline">\(\hat y_0\)</span>, at point <span class="math inline">\(x_0\)</span> is <span class="math display">\[\hat y_0 = \hat \beta_0 + \hat \beta_1 x_0\]</span></li>
<li>we can construct two prediction intervals
<ol type="1">
<li>interval for <strong>the line</strong> at <span class="math inline">\(x_0\)</span> <span class="math display">\[
\begin{aligned}
\mbox{Interval}: &amp; \hat y_0 \pm t_{n-2, 1-\alpha/2} \times SE_{line}  \\
\mbox{where } &amp; \hat y_0 = \hat \beta_0 + \hat \beta_1 x_0 \\
\mbox{and } &amp; SE_{line} = \hat \sigma\sqrt{\frac{1}{n} + \frac{(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}\\
\end{aligned}
\]</span>
<ul>
<li>interval has <strong><em>varying width</em></strong></li>
<li>interval is narrow as we are quite confident in the regression line</li>
<li>as <span class="math inline">\(n\)</span> increases, the interval becomes <strong><em>narrower</em></strong>, which makes sense because as more data is collected, we are able to get a better regression line
<ul>
<li><em><strong>Note</strong>: if we knew <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, this interval would have zero width </em></li>
</ul></li>
</ul></li>
<li>interval for the <strong>predicted value</strong>, <span class="math inline">\(\hat y_0\)</span>, at <span class="math inline">\(x_0\)</span> <span class="math display">\[
\begin{aligned}
\mbox{Interval}: &amp; \hat y_0 \pm t_{n-2, 1-\alpha/2} \times SE_{\hat y_0}  \\
\mbox{where } &amp; \hat y_0 = \hat \beta_0 + \hat \beta_1 x_0 \\
\mbox{and } &amp; SE_{\hat y_0} = \hat \sigma\sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}\\
\end{aligned}
\]</span>
<ul>
<li>interval has <strong><em>varying width</em></strong></li>
<li>the <span class="math inline">\(1\)</span> part in the <span class="math inline">\(SE_{\hat y_0}\)</span> formula represents the inherent variability in the data
<ul>
<li>no matter how good of a regression line we get, we still can not get rid of the variability in the data</li>
<li><em><strong>Note</strong>: even if we know <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the interval would still have width due to data variance </em></li>
</ul></li>
</ul></li>
</ol></li>
<li><code>predict(fitModel, data, interval = (&quot;confidence&quot;))</code> = returns a 3-column matrix with data for <code>fit</code> (regression line), <code>lwr</code> (lower bound of interval), and <code>upr</code> (upper bound of interval)
<ul>
<li><code>interval = (&quot;confidence&quot;)</code> = returns interval for the line</li>
<li><code>interval = (&quot;prediction&quot;)</code> = returns interval for the prediction</li>
<li><code>data</code> = must be a new data frame with the values you would like to predict</li>
</ul></li>
<li><strong><em>example (<code>ggplot2</code>)</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># create a sequence of values that we want to predict at</span><br><span class="line">newx = data.frame(x = seq(min(x), max(x), length = 100))</span><br><span class="line"># calculate values for both intervals</span><br><span class="line">p1 = data.frame(predict(fit, newdata= newx,interval = (&quot;confidence&quot;)))</span><br><span class="line">p2 = data.frame(predict(fit, newdata = newx,interval = (&quot;prediction&quot;)))</span><br><span class="line"># add column for interval labels</span><br><span class="line">p1$interval = &quot;confidence&quot;; p2$interval = &quot;prediction&quot;</span><br><span class="line"># add column for the x values we want to predict</span><br><span class="line">p1$x = newx$x; p2$x = newx$x</span><br><span class="line"># combine the two dataframes</span><br><span class="line">dat = rbind(p1, p2)</span><br><span class="line"># change the name of the first column to y</span><br><span class="line">names(dat)[1] = &quot;y&quot;</span><br><span class="line"># plot the data</span><br><span class="line">g &lt;- ggplot(dat, aes(x = x, y = y))</span><br><span class="line">g &lt;- g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)</span><br><span class="line">g &lt;- g + geom_line()</span><br><span class="line">g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em>example (<code>base</code>)</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># plot the x and y values</span><br><span class="line">plot(x,y,frame=FALSE,xlab=&quot;Carat&quot;,ylab=&quot;Dollars&quot;,pch=21,col=&quot;black&quot;,bg=&quot;lightblue&quot;,cex=2)</span><br><span class="line"># add the fit line</span><br><span class="line">abline(fit,lwd=2)</span><br><span class="line"># create sequence of x values that we want to predict at</span><br><span class="line">xVals&lt;-seq(min(x),max(x),by=.01)</span><br><span class="line"># calculate the predicted y values</span><br><span class="line">yVals&lt;-beta0+beta1*xVals</span><br><span class="line"># calculate the standard errors for the interval for the line</span><br><span class="line">se1&lt;-sigma*sqrt(1/n+(xVals-mean(x))^2/ssx)</span><br><span class="line"># calculate the standard errors for the interval for the predicted values</span><br><span class="line">se2&lt;-sigma*sqrt(1+1/n+(xVals-mean(x))^2/ssx)</span><br><span class="line"># plot the upper and lower bounds of both intervals</span><br><span class="line">lines(xVals,yVals+2*se1); lines(xVals,yVals-2*se1)</span><br><span class="line">lines(xVals,yVals+2*se2); lines(xVals,yVals-2*se2)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="multivariate-regression">Multivariate Regression</h2>
<ul>
<li>linear models = most important applied statistical/machine learning technique</li>
<li><strong>generalized linear model</strong> extends simple linear regression (SLR) model <span class="math display">\[Y_i =  \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots +\beta_{p} X_{pi} + \epsilon_{i} = \sum_{k=1}^p X_{ik} \beta_j + \epsilon_{i}\]</span> where <span class="math inline">\(X_{1i}=1\)</span> typically, so that an intercept is included</li>
<li>least squares/MLE for the model (under IID Gaussian errors) minimizes <span class="math display">\[\sum_{i=1}^n \left(Y_i - \sum_{k=1}^p X_{ki} \beta_j\right)^2\]</span></li>
<li><strong>linearity of coefficients</strong> is what defines a linear model as transformations of the variables (i.e. squaring) still yields a linear model<span class="math display">\[Y_i =  \beta_1 X_{1i}^2 + \beta_2 X_{2i}^2 + \ldots +\beta_{p} X_{pi}^2 + \epsilon_{i} \]</span></li>
<li>performing multivariate regression = pick any regressor and replace the outcome and all other regressors by their residuals against the chosen one</li>
</ul>
<h3 id="derivation-of-coefficients">Derivation of Coefficients</h3>
<ul>
<li>we know for simple univariate regression through the origin <span class="math display">\[E[Y_i]=X_{1i}\beta_1 \\ \hat \beta_1 =  \frac{\sum_{i=1}^n X_i Y_i}{\sum_{i=1}^n X_i^2}\]</span></li>
<li>we want to minimize <span class="math display">\[\sum_{i=1}^n (Y_i - X_{1i}\beta_1 - X_{2i}\beta_2 - \ldots - X_{pi}\beta_p)^2\]</span></li>
<li>we begin by looking at the two variable model where <span class="math display">\[E[Y_i] = \mu_i  = X_{1i}\beta_1 + X_{2i}\beta_2 \\ \hat \mu_i = X_{1i}\hat \beta_1 + X_{2i}\hat \beta_2\]</span></li>
<li>from our previous derivations, to minimize the sum of squares, the following has to true <span class="math display">\[\sum_{i=1}^n (Y_i - \hat \mu_i) (\hat \mu_i - \mu_i) = 0\]</span></li>
<li>plugging in <span class="math inline">\(\hat \mu_i\)</span> and <span class="math inline">\(\mu_i\)</span> <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \hat \mu_i) (\hat \mu_i - \mu_i) &amp; = \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2)(X_{1i}\hat \beta_1 + X_{2i}\hat \beta_2 - X_{1i}\beta_1 - X_{2i}\beta_2)\\
(simplifying) &amp; = \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2)\left[ X_{1i}(\hat \beta_1 - \beta_1) + X_{2i} (\hat \beta_2 - \beta_2)\right]\\
(expanding) &amp; = \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{1i}(\hat \beta_1 - \beta_1) + \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{2i} (\hat \beta_2 - \beta_2)\\
\end{aligned}\]</span></li>
<li>for the entire expression to equal to zero, <span class="math display">\[\begin{aligned}
\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{1i}(\hat \beta_1 - \beta_1) &amp; = 0 \\
(since~\hat \beta_1,~\beta_1~don&#39;t~depend~on~i) \Rightarrow \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{1i}&amp; = 0 ~~~~~~\mbox{(1)}\\
\\
\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{2i}(\hat \beta_2 - \beta_2) &amp; = 0 \\
(since~\hat \beta_2,~\beta_2~don&#39;t~depend~on~i) \Rightarrow \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{2i} &amp; = 0 ~~~~~~\mbox{(2)}\\
\end{aligned} \]</span></li>
<li><p>we can hold <span class="math inline">\(\hat \beta_1\)</span> fixed and solve <strong>(2)</strong> for <span class="math inline">\(\hat \beta_2\)</span> <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1) X_{2i} - \sum_{i=1}^n X_{2i}^2 \hat \beta_2&amp; = 0\\
\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1) X_{2i} &amp;= \sum_{i=1}^n X_{2i}^2 \hat \beta_2   \\
\hat \beta_2 &amp; = \frac{\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1) X_{2i}}{\sum_{i=1}^n X_{2i}^2} \\
\end{aligned}
\]</span></p></li>
<li>plugging <span class="math inline">\(\hat \beta_2\)</span> back into <strong>(1)</strong>, we get <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n \left[Y_i - X_{1i}\hat \beta_1 - \frac{\sum_{j=1}^n (Y_j - X_{1j}\hat \beta_1) X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right] X_{1i} &amp; = 0 \\
\sum_{i=1}^n \left[Y_i - X_{1i}\hat \beta_1 - \frac{\sum_{j=1}^n Y_jX_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i} + \frac{\sum_{j=1}^n X_{1j} X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\hat \beta_1\right] X_{1i} &amp; = 0  \\
\sum_{i=1}^n \left[\left(Y_i - \frac{\sum_{j=1}^n Y_jX_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right) - \hat \beta_1 \left(X_{1i} - \frac{\sum_{j=1}^n X_{1j} X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right)\right] X_{1i} &amp; = 0 \\
\Rightarrow \left(Y_i - \frac{\sum_{j=1}^n Y_jX_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right) &amp; = \mbox{residual for } Y_i = X_{i2} \hat \beta_2 + \epsilon_i\\
\Rightarrow \left(X_{1i} - \frac{\sum_{j=1}^n X_{1j} X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right) &amp; = \mbox{residual for } X_{i1} = X_{i2} \hat \gamma + \epsilon_i\\
\end{aligned}
\]</span></li>
<li><p>we can rewrite <span class="math display">\[\sum_{i=1}^n \left[\left(Y_i - \frac{\sum_{j=1}^n Y_jX_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right) - \hat \beta_1 \left(X_{1i} - \frac{\sum_{j=1}^n X_{1j} X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right)\right] X_{1i}  = 0 ~~~~~~(3)\]</span> as <span class="math display">\[ \sum_{i=1}^n \left[ e_{i, Y|X_1} -\hat \beta_1 e_{i, X_1|X_2}  \right] X_{1i}= 0\]</span> where <span class="math display">\[e_{i, a|b} = a_i - \frac{ \sum_{j=1}^n a_j b_j}{ \sum_{j=1}^n b_j^2}b_i\]</span> which is interpreted as the residual when regressing b from a without an intercept</p></li>
<li><p>solving <strong>(3)</strong>, we get <span class="math display">\[\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2}X_{1i}} ~~~~~~(4)\]</span></p></li>
<li><p>to simplify the denominator, we will look at <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n e_{i, X_1 | X_2}^2 &amp; = \sum_{i=1}^n e_{i, X_1 | X_2}\left(X_{1i} - \frac{\sum_{j=1}^n X_{1j}X_{2j} }{ \sum_{j=1}^n X_{2j}^2 } X_{2i}\right) \\
&amp; = \sum_{i=1}^n e_{i, X_1 | X_2}X_{1i} - \frac{\sum_{j=1}^n X_{1j}X_{2j} }{ \sum_{j=1}^n X_{2j}^2 } \sum_{i=1}^n e_{i, X_1 | X_2} X_{2i}\\
&amp; (recall~that~ \sum_{i=1}^n e_{i}X_i = 0,~so~the~2^{nd}~term~is~0) \\
\Rightarrow \sum_{i=1}^n e_{i, X_1 | X_2}^2 &amp; = \sum_{i=1}^n e_{i, X_1 | X_2}X_{1i}\\
\end{aligned}
\]</span></p></li>
<li><p>plugging the above equation back in to <strong>(4)</strong>, we get <span class="math display">\[\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2}^2}\]</span></p></li>
<li><strong>general case</strong>
<ul>
<li>pick one regressor and to replace all other variables by the residuals of their regressions against that one <span class="math display">\[\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - \ldots - X_{pi}\hat \beta_p)X_k = 0\]</span> for <span class="math inline">\(k = 1, \ldots, p\)</span> yields <span class="math inline">\(p\)</span> equations with <span class="math inline">\(p\)</span> unknowns</li>
<li>holding <span class="math inline">\(\hat \beta_1, \ldots, \hat \beta_{p-1}\)</span> constant, we get <span class="math display">\[\hat \beta_p = \frac{ \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - \ldots - X_{p-1, i}\hat \beta_{p-1})X_{pi}}{\sum_{i=1}^n X_{pi}^2}\]</span></li>
<li>plugging <span class="math inline">\(\hat \beta_p\)</span> back into the equation <span class="math display">\[\sum_{i=1}^n (e_{i,Y|X_p} - e_{i,X_1|X_p}\hat \beta_1 - \ldots - e_{i,X_{p-1}|X_p}\hat \beta_{p-1} )X_k = 0\]</span></li>
<li>since we know that <span class="math display">\[X_k = e_{i,X_i|X_p} + \frac{\sum_{i=1}^n X_{ki}X_{pi}}{\sum_{i=1}^n X_{pi}^2}X_p\]</span> and that <span class="math display">\[\sum_{i=1}^n e_{i,X_i|X_p}X_{pi} = 0\]</span> the equation becomes <span class="math display">\[\sum_{i=1}^n (e_{i,Y|X_p} - e_{i,X_1|X_p}\hat \beta_1 - \ldots - e_{i,X_{p-1}|X_p}\hat \beta_{p-1} )e_{i, X_k|X_p} = 0\]</span></li>
<li>this procedure reduces <span class="math inline">\(p\)</span> LS equations and p unknowns to <span class="math inline">\(p-1\)</span> LS equations and <span class="math inline">\(p-1\)</span> unknowns
<ul>
<li>every variable is replaced by its residual with <span class="math inline">\(X_p\)</span></li>
<li>process iterates until <strong>only <span class="math inline">\(Y\)</span></strong> and <strong>one variable remains</strong></li>
<li>or more intuitively, we take residuals over the confounding variables and do regression through the origin</li>
</ul></li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>for simple linear regression, <span class="math inline">\(Y_{i} = \beta_1 X_{1i} + \beta_2 X_{2i}\)</span> where <span class="math inline">\(X_{2i} = 1\)</span> is an intercept term</li>
<li>the residuals <span class="math display">\[e_{i, Y | X_2} = Y_i - \frac{ \sum_{j=1}^n Y_j X_{2j}}{ \sum_{j=1}^n X_{2j}^2}X_{2i} = Y_i - \bar Y\]</span>
<ul>
<li><em><strong>Note</strong>: this is according to previous derivation of the slope of a regression line through the origin </em></li>
</ul></li>
<li>the residuals <span class="math inline">\(e_{i, X_1 | X_2}= X_{1i} - \frac{\sum_{j=1}^n X_{1j}X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i} = X_{1i} - \bar X_1\)</span>
<ul>
<li><em><strong>Note</strong>: this is according to previous derivation of the slope of a regression line through the origin </em></li>
</ul></li>
<li>Thus <span class="math display">\[\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2}^2} = \frac{\sum_{i=1}^n (X_i - \bar X)(Y_i - \bar Y)}{\sum_{i=1}^n (X_i - \bar X)^2} = Cor(X, Y) \frac{Sd(Y)}{Sd(X)}\]</span></li>
</ul></li>
</ul>
<h3 id="interpretation-of-coefficients">Interpretation of Coefficients</h3>
<ul>
<li>from the derivation in the previous section, we have <span class="math display">\[\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2}^2}\]</span></li>
<li><p>this is interpreted as the effect of variable <span class="math inline">\(X_1\)</span> when the effects of all other variables have been removed from <span class="math inline">\(X_1\)</span> and the predicted result <span class="math inline">\(Y\)</span> (holding everything else constant/adjusting for all other variables)</p></li>
<li>the expected response is as follows <span class="math display">\[E[Y | X_1 = x_1, \ldots, X_p = x_p] = \sum_{k=1}^p x_{k} \beta_k\]</span> so the expected change in the response through change in one variable is <span class="math display">\[
\begin{aligned}
&amp; E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p]  - E[Y | X_1 = x_1, \ldots, X_p = x_p] \\
= &amp; (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k - \sum_{k=1}^p x_{k} \beta_k \\
= &amp; (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k - (x_1 \beta_1 + \sum_{k=2}^p x_{k} \beta_k) \\
= &amp; \beta_1 \\
\end{aligned}
\]</span></li>
<li>therefore, interpretation of a multivariate regression coefficient <span class="math inline">\(\rightarrow\)</span> expected change in the response per unit change in the regressor, holding all of the other regressors fixed</li>
<li>all of the SLR properties/calculations extend to generalized linear model
<ul>
<li><strong><em>model</em></strong> = <span class="math inline">\(Y_i = \sum_{k=1}^p X_{ik} \beta_{k} + \epsilon_{i}\)</span> where <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span></li>
<li><strong><em>fitted response</em></strong> = <span class="math inline">\(\hat Y_i = \sum_{k=1}^p X_{ik} \hat \beta_{k}\)</span></li>
<li><strong><em>residual</em></strong> = <span class="math inline">\(e_i = Y_i - \hat Y_i\)</span></li>
<li><strong><em>variance estimate</em></strong> = <span class="math inline">\(\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i ^2\)</span></li>
<li><strong><em>predicted responses at new values, <span class="math inline">\(x_1, \ldots, x_p\)</span></em></strong> = plug <span class="math inline">\(x\)</span> values into <span class="math inline">\(\sum_{k=1}^p x_{k} \hat \beta_{k}\)</span></li>
<li><strong><em>standard errors of coefficients</em></strong> = <span class="math inline">\(\hat \sigma_{\hat \beta_k}\)</span></li>
<li><strong><em>test/CI statistic</em></strong> = <span class="math inline">\(\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}\)</span> follows a <span class="math inline">\(T\)</span> distribution with <span class="math inline">\(n-p\)</span> degrees of freedom</li>
<li><strong><em>predicted/expected response intervals</em></strong> = calculated using standard errors of predicted responses of <span class="math inline">\(\hat Y_i\)</span></li>
</ul></li>
</ul>
<h3 id="example-linear-model-with-2-variables-and-intercept">Example: Linear Model with 2 Variables and Intercept</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># simulate the data</span><br><span class="line">n = 100; x = rnorm(n); x2 = rnorm(n); x3 = rnorm(n)</span><br><span class="line"># equation = intercept + var1 + var2 + var3 + error</span><br><span class="line">y = 1 + x + x2 + x3 + rnorm(n, sd = .1)</span><br><span class="line"># residual of y regressed on var2 and var3</span><br><span class="line">ey = resid(lm(y ~ x2 + x3))</span><br><span class="line"># residual of y regressed on var2 and var3</span><br><span class="line">ex = resid(lm(x ~ x2 + x3))</span><br><span class="line"># estimate beta1 for var1</span><br><span class="line">sum(ey * ex) / sum(ex ^ 2)</span><br><span class="line"># regression through the origin with xva1 with var2 var3 effect removed</span><br><span class="line">coef(lm(ey ~ ex - 1))</span><br><span class="line"># regression for all three variables</span><br><span class="line">coef(lm(y ~ x + x2 + x3))</span><br></pre></td></tr></table></figure>
<h3 id="example-coefficients-that-reverse-signs">Example: Coefficients that Reverse Signs</h3>
<ul>
<li><em><strong>Note</strong>: more information can be found at <code>?swiss</code> </em></li>
<li>data set is composed of standardized fertility measure and socio-economic indicators for each of 47 French-speaking provinces of Switzerland at about 1888</li>
<li>data frame has 47 observations on 6 variables, each of which is in percent [0, 100]
<ul>
<li><strong><em>Fertility</em></strong> = common standardized fertility measure <span class="math inline">\(\rightarrow\)</span> outcome</li>
<li><strong><em>Agriculture</em></strong> = % of males involved in agriculture as occupation</li>
<li><strong><em>Examination</em></strong> = % draftees receiving highest mark on army examination</li>
<li><strong><em>Education</em></strong> = % education beyond primary school for draftees</li>
<li><strong><em>Catholic</em></strong> = % catholic vs protestant</li>
<li><strong><em>Infant.Mortality</em></strong> = live births who live less than 1 year</li>
</ul></li>
<li>[<code>GGally</code> package] <code>ggpairs(data)</code> = produces pair wise plot for the predictors similar to <code>pairs</code> in base package</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># load dataset</span><br><span class="line">require(datasets); data(swiss); require(GGally)</span><br><span class="line"># produce pairwise plot using ggplot2</span><br><span class="line">ggpairs(swiss, lower = list(continuous = &quot;smooth&quot;),params = c(method = &quot;loess&quot;))</span><br><span class="line"># print coefficients of regression of fertility on all predictors</span><br><span class="line">summary(lm(Fertility ~ . , data = swiss))$coefficients</span><br></pre></td></tr></table></figure>
<ul>
<li>interpretation for Agriculture coefficient
<ul>
<li>we expect an <code>r round(summary(lm(Fertility~.,data=swiss))$coefficients[2,1],2)</code> <strong>decrease</strong> in standardized <strong><em>fertility</em></strong> for every 1% increase in percentage of <strong><em>males involved in agriculture</em></strong> in holding the remaining variables constant</li>
<li>since the p-value is <code>r summary(lm(Fertility~.,data=swiss))$coefficients[2,4]</code>, the t-test for <span class="math inline">\(H_0: \beta_{Agri} = 0\)</span> versus <span class="math inline">\(H_a: \beta_{Agri} \neq 0\)</span> is <strong><em>significant</em></strong></li>
</ul></li>
<li>however, if we look at the unadjusted estimate (marginal regression) for the coefficient for Agriculture</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># run marginal regression on Agriculture</span><br><span class="line">summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients</span><br></pre></td></tr></table></figure>
<ul>
<li>interpretation for Agriculture coefficient
<ul>
<li>we expect an <code>r round(summary(lm(Fertility~Agriculture,data=swiss))$coefficients[2,1],2)</code> <strong>increase</strong> in standardized <strong><em>fertility</em></strong> for every 1% increase in percentage of <strong><em>males involved in agriculture</em></strong> in holding the remaining variables constant
<ul>
<li><em><strong>Note</strong>: the coefficient <strong>flipped signs</strong> </em></li>
</ul></li>
<li>since the p-value is <code>r summary(lm(Fertility~Agriculture,data=swiss))$coefficients[2,4]</code>, the t-test for <span class="math inline">\(H_0: \beta_{Agri} = 0\)</span> versus <span class="math inline">\(H_a: \beta_{Agri} \neq 0\)</span> is <strong><em>significant</em></strong></li>
</ul></li>
<li>to see intuitively how a <strong>sign change</strong> is possible, we can look at the following simulated example</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; x2 &lt;- 1 : n; x1 &lt;- .01 * x2 + runif(n, -.1, .1); y = -x1 + x2 + rnorm(n, sd = .01)</span><br><span class="line"># print coefficients</span><br><span class="line">c(&quot;with x1&quot; = summary(lm(y ~ x1))$coef[2,1],</span><br><span class="line">	&quot;with x1 and x2&quot; = summary(lm(y ~ x1 + x2))$coef[2,1])</span><br><span class="line"># print p-values</span><br><span class="line">c(&quot;with x1&quot; = summary(lm(y ~ x1))$coef[2,4],</span><br><span class="line">	&quot;with x1 and x2&quot; = summary(lm(y ~ x1 + x2))$coef[2,4])</span><br><span class="line"># store all data in one data frame (ey and ex1 are residuals with respect to x2)</span><br><span class="line">dat &lt;- data.frame(y = y, x1 = x1, x2 = x2, ey = resid(lm(y ~ x2)), ex1 = resid(lm(x1 ~ x2)))</span><br><span class="line"># plot y vs x1</span><br><span class="line">g &lt;- ggplot(dat, aes(y = y, x = x1, colour = x2)) +</span><br><span class="line">	geom_point(colour=&quot;grey50&quot;, size = 2) +</span><br><span class="line">	geom_smooth(method = lm, se = FALSE, colour = &quot;black&quot;) + geom_point(size = 1.5) +</span><br><span class="line">	ggtitle(&quot;unadjusted = y vs x1&quot;)</span><br><span class="line"># plot residual of y adjusted for x2 vs residual of x1 adjusted for x2</span><br><span class="line">g2 &lt;- ggplot(dat, aes(y = ey, x = ex1, colour = x2)) +</span><br><span class="line">	geom_point(colour=&quot;grey50&quot;, size = 2) +</span><br><span class="line">	geom_smooth(method = lm, se = FALSE, colour = &quot;black&quot;) + geom_point(size = 1.5) +</span><br><span class="line">	ggtitle(&quot;adjusted = y, x1 residuals with x2 removed&quot;) + labs(x = &quot;resid(x1~x2)&quot;,</span><br><span class="line">		y = &quot;resid(y~x2)&quot;)</span><br><span class="line"># combine plots</span><br><span class="line">multiplot(g, g2, cols = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the correlation between <code>y</code> and <code>x1</code> flips signs when adjusting for <code>x2</code></li>
<li><p>this effectively means that within each consecutive group/subset of points (each color gradient) on the left hand plot (unadjusted), there exists a negative relationship between the points while the overall trend is going up</p></li>
<li><strong><em>going back to the swiss data set</em></strong>, the sign of the coefficient for Agriculture <em>reverses</em> itself with the inclusion of Examination and Education (both are <strong><em>negatively correlated</em></strong> with Agriculture)
<ul>
<li>correlation between Agriculture and Education: <code>r round(cor(swiss$Education,swiss$Agriculture),2)</code></li>
<li>correlation between Agriculture and Examination: <code>r round(cor(swiss$Examination,swiss$Agriculture),2)</code></li>
<li>correlation between Education and Examination: <code>r round(cor(swiss$Education, swiss$Examination),2)</code>
<ul>
<li>this means that the two variables are likely to be measuring the same things</li>
</ul></li>
</ul></li>
<li><p><em><strong>Note</strong>: it is <strong>difficult to interpret</strong> and determine which one is the correct model <span class="math inline">\(\rightarrow\)</span> one <strong>should not</strong> claim positive correlation between Agriculture and Fertility simply based on marginal regression <code>lm(Fertility ~ Agriculture, data=swiss)</code> </em></p></li>
</ul>
<h3 id="example-unnecessary-variables">Example: Unnecessary Variables</h3>
<ul>
<li><strong>unnecessary predictors</strong> = variables that don’t provide any new linear information, meaning that the variable are simply <strong><em>linear combinations</em></strong> (multiples, sums) of other predictors/variables</li>
<li>when running a linear regression with unnecessary variables, R automatically drops the linear combinations and returns <code>NA</code> as their coefficients</li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># add a linear combination of agriculture and education variables</span><br><span class="line">z &lt;- swiss$Agriculture + swiss$Education</span><br><span class="line"># run linear regression with unnecessary variables</span><br><span class="line">lm(Fertility ~ . + z, data = swiss)$coef</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see above, the R dropped the unnecessary variable <code>z</code> by excluding it from the linear regression <span class="math inline">\(\rightarrow\)</span> <code>z</code>’s coefficient is <code>NA</code></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="dummy-variables">Dummy Variables</h2>
<ul>
<li><strong>dummy variables</strong> = binary variables that take on value of <strong>1</strong> when the measurement is in a particular group, and <strong>0</strong> when the measurement is not (i.e. in clinical trials, treated = 1, untreated = 1)</li>
<li>in linear model form, <span class="math display">\[Y_i = \beta_0 + X_{i1} \beta_1 + \epsilon_{i}\]</span> where <span class="math inline">\(X_{i1}\)</span> is a binary/dummy variable so that it is a 1 if measurement <span class="math inline">\(i\)</span> is in a group and 0 otherwise
<ul>
<li>for people in the group, the mean or <span class="math inline">\(\mu_{X_{i1} = 1} = E[Y_i] = \beta_0 + \beta_1\)</span></li>
<li>for people <strong><em>not</em></strong> in the group, the mean or <span class="math inline">\(\mu_{X_{i1} = 0} = E[Y_i] = \beta_0\)</span></li>
<li>predicted mean for group = <span class="math inline">\(\hat \beta_0 + \hat \beta_1\)</span></li>
<li>predicted mean for not in group = <span class="math inline">\(\hat \beta_0\)</span></li>
<li>coefficient <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(X_{i1}\)</span> is interpreted as the <strong><em>increase or decrease</em></strong> in the <strong>mean</strong> when comparing two groups (in vs not)</li>
<li><em><strong>Note</strong>: including a dummy variable that is 1 for not in the group would be <strong>redundant</strong> as it would simply be a linear combination <span class="math inline">\(1 - X_{i1}\)</span> </em></li>
</ul></li>
</ul>
<h3 id="more-than-2-levels">More Than 2 Levels</h3>
<ul>
<li>for 3 factor levels, we would need 2 dummy variables and the model would be <span class="math display">\[Y_i = \beta_0 + X_{i1} \beta_1 + X_{i2} \beta_2 + \epsilon_i\]</span></li>
<li>for this example, we will use the above model to analyze US political party affiliations (Democrats vs Republicans vs independents) and denote the variables as follows:
<ul>
<li><span class="math inline">\(X_{i1} = 1\)</span> for Republicans and <span class="math inline">\(0\)</span> otherwise</li>
<li><span class="math inline">\(X_{i2} = 1\)</span> for Democrats and <span class="math inline">\(0\)</span> otherwise</li>
<li>If <span class="math inline">\(i\)</span> is Republican, <span class="math inline">\(X_{i1} = 1\)</span>, <span class="math inline">\(X_{i2} = 0\)</span>, <span class="math inline">\(E[Y_i] = \beta_0 +\beta_1\)</span></li>
<li>If <span class="math inline">\(i\)</span> is Democrat, <span class="math inline">\(X_{i1} = 0\)</span>, <span class="math inline">\(X_{i2} = 1\)</span>, <span class="math inline">\(E[Y_i] = \beta_0 + \beta_2\)</span></li>
<li>If <span class="math inline">\(i\)</span> is independent, <span class="math inline">\(X_{i1} = 0\)</span>, <span class="math inline">\(X_{i2} = 0\)</span>, <span class="math inline">\(E[Y_i] = \beta_0\)</span></li>
<li><span class="math inline">\(\beta_1\)</span> compares Republicans to independents</li>
<li><span class="math inline">\(\beta_2\)</span> compares Democrats to independents</li>
<li><span class="math inline">\(\beta_1 - \beta_2\)</span> compares Republicans to Democrats</li>
</ul></li>
<li><em><strong>Note</strong>: choice of reference category (independent in this case) changes the interpretation </em>
<ul>
<li><strong>reference category</strong> = the group whose binary variable has been eliminated</li>
</ul></li>
<li>the same principles explained above can be expanded to <span class="math inline">\(p\)</span> level model <span class="math display">\[Y_i = \beta_0 + X_{i1} \beta_1 + X_{i2} \beta_2 + \ldots + X_{ip} \beta_p + \epsilon_i\]</span></li>
</ul>
<h3 id="example-6-factor-level-insect-spray-data">Example: 6 Factor Level Insect Spray Data</h3>
<ul>
<li>below is a violin plot of the 6 different types (A, B, C, D, E, and F) of insect sprays and their potency (kill count) from <code>InsectSprays</code> data set
<ul>
<li><em><strong>Note</strong>: the varying width of each bar indicates the density of measurement at each value </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># load insect spray data</span><br><span class="line">data(InsectSprays)</span><br><span class="line">ggplot(data = InsectSprays, aes(y = count, x = spray, fill  = spray)) +</span><br><span class="line">	geom_violin(colour = &quot;black&quot;, size = 2) + xlab(&quot;Type of spray&quot;) +</span><br><span class="line">	ylab(&quot;Insect count&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>Linear model fit with group A as reference category</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># linear fit with 5 dummy variables</span><br><span class="line">summary(lm(count ~ spray, data = InsectSprays))$coefficients</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: R automatically converts factor variables into <span class="math inline">\(n-1\)</span> dummy variables and uses the first category as reference </em>
<ul>
<li>mean of group A is therefore the default intercept</li>
</ul></li>
<li>the above coefficients can be interpreted as the difference in means between each group (B, C, D, E, and F) and group A (the intercept)
<ul>
<li>example: the mean of group B is <code>r round(lm(count ~ spray, data = InsectSprays)$coefficients[2],2)</code> higher than the mean of group A, which is <code>r round(lm(count ~ spray, data = InsectSprays)$coefficients[1],2)</code></li>
<li>means for group B/C/D/E/F = the intercept + their respective coefficient</li>
</ul></li>
<li>all t-tests are for comparisons of Sprays versus Spray A</li>
</ul>
<p><strong>Hard-coding the dummy variables</strong></p>
<ul>
<li>this produces the exact same result as the command <code>lm(count ~ spray, data = InsectSprays)</code></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># hard coding dummy variables</span><br><span class="line">lm(count ~ I(1 * (spray == &apos;B&apos;)) + I(1 * (spray == &apos;C&apos;)) +</span><br><span class="line">           I(1 * (spray == &apos;D&apos;)) + I(1 * (spray == &apos;E&apos;)) +</span><br><span class="line">           I(1 * (spray == &apos;F&apos;)), data = InsectSprays)$coefficients</span><br></pre></td></tr></table></figure>
<p><strong>Linear model fit with all 6 categories</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># linear fit with 6 dummy variables</span><br><span class="line">lm(count ~ I(1 * (spray == &apos;B&apos;)) + I(1 * (spray == &apos;C&apos;)) +</span><br><span class="line">   		   I(1 * (spray == &apos;D&apos;)) + I(1 * (spray == &apos;E&apos;)) +</span><br><span class="line">   		   I(1 * (spray == &apos;F&apos;)) + I(1 * (spray == &apos;A&apos;)),</span><br><span class="line">   		   data = InsectSprays)$coefficients</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the coefficient for group <code>A</code> is <code>NA</code></li>
<li>this is because <span class="math inline">\(X_{iA} = 1 - X_{iB} - X_{iC}- X_{iD}- X_{iE}- X_{iF}\)</span>, or the dummy variable for A is a linear combination of the rest of the dummy variables</li>
</ul>
<p><strong>Linear model fit with omitted intercept</strong></p>
<ul>
<li>eliminating the intercept would mean that each group is compared to the value 0, which would yield 6 variables since A is no longer the reference category</li>
<li>this means that the coefficients for the 6 variables are simply the <strong><em>mean</em></strong> of each group
<ul>
<li>when <span class="math inline">\(X_{iA} = 1\)</span>, all the other dummy variables become 0, which means the linear model becomes <span class="math display">\[Y_i = \beta_A + \epsilon_i\]</span></li>
<li>then <span class="math inline">\(E[Y_i] = \beta_A = \mu_A\)</span></li>
<li>this makes sense because the <strong><em>best prediction</em></strong> for the kill count of spray of type A is the <strong><em>mean</em></strong> of recorded kill counts of A spray</li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># linear model with omitted intercept</span><br><span class="line">summary(lm(count ~ spray - 1, data = InsectSprays))$coefficients</span><br><span class="line"># actual means of count by each variable</span><br><span class="line">round(tapply(InsectSprays$count, InsectSprays$spray, mean), 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>all t-tests are for whether the groups are different than zero (i.e. are the expected counts 0 for that spray?)</li>
<li><p>to compare between different categories, say B vs C, we can simply subtract the coefficients</p></li>
<li>to reorient the model with other groups as reference categories, we can simply <strong>reorder the levels</strong> for the factor variable
<ul>
<li><code>relevel(var, &quot;l&quot;)</code> = reorders the factor levels within the factor variable <code>var</code> such that the specified level “l” is the reference/base/lowest level
<ul>
<li><em><strong>Note</strong>: R automatically uses the first/base level as the reference category during a linear regression </em></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># reorder the levels of spray variable such that C is the lowest level</span><br><span class="line">spray2 &lt;- relevel(InsectSprays$spray, &quot;C&quot;)</span><br><span class="line"># rerun linear regression with releveled factor</span><br><span class="line">summary(lm(count ~ spray2, data = InsectSprays))$coef</span><br></pre></td></tr></table></figure>
<ul>
<li>it is important to note in this example that
<ul>
<li>counts are bounded from below by 0 <span class="math inline">\(\rightarrow\)</span> <strong><em>violates</em></strong> the assumption of normality of the errors</li>
<li>there are counts near zero <span class="math inline">\(\rightarrow\)</span> <strong><em>violates</em></strong> intent of the assumption <span class="math inline">\(\rightarrow\)</span> not acceptable in assuming normal distribution</li>
<li>variance does not appear to be constant across different type of groups <span class="math inline">\(\rightarrow\)</span> violates assumption
<ul>
<li>taking log(counts) + 1 may help (+1 since there are the zero values)</li>
</ul></li>
<li><strong><em>Poisson GLMs</em></strong> are better (don’t have to worry about the assumptions) for fitting count data</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="interactions">Interactions</h2>
<ul>
<li><strong>interactions</strong> between variables can be added to a regression model to test how the outcomes change under different conditions</li>
<li>we will use the data set from the Millennium Development Goal from the UN which can be found <a href="https://github.com/bcaffo/courses/blob/master/07_RegressionModels/02_02_multivariateExamples/hunger.csv" target="_blank" rel="noopener">here</a>
<ul>
<li><code>Numeric</code> = values for children aged &lt;5 years underweight (%)</li>
<li><code>Sex</code> = records whether</li>
<li><code>Year</code> = year when data was recorded</li>
<li><code>Income</code> = income for the child’s parents</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># load in hunger data</span><br><span class="line">hunger &lt;- read.csv(&quot;hunger.csv&quot;)</span><br><span class="line"># exclude the data with &quot;Both Sexes&quot; as values (only want Male vs Female)</span><br><span class="line">hunger &lt;- hunger[hunger$Sex!=&quot;Both sexes&quot;, ]</span><br><span class="line"># structure of data</span><br><span class="line">str(hunger)</span><br></pre></td></tr></table></figure>
<h3 id="model-hungry-year-by-sex">Model: % Hungry ~ Year by Sex</h3>
<ul>
<li>this will include 2 models with 2 separate lines</li>
<li>model for % hungry (<span class="math inline">\(H_F\)</span>) vs year (<span class="math inline">\(Y_F\)</span>) for females is <span class="math display">\[H_{Fi} = \beta_{F0} + \beta_{F1} Y_{Fi} + \epsilon_{Fi}\]</span>
<ul>
<li><span class="math inline">\(\beta_{F0}\)</span> = % of females hungry at year 0</li>
<li><span class="math inline">\(\beta_{F1}\)</span> = decrease in % females hungry per year</li>
<li><span class="math inline">\(\epsilon_{Fi}\)</span> = standard error (or everything we didn’t measure)</li>
</ul></li>
<li>model for % hungry (<span class="math inline">\(H_M\)</span>) vs year (<span class="math inline">\(Y_M\)</span>) for males is <span class="math display">\[H_{Mi} = \beta_{M0} + \beta_{M1} Y_{Mi} + \epsilon_{Mi}\]</span>
<ul>
<li><span class="math inline">\(\beta_{M0}\)</span> = % of males hungry at year 0</li>
<li><span class="math inline">\(\beta_{M1}\)</span> = decrease in % males hungry per year</li>
<li><span class="math inline">\(\epsilon_{Mi}\)</span> = standard error (or everything we didn’t measure)</li>
</ul></li>
<li>each line has <strong>different</strong> residuals, standard errors, and variances</li>
<li><em><strong>Note</strong>: <span class="math inline">\(\beta_{F0}\)</span> and <span class="math inline">\(\beta_{M0}\)</span> are the interpolated intercept at year 0, which does hold any interpretable value for the model </em>
<ul>
<li>it’s possible to subtract the model by a meaningful value (% hungry at 1970, or average), which moves the intercept of the lines to something interpretable</li>
</ul></li>
<li><em><strong>Note</strong>: we are also assuming the error terms <span class="math inline">\(\epsilon_{Fi}\)</span> and <span class="math inline">\(\epsilon_{Mi}\)</span> are Gaussian distributions <span class="math inline">\(\rightarrow\)</span> mean = 0 </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># run linear model with Numeric vs Year for male and females</span><br><span class="line">male.fit &lt;- lm(Numeric ~ Year, data = hunger[hunger$Sex == &quot;Male&quot;, ])</span><br><span class="line">female.fit &lt;- lm(Numeric ~ Year, data = hunger[hunger$Sex == &quot;Female&quot;, ])</span><br><span class="line"># plot % hungry vs the year</span><br><span class="line">plot(Numeric ~ Year, data = hunger, pch = 19, col=(Sex==&quot;Male&quot;)*1+1)</span><br><span class="line"># plot regression lines for both</span><br><span class="line">abline(male.fit, lwd = 3, col = &quot;black&quot;)</span><br><span class="line">abline(female.fit, lwd = 3, col = &quot;red&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="model-hungry-year-sex-binary-variable">Model: % Hungry ~ Year + Sex (Binary Variable)</h3>
<ul>
<li>this will include 1 model with 2 separate lines with the <strong><em>same</em></strong> slope</li>
<li><p>model for % hungry (<span class="math inline">\(H\)</span>) vs year (<span class="math inline">\(Y\)</span>) and dummy variable for sex (<span class="math inline">\(X\)</span>) is <span class="math display">\[H_{i} = \beta_{0} + \beta_{1}X_i + \beta_{2} Y_{i} + \epsilon_{i}^*\]</span></p>
<ul>
<li><span class="math inline">\(\beta_{0}\)</span> = % of females hungry at year 0</li>
<li><span class="math inline">\(\beta_{0} + \beta_{1}\)</span> = % of males hungry at year 0
<ul>
<li><em><strong>Note</strong>: the term <span class="math inline">\(\beta_{1}X_i\)</span> is effectively an <strong>adjustment</strong> for the intercept for males and DOES NOT alter the slope in anyway </em></li>
<li><span class="math inline">\(\beta_{1}\)</span> = difference in means of males vs females</li>
</ul></li>
<li><span class="math inline">\(\beta_{2}\)</span> = decrease in % hungry (males and females) per year
<ul>
<li>this means that the slope is <strong><em>constant</em></strong> for both females and males</li>
</ul></li>
<li><span class="math inline">\(\epsilon_{i}^*\)</span> = standard error (or everything we didn’t measure)
<ul>
<li>we are still assuming Gaussian error term</li>
</ul></li>
</ul></li>
<li><code>abline(intercept, slope)</code> = adds a line to the existing plot based on the intercept and slope provided
<ul>
<li><code>abline(lm)</code> = plots the linear regression line on the plot</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># run linear model with Numeric vs Year and Sex</span><br><span class="line">both.fit &lt;- lm(Numeric ~ Year+Sex, data = hunger)</span><br><span class="line"># print fit</span><br><span class="line">both.fit$coef</span><br><span class="line"># plot % hungry vs the year</span><br><span class="line">plot(Numeric ~ Year, data = hunger, pch = 19, col=(Sex==&quot;Male&quot;)*1+1)</span><br><span class="line"># plot regression lines for both (same slope)</span><br><span class="line">abline(both.fit$coef[1], both.fit$coef[2], lwd = 3, col = &quot;black&quot;)</span><br><span class="line">abline(both.fit$coef[1]+both.fit$coef[3], both.fit$coef[2], lwd = 3, col = &quot;red&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="model-hungry-year-sex-year-sex-binary-interaction">Model: % Hungry ~ Year + Sex + Year * Sex (Binary Interaction)</h3>
<ul>
<li>this will include 1 model with an interaction term with binary variable, which produces 2 lines with <strong><em>different</em></strong> slopes</li>
<li>we can introduce an interaction term to the previous model to capture the different slopes between males and females</li>
<li>model for % hungry (<span class="math inline">\(H\)</span>) vs year (<span class="math inline">\(Y\)</span>), sex (<span class="math inline">\(X\)</span>), and interaction between year and sex (<span class="math inline">\(Y \times X\)</span>) is <span class="math display">\[H_{i} = \beta_{0} + \beta_{1}X_i + \beta_{2} Y_{i} + \beta_{3}X_i Y_{i} + \epsilon_{i}^+\]</span>
<ul>
<li><span class="math inline">\(\beta_{0}\)</span> = % of females hungry at year 0</li>
<li><span class="math inline">\(\beta_{0} + \beta_{1}\)</span> = % of males hungry at year 0
<ul>
<li><span class="math inline">\(\beta_{1}\)</span> = change in <strong><em>intercept</em></strong> for males</li>
</ul></li>
<li><span class="math inline">\(\beta_{2}\)</span> = decrease in % hungry (females) per year</li>
<li><span class="math inline">\(\beta_{2} + \beta_{3}\)</span> = decrease in % hungry (males) per year
<ul>
<li><span class="math inline">\(\beta_{3}\)</span> = change in <strong><em>slope</em></strong> for males</li>
</ul></li>
<li><span class="math inline">\(\epsilon_{i}^+\)</span> = standard error (or everything we didn’t measure)</li>
</ul></li>
<li>expected value for males is <span class="math inline">\(E[H_i]_M = (\beta_0 + \beta_1) + (\beta_2 + \beta_3) Y_i\)</span></li>
<li>expected value for females is <span class="math inline">\(E[H_i]_F = \beta_0 + \beta_2 Y_i\)</span>
<ul>
<li><span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_3\)</span> are effectively adjusting the intercept and slope for males</li>
</ul></li>
<li><code>lm(outcome ~ var1*var2)</code> = whenever an interaction is specified in <code>lm</code> function using the <code>*</code> operator, the individual terms are added automatically
<ul>
<li><code>lm(outcome ~ var1+var2+var1*var2)</code> = builds the exact same model</li>
<li><code>lm(outcome ~ var1:var2)</code> = builds linear model with <strong><em>only</em></strong> the interaction term (specified by <code>:</code> operator)</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># run linear model with Numeric vs Year and Sex and interaction term</span><br><span class="line">interaction.fit &lt;- lm(Numeric ~ Year*Sex, data = hunger)</span><br><span class="line"># print fit</span><br><span class="line">interaction.fit$coef</span><br><span class="line"># plot % hungry vs the year</span><br><span class="line">plot(Numeric ~ Year, data = hunger, pch = 19, col=(Sex==&quot;Male&quot;)*1+1)</span><br><span class="line"># plot regression lines for both (different slope)</span><br><span class="line">abline(interaction.fit$coef[1], interaction.fit$coef[2], lwd = 3, col = &quot;black&quot;)</span><br><span class="line">abline(interaction.fit$coef[1]+interaction.fit$coef[3],</span><br><span class="line">	interaction.fit$coef[2]+interaction.fit$coef[4], lwd = 3, col = &quot;red&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="example-hungry-year-income-year-income-continuous-interaction">Example: % Hungry ~ Year + Income + Year * Income (Continuous Interaction)</h3>
<ul>
<li>this will include 1 model with an interaction term with continuous variable, which produces a curve through the plot</li>
<li>for <strong>continuous interactions</strong> (two continuous variables) with model <span class="math display">\[Y{i} = \beta_{0} + \beta_{1} X_{1i} + \beta_{2} X_{2i} + \beta_{3} X_{1i} X_{2i} + \epsilon_{i}\]</span> the expected value for a given set of values <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> is defined as <span class="math display">\[E[Y_i|X_{1i}=x_1, X_{2i}=x_2] = \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \beta_{3} x_{1} x_{2}\]</span></li>
<li>holding <span class="math inline">\(X_2\)</span> constant and varying <span class="math inline">\(X_1\)</span> by <span class="math inline">\(1\)</span>, we have <span class="math display">\[\begin{aligned}
\frac{\partial Y_i}{\partial X_{1i}} &amp; = E[Y_i|X_{1i}=x_1 + 1, X_{2i}=x_2] - E[Y_i|X_{1i}=x_1, X_{2i}=x_2] \\
&amp; = \beta_{0} + \beta_{1} (x_{1}+1) + \beta_{2} x_{2} + \beta_{3} (x_{1}+1) x_{2} - [\beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \beta_{3} x_{1} x_{2}]\\
&amp; = \beta_{1} + \beta_{3} x_{2}\\
\end{aligned}\]</span>
<ul>
<li><em><strong>Note</strong>: this means that slope for <span class="math inline">\(X_{1i}\)</span> <strong>not</strong> a constant and is <strong>dependent</strong> on <span class="math inline">\(X_{2i}\)</span> </em></li>
<li><span class="math inline">\(\beta_1\)</span> is the slope for <span class="math inline">\(X_{1i}\)</span> when <span class="math inline">\(X_{2i}\)</span> = 0</li>
</ul></li>
<li>by the same logic, if we vary <span class="math inline">\(X_1\)</span> by <span class="math inline">\(1\)</span> and find the change, and vary <span class="math inline">\(X_2\)</span> by <span class="math inline">\(1\)</span> and find the change, we get <span class="math display">\[\begin{aligned}
\frac{\partial}{\partial X_{2i}} \left(\frac{\partial Y_i}{\partial X_{1i}}\right) &amp; = E[Y_i|X_{1i}=x_1 + 1, X_{2i}=x_2+1] - E[Y_i|X_{1i}=x_1, X_{2i}=x_2+1]\\
&amp; \qquad - \Big(E[Y_i|X_{1i}=x_1+1, X_{2i}=x_2] - E[Y_i|X_{1i}=x_1, X_{2i}=x_2]\Big)\\
&amp; = \beta_{0} + \beta_{1} (x_{1}+1) + \beta_{2} (x_{2}+1) + \beta_{3} (x_{1}+1) (x_{2}+1) - [\beta_{0} + \beta_{1} x_{1} + \beta_{2} (x_{2}+1) + \beta_{3} x_{1} (x_{2}+1)]\\
&amp; \qquad - \Big(\beta_{0} + \beta_{1} (x_{1}+1) + \beta_{2} x_{2} + \beta_{3} (x_{1}+1) x_{2} - [\beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \beta_{3} x_{1} x_{2}]\Big)\\
&amp; = \beta_{3}\\
\end{aligned}\]</span>
<ul>
<li>this can be interpreted as <span class="math inline">\(\beta_3\)</span> = the <strong>expected change in <span class="math inline">\(Y\)</span></strong> per <strong>unit change in <span class="math inline">\(X_1\)</span></strong> per <strong>unit change</strong> of <span class="math inline">\(X_2\)</span></li>
<li>in other words, <span class="math inline">\(\beta_3\)</span> = the change in slope of <span class="math inline">\(X_1\)</span> per unit change of <span class="math inline">\(X_2\)</span></li>
</ul></li>
<li>coming back to the hunger data, model for % hungry (<span class="math inline">\(H\)</span>) vs year (<span class="math inline">\(Y\)</span>), income (<span class="math inline">\(I\)</span>), and interaction between year and income (<span class="math inline">\(Y \times I\)</span>) is <span class="math display">\[H_{i} = \beta_{0} + \beta_{1}I_i + \beta_{2} Y_{i} + \beta_{3}I_i Y_{i} + \epsilon_{i}^+\]</span>
<ul>
<li><span class="math inline">\(\beta_{0}\)</span> = % hungry children (whose parents have no income) at year 0</li>
<li><span class="math inline">\(\beta_{1}\)</span> = change in % hungry children for <strong>each dollar in income</strong> in year 0</li>
<li><span class="math inline">\(\beta_{2}\)</span> = change in % hungry children (whose parents have no income) <strong>per year</strong></li>
<li><span class="math inline">\(\beta_{3}\)</span> = change in % hungry children <strong>per year</strong> and for <strong>each dollar in income</strong>
<ul>
<li>if income is <span class="math inline">\(\$10,000\)</span>, then the change in % hungry children <strong>per year</strong> will be <span class="math inline">\(\beta_1 - 10000 \times \beta_3\)</span></li>
</ul></li>
<li><span class="math inline">\(\epsilon_{i}^+\)</span> = standard error (or everything we didn’t measure)</li>
</ul></li>
<li><em><strong>Note</strong>: much care needs to be taken when interpreting these coefficients </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># generate some income data</span><br><span class="line">hunger$Income &lt;- 1:nrow(hunger)*10 + 500*runif(nrow(hunger), 0, 10) +</span><br><span class="line">    runif(nrow(hunger), 0, 500)^1.5</span><br><span class="line"># run linear model with Numeric vs Year and Income and interaction term</span><br><span class="line">lm(Numeric ~ Year*Income, data = hunger)$coef</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="multivariable-simulation">Multivariable Simulation</h2>
<ul>
<li>we will generate a series of simulated data so that we know the true relationships, and then run linear regressions to interpret and compare the results to truth</li>
<li><strong>treatment effect</strong> = effect of adding the treatment variable <span class="math inline">\(t\)</span> to the regression model (i.e. how adding <span class="math inline">\(t\)</span> changes the regression lines)
<ul>
<li>effectively measures how much the regression lines for the two groups separate with regression <code>lm(y ~ x + t)</code></li>
</ul></li>
<li><strong>adjustment effect</strong> = adjusting the regression for effects of <span class="math inline">\(x\)</span> such that we just look at how <span class="math inline">\(t\)</span> is <strong><em>marginally related</em></strong> to <span class="math inline">\(Y\)</span>
<ul>
<li>ignore all variation of <span class="math inline">\(x\)</span> and simply look at the group means of <span class="math inline">\(t = 1\)</span> vs <span class="math inline">\(t = 0\)</span></li>
</ul></li>
</ul>
<h3 id="simulation-1---treatment-adjustment-effect">Simulation 1 - Treatment = Adjustment Effect</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \beta_2 t_i + \epsilon_i\]</span> where <span class="math inline">\(t = \{0, 1\} \rightarrow\)</span> binary variable</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2), runif(n/2));</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 2; beta2 &lt;- 1; sigma &lt;- .2</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x * beta1 + t * beta2 + rnorm(n, sd = sigma)</span><br><span class="line"># set up axes</span><br><span class="line">plot(x, y, type = &quot;n&quot;, frame = FALSE)</span><br><span class="line"># plot linear fit of y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2, col = &quot;blue&quot;)</span><br><span class="line"># plot means of the two groups (t = 0 vs t = 1)</span><br><span class="line">abline(h = mean(y[1 : (n/2)]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line">abline(h = mean(y[(n/2 + 1) : n]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line"># plot linear fit of y vs x and t</span><br><span class="line">fit &lt;- lm(y ~ x + t)</span><br><span class="line"># plot the two lines corresponding to (t = 0 vs t = 1)</span><br><span class="line">abline(coef(fit)[1], coef(fit)[2], lwd = 3)</span><br><span class="line">abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)</span><br><span class="line"># add in the actual data points</span><br><span class="line">points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 1)</span><br><span class="line">points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 1)</span><br><span class="line"># print treatment and adjustment effects</span><br><span class="line">rbind(&quot;Treatment Effect&quot; = lm(y~t+x)$coef[2], &quot;Adjustment Effect&quot; = lm(y~t)$coef[2])</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, the elements are as follows:
<ul>
<li><span class="math inline">\(red\)</span> lines = means for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t)</code></li>
<li><span class="math inline">\(black\)</span> lines = regression lines for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t + x)</code></li>
<li><span class="math inline">\(blue\)</span> line = overall regression of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> line representing <code>lm(y ~ x)</code></li>
</ul></li>
<li>from the graph, we can see that
<ul>
<li><span class="math inline">\(x\)</span> variable is <strong><em>unrelated</em></strong> to group status <span class="math inline">\(t\)</span>
<ul>
<li>distribution of each group (salmon vs light blue) of <span class="math inline">\(Y\)</span> vs <span class="math inline">\(X\)</span> is effectively the same</li>
</ul></li>
<li><span class="math inline">\(x\)</span> variable is <strong><em>related</em></strong> to <span class="math inline">\(Y\)</span>, but the intercept <strong>depends</strong> on group status <span class="math inline">\(t\)</span></li>
<li>group variable <span class="math inline">\(t\)</span> is <strong><em>related</em></strong> to <span class="math inline">\(Y\)</span>
<ul>
<li>relationship between <span class="math inline">\(t\)</span> and <span class="math inline">\(Y\)</span> disregarding <span class="math inline">\(x \approx\)</span> the same as holding <span class="math inline">\(x\)</span> constant</li>
<li>difference in group means <span class="math inline">\(\approx\)</span> difference in regression lines</li>
<li><strong><em>treatment effect</em></strong> (difference in regression lines) <span class="math inline">\(\approx\)</span> <strong><em>adjustment effect</em></strong> (difference in group means)</li>
</ul></li>
</ul></li>
</ul>
<h3 id="simulation-2---no-treatment-effect">Simulation 2 - No Treatment Effect</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \beta_2 t_i + \epsilon_i\]</span> where <span class="math inline">\(t = \{0, 1\} \rightarrow\)</span> binary variable</li>
<li>in this case, <span class="math inline">\(\beta_2\)</span> is set to <span class="math inline">\(0\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2), 1.5 + runif(n/2));</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 2; beta2 &lt;- 0; sigma &lt;- .2</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x * beta1 + t * beta2 + rnorm(n, sd = sigma)</span><br><span class="line"># set up axes</span><br><span class="line">plot(x, y, type = &quot;n&quot;, frame = FALSE)</span><br><span class="line"># plot linear fit of y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2, col = &quot;blue&quot;)</span><br><span class="line"># plot means of the two groups (t = 0 vs t = 1)</span><br><span class="line">abline(h = mean(y[1 : (n/2)]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line">abline(h = mean(y[(n/2 + 1) : n]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line"># plot linear fit of y vs x and t</span><br><span class="line">fit &lt;- lm(y ~ x + t)</span><br><span class="line"># plot the two lines corresponding to (t = 0 vs t = 1)</span><br><span class="line">abline(coef(fit)[1], coef(fit)[2], lwd = 3)</span><br><span class="line">abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)</span><br><span class="line"># add in the actual data points</span><br><span class="line">points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 1)</span><br><span class="line">points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 1)</span><br><span class="line"># print treatment and adjustment effects</span><br><span class="line">rbind(&quot;Treatment Effect&quot; = lm(y~t+x)$coef[2], &quot;Adjustment Effect&quot; = lm(y~t)$coef[2])</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, the elements are as follows:
<ul>
<li><span class="math inline">\(red\)</span> lines = means for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t)</code></li>
<li><span class="math inline">\(black\)</span> lines = regression lines for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t + x)</code>
<ul>
<li>in this case, both lines correspond to <code>lm(y ~ x)</code> since coefficient of <span class="math inline">\(t\)</span> or <span class="math inline">\(\beta_2 = 0\)</span></li>
</ul></li>
<li><span class="math inline">\(blue\)</span> line = overall regression of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> line representing <code>lm(y ~ x)</code>
<ul>
<li>this is overwritten by the <span class="math inline">\(black\)</span> lines</li>
</ul></li>
</ul></li>
<li>from the graph, we can see that
<ul>
<li><span class="math inline">\(x\)</span> variable is <strong><em>highly related</em></strong> to group status <span class="math inline">\(t\)</span>
<ul>
<li>clear shift in <span class="math inline">\(x\)</span> with salmon vs light blue groups</li>
</ul></li>
<li><span class="math inline">\(x\)</span> variable is <strong><em>related</em></strong> to <span class="math inline">\(Y\)</span>, but the intercept <strong>does not depend</strong> on group status <span class="math inline">\(t\)</span>
<ul>
<li>intercepts for both lines are the same</li>
</ul></li>
<li><span class="math inline">\(x\)</span> variable shows <strong><em>similar relationships</em></strong> to <span class="math inline">\(Y\)</span> for both groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>, or salmon vs lightblue)
<ul>
<li>the <span class="math inline">\(x\)</span> values of the two groups of points both seem to be linearly correlated with <span class="math inline">\(Y\)</span></li>
</ul></li>
<li>group variable <span class="math inline">\(t\)</span> is <strong><em>marginally related</em></strong> to <span class="math inline">\(Y\)</span> when disregarding X
<ul>
<li><span class="math inline">\(x\)</span> values capture most of the variation</li>
<li><strong><em>adjustment effect</em></strong> (difference in group means) is very large</li>
</ul></li>
<li>group variable <span class="math inline">\(t\)</span> is <strong><em>unrelated or has very little</em></strong> effect on <span class="math inline">\(Y\)</span>
<ul>
<li><strong><em>treatment effect</em></strong> is very small or non-existent</li>
<li><em><strong>Note</strong>: the groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) are <strong>incomparable</strong> since there is no data to inform the relationship between <span class="math inline">\(t\)</span> and <span class="math inline">\(Y\)</span> </em></li>
<li>the groups (salmon vs lightblue) don’t have any overlaps so we have no idea how they behave</li>
<li>this conclusion is based on the constructed alone</li>
</ul></li>
</ul></li>
</ul>
<h3 id="simulation-3---treatment-reverses-adjustment-effect">Simulation 3 - Treatment Reverses Adjustment Effect</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \beta_2 t_i + \epsilon_i\]</span> where <span class="math inline">\(t = \{0, 1\} \rightarrow\)</span> binary variable</li>
<li>in this case, <span class="math inline">\(\beta_0\)</span> is set to <span class="math inline">\(0\)</span> <span class="math inline">\(\rightarrow\)</span> no intercept</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2), .9 + runif(n/2));</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 2; beta2 &lt;- -1; sigma &lt;- .2</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x * beta1 + t * beta2 + rnorm(n, sd = sigma)</span><br><span class="line"># set up axes</span><br><span class="line">plot(x, y, type = &quot;n&quot;, frame = FALSE)</span><br><span class="line"># plot linear fit of y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2, col = &quot;blue&quot;)</span><br><span class="line"># plot means of the two groups (t = 0 vs t = 1)</span><br><span class="line">abline(h = mean(y[1 : (n/2)]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line">abline(h = mean(y[(n/2 + 1) : n]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line"># plot linear fit of y vs x and t</span><br><span class="line">fit &lt;- lm(y ~ x + t)</span><br><span class="line"># plot the two lines corresponding to (t = 0 vs t = 1)</span><br><span class="line">abline(coef(fit)[1], coef(fit)[2], lwd = 3)</span><br><span class="line">abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)</span><br><span class="line"># add in the actual data points</span><br><span class="line">points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 1)</span><br><span class="line">points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 1)</span><br><span class="line"># print treatment and adjustment effects</span><br><span class="line">rbind(&quot;Treatment Effect&quot; = lm(y~t+x)$coef[2], &quot;Adjustment Effect&quot; = lm(y~t)$coef[2])</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, the elements are as follows:
<ul>
<li><span class="math inline">\(red\)</span> lines = means for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t)</code></li>
<li><span class="math inline">\(black\)</span> lines = regression lines for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t + x)</code></li>
<li><span class="math inline">\(blue\)</span> line = overall regression of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> line representing <code>lm(y ~ x)</code></li>
</ul></li>
<li>from the graph, we can see that
<ul>
<li>disregarding/adjusting for <span class="math inline">\(x\)</span>, the mean for salmon group is <strong><em>higher</em></strong> than the mean of the blue group (<strong><em>adjustment effect</em></strong> is positive)</li>
<li>when adding <span class="math inline">\(t\)</span> into the linear model, the treatment actually reverses the orders of the group <span class="math inline">\(\rightarrow\)</span> the mean for salmon group is <strong><em>lower</em></strong> than the mean of the blue group (<strong><em>treatment effect</em></strong> is negative)</li>
<li>group variable <span class="math inline">\(t\)</span> is <strong><em>related</em></strong> to <span class="math inline">\(x\)</span></li>
<li>some points overlap so it is possible to compare the subsets two groups holding <span class="math inline">\(x\)</span> fixed</li>
</ul></li>
</ul>
<h3 id="simulation-4---no-adjustment-effect">Simulation 4 - No Adjustment Effect</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \beta_2 t_i + \epsilon_i\]</span> where <span class="math inline">\(t = \{0, 1\} \rightarrow\)</span> binary variable</li>
<li>in this case, <span class="math inline">\(\beta_0\)</span> is set to <span class="math inline">\(0\)</span> <span class="math inline">\(\rightarrow\)</span> no intercept</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(.5 + runif(n/2), runif(n/2));</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 2; beta2 &lt;- 1; sigma &lt;- .2</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x * beta1 + t * beta2 + rnorm(n, sd = sigma)</span><br><span class="line"># set up axes</span><br><span class="line">plot(x, y, type = &quot;n&quot;, frame = FALSE)</span><br><span class="line"># plot linear fit of y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2, col = &quot;blue&quot;)</span><br><span class="line"># plot means of the two groups (t = 0 vs t = 1)</span><br><span class="line">abline(h = mean(y[1 : (n/2)]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line">abline(h = mean(y[(n/2 + 1) : n]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line"># plot linear fit of y vs x and t</span><br><span class="line">fit &lt;- lm(y ~ x + t)</span><br><span class="line"># plot the two lines corresponding to (t = 0 vs t = 1)</span><br><span class="line">abline(coef(fit)[1], coef(fit)[2], lwd = 3)</span><br><span class="line">abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)</span><br><span class="line"># add in the actual data points</span><br><span class="line">points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 1)</span><br><span class="line">points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 1)</span><br><span class="line"># print treatment and adjustment effects</span><br><span class="line">rbind(&quot;Treatment Effect&quot; = lm(y~t+x)$coef[2], &quot;Adjustment Effect&quot; = lm(y~t)$coef[2])</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, the elements are as follows:
<ul>
<li><span class="math inline">\(red\)</span> lines = means for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t)</code></li>
<li><span class="math inline">\(black\)</span> lines = regression lines for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t + x)</code></li>
<li><span class="math inline">\(blue\)</span> line = overall regression of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> line representing <code>lm(y ~ x)</code></li>
</ul></li>
<li>from the graph, we can see that
<ul>
<li>no <strong><em>clear relationship</em></strong> between group variable <span class="math inline">\(t\)</span> and <span class="math inline">\(Y\)</span>
<ul>
<li>two groups have similar distributions with respect to <span class="math inline">\(Y\)</span></li>
</ul></li>
<li><strong><em>treatment effect</em></strong> is substantial
<ul>
<li>separation of regression lines is large</li>
</ul></li>
<li><strong><em>adjustment effect</em></strong> is effectively 0 as there are no large differences between the means of the groups</li>
<li>group variable <span class="math inline">\(t\)</span> is <strong><em>not related</em></strong> to <span class="math inline">\(x\)</span>
<ul>
<li>distribution of each group (salmon vs light blue) of <span class="math inline">\(Y\)</span> vs <span class="math inline">\(X\)</span> is effectively the same</li>
</ul></li>
<li>lots of direct evidence for comparing two groups holding <span class="math inline">\(X\)</span> fixed</li>
</ul></li>
</ul>
<h3 id="simulation-5---binary-interaction">Simulation 5 - Binary Interaction</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \beta_2 t_i + \beta_3 x_i t_i + \epsilon_i\]</span> where <span class="math inline">\(t = \{0, 1\} \rightarrow\)</span> binary variable</li>
<li>in this case, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_2\)</span> are set to <span class="math inline">\(0\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2, -1, 1), runif(n/2, -1, 1));</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 2; beta2 &lt;- 0; beta3 &lt;- -4; sigma &lt;- .2</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x * beta1 + t * beta2 + t * x * beta3 + rnorm(n, sd = sigma)</span><br><span class="line"># set up axes</span><br><span class="line">plot(x, y, type = &quot;n&quot;, frame = FALSE)</span><br><span class="line"># plot linear fit of y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2, col = &quot;blue&quot;)</span><br><span class="line"># plot means of the two groups (t = 0 vs t = 1)</span><br><span class="line">abline(h = mean(y[1 : (n/2)]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line">abline(h = mean(y[(n/2 + 1) : n]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line"># plot linear fit of y vs x and t and interaction term</span><br><span class="line">fit &lt;- lm(y ~ x + t + I(x * t))</span><br><span class="line"># plot the two lines corresponding to (t = 0 vs t = 1)</span><br><span class="line">abline(coef(fit)[1], coef(fit)[2], lwd = 3)</span><br><span class="line">abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2] + coef(fit)[4], lwd = 3)</span><br><span class="line"># add in the actual data points</span><br><span class="line">points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 1)</span><br><span class="line">points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 1)</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, the elements are as follows:
<ul>
<li><span class="math inline">\(red\)</span> lines = means for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t)</code></li>
<li><span class="math inline">\(black\)</span> lines = regression lines for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t + x + t*x)</code></li>
<li><span class="math inline">\(blue\)</span> line = overall regression of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> line representing <code>lm(y ~ x)</code>
<ul>
<li>this is completely meaningless in this case</li>
</ul></li>
</ul></li>
<li>from the graph, we can see that
<ul>
<li><strong><em>treatment effect</em></strong> does not apply since it varies with <span class="math inline">\(x\)</span>
<ul>
<li>impact of treatment/group variable <strong>reverses itself</strong> depending on <span class="math inline">\(x\)</span></li>
</ul></li>
<li><strong><em>adjustment effect</em></strong> is effectively zero as the means of the two groups are very similar</li>
<li>both intercept and slope of the two lines depend on the group variable <span class="math inline">\(t\)</span></li>
<li>group variable and <span class="math inline">\(x\)</span> are <strong><em>unrelated</em></strong></li>
<li>lots of information for comparing group effects holding <span class="math inline">\(x\)</span> fixed</li>
</ul></li>
</ul>
<h3 id="simulation-6---continuous-adjustment">Simulation 6 - Continuous Adjustment</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">p &lt;- 1; n &lt;- 100; x2 &lt;- runif(n); x1 &lt;- p * runif(n) - (1 - p) * x2</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 1; beta2 &lt;- 4 ; sigma &lt;- .01</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x1 * beta1 + beta2 * x2 + rnorm(n, sd = sigma)</span><br><span class="line"># plot y vs x1 and x2</span><br><span class="line">qplot(x1, y) + geom_point(aes(colour=x2)) + geom_smooth(method = lm)</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, we plotted <span class="math inline">\(y\)</span> vs <span class="math inline">\(x_{1}\)</span> with <span class="math inline">\(x_{2}\)</span> denoted as the gradient of color from blue to white</li>
<li>to investigate the bivariate relationship more clearly, we can use the following command from the <code>rgl</code> package to generate <strong><em>3D plots</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgl::plot3d(x1, x2, y)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_{1}\)</span></em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/5.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_{2}\)</span></em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/6.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em><span class="math inline">\(x_{1}\)</span> vs <span class="math inline">\(x_{2}\)</span></em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/7.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>residual plot with effect of <span class="math inline">\(x_2\)</span> removed from both <span class="math inline">\(y\)</span> and <span class="math inline">\(x_1\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># plot the residuals for y and x1 with x2 removed</span><br><span class="line">plot(resid(lm(x1 ~ x2)), resid(lm(y ~ x2)), frame = FALSE,</span><br><span class="line">	col = &quot;black&quot;, bg = &quot;lightblue&quot;, pch = 21, cex = 1)</span><br><span class="line"># add linear fit line</span><br><span class="line">abline(lm(I(resid(lm(y ~ x2))) ~ I(resid(lm(x1 ~ x2)))), lwd = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>from the generated plots above, we can see that
<ul>
<li><span class="math inline">\(x_{1}\)</span> is <strong><em>unrelated</em></strong> to <span class="math inline">\(x_{2}\)</span></li>
<li><span class="math inline">\(x_{2}\)</span> <strong><em>strongly correlated</em></strong> to <span class="math inline">\(y\)</span></li>
<li>relationship between <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(y\)</span> (loosely correlated <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(R^2\)</span> = <code>r round(summary(lm(y ~ x1))$r.squared,2)</code>) <strong><em>largely unchanged</em></strong> by when <span class="math inline">\(x_{2}\)</span> is considered
<ul>
<li><span class="math inline">\(x_{2}\)</span> captures the vast majority of variation in data</li>
<li>there exists almost no residual variability after removing <span class="math inline">\(x_{2}\)</span></li>
</ul></li>
</ul></li>
</ul>
<h3 id="summary-and-considerations">Summary and Considerations</h3>
<ul>
<li>modeling multivariate relationships is <strong><em>difficult</em></strong>
<ul>
<li>modeling for prediction is fairly straight forward</li>
<li>interpreting the regression lines is much harder, as adjusting for variables can have profound effect on variables of interest</li>
</ul></li>
<li>it is often recommended to explore with simulations to see how inclusion or exclusion of another variable affects the relationship of variable of interest and the outcome</li>
<li>variable selection simply affects associations between outcome and predictors, using the model to formulate causal relationship are even more difficult (entire field dedicated to this <span class="math inline">\(\rightarrow\)</span> <strong><em>causal inference</em></strong>)</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="residuals-and-diagnostics">Residuals and Diagnostics</h2>
<ul>
<li>recall that the <strong>generalized linear model</strong> is defined as <span class="math display">\[Y_i =  \sum_{k=1}^p X_{ik} \beta_j + \epsilon_{i}\]</span> where <span class="math inline">\(\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\)</span></li>
<li>the <strong>predicted outcome</strong>, <span class="math inline">\(\hat Y_i\)</span>, is defined as <span class="math display">\[\hat Y_i =  \sum_{k=1}^p X_{ik} \hat \beta_j\]</span></li>
<li>the <strong>residuals</strong>, <span class="math inline">\(e_i\)</span>, is defined as <span class="math display">\[e_i = Y_i -  \hat Y_i =  Y_i - \sum_{k=1}^p X_{ik} \hat \beta_j\]</span></li>
<li>the unbiased estimate for <strong>residual variation</strong> is defined as <span class="math display">\[\hat \sigma^2_{resid} = \frac{\sum_{i=1}^n e_i^2}{n-p}\]</span> where the denominator is <span class="math inline">\(n-p\)</span> so that <span class="math inline">\(E[\hat \sigma^2] = \sigma^2\)</span></li>
<li>to evaluate the fit and residuals of a linear model generated by R (i.e. <code>fit &lt;- lm(y~x)</code>, we can use the <code>plot(fit)</code> to produce a series of <strong><em>4 diagnostic plots</em></strong>
<ul>
<li><strong><em>Residuals vs Fitted</em></strong> = plots ordinary residuals vs fitted values <span class="math inline">\(\rightarrow\)</span> used to detect patterns for missing variables, heteroskedasticity, etc</li>
<li><strong><em>Scale-Location</em></strong> = plots standardized residuals vs fitted values <span class="math inline">\(\rightarrow\)</span> similar residual plot, used to detect patterns in residuals</li>
<li><strong><em>Normal Q-Q</em></strong> = plots theoretical quantiles for standard normal vs actual quantiles of standardized residuals <span class="math inline">\(\rightarrow\)</span> used to evaluate normality of the errors</li>
<li><strong><em>Residuals vs Leverage</em></strong> = plots cooks distances comparison of fit at that point vs potential for influence of that point <span class="math inline">\(\rightarrow\)</span> used to detect any points that have substantial influence on the regression model</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># load swiss data and</span><br><span class="line">data(swiss)</span><br><span class="line"># run linear regression on Fertility vs all other predictors</span><br><span class="line">fit &lt;- lm(Fertility ~ . , data = swiss)</span><br><span class="line"># generate diagnostic plots in 2 x 2 panels</span><br><span class="line">par(mfrow = c(2, 2)); plot(fit)</span><br></pre></td></tr></table></figure>
<h3 id="outliers-and-influential-points">Outliers and Influential Points</h3>
<ul>
<li><strong>outlier</strong> = an observation that is distant from the other observations of the data set
<ul>
<li>can be results of <a href="http://en.wikipedia.org/wiki/Spurious_relationship" target="_blank" rel="noopener">spurious</a> or real processes</li>
<li>can conform to the regression relationship (i.e being marginally outlying in X or Y, but not outlying given the regression relationship)</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># generate data</span><br><span class="line">n &lt;- 100; x &lt;- rnorm(n); y &lt;- x + rnorm(n, sd = .3)</span><br><span class="line"># set up axes</span><br><span class="line">plot(c(-3, 6), c(-3, 6), type = &quot;n&quot;, frame = FALSE, xlab = &quot;X&quot;, ylab = &quot;Y&quot;)</span><br><span class="line"># plot regression line for y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2)</span><br><span class="line"># plot actual (x, y) pairs</span><br><span class="line">points(x, y, cex = 1, bg = &quot;lightblue&quot;, col = &quot;black&quot;, pch = 21)</span><br><span class="line"># plot 4 points of interest</span><br><span class="line">points(0, 0, cex = 1.5, bg = &quot;darkorange&quot;, col = &quot;black&quot;, pch = 21)</span><br><span class="line">points(0, 5, cex = 1.5, bg = &quot;darkorange&quot;, col = &quot;black&quot;, pch = 21)</span><br><span class="line">points(5, 5, cex = 1.5, bg = &quot;darkorange&quot;, col = &quot;black&quot;, pch = 21)</span><br><span class="line">points(5, 0, cex = 1.5, bg = &quot;darkorange&quot;, col = &quot;black&quot;, pch = 21)</span><br></pre></td></tr></table></figure>
<ul>
<li>different outliers can have <strong><em>varying</em></strong> degrees of <strong><em>influence</em></strong>
<ul>
<li>influence = actual effect on model fit</li>
<li>leverage = potential for influence</li>
</ul></li>
<li>in the plot above, we examine 4 different points of interest (in orange)
<ul>
<li><strong>lower left</strong>: low leverage, low influence, <strong><em>not</em></strong> an outlier in any sense</li>
<li><strong>upper left</strong>: low leverage, low influence, classified as outlier because it does not conform to the regression relationship
<ul>
<li><em><strong>Note</strong>: this point, though far away from the rest, <strong>does not</strong> impact the regression line since it lies in the middle of the data range because the regression line must always pass through the mean/center of observations </em></li>
</ul></li>
<li><strong>upper right</strong>: high leverage, low influence, classified as outlier because it lies far away from the rest of the data
<ul>
<li><em><strong>Note</strong>: this point has low influence on regression line because it conforms to the overall regression relationship </em></li>
</ul></li>
<li><strong>lower right</strong>: high leverage, high influence, classified as outlier because it lies far away from the rest of the data AND it does not conform to the regression relationship</li>
</ul></li>
</ul>
<h3 id="influence-measures">Influence Measures</h3>
<ul>
<li>there exists many pre-written functions to measure influence of observations already in the <code>stats</code> package in R
<ul>
<li><em><strong>Note</strong>: typing in <code>?influence.measures</code> in R will display the detailed documentation on all available functions to measure influence </em></li>
<li><em><strong>Note</strong>: the <code>model</code> argument referenced in the following functions is simply the linear fit model generated by the <code>lm</code> function (i.e. <code>model &lt;- lm(y~x</code>) </em></li>
<li><code>rstandard(model)</code> = standardized residuals <span class="math inline">\(\rightarrow\)</span> residuals divided by their standard deviations</li>
<li><code>rstudent(model)</code> = standardized residuals <span class="math inline">\(\rightarrow\)</span> residuals divided by their standard deviations, where the <span class="math inline">\(i^{th}\)</span> data point was deleted in the calculation of the standard deviation for the residual to follow a t distribution</li>
<li><code>hatvalues(model)</code> = measures of leverage</li>
<li><code>dffits(model)</code> = change in the predicted response when the <span class="math inline">\(i^{th}\)</span> point is deleted in fitting the model
<ul>
<li>effectively measures influence of a point on prediction</li>
</ul></li>
<li><code>dfbetas(model)</code> = change in individual coefficients when the <span class="math inline">\(i^{th}\)</span> point is deleted in fitting the model
<ul>
<li>effectively measures influence of the individual coefficients</li>
</ul></li>
<li><code>cooks(model).distance</code> = overall change in coefficients when the <span class="math inline">\(i^{th}\)</span> point is deleted</li>
<li><code>resid(model)</code> = returns ordinary residuals</li>
<li><code>resid(model)/(1-hatvalues(model))</code> = returns <em>PRESS</em> residuals (i.e. the leave one out cross validation residuals)
<ul>
<li>PRESS residuals measure the differences in the response and the predicted response at data point <span class="math inline">\(i\)</span>, where it was not included in the model fitting</li>
<li>effectively measures the prediction error based on model constructed with every other point but the one of interest</li>
</ul></li>
</ul></li>
</ul>
<h3 id="using-influence-measures">Using Influence Measures</h3>
<ul>
<li>the purpose of these functions are to probe the given data in different ways to <strong><em>diagnose</em></strong> different problems
<ul>
<li>patterns in <strong>residual plots</strong> (most important tool) <span class="math inline">\(\rightarrow\)</span> generally indicate some poor aspect of model fit
<ul>
<li>heteroskedasticity <span class="math inline">\(\rightarrow\)</span> non-constant variance</li>
<li>missing model terms</li>
<li>temporal patterns <span class="math inline">\(\rightarrow\)</span> residuals versus collection order/index exhibit pattern</li>
</ul></li>
<li><strong>residual Q-Q plots</strong> plots theoretical quantile vs actual quantiles of residuals
<ul>
<li>investigates whether the errors follow the standard normal distribution</li>
</ul></li>
<li><strong>leverage measures</strong> (hat values) measures the potential to influence the regression model
<ul>
<li>only depend on <span class="math inline">\(x\)</span> or predictor variables</li>
<li>can be useful for diagnosing data entry errors</li>
</ul></li>
<li><strong>influence measures</strong> (i.e. dfbetas) measures actual influence of points on the regression model
<ul>
<li>evaluates how deleting or including this point impact a particular aspect of the model</li>
</ul></li>
</ul></li>
<li>it is important to to understand these functions/tools and use carefully in the <strong><em>appropriate context</em></strong></li>
<li>not all measures have <strong><em>meaningful absolute scales</em></strong>, so it may be useful to apply these measures to different values in the same data set but <strong><em>almost never</em></strong> to different datasets</li>
</ul>
<h3 id="example---outlier-causing-linear-relationship">Example - Outlier Causing Linear Relationship</h3>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># generate random data and point (10, 10)</span><br><span class="line">x &lt;- c(10, rnorm(n)); y &lt;- c(10, c(rnorm(n)))</span><br><span class="line"># plot y vs x</span><br><span class="line">plot(x, y, frame = FALSE, cex = 1, pch = 21, bg = &quot;lightblue&quot;, col = &quot;black&quot;)</span><br><span class="line"># perform linear regression</span><br><span class="line">fit &lt;- lm(y ~ x)</span><br><span class="line"># add regression line to plot</span><br><span class="line">abline(fit)</span><br></pre></td></tr></table></figure>
<ul>
<li>data generated
<ul>
<li>100 points are randomly generated from the standard normal distribution</li>
<li>point (10, 10) added to the data set</li>
</ul></li>
<li>there is no regression relationship between X and Y as the points are simply random noise</li>
<li>the regression relationship was able to be constructed precisely because of the presence of the point (10, 10)
<ul>
<li><span class="math inline">\(R^2\)</span> = <code>r summary(fit)$r.squared</code></li>
<li>a single point has created a strong regression relationship where there shouldn’t be one
<ul>
<li>point (10, 10) has high leverage and high influence</li>
</ul></li>
<li>we can use diagnostics to detect this kind of behavior</li>
</ul></li>
<li><code>dfbetas(fit)</code> = difference in coefficients for including vs excluding each data point
<ul>
<li>the function will return a <code>n x m</code> dataframe, where n = number of values in the original dataset, and m = number of coefficients</li>
<li>for this example, the coefficients are <span class="math inline">\(\beta_0\)</span> (intercept), and <span class="math inline">\(\beta_1\)</span> (slope), and we are interested in the slope</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># calculate the dfbetas for the slope the first 10 points</span><br><span class="line">round(dfbetas(fit)[1 : 10, 2], 3)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>as we can see from above, the slope coefficient would <strong><em>change dramatically</em></strong> if the first point (10, 10) is left out</p></li>
<li><p><code>hatvalues(fit)</code> = measures the potential for influence for each point</p></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># calculate the hat values for the first 10 points</span><br><span class="line">round(hatvalues(fit)[1 : 10], 3)</span><br></pre></td></tr></table></figure>
<ul>
<li>again, as we can see from above, the <strong><em>potential for influence is very large</em></strong> for the first point (10, 10)</li>
</ul>
<h3 id="example---real-linear-relationship">Example - Real Linear Relationship</h3>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># generate data</span><br><span class="line">x &lt;- rnorm(n); y &lt;- x + rnorm(n, sd = .3)</span><br><span class="line"># add an outlier that fits the relationship</span><br><span class="line">x &lt;- c(5, x); y &lt;- c(5, y)</span><br><span class="line"># plot the (x, y) pairs</span><br><span class="line">plot(x, y, frame = FALSE, cex = 1, pch = 21, bg = &quot;lightblue&quot;, col = &quot;black&quot;)</span><br><span class="line"># perform the linear regression</span><br><span class="line">fit2 &lt;- lm(y ~ x)</span><br><span class="line"># add the regression line to the plot</span><br><span class="line">abline(fit2)</span><br></pre></td></tr></table></figure>
<ul>
<li>data generated
<ul>
<li>100 directly correlated points are generated</li>
<li>point (5, 5) added to the data set</li>
</ul></li>
<li>there is a linear relationship between X and Y
<ul>
<li><span class="math inline">\(R^2\)</span> = <code>r summary(fit2)$r.squared</code></li>
<li>point (5, 5) has high leverage and low influence</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># calculate the dfbetas for the slope the first 10 points</span><br><span class="line">round(dfbetas(fit2)[1 : 10, 2], 3)</span><br><span class="line"># calculate the hat values for the first 10 points</span><br><span class="line">round(hatvalues(fit2)[1 : 10], 3)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the point (5, 5) no longer has a large <code>dfbetas</code> value (indication of low influence) but still has a substantial <code>hatvalue</code> (indication of high leverage)
<ul>
<li>this is in line with out expectations</li>
</ul></li>
</ul>
<h3 id="example---stefanski-tas-2007">Example - Stefanski TAS 2007</h3>
<ul>
<li>taken from Leonard A. Stefanski’s paper <a href="http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/Residual_Surrealism_TAS_2007.pdf" target="_blank" rel="noopener">Residual (Sur)Realism</a></li>
<li>data set can be found <a href="http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/orly_owl_files/orly_owl_Lin_4p_5_flat.txt" target="_blank" rel="noopener">here</a></li>
<li>the data itself exhibit no sign of correlation between the variables</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># read data</span><br><span class="line">data &lt;- read.table(&apos;http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/orly_owl_files/orly_owl_Lin_4p_5_flat.txt&apos;,</span><br><span class="line">	header = FALSE)</span><br><span class="line"># construct pairwise plot</span><br><span class="line">pairs(data)</span><br><span class="line"># perform regression on V1 with all other predictors (omitting the intercept)</span><br><span class="line">fit &lt;- lm(V1 ~ . - 1, data = data)</span><br><span class="line"># print the coefficient for linear model</span><br><span class="line">summary(fit)$coef</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the p-values for the coefficients indicate that they are significant</li>
<li>if we take a look at the residual plot, an interesting pattern appears</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot the residuals vs fitted values</span><br><span class="line">plot(predict(fit), resid(fit), pch = &apos;.&apos;)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="model-selection">Model Selection</h2>
<blockquote>
<p>“A model is a lense through which to look at your data” – <strong>Scott Zeger</strong></p>
</blockquote>
<blockquote>
<p>“There’s no such thing as a correct model” – <strong>Brian Caffo</strong></p>
</blockquote>
<ul>
<li><strong>goal for modeling</strong> = find <strong><em>parsimonious, interpretable representations</em></strong> of data that enhance our understanding</li>
<li>whichever model connects data to a true, parsimonious statement would be <strong><em>best</em></strong> model</li>
<li>like nearly all aspects of statistics, good modeling decisions are context dependent</li>
<li>good model for prediction <span class="math inline">\(\neq\)</span> model to establish causal effects
<ul>
<li>prediction model may tolerate more variables and variability</li>
</ul></li>
</ul>
<h3 id="rumsfeldian-triplet">Rumsfeldian Triplet</h3>
<blockquote>
<p>“There are known knowns. These are things we know that we know. There are known unknowns. That is to say, there are things that we know we don’t know. But there are also unknown unknowns. There are things we don’t know we don’t know.” – <strong>Donald Rumsfeld</strong></p>
</blockquote>
<ul>
<li><strong>known knowns</strong> = regressors that we know and have, which will be evaluated to be included in the model</li>
<li><strong>known unknowns</strong> = regressors that we but don’t have but would like to include in the model
<ul>
<li>didn’t or couldn’t collect the data</li>
</ul></li>
<li><strong>unknown unknowns</strong> = regressors that we don’t know about that we should have included in the model</li>
</ul>
<h3 id="general-rules">General Rules</h3>
<ul>
<li><strong>omitting variables</strong> <span class="math inline">\(\rightarrow\)</span> generally results in <strong><em>increased bias</em></strong> in coefficients of interest
<ul>
<li>exceptions are when the omitted variables are uncorrelated with the regressors (variables of interest/included in model)
<ul>
<li><em><strong>Note</strong>: this is why randomize treatments/trials/experiments are the norm; it’s the best strategy to balance confounders, or maximizing the probability that the treatment variable is uncorrelated with variables not in the model </em></li>
<li>often times, due to experiment conditions or data availability, we cannot randomize</li>
<li>however, if there are too many unobserved confounding variables, even randomization won’t help</li>
</ul></li>
</ul></li>
<li><strong>including irrelevant/unnecessary variables</strong> <span class="math inline">\(\rightarrow\)</span> generally <strong><em>increases standard errors</em></strong> (estimated standard deviation) of the coefficients
<ul>
<li><em><strong>Note</strong>: including <strong>any</strong> new variables increases true standard errors of other regressors, so it is not wise to idly add variables into model </em></li>
</ul></li>
<li>whenever highly correlated variables are included in the same model <span class="math inline">\(\rightarrow\)</span> the standard error and therefore the<strong><em>variance</em></strong> of the model <strong><em>increases</em></strong> <span class="math inline">\(\rightarrow\)</span> this is known as <strong>variance inflation</strong>
<ul>
<li>actual increase in standard error of coefficients for adding a regressor = estimated by the ratio of the estimated standard errors minus 1, or in other words <span class="math display">\[\Delta_{\sigma~|~adding~x_2} = \frac{\hat \sigma_{y \sim x_1+x_2}}{\hat \sigma_{y \sim x_1}} - 1\]</span> for all coefficients for the regression model
<ul>
<li><strong><em>example</em></strong>: if standard error of the <span class="math inline">\(\beta_1\)</span> of <code>y~x1+x2</code> = 1.5 and standard error for the <span class="math inline">\(\beta_1\)</span> of <code>y~x1</code> = 0.5, then the actual increase in standard error of the <span class="math inline">\(\beta_1\)</span> = 1.5/0.5 - 1 = 200%</li>
</ul></li>
<li><em><strong>Note</strong>: when the regressors added are orthogonal (statistically independent) to the regressor of interest, then there is no variance inflation </em>
<ul>
<li><strong>variance inflation factor</strong> (VIF) = the increase in the variance for the <span class="math inline">\(i_{th}\)</span> regressor compared to the ideal setting where it is orthogonal to the other regressors</li>
<li><span class="math inline">\(\sqrt{VIF}\)</span> = increase in standard error</li>
</ul></li>
<li><em><strong>Note</strong>: variance inflation is only part of the picture in that sometimes we will <strong>include variables</strong> even though they dramatically inflate the variation because it is an <strong>empirical part of the relationship</strong> we are attempting to model </em></li>
</ul></li>
<li>as the number of <em>non-redundant</em> variables increases or approaches <span class="math inline">\(n\)</span>, the model <strong><em>approaches a perfect fit</em></strong> for the data
<ul>
<li><span class="math inline">\(R^2\)</span> monotonically increases as more regressors are included</li>
<li>Sum of Squared Errors (SSE) monotonically decreases as more regressors are included</li>
</ul></li>
</ul>
<h3 id="example---r2-v-n">Example - <span class="math inline">\(R^2\)</span> v <span class="math inline">\(n\)</span></h3>
<ul>
<li>for the simulation below, no actual regression relationship exist as the data generated are simply standard normal noise</li>
<li>it is clear, however, that as <span class="math inline">\(p\)</span>, the number of regressors included in the model, approaches <span class="math inline">\(n\)</span>, the <span class="math inline">\(R^2\)</span> value approaches 1.0, which signifies perfect fit</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># set number of measurements</span><br><span class="line">n &lt;- 100</span><br><span class="line"># set up the axes of the plot</span><br><span class="line">plot(c(1, n), 0 : 1, type = &quot;n&quot;, xlab = &quot;p&quot;, ylab = expression(R^2),</span><br><span class="line">	main = expression(paste(R^2, &quot; vs n&quot;)))</span><br><span class="line"># for each value of p from 1 to n</span><br><span class="line">r &lt;- sapply(1 : n, function(p)&#123;</span><br><span class="line">	# create outcome and p regressors</span><br><span class="line">	y &lt;- rnorm(n); x &lt;- matrix(rnorm(n * p), n, p)</span><br><span class="line">	# calculate the R^2</span><br><span class="line">	summary(lm(y ~ x))$r.squared</span><br><span class="line">&#125;)</span><br><span class="line"># plot the R^2 values and connect them with a line</span><br><span class="line">lines(1 : n, r, lwd = 2)</span><br></pre></td></tr></table></figure>
<h3 id="adjusted-r2">Adjusted <span class="math inline">\(R^2\)</span></h3>
<ul>
<li>recall that <span class="math inline">\(R^2\)</span> is defined as the percent of total variability that is explained by the regression model, or <span class="math display">\[R^2 = \frac{\mbox{regression variation}}{\mbox{total variation}} =  1- \frac{\sum_{i=1}^n (Y_i - \hat Y_i)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2} = 1 - \frac{Var(e_i)}{Var(Y_i)}\]</span></li>
<li>Estimating <span class="math inline">\(R^2\)</span> with the above definition is <strong><em>acceptable</em></strong> when there is a <em>single</em> variable, but it becomes less and <strong><em>less helpful</em></strong> as the <em>number of variables increases</em>
<ul>
<li>as we have shown previously, <span class="math inline">\(R^2\)</span> always increases as more variables are introduced and is thus <strong><em>biased</em></strong></li>
</ul></li>
<li><strong>adjusted <span class="math inline">\(R^2\)</span></strong> is a better estimate of variability explained by the model and is defined as <span class="math display">\[R^2_{adj} = 1 - \frac{Var(e_i)}{Var(Y_i)} \times \frac{n-1}{n-k-1}\]</span> where <span class="math inline">\(n\)</span> = number of observations, and <span class="math inline">\(k\)</span> = number of predictors in the model
<ul>
<li>since <span class="math inline">\(k\)</span> is always greater than zero, the adjusted <span class="math inline">\(R^2\)</span> is <strong><em>always smaller</em></strong> than the unadjusted <span class="math inline">\(R^2\)</span></li>
<li>adjusted <span class="math inline">\(R^2\)</span> also penalizes adding large numbers of regressors, which would have inflated the unadjusted <span class="math inline">\(R^2\)</span></li>
</ul></li>
</ul>
<h3 id="example---unrelated-regressors">Example - Unrelated Regressors</h3>
<ul>
<li>in the simulation below, outcome <span class="math inline">\(y\)</span> is only related to <span class="math inline">\(x_1\)</span>
<ul>
<li><span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> are random noise</li>
</ul></li>
<li>we will run 1000 simulations of 3 linear regression models, and calculate the <strong><em>standard error of the slope</em></strong>
<ul>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1\)</span></li>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1 + x_2\)</span></li>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1 + x_2 + x_3\)</span></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; nosim &lt;- 1000</span><br><span class="line"># generate 3 random noise, unrelated variables</span><br><span class="line">x1 &lt;- rnorm(n); x2 &lt;- rnorm(n); x3 &lt;- rnorm(n);</span><br><span class="line"># calculate beta1s of three different regression</span><br><span class="line">betas &lt;- sapply(1 : nosim, function(i)&#123;</span><br><span class="line">	# generate outcome as only related to x1</span><br><span class="line">	y &lt;- x1 + rnorm(n, sd = .3)</span><br><span class="line">	# store beta1 of linear regression on y vs x1</span><br><span class="line">	c(coef(lm(y ~ x1))[2],</span><br><span class="line">		# store beta1 of linear regression on y vs x1 and x2</span><br><span class="line">		coef(lm(y ~ x1 + x2))[2],</span><br><span class="line">		# store beta1 of linear regression on y vs x1 x2 and x3</span><br><span class="line">		coef(lm(y ~ x1 + x2 + x3))[2])</span><br><span class="line">&#125;)</span><br><span class="line"># calculate the standard error of the beta1s for the three regressions</span><br><span class="line">beta1.se &lt;- round(apply(betas, 1, sd), 5)</span><br><span class="line"># print results</span><br><span class="line">rbind(&quot;y ~ x1&quot; = c(&quot;beta1SE&quot; = beta1.se[1]),</span><br><span class="line">      &quot;y ~ x1 + x2&quot; = beta1.se[2],</span><br><span class="line">      &quot;y ~ x1 + x2 + x3&quot; = beta1.se[3])</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from the above result, if we include unrelated regressors <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>, the <strong><em>standard error increases</em></strong></li>
</ul>
<h3 id="example---highly-correlated-regressors-variance-inflation">Example - Highly Correlated Regressors / Variance Inflation</h3>
<ul>
<li>in the simulation below, outcome <span class="math inline">\(y\)</span> is related to <span class="math inline">\(x_1\)</span>
<ul>
<li><span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> are highly correlated with <span class="math inline">\(x_1\)</span></li>
<li><span class="math inline">\(x_3\)</span> is more correlated with <span class="math inline">\(x_1\)</span> than <span class="math inline">\(x_2\)</span></li>
</ul></li>
<li>we will run 1000 simulations of 3 linear regression models, and calculate the <strong><em>standard error of <span class="math inline">\(\beta_1\)</span></em></strong>, the coefficient of <span class="math inline">\(x_1\)</span>
<ul>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1\)</span></li>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1 + x_2\)</span></li>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1 + x_2 + x_3\)</span></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># generate number of measurements and trials</span><br><span class="line">n &lt;- 100; nosim &lt;- 1000</span><br><span class="line"># generate random variables that are correlated with each other</span><br><span class="line">x1 &lt;- rnorm(n); x2 &lt;- x1/sqrt(2) + rnorm(n) /sqrt(2)</span><br><span class="line">x3 &lt;- x1 * 0.95 + rnorm(n) * sqrt(1 - 0.95^2);</span><br><span class="line"># calculate the beta1s for 1000 trials</span><br><span class="line">betas &lt;- sapply(1 : nosim, function(i)&#123;</span><br><span class="line">	# generate outcome as only related to x1</span><br><span class="line">	y &lt;- x1 + rnorm(n, sd = .3)</span><br><span class="line">	# store beta1 of linear regression on y vs x1</span><br><span class="line">	c(coef(lm(y ~ x1))[2],</span><br><span class="line">		# store beta1 of linear regression on y vs x1 and x2</span><br><span class="line">		coef(lm(y ~ x1 + x2))[2],</span><br><span class="line">		# store beta1 of linear regression on y vs x1 x2 and x3</span><br><span class="line">		coef(lm(y ~ x1 + x2 + x3))[2])</span><br><span class="line">&#125;)</span><br><span class="line"># calculate the standard error of the beta1 for the three regressions</span><br><span class="line">beta1.se &lt;- round(apply(betas, 1, sd), 5)</span><br><span class="line"># print results</span><br><span class="line">rbind(&quot;y ~ x1&quot; = c(&quot;beta1SE&quot; = beta1.se[1]),</span><br><span class="line">      &quot;y ~ x1 + x2&quot; = beta1.se[2],</span><br><span class="line">      &quot;y ~ x1 + x2 + x3&quot; = beta1.se[3])</span><br></pre></td></tr></table></figure>
<ul>
<li><p>as we can see from above, adding highly correlated regressors <strong><em>drastically increases</em></strong> the standard errors of the coefficients</p></li>
<li>to estimate the actual change in variance, we can use the ratio of estimated variances for the <span class="math inline">\(\beta_1\)</span> coefficient for the different models
<ul>
<li><code>summary(fit)$cov.unscaled</code> = returns p x p covariance matrix for p coefficients, with the diagonal values as the true variances of coefficients
<ul>
<li><code>summary(fit)$cov.unscaled[2,2]</code> = true variance for the <span class="math inline">\(\beta_1\)</span></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># generate outcome that is correlated with x1</span><br><span class="line">y &lt;- x1 + rnorm(n, sd = .3)</span><br><span class="line"># store the variance of beta1 for the 1st model</span><br><span class="line">a &lt;- summary(lm(y ~ x1))$cov.unscaled[2,2]</span><br><span class="line"># calculate the ratio of variances of beta1 for 2nd and 3rd models with respect to 1st model</span><br><span class="line">c(summary(lm(y ~ x1 + x2))$cov.unscaled[2,2],</span><br><span class="line">	summary(lm(y~ x1 + x2 + x3))$cov.unscaled[2,2]) / a - 1</span><br><span class="line"># alternatively, the change in variance can be estimated by calculating ratio of trials variance</span><br><span class="line">temp &lt;- apply(betas, 1, var); temp[2 : 3] / temp[1] - 1</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from the above results
<ul>
<li>adding <span class="math inline">\(x_2\)</span> increases the variance by approximately <code>r round(temp[2]/ temp[1] - 1)</code> fold</li>
<li>adding <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> increases the variance by approximately <code>r round(temp[3]/ temp[1] - 1)</code> folds</li>
</ul></li>
<li>the estimated values from the 1000 trials are <strong><em>different but close</em></strong> to the true increases, and they will approach the true values as the number of trials increases</li>
</ul>
<h3 id="example-variance-inflation-factors">Example: Variance Inflation Factors</h3>
<ul>
<li>we will use the <code>swiss</code> data set for this example, and compare the following models
<ul>
<li>Fertility vs Agriculture</li>
<li>Fertility vs Agriculture + Examination</li>
<li>Fertility vs Agriculture + Examination + Education</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>warning </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># load swiss data</span><br><span class="line">data(swiss)</span><br><span class="line"># run linear regression for Fertility vs Agriculture</span><br><span class="line">fit &lt;- lm(Fertility ~ Agriculture, data = swiss)</span><br><span class="line"># variance for coefficient of Agriculture</span><br><span class="line">a &lt;- summary(fit)$cov.unscaled[2,2]</span><br><span class="line"># run linear regression for Fertility vs Agriculture + Examination</span><br><span class="line">fit2 &lt;- update(fit, Fertility ~ Agriculture + Examination)</span><br><span class="line"># run linear regression for Fertility vs Agriculture + Examination + Education</span><br><span class="line">fit3 &lt;- update(fit, Fertility ~ Agriculture + Examination + Education)</span><br><span class="line"># calculate ratios of variances for Agriculture coef for fit2 and fit3 w.r.t fit1</span><br><span class="line">c(summary(fit2)$cov.unscaled[2,2], summary(fit3)$cov.unscaled[2,2]) / a - 1</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above
<ul>
<li>adding Examination variable to the model increases the variance by <code>r round(summary(fit2)$cov.unscaled[2,2]/a-1, 2)*100</code>%</li>
<li>adding Examination and Education variables to the model increases the variance by <code>r round(summary(fit3)$cov.unscaled[2,2]/a-1, 2)*100</code>%</li>
</ul></li>
<li>we can also calculate the <strong>variance inflation factors</strong> for all the predictors and see how variance will change by adding each predictor (assuming all predictor are orthogonal/independent of each other)
<ul>
<li>[<code>car</code> library] <code>vit(fit)</code> = returns the variance inflation factors for the predictors of the given linear model</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># load car library</span><br><span class="line">library(car)</span><br><span class="line"># run linear regression on Fertility vs all other predictors</span><br><span class="line">fit &lt;- lm(Fertility ~ . , data = swiss)</span><br><span class="line"># calculate the variance inflation factors</span><br><span class="line">vif(fit)</span><br><span class="line"># calculate the standard error inflation factors</span><br><span class="line">sqrt(vif(fit))</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from the above results, Education and Examination both have relatively higher inflation factors, which makes sense as the two variables are likely to be correlated with each other</li>
</ul>
<h3 id="residual-variance-estimates">Residual Variance Estimates</h3>
<ul>
<li>assuming that the model is linear with additive iid errors (with finite variance), we can mathematically describe the impact of omitting necessary variables or including unnecessary ones
<ul>
<li><strong>underfitting</strong> the model <span class="math inline">\(\rightarrow\)</span> variance estimate is <strong><em>biased</em></strong> <span class="math inline">\(\rightarrow ~ E[\hat \sigma^2] \neq \sigma^2\)</span></li>
<li><strong>correctly fitting</strong> or <strong>overfitting</strong> the model <span class="math inline">\(\rightarrow\)</span> variance estimate is <strong><em>unbiased</em></strong> <span class="math inline">\(\rightarrow ~ E[\hat \sigma^2] = \sigma^2\)</span>
<ul>
<li>however, if unnecessary variables are included, the variance estimate is <strong><em>larger</em></strong> than that of the correctly fitted variables <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(Var(\hat \sigma_{overfitted}) \geq Var(\hat \sigma_{correct})\)</span></li>
<li>in other words, adding unnecessary variables increases the variability of estimate for the true model</li>
</ul></li>
</ul></li>
</ul>
<h3 id="covariate-model-selection">Covariate Model Selection</h3>
<ul>
<li>automated covariate/predictor selection is difficult
<ul>
<li>the space of models explodes quickly with interactions and polynomial terms</li>
<li><em><strong>Note</strong>: in the <strong>Practical Machine Learning</strong> class, many modern methods for traversing large model spaces for the purposes of prediction will be covered </em></li>
</ul></li>
<li>principal components analysis (PCA) or factor analytic models on covariates are often useful for reducing complex covariate spaces
<ul>
<li>find linear combinations of variables that captures the most variation</li>
</ul></li>
<li>good experiment design can often eliminate the need for complex model searches during analyses
<ul>
<li>randomization, stratification can help simply the end models</li>
<li>unfortunately, control over the design is <strong><em>often limited</em></strong></li>
</ul></li>
<li>it is also viable to manually explore the covariate space based on understanding of the data
<ul>
<li>use covariate adjustment and multiple models to probe that effect of adding a particular predictor on the model</li>
<li><em><strong>Note</strong>: this isn’t a terribly systematic or efficient approach, but it tends to teach you a lot about the the data through the process </em></li>
</ul></li>
<li>if the models of interest are nested (i.e. one model is a special case of another with one or more coefficients set to zero) and without lots of parameters differentiating them, it’s fairly possible to use nested likelihood ratio tests (ANOVA) to help find the best model
<ul>
<li><strong>Analysis of Variance</strong> (ANOVA) works well when adding one or two regressors at a time
<ul>
<li><code>anova(fit1, fit2, fit3)</code> = performs ANOVA or analysis of variance (or deviance) tables for a series of nested linear regressions models</li>
</ul></li>
<li><em><strong>Note</strong>: it is extremely important to get the order of the models correct to ensure the results are sensible </em></li>
<li>an example can be found <a href="#example-anova">here</a></li>
</ul></li>
<li>another alternative to search through different models is the <strong>step-wise search</strong> algorithm that repeatedly adds/removes regressors one at a time to find the best model with the least <a href="http://en.wikipedia.org/wiki/Akaike_information_criterion" target="_blank" rel="noopener">Akaike Information Criterion (AIC)</a>
<ul>
<li><code>step(lm, k=df)</code> = performs step wise regression on a given linear model to find and return best linear model
<ul>
<li><code>k=log(n)</code> = specifying the value of <code>k</code> as the log of the number of observation will force the step-wise regression model to use Bayesian Information Criterion (BIC) instead of the AIC</li>
<li><em><strong>Note</strong>: both BIC and AIC penalizes adding parameters to the regression model with an additional penalty term; the penalty is <strong>larger</strong> for BIC than AIC </em></li>
</ul></li>
<li><code>MASS::stepAIC(lm, k = df)</code> = more versatile, rigorous implementation of the step wise regression</li>
<li>an example can be found <a href="#example-step-wise-model-search">here</a></li>
</ul></li>
</ul>
<h3 id="example-anova">Example: ANOVA</h3>
<ul>
<li>we will use the <code>swiss</code> data set for this example, and compare the following nested models
<ul>
<li>Fertility vs Agriculture</li>
<li>Fertility vs Agriculture + Examination + Education</li>
<li>Fertility vs Agriculture + Examination + Education + Catholic + Infant.Mortality</li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># three different regressions that are nested</span><br><span class="line">fit1 &lt;- lm(Fertility ~ Agriculture, data = swiss)</span><br><span class="line">fit3 &lt;- update(fit, Fertility ~ Agriculture + Examination + Education)</span><br><span class="line">fit5 &lt;- update(fit, Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality)</span><br><span class="line"># perform ANOVA</span><br><span class="line">anova(fit1, fit3, fit5)</span><br></pre></td></tr></table></figure>
<ul>
<li>the ANOVA function returns a formatted table with the follow information
<ul>
<li><code>Res.Df</code> = residual degrees of freedom for the models</li>
<li><code>RSS</code> = residual sum of squares for the models, measure of fit</li>
<li><code>Df</code> = change in degrees of freedom from one model to the next</li>
<li><code>Sum of Sq</code> = difference/change in residual sum of squares from one model to the next</li>
<li><code>F</code> = F statistic, measures the ratio of two scaled sums of squares reflecting different sources of variability <span class="math display">\[F = \frac{\frac{RSS_1 - RSS_2}{p_2 - p_1}}{\frac{RSS_2}{n-p_2}}\]</span> where <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> = number of parameters in the two models for comparison, and <span class="math inline">\(n\)</span> = number of observations</li>
<li><code>Pr(&gt;F)</code> = p-value for the F statistic to indicate whether the change in model is significant or not</li>
</ul></li>
<li>from the above result, we can see that both going from first to second, and second to third models result in significant reductions in RSS and <strong><em>better model fits</em></strong></li>
</ul>
<h3 id="example-step-wise-model-search">Example: Step-wise Model Search</h3>
<ul>
<li>we will use the <code>mtcars</code> data set for this example, and perform step-wise regression/model selection algorithm on the following initial model
<ul>
<li>Miles Per Gallon vs Number of Cylinder + Displacement + Gross Horse Power + Rear Axle Ratio + Weight</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># load the mtcars data starting regression model</span><br><span class="line">data(mtcars); fit &lt;- lm(mpg ~ cyl + disp + hp + drat + wt, data = mtcars)</span><br><span class="line"># step-wise search using BIC</span><br><span class="line">step(fit, k = log(nrow(mtcars)))</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the best model that captures most of the variability in the data is simply <code>mpg ~ cyl + wt</code> <span class="math inline">\(\pagebreak\)</span></li>
</ul>
<h2 id="general-linear-models-overview">General Linear Models Overview</h2>
<ul>
<li>limitations of linear models:
<ul>
<li>response can be discrete (i.e. 0, 1, etc.) or strictly positive <span class="math inline">\(\rightarrow\)</span> linear response models don’t make much sense</li>
<li>if outcome must be positive, Gaussian errors (<span class="math inline">\(\pm\)</span> errors) don’t make sense as negative outcomes are possible</li>
<li>transformations on predictors (log + 1) are often hard to interpret
<ul>
<li>modeling the data on the scale that it was collected is most ideal</li>
<li>even for interpretable transformations, <em>natural logarithms</em> specifically, aren’t applicable for negative/zero values</li>
</ul></li>
</ul></li>
<li><strong>general linear models</strong> = introduced in 1972 RSSB paper by Nelder and Wedderburn and has <strong><em>3</em></strong> parts
<ol type="1">
<li>exponential family model for response/outcome (i.e. Gaussian, Bernoulli distribution)</li>
<li>systematic component for linear predictor <span class="math inline">\(\rightarrow\)</span> incorporates the information about the independent variables into the model
<ul>
<li>denoted by <span class="math inline">\(\eta = X \beta\)</span> where <span class="math inline">\(X\)</span> is a matrix of independent variables/predictors and <span class="math inline">\(\beta\)</span> is the coefficients</li>
</ul></li>
<li>link function that connects means of the outcome/distribution to linear predictor
<ul>
<li>the relationship is defined as <span class="math inline">\(\eta = g(\mu)\)</span>, or the linear predictor <span class="math inline">\(\eta\)</span> is a function of the mean of the distribution <span class="math inline">\(\mu\)</span></li>
</ul></li>
</ol></li>
</ul>
<h3 id="simple-linear-model">Simple Linear Model</h3>
<ul>
<li><em>exponential family distribution</em>: Gaussian distribution, assumed <span class="math inline">\(Y_i \sim N(\mu_i, \sigma^2)\)</span></li>
<li><em>linear predictor</em>: <span class="math inline">\(\eta_i = \sum_{k=1}^p X_{ik} \beta_k\)</span></li>
<li><em>link function</em> : <span class="math inline">\(g(\mu) = \eta = \mu\)</span>
<ul>
<li>for linear models, <span class="math inline">\(g(\mu) = \mu\)</span>, so <span class="math inline">\(\eta_i = \mu_i\)</span></li>
</ul></li>
<li><strong>result</strong>: the same likelihood model (see <a href="#derivation-for-maximum-likelihood-estimator">derivation</a>)as the additive Gaussian error linear model <span class="math display">\[Y_i = \sum_{k=1}^p X_{ik} \beta_k + \epsilon_{i}\]</span> where <span class="math inline">\(\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\)</span></li>
</ul>
<h3 id="logistic-regression">Logistic Regression</h3>
<ul>
<li><em>exponential family distribution</em>: binomial/Bernoulli distribution, assumed <span class="math inline">\(Y_i \sim Bernoulli(\mu_i)\)</span> where the probability of success is <span class="math inline">\(\mu_i\)</span>
<ul>
<li>due to the properties of the binomial/Bernoulli distribution, <span class="math inline">\(E[Y_i] = \mu_i\)</span> where <span class="math inline">\(0 \leq \mu_i \leq 1\)</span></li>
</ul></li>
<li><em>linear predictor</em>: <span class="math inline">\(\eta_i = \sum_{k=1}^p X_{ik} \beta_k\)</span></li>
<li><em>link function</em> : <span class="math inline">\(g(\mu) = \eta = \log\left(\frac{\mu}{1 - \mu}\right)\)</span>
<ul>
<li><strong>odds</strong> for success for a binomial/Bernoulli distribution is defined as <span class="math display">\[\mbox{odds} = \frac{p}{1-p}\]</span></li>
<li><strong>logit</strong> is defined as <span class="math display">\[\log(\mbox{odds}) = \log \frac{\mu}{1-\mu}\]</span>
<ul>
<li><em><strong>Note</strong>: the <span class="math inline">\(\log\)</span> here is the <strong>natural</strong> log </em></li>
</ul></li>
<li><strong>inverse logit</strong> is defined as <span class="math display">\[\mu_i = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\]</span></li>
<li>complement of inverse logit is <span class="math display">\[1 - \mu_i = \frac{1}{1 + \exp(\eta_i)}\]</span></li>
</ul></li>
<li><strong>result</strong>: the likelihood model <span class="math display">\[\begin{aligned}
L(\beta) &amp; = \prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1-y_i}  \\
(plug~in~\mu_i~and~1 - \mu_i~from~above)&amp; = \prod_{i=1}^n \left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right)^{y_i} \left(\frac{1}{1 + \exp(\eta_i)}\right)^{1-y_i}\\
(multiply~2^{nd}~term~by~\frac{exp(\eta_i)}{exp(\eta_i)})&amp; = \prod_{i=1}^n \left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right)^{y_i} \left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right)^{1-y_i} \left(\frac{1}{\exp(\eta_i)}\right)^{1-y_i}\\
(simplify) &amp; = \prod_{i=1}^n \left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right) \left(\frac{1}{\exp(\eta_i)}\right)^{1-y_i}\\
(simplify) &amp; = \prod_{i=1}^n \left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right) \exp(\eta_i)^{y_i-1}\\
(simplify) &amp; = \prod_{i=1}^n \frac{\exp(\eta_i)^{y_i}}{1 + \exp(\eta_i)} \\
(change~form~of~numerator) &amp; = \exp\left(\sum_{i=1}^n y_i \eta_i \right)\prod_{i=1}^n \frac{1}{1 + \exp(\eta_i)}\\
(substitute~\eta_i) \Rightarrow L(\beta) &amp; = \exp\left(\sum_{i=1}^n y_i \big(\sum_{k=1}^p X_{ik} \beta_k \big) \right)\prod_{i=1}^n \frac{1}{1 + \exp\big(\sum_{k=1}^p X_{ik} \beta_k \big)}\\
\end{aligned}\]</span>
<ul>
<li>maximizing the likelihood <span class="math inline">\(L(\beta)\)</span> (solving for <span class="math inline">\(\frac{\partial L}{\partial \beta} = 0)\)</span> would return a set of optimized coefficients <span class="math inline">\(\beta\)</span> that will fit the data</li>
</ul></li>
</ul>
<h3 id="poisson-regression">Poisson Regression</h3>
<ul>
<li><em>exponential family distribution</em>: Poisson distribution, assumed <span class="math inline">\(Y_i \sim Poisson(\mu_i)\)</span> where <span class="math inline">\(E[Y_i] = \mu_i\)</span></li>
<li><em>linear predictor</em>: <span class="math inline">\(\eta_i = \sum_{k=1}^p X_{ik} \beta_k\)</span></li>
<li><em>link function</em> : <span class="math inline">\(g(\mu) = \eta = \log(\mu)\)</span>
<ul>
<li><em><strong>Note</strong>: the <span class="math inline">\(\log\)</span> here is the <strong>natural</strong> log </em></li>
<li>since <span class="math inline">\(e^x\)</span> is the inverse of <span class="math inline">\(\log(x)\)</span>, then <span class="math inline">\(\eta_i = \log(\mu_i)\)</span> can be transformed into <span class="math inline">\(\mu_i = e^{\eta_i}\)</span></li>
</ul></li>
<li><strong>result</strong>: the likelihood model <span class="math display">\[\begin{aligned}
L(\beta) &amp; = \prod_{i=1}^n (y_i !)^{-1} \mu_i^{y_i}e^{-\mu_i}\\
(substitute~\mu_i = e^{\eta_i}) &amp; = \prod_{i=1}^n \frac{(e^{\eta_i})^{y_i}}{y_i! e^{e^{\eta_i}}}\\
(transform) &amp;= \prod_{i=1}^n \frac{\exp(\eta_i y_i)}{y_i! \exp(e^{\eta_i})}\\
(taking~\log~of~both~sides)~\mathcal{L}(\beta) &amp; = \sum_{i=1}^n \eta_i y_i - \sum_{i=1}^n e^{\eta_i} - \sum_{i=1}^n log(y_i!) \\
(since~y_i~is~given,~we~can~ignore~\log y_i!)~\mathcal{L}(\beta) &amp; \propto \sum_{i=1}^n \eta_i y_i - \sum_{i=1}^n e^{\eta_i}\\
(substitute~\eta_i= \sum_{k=1}^p X_{ik} \beta_k) \Rightarrow  \mathcal{L}(\beta) &amp; \propto \sum_{i=1}^n y_i \left(\sum_{k=1}^p X_{ik}\beta_k\right) - \sum_{i=1}^n \exp \left(\sum_{k=1}^p X_{ik} \beta_k \right) \\
\end{aligned}\]</span>
<ul>
<li>maximizing the log likelihood <span class="math inline">\(\mathcal{L}(beta)\)</span> (solving for <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \beta} = 0)\)</span> would return a set of optimized coefficients <span class="math inline">\(\beta\)</span> that will fit the data</li>
</ul></li>
</ul>
<h3 id="variances-and-quasi-likelihoods">Variances and Quasi-Likelihoods</h3>
<ul>
<li>in each of the linear/Bernoulli/Poisson cases, the <strong><em>only</em></strong> term in the likelihood functions that depend on the <strong><em>data</em></strong> is <span class="math display">\[\sum_{i=1}^n y_i \eta_i =\sum_{i=1}^n y_i\sum_{k=1}^p X_{ik} \beta_k = \sum_{k=1}^p \beta_k\sum_{i=1}^n X_{ik} y_i\]</span></li>
<li>this means that we don’t need need all of the data collected to maximize the likelihoods/find the coefficients <span class="math inline">\(\beta\)</span>, but <strong><em>only</em></strong> need <span class="math inline">\(\sum_{i=1}^n X_{ik} y_i\)</span>
<ul>
<li><em><strong>Note</strong>: this simplification is a consequence of choosing “<strong>canonical</strong>” link functions, <span class="math inline">\(g(\mu)\)</span>, to be in specific forms </em></li>
</ul></li>
<li>[Derivation needed] all models achieve their <strong><em>maximum</em></strong> at the root of the <strong>normal equations</strong> <span class="math display">\[\sum_{i=1}^n \frac{(Y_i - \mu_i)}{Var(Y_i)}W_i = 0\]</span> where <span class="math inline">\(W_i = \frac{\partial g^{-1}(\mu_i)}{\mu_i}\)</span> or the derivative of the inverse of the link function
<ul>
<li><em><strong>Note</strong>: this is similar to deriving the least square equation where the middle term must be set to 0 to find the solution (see <a href="#derivation-for-">Derivation for <span class="math inline">\(\beta\)</span></a>) </em></li>
<li><em><strong>Note</strong>: <span class="math inline">\(\mu_i = g^{-1}(\eta_i) =g^{-1}\left(\sum_{k=1}^p X_{ik} \beta_k\right)\)</span>, the normal functions are really functions of <span class="math inline">\(\beta\)</span> </em></li>
</ul></li>
<li>the variance, <span class="math inline">\(Var(Y_i)\)</span>, is defined as
<ul>
<li><strong><em>linear model</em></strong>: <span class="math inline">\(Var(Y_i) = \sigma^2\)</span>, where <span class="math inline">\(\sigma\)</span> is constant</li>
<li><strong><em>binomial model</em></strong>: <span class="math inline">\(Var(Y_i) = \mu_i (1 - \mu_i)\)</span></li>
<li><strong><em>Poisson model</em></strong>: <span class="math inline">\(Var(Y_i) = \mu_i\)</span></li>
</ul></li>
<li>for binomial and Poisson models, there are <strong><em>strict relationships</em></strong> between the mean and variance that can be easily tested from the data:
<ul>
<li>binomial: mean = <span class="math inline">\(\mu_i\)</span>, variance = <span class="math inline">\(\mu_i (1 - \mu_i)\)</span></li>
<li>Poisson: mean = <span class="math inline">\(\mu_i\)</span>, variance = <span class="math inline">\(\mu_i\)</span></li>
</ul></li>
<li>it is often relevant to have a <strong><em>more flexible</em></strong> variance model (i.e. data doesn’t follow binomial/Poisson distributions exactly but are approximated), even if it doesn’t correspond to an actual likelihood, so we can add an extra parameter, <span class="math inline">\(\phi\)</span>, to the normal equations to form <strong>quasi-likelihood normal equations</strong> <span class="math display">\[
binomial:~\sum_{i=1}^n \frac{(Y_i - \mu_i)}{\phi \mu_i (1 - \mu_i ) } W_i=0 \\
Poisson:~\sum_{i=1}^n \frac{(Y_i - \mu_i)}{\phi \mu_i} W_i=0
\]</span> where <span class="math inline">\(W_i = \frac{\partial g^{-1}(\mu_i)}{\mu_i}\)</span> or the derivative of the inverse of the link function
<ul>
<li>for R function <code>glm()</code>, its possible to specify for the model to solve using quasi-likelihood normal equations instead of normal equations through the parameter <code>family = quasi-binomial</code> and <code>family = quasi-poisson</code> respectively</li>
<li><em><strong>Note</strong>: the quasi-likelihoods models generally same properties as normal GLM </em></li>
</ul></li>
</ul>
<h3 id="solving-for-normal-and-quasi-likelihood-normal-equations">Solving for Normal and Quasi-Likelihood Normal Equations</h3>
<ul>
<li>normal equations have to be solved <strong><em>iteratively</em></strong>
<ul>
<li>the results are <span class="math inline">\(\hat \beta_k\)</span>, estimated coefficients for the predictors</li>
<li>for quasi-likelihood normal equations, <span class="math inline">\(\hat \phi\)</span> will be part of the results as well</li>
<li>in R, <a href="http://en.wikipedia.org/wiki/Newton%27s_method" target="_blank" rel="noopener">Newton/Raphson’s algorithm</a> is used to solve the equations</li>
<li><strong><em>asymptotics</em></strong> are used for inference of results to broader population (see <strong><em>Statistical Inference</em></strong> course)</li>
<li><em><strong>Note</strong>: many of the ideas, interpretation, and conclusions derived from simple linear models are applicable to GLMs </em></li>
</ul></li>
<li><strong>predicted linear predictor responses</strong> are defined as <span class="math display">\[\hat \eta = \sum_{k=1}^p X_k \hat \beta_k\]</span></li>
<li><strong>predicted mean responses</strong> can be solved from <span class="math display">\[\hat \mu = g^{*1}(\hat \eta)\]</span></li>
<li><strong>coefficients</strong> are interpreted as the <strong><em>expected change in the link function</em></strong> of the expected response <strong><em>per unit change</em></strong> in <span class="math inline">\(X_k\)</span> holding other regressors constant, or <span class="math display">\[\beta_k = g(E[Y | X_k = x_k + 1, X_{\sim k} = x_{\sim k}]) - g(E[Y | X_k = x_k, X_{\sim k}=x_{\sim k}])\]</span></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="general-linear-models---binary-models">General Linear Models - Binary Models</h2>
<ul>
<li><strong>Bernoulli/binary</strong> models are frequently used to model outcomes that have two values
<ul>
<li>alive vs dead</li>
<li>win vs loss</li>
<li>success vs failure</li>
<li>disease vs healthy</li>
</ul></li>
<li><strong>binomial outcomes</strong> = collection of exchangeable binary outcomes (i.e. flipping coins repeatedly) for the same covariate data
<ul>
<li>in other words, we are interested in the count of predicted <span class="math inline">\(1\)</span>s vs <span class="math inline">\(0\)</span>s rather individual outcomes of <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span></li>
</ul></li>
</ul>
<h3 id="odds">Odds</h3>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Odds_ratio" target="_blank" rel="noopener"><strong>odds</strong></a> are useful in constructing logistic regression models and fairly easy to interpret
<ul>
<li>imagine flipping a coin with success probability <span class="math inline">\(p\)</span>
<ul>
<li>if heads, you win <span class="math inline">\(X\)</span></li>
<li>if tails, you lose <span class="math inline">\(Y\)</span></li>
</ul></li>
<li>how should <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be set so that the game is <strong><em>fair</em></strong>? <span class="math display">\[E[earnings]= X p - Y (1 - p) = 0 \Rightarrow \frac{Y}{X} = \frac{p}{1 - p}\]</span></li>
<li>odds can be interpreted as “How much should you be willing to pay for a <span class="math inline">\(p\)</span> probability of winning a dollar?”
<ul>
<li>if <span class="math inline">\(p &gt; 0.5\)</span>, you have to pay more if you lose than you get if you win</li>
<li>if <span class="math inline">\(p &lt; 0.5\)</span>, you have to pay less if you lose than you get if you win</li>
</ul></li>
</ul></li>
<li>odds are <strong>NOT</strong> probabilities</li>
<li>odds ratio of 1 = no difference in odds or 50% - 50%
<ul>
<li><span class="math inline">\(p = 0.5 \Rightarrow odds = \frac{0.5}{1-0.5} = 1\)</span></li>
<li>log odds ratio of 0 = no difference in odds
<ul>
<li><span class="math inline">\(p = 0.5 \Rightarrow odds = \log\left(\frac{0.5}{1-0.5}\right) = \log(1) = 0\)</span></li>
</ul></li>
</ul></li>
<li>odds ratio &lt; 0.5 or &gt; 2 commonly a “moderate effect”</li>
<li><strong>relative risk</strong> = ratios of probabilities instead of odds, and are often easier to interpret but harder to estimate <span class="math display">\[\frac{Pr(W_i | S_i = 10)}{Pr(W_i | S_i = 0)}\]</span>
<ul>
<li><em><strong>Note</strong>: relative risks often have <strong>boundary problems</strong> as the range of <span class="math inline">\(\log(p)\)</span> is <span class="math inline">\((-\infty,~0]\)</span> where as the range of logit <span class="math inline">\(\frac{p}{1-p}\)</span> is <span class="math inline">\((-\infty,\infty)\)</span> </em></li>
<li>for small probabilities Relative Risk <span class="math inline">\(\approx\)</span> Odds Ratio but <strong>they are not the same</strong>!</li>
</ul></li>
</ul>
<h3 id="example---baltimore-ravens-win-vs-loss">Example - Baltimore Ravens Win vs Loss</h3>
<ul>
<li>the data for this example can be found <a href="https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda" target="_blank" rel="noopener">here</a>
<ul>
<li>the data contains the records 20 games for Baltimore Ravens, a professional American Football team</li>
<li>there are 4 columns
<ul>
<li><code>ravenWinNum</code> = 1 for Raven win, 0 for Raven loss</li>
<li><code>ravenWin</code> = W for Raven win, L for Raven loss</li>
<li><code>ravenScore</code> = score of the Raven team during the match</li>
<li><code>opponentScore</code> = score of the Raven team during the match</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># load the data</span><br><span class="line">load(&quot;ravensData.rda&quot;)</span><br><span class="line">head(ravensData)</span><br></pre></td></tr></table></figure>
<h3 id="example---simple-linear-regression">Example - Simple Linear Regression</h3>
<ul>
<li><strong>simple linear regression</strong> can be used model win vs loss for the Ravens <span class="math display">\[ W_i = \beta_0 + \beta_1 S_i + \epsilon_i \]</span>
<ul>
<li><span class="math inline">\(W_i\)</span> = binary outcome, 1 if a Ravens win, 0 if not</li>
<li><span class="math inline">\(S_i\)</span> = number of points Ravens scored</li>
<li><span class="math inline">\(\beta_0\)</span> = probability of a Ravens win if they score 0 points</li>
<li><span class="math inline">\(\beta_1\)</span> = increase in probability of a Ravens win for each additional point</li>
<li><span class="math inline">\(\epsilon_i\)</span> = residual variation, error</li>
</ul></li>
<li>the expected value for the model is defined as <span class="math display">\[E[W_i | S_i, \beta_0, \beta_1] = \beta_0 + \beta_1 S_i\]</span></li>
<li>however, the model wouldn’t work well as the predicted results <strong><em>won’t</em></strong> be 0 vs 1
<ul>
<li>the error term, <span class="math inline">\(\epsilon_i\)</span>, is assumed to be continuous and normally distributed, meaning that the prediction will likely be a decimal</li>
<li>therefore, this is <strong><em>not</em></strong> a good assumption for the model</li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># perform linear regression</span><br><span class="line">summary(lm(ravenWinNum ~ ravenScore, data = ravensData))</span><br></pre></td></tr></table></figure>
<ul>
<li>as expected, the model produces a poor fit for the data (<span class="math inline">\(R^2_{adj} =\)</span> <code>r round(summary(lm(ravenWinNum ~ ravenScore, data = ravensData))$adj.r.squared, 4)</code>)</li>
<li>adding a threshold to the predicted outcome (i.e. if <span class="math inline">\(\hat W_i &lt; 0.5, \hat W_i = 0\)</span>) and using the model to predict the results would be <strong><em>viable</em></strong>
<ul>
<li>however, the coefficients for the model are <strong><em>not very interpretable</em></strong></li>
</ul></li>
</ul>
<h3 id="example---logistic-regression">Example - Logistic Regression</h3>
<ul>
<li><strong>probability</strong> of Ravens win is defined as <span class="math display">\[Pr(W_i | S_i, \beta_0, \beta_1)\]</span></li>
<li><strong>odds</strong> is defined as <span class="math display">\[\frac{Pr(W_i | S_i, \beta_0, \beta_1 )}{1-Pr(W_i | S_i, \beta_0, \beta_1)}\]</span> which ranges from 0 to <span class="math inline">\(\infty\)</span></li>
<li>log odds or <strong>logit</strong> is defined as <span class="math display">\[\log\left(\frac{Pr(W_i | S_i, \beta_0, \beta_1 )}{1-Pr(W_i | S_i, \beta_0, \beta_1)}\right)\]</span> which ranges from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span></li>
<li>we can use the link function and linear predictors to construct the <strong>logistic regression</strong> model <span class="math display">\[\begin{aligned}
g(\mu_i) &amp; = \log \left(\frac{\mu_i}{1 - \mu_i} \right) = \eta_i\\
(substitute~\mu_i = Pr(W_i | S_i, \beta_0, \beta_1))~g(\mu_i) &amp; = \log\left(\frac{Pr(W_i | S_i, \beta_0, \beta_1)}{1-Pr(W_i | S_i, \beta_0, \beta_1)}\right) = \eta_i \\
(substitute~\eta_i=\beta_0 + \beta_1 S_i) \Rightarrow ~g(\mu_i) &amp; = \log\left(\frac{Pr(W_i | S_i, \beta_0, \beta_1)}{1-Pr(W_i | S_i, \beta_0, \beta_1)}\right) = \beta_0 + \beta_1 S_i\\
\end{aligned}
 \]</span> which can also be written as <span class="math display">\[Pr(W_i | S_i, \beta_0, \beta_1 ) = \frac{\exp(\beta_0 + \beta_1 S_i)}{1 + \exp(\beta_0 + \beta_1 S_i)}\]</span></li>
<li>for the model <span class="math display">\[\log\left(\frac{Pr(W_i | S_i, \beta_0, \beta_1)}{1-Pr(W_i | S_i, \beta_0, \beta_1)}\right) = \beta_0 + \beta_1 S_i\]</span>
<ul>
<li><span class="math inline">\(\beta_0\)</span> = log odds of a Ravens win if they score zero points</li>
<li><span class="math inline">\(\beta_1\)</span> = log odds ratio of win probability for each point scored (compared to zero points) <span class="math display">\[\beta_1 = \log\left(odds(S_i = S_i+1)\right) - \log\left(odds(S_i = S_i)\right) = \log\left(\frac{odds(S_i = S_i+1)}{odds(S_i = S_i)} \right)\]</span></li>
<li><span class="math inline">\(\exp(\beta_1)\)</span> = odds ratio of win probability for each point scored (compared to zero points) <span class="math display">\[\exp(\beta_1) =  \frac{odds(S_i = S_i+1)}{odds(S_i = S_i)}\]</span></li>
</ul></li>
<li>we can leverage the <code>manupulate</code> function vary <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> to fit logistic regression curves for simulated data</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set x values for the points to be plotted</span></span><br><span class="line">x &lt;- seq(-<span class="number">10</span>, <span class="number">10</span>, length = <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># "library(manipulate)" is needed to use the manipulate function</span></span><br><span class="line">manipulate(</span><br><span class="line">	<span class="comment"># plot the logistic regression curve</span></span><br><span class="line">    plot(x, exp(beta0 + beta1 * x) / (<span class="number">1</span> + exp(beta0 + beta1 * x)),</span><br><span class="line">         type = <span class="string">"l"</span>, lwd = <span class="number">3</span>, frame = <span class="literal">FALSE</span>),</span><br><span class="line">    <span class="comment"># slider for beta1</span></span><br><span class="line">    beta1 = slider(-<span class="number">2</span>, <span class="number">2</span>, step = <span class="number">.1</span>, initial = <span class="number">2</span>),</span><br><span class="line">    <span class="comment"># slider for beta0</span></span><br><span class="line">    beta0 = slider(-<span class="number">2</span>, <span class="number">2</span>, step = <span class="number">.1</span>, initial = <span class="number">0</span>)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/8.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>we can use the <code>glm(outcome ~ predictor, family = &quot;binomial&quot;)</code> to fit a logistic regression to the data</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># run logistic regression on data</span><br><span class="line">logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data = ravensData,family=&quot;binomial&quot;)</span><br><span class="line"># print summary</span><br><span class="line">summary(logRegRavens)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see above, the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are <code>r round(logRegRavens$coef,3)</code>, which are interpreted to be the log odds ratios</li>
<li>we can convert the log ratios as well as the log confidence intervals to ratios and confidence intervals (in the same units as the data)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># take e^coefs to find the log ratios</span><br><span class="line">exp(logRegRavens$coeff)</span><br><span class="line"># take e^log confidence interval to find the confidence intervals</span><br><span class="line">exp(confint(logRegRavens))</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(\exp(x) \approx 1 + x\)</span> for small values (close to 0) of x, this can be a quick way to estimate the coefficients </em></li>
<li>we can interpret the slope, <span class="math inline">\(\beta_1\)</span> as <code>r round((exp(logRegRavens$coeff)[2]-1)*100, 3)</code> % increase in probability of winning for every point scored</li>
<li>we can interpret the intercept, <span class="math inline">\(\beta_0\)</span> as <code>r round(exp(logRegRavens$coeff)[1], 3)</code> is the odds for Ravens winning if they scored 0 points
<ul>
<li><em><strong>Note</strong>: similar to the intercept of a simple linear regression model, the intercept should be interpreted carefully as it is an extrapolated value from the model and may not hold practical meaning </em></li>
</ul></li>
<li>to calculate specific probability of winning for a given number of points <span class="math display">\[Pr(W_i | S_i, \hat \beta_0, \hat \beta_1) = \frac{\exp(\hat \beta_0 + \hat \beta_1 S_i)}{1 + \exp(\hat \beta_0 + \hat \beta_1 S_i)}\]</span></li>
<li>the resulting logistic regression curve can be seen below</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot the logistic regression</span><br><span class="line">plot(ravensData$ravenScore,logRegRavens$fitted,pch=19,col=&quot;blue&quot;,xlab=&quot;Score&quot;,ylab=&quot;Prob Ravens Win&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="example---anova-for-logistic-regression">Example - ANOVA for Logistic Regression</h3>
<ul>
<li>ANOVA can be performed on a single logistic regression, in which it will analyze the change in variances with addition of parameters in the model, or multiple nested logistic regression (similar to linear models)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># perform analysis of variance</span><br><span class="line">anova(logRegRavens,test=&quot;Chisq&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>ANOVA returns information about the model, link function, response, as well as analysis of variance for adding terms
<ul>
<li><code>Df</code> = change in degrees of freedom
<ul>
<li>the value 1 refers to adding the <code>ravenScore</code> parameter (slope)</li>
</ul></li>
<li><code>Deviance</code> = measure of goodness of model fit compare to the previous model</li>
<li><code>Resid. Dev</code> = residual deviance for current model</li>
<li><code>Pr(&gt;Chi)</code> = used to evaluate the significance of the added parameter
<ul>
<li>in this case, the <code>Deviance</code> value of 3.54 is used to find the corresponding p-value from the Chi Squared distribution, which is 0.06
<ul>
<li><em><strong>Note</strong>: Chi Squared distribution with 1 degree of freedom is simply the squared of normal distribution, so z statistic of 2 corresponds to 95% for normal distribution indicates that deviance of 4 corresponds to approximately 5% in the Chi Squared distribution (which is what our result shows) </em></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="further-resources">Further resources</h3>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Logistic_regression" target="_blank" rel="noopener">Wikipedia on Logistic Regression</a></li>
<li><a href="http://data.princeton.edu/R/glms.html" target="_blank" rel="noopener">Logistic regression and GLMs in R</a></li>
<li>Brian Caffo’s lecture notes on: <a href="http://ocw.jhsph.edu/courses/MethodsInBiostatisticsII/PDFs/lecture23.pdf" target="_blank" rel="noopener">Simpson’s Paradox</a>, <a href="http://ocw.jhsph.edu/courses/MethodsInBiostatisticsII/PDFs/lecture24.pdf" target="_blank" rel="noopener">Retrospective Case-control Studies</a></li>
<li><a href="http://www.openintro.org/stat/down/oiStat2_08.pdf" target="_blank" rel="noopener">Open Intro Chapter on Logistic Regression</a></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="general-linear-models---poisson-models">General Linear Models - Poisson Models</h2>
<ul>
<li><strong>Poisson distribution</strong> is a useful model for counts and rates
<ul>
<li><strong>rate</strong> = count per unit of time</li>
<li>linear regression with transformation is an alternative</li>
</ul></li>
<li>count data examples
<ul>
<li>calls to a call center</li>
<li>number of flu cases in an area</li>
<li>number of cars that cross a bridge</li>
</ul></li>
<li>rate data examples
<ul>
<li>percent of children passing a test</li>
<li>percent of hits to a website from a country</li>
<li>radioactive decay</li>
</ul></li>
<li>Poisson model examples
<ul>
<li>modeling web traffic hits incidence rates</li>
<li>approximating binomial probabilities with small <span class="math inline">\(p\)</span> and large <span class="math inline">\(n\)</span></li>
<li>analyzing contingency table data (tabulated counts for categorical variables)</li>
</ul></li>
</ul>
<h3 id="properties-of-poisson-distribution">Properties of Poisson Distribution</h3>
<ul>
<li>a set of data <span class="math inline">\(X\)</span> is said to follow the Poisson distribution, or <span class="math inline">\(X \sim Poisson(t\lambda)\)</span>, if <span class="math display">\[P(X = x) = \frac{(t\lambda)^x e^{-t\lambda}}{x!}\]</span> where <span class="math inline">\(x = 0, 1, \ldots\)</span>
<ul>
<li><span class="math inline">\(\lambda\)</span> = rate or expected count per unit time</li>
<li><span class="math inline">\(t\)</span> = monitoring time</li>
</ul></li>
<li><strong>mean</strong> of Poisson distribution is <span class="math inline">\(E[X] = t\lambda\)</span>, thus <span class="math inline">\(E[X / t] = \lambda\)</span></li>
<li><strong>variance</strong> of the Poisson is <span class="math inline">\(Var(X) = t\lambda = \mu (mean)\)</span>
<ul>
<li><em><strong>Note</strong>: Poisson approaches a Gaussian/normal distribution as <span class="math inline">\(t\lambda\)</span> gets large </em></li>
</ul></li>
<li>below are the Poisson distributions for various values of <span class="math inline">\(\lambda\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># set up 1x3 panel plot</span><br><span class="line">par(mfrow = c(1, 3))</span><br><span class="line"># Poisson distribution for t = 1, and lambda = 2</span><br><span class="line">plot(0 : 10, dpois(0 : 10, lambda = 2), type = &quot;h&quot;, frame = FALSE)</span><br><span class="line"># Poisson distribution for t = 1, and lambda = 10</span><br><span class="line">plot(0 : 20, dpois(0 : 20, lambda = 10), type = &quot;h&quot;, frame = FALSE)</span><br><span class="line"># Poisson distribution for t = 1, and lambda = 100</span><br><span class="line">plot(0 : 200, dpois(0 : 200, lambda = 100), type = &quot;h&quot;, frame = FALSE)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, for large values of <span class="math inline">\(\lambda\)</span>, the distribution looks like the Gaussian</li>
</ul>
<h3 id="example---leek-group-website-traffic">Example - Leek Group Website Traffic</h3>
<ul>
<li>for this example, we will be modeling the daily traffic to Jeff Leek’s web site: <a href="http://biostat.jhsph.edu/~jleek/" class="uri" target="_blank" rel="noopener">http://biostat.jhsph.edu/~jleek/</a>
<ul>
<li>the data comes from Google Analytics and was extracted by the <code>rga</code> package that can be found at <a href="http://skardhamar.github.com/rga/" class="uri" target="_blank" rel="noopener">http://skardhamar.github.com/rga/</a></li>
</ul></li>
<li>for the purpose of the example, the time is <strong><em>always</em></strong> one day, so <span class="math inline">\(t = 1\)</span>, Poisson mean is interpreted as web hits per day
<ul>
<li>if <span class="math inline">\(t = 24\)</span>, we would be modeling web hits per hour</li>
</ul></li>
<li>the data can be found <a href="https://dl.dropboxusercontent.com/u/7710864/data/gaData.rda" target="_blank" rel="noopener">here</a></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># laod data</span><br><span class="line">load(&quot;gaData.rda&quot;)</span><br><span class="line"># convert the dates to proper formats</span><br><span class="line">gaData$julian &lt;- julian(gaData$date)</span><br><span class="line"># plot visits vs dates</span><br><span class="line">plot(gaData$julian,gaData$visits,pch=19,col=&quot;darkgrey&quot;,xlab=&quot;Julian&quot;,ylab=&quot;Visits&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="example---linear-regression">Example - Linear Regression</h3>
<ul>
<li>the traffic can be modeled using linear model as follows <span class="math display">\[ NH_i = \beta_0 + \beta_1 JD_i + \epsilon_i \]</span>
<ul>
<li><span class="math inline">\(NH_i\)</span> = number of hits to the website</li>
<li><span class="math inline">\(JD_i\)</span> = day of the year (Julian day)</li>
<li><span class="math inline">\(\beta_0\)</span> = number of hits on Julian day 0 (1970-01-01)</li>
<li><span class="math inline">\(\beta_1\)</span> = increase in number of hits per unit day</li>
<li><span class="math inline">\(\epsilon_i\)</span> = variation due to everything we didn’t measure</li>
</ul></li>
<li>the expected outcome is defined as <span class="math display">\[ E[NH_i | JD_i, \beta_0, \beta_1] = \beta_0 + \beta_1 JD_i\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># plot the visits vs dates</span><br><span class="line">plot(gaData$julian,gaData$visits,pch=19,col=&quot;darkgrey&quot;,xlab=&quot;Julian&quot;,ylab=&quot;Visits&quot;)</span><br><span class="line"># perform linear regression</span><br><span class="line">lm1 &lt;- lm(gaData$visits ~ gaData$julian)</span><br><span class="line"># plot regression line</span><br><span class="line">abline(lm1,col=&quot;red&quot;,lwd=3)</span><br></pre></td></tr></table></figure>
<h3 id="example---log-outcome">Example - log Outcome</h3>
<ul>
<li>if we are interested in relative increases in web traffic, we can the natural log of the outcome, so the linear model becomes <span class="math display">\[ \log(NH_i) = \beta_0 + \beta_1 JD_i + \epsilon_i\]</span>
<ul>
<li><span class="math inline">\(\log(NH_i)\)</span> = number of hits to the website</li>
<li><span class="math inline">\(JD_i\)</span> = day of the year (Julian day)</li>
<li><span class="math inline">\(\beta_0\)</span> = log number of hits on Julian day 0 (1970-01-01)</li>
<li><span class="math inline">\(\beta_1\)</span> = increase in log number of hits per unit day</li>
<li><span class="math inline">\(\epsilon_i\)</span> = variation due to everything we didn’t measure</li>
</ul></li>
<li>when we take the natural log of outcomes and fit a regression model, the exponentiated coefficients estimate quantities based on the geometric means rather than the measured values
<ul>
<li><span class="math inline">\(e^{E[\log(Y)]}\)</span> = geometric mean of <span class="math inline">\(Y\)</span>
<ul>
<li>geometric means are defined as <span class="math display">\[e^{\frac{1}{n}\sum_{i=1}^n \log(y_i)} = (\prod_{i=1}^n y_i)^{1/n}\]</span> which is the estimate for the <strong>population geometric mean</strong></li>
<li>as we collect infinite amount of data, <span class="math inline">\(\prod_{i=1}^n y_i)^{1/n} \to E[\log(Y)]\)</span></li>
</ul></li>
<li><span class="math inline">\(e^{\beta_0}\)</span> = estimated geometric mean hits on day 0</li>
<li><span class="math inline">\(e^{\beta_1}\)</span> = estimated relative increase or decrease in geometric mean hits per day</li>
<li><em><strong>Note</strong>: not we can not take the natural log of zero counts, so often we need to adding a constant (i.e. 1) to construct the log model </em>
<ul>
<li>adding the constant changes the interpretation of coefficient slightly</li>
<li><span class="math inline">\(e^{\beta_1}\)</span> is now the relative increase or decrease in geometric mean hits <strong>+ 1</strong> per day</li>
</ul></li>
</ul></li>
<li>the expected outcome is <span class="math display">\[E[\log(NH_i | JD_i, \beta_0, \beta_1)] = \beta_0 + \beta_1 JD_i \]</span></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">round(exp(coef(lm(I(log(gaData$visits + 1)) ~ gaData$julian))), 5)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the daily increase in hits is 0.2%</li>
</ul>
<h3 id="example---poisson-regression">Example - Poisson Regression</h3>
<ul>
<li>the Poisson model can be constructed as log of the mean <span class="math display">\[\log\left(E[NH_i | JD_i, \beta_0, \beta_1]\right) = \beta_0 + \beta_1 JD_i\]</span> or in other form <span class="math display">\[E[NH_i | JD_i, \beta_0, \beta_1] = \exp\left(\beta_0 + \beta_1 JD_i\right)\]</span>
<ul>
<li><span class="math inline">\(NH_i\)</span> = number of hits to the website</li>
<li><span class="math inline">\(JD_i\)</span> = day of the year (Julian day)</li>
<li><span class="math inline">\(\beta_0\)</span> = expected number of hits on Julian day 0 (1970-01-01)</li>
<li><span class="math inline">\(\beta_1\)</span> = expected increase in number of hits per unit day</li>
<li><em><strong>Note</strong>: Poisson model differs from the log outcome model in that the coefficients are interpreted naturally as expected value of outcome where as the log model is interpreted on the log scale of outcome </em></li>
</ul></li>
<li>we can transform the Poisson model to <span class="math display">\[E[NH_i | JD_i, \beta_0, \beta_1] = \exp\left(\beta_0 + \beta_1 JD_i\right) = \exp\left(\beta_0 \right)\exp\left(\beta_1 JD_i\right)\]</span>
<ul>
<li><span class="math inline">\(\beta_1 = E[NH_i | JD_i+1, \beta_0, \beta_1] - E[NH_i | JD_i, \beta_0, \beta_1]\)</span></li>
<li><span class="math inline">\(\beta_1\)</span> can therefore be interpreted as the <strong>relative</strong> increase/decrease in web traffic hits per one day increase</li>
</ul></li>
<li><code>glm(outcome~predictor, family = &quot;poisson&quot;)</code> = performs Poisson regression</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># plot visits vs dates</span><br><span class="line">plot(gaData$julian,gaData$visits,pch=19,col=&quot;darkgrey&quot;,xlab=&quot;Julian&quot;,ylab=&quot;Visits&quot;)</span><br><span class="line"># construct Poisson regression model</span><br><span class="line">glm1 &lt;- glm(gaData$visits ~ gaData$julian,family=&quot;poisson&quot;)</span><br><span class="line"># plot linear regression line in red</span><br><span class="line">abline(lm1,col=&quot;red&quot;,lwd=3)</span><br><span class="line"># plot Poisson regression line in</span><br><span class="line">lines(gaData$julian,glm1$fitted,col=&quot;blue&quot;,lwd=3)</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: the Poisson fit is non-linear since it is linear only on the log of the mean scale </em></li>
</ul>
<h3 id="example---robust-standard-errors-with-poisson-regression">Example - Robust Standard Errors with Poisson Regression</h3>
<ul>
<li>variance of the Poisson distribution is defined to be the mean of the distribution, so we would expect the variance to increase with higher values of <span class="math inline">\(X\)</span></li>
<li>below is the residuals vs fitted value plot for the Poisson regression model</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot residuals vs fitted values</span><br><span class="line">plot(glm1$fitted,glm1$residuals,pch=19,col=&quot;grey&quot;,ylab=&quot;Residuals&quot;,xlab=&quot;Fitted&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the residuals don’t appear to be increasing with higher fitted values</li>
<li>even if the mean model is correct in principle, there could always be a certain degree of <strong><em>model mis-specification</em></strong></li>
<li>to account for mis-specifications for the model, we can use
<ol type="1">
<li><code>glm(outcome~predictor, family = &quot;quasi-poisson&quot;)</code> = introduces an additional multiplicative factor <span class="math inline">\(\phi\)</span> to denominator of model so that the variance is <span class="math inline">\(\phi \mu\)</span> rather than just <span class="math inline">\(\mu\)</span> (see <a href="#variances-and-quasi-likelihoods">Variances and Quasi-Likelihoods</a>)</li>
<li>more generally, <em>robust standard errors</em> (effectively constructing wider confidence intervals) can be used</li>
</ol></li>
<li><strong>model agnostic standard errors</strong>, implemented through the <code>sandwich</code> package, is one way to calculate the robust standard errors
<ul>
<li>algorithm assumes the mean relationship is specified correctly and attempts to get a general estimates the variance that isn’t highly dependent on the model</li>
<li>it uses assumption of large sample sizes and asymptotics to estimate the confidence intervals that is robust to model mis-specification</li>
<li><em><strong>Note</strong>: more information can be found at <a href="http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval" class="uri" target="_blank" rel="noopener">http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval</a> </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># load sandwich package</span><br><span class="line">library(sandwich)</span><br><span class="line"># compute</span><br><span class="line">confint.agnostic &lt;- function (object, parm, level = 0.95, ...)</span><br><span class="line">&#123;</span><br><span class="line">    cf &lt;- coef(object); pnames &lt;- names(cf)</span><br><span class="line">    if (missing(parm))</span><br><span class="line">        parm &lt;- pnames</span><br><span class="line">    else if (is.numeric(parm))</span><br><span class="line">        parm &lt;- pnames[parm]</span><br><span class="line">    a &lt;- (1 - level)/2; a &lt;- c(a, 1 - a)</span><br><span class="line">    pct &lt;- stats:::format.perc(a, 3)</span><br><span class="line">    fac &lt;- qnorm(a)</span><br><span class="line">    ci &lt;- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,</span><br><span class="line">                                                               pct))</span><br><span class="line">    ses &lt;- sqrt(diag(sandwich::vcovHC(object)))[parm]</span><br><span class="line">    ci[] &lt;- cf[parm] + ses %o% fac</span><br><span class="line">    ci</span><br><span class="line">&#125;</span><br><span class="line"># regular confidence interval from Poisson Model</span><br><span class="line">confint(glm1)</span><br><span class="line"># model agnostic standard errors</span><br><span class="line">confint.agnostic(glm1)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the robust standard error produced slightly wider confidence intervals</li>
</ul>
<h3 id="example---rates">Example - Rates</h3>
<ul>
<li>if we were to model the percentage of total web hits that are coming from the <em>Simply Statistics</em> blog, we could construct the following model <span class="math display">\[\begin{aligned}
E[NHSS_i | JD_i, \beta_0, \beta_1]/NH_i &amp; = \exp\left(\beta_0 + \beta_1 JD_i\right) \\
(take~\log~of~both~sides)~\log\left(E[NHSS_i | JD_i, \beta_0, \beta_1]\right) - \log(NH_i) &amp; = \beta_0 + \beta_1 JD_i \\
(move~\log(NH_i)~to~right~side)~\log\left(E[NHSS_i | JD_i, \beta_0, \beta_1]\right) &amp; = \log(NH_i) + \beta_0 + \beta_1 JD_i \\
\end{aligned}\]</span>
<ul>
<li>when <strong>offset</strong> term, <span class="math inline">\(\log(NH_i)\)</span>, is present in the Poisson model, the interpretation of the coefficients will be relative to the offset quantity</li>
<li>it’s important to recognize that the fitted response doesn’t change</li>
<li><strong><em>example</em></strong>: to convert the outcome from daily data to hourly, we can add a factor 24 so that the model becomes <span class="math display">\[\begin{aligned}
E[NHSS_i | JD_i, \beta_0, \beta_1]/24 &amp; = \exp\left(\beta_0 + \beta_1 JD_i\right) \\
(take~\log~of~both~sides)~\log\left(E[NHSS_i | JD_i, \beta_0, \beta_1]\right) - \log(24) &amp; = \beta_0 + \beta_1 JD_i \\
(move~\log(24)~to~right~side)~\log\left(E[NHSS_i | JD_i, \beta_0, \beta_1]\right) &amp; = \log(24) + \log(NH_i) + \beta_0 + \beta_1 JD_i \\
\end{aligned}\]</span></li>
</ul></li>
<li>back to the rates model, we fit the Poisson model now with an offset so that the model is interpreted with respect to the number of visits
<ul>
<li><code>glm(outcome ~ predictor, offset = log(offset), family = &quot;poisson&quot;)</code> = perform Poisson regression with offset</li>
<li><code>glm(outcome ~ predictor + log(offset))</code> = produces the same result</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># perform Poisson regression with offset for number of visits</span><br><span class="line">glm2 &lt;- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),</span><br><span class="line">            family=&quot;poisson&quot;,data=gaData)</span><br><span class="line"># plot the fitted means (from simply statistics)</span><br><span class="line">plot(julian(gaData$date),glm2$fitted,col=&quot;blue&quot;,pch=19,xlab=&quot;Date&quot;,ylab=&quot;Fitted Counts&quot;)</span><br><span class="line"># plot the fitted means (total visit)</span><br><span class="line">points(julian(gaData$date),glm1$fitted,col=&quot;red&quot;,pch=19)</span><br><span class="line"># plot the rates for simply stats</span><br><span class="line">plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col=&quot;grey&quot;,xlab=&quot;Date&quot;,</span><br><span class="line">     ylab=&quot;Fitted Rates&quot;,pch=19)</span><br><span class="line"># plot the fitted rates for simply stats (visit/day)</span><br><span class="line">lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col=&quot;blue&quot;,lwd=3)</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: we added 1 to the <code>log(visits)</code> to address 0 values </em></li>
</ul>
<h3 id="further-resources-1">Further Resources</h3>
<ul>
<li><a href="http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html" target="_blank" rel="noopener">Log-linear mMdels and Multi-way Tables</a></li>
<li><a href="http://en.wikipedia.org/wiki/Poisson_regression" target="_blank" rel="noopener">Wikipedia on Poisson Regression</a></li>
<li><a href="http://en.wikipedia.org/wiki/Overdispersion" target="_blank" rel="noopener">Wikipedia on Overdispersion</a></li>
<li><a href="http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf" target="_blank" rel="noopener">Regression Models for count data in R</a></li>
<li><a href="http://cran.r-project.org/web/packages/pscl/index.html" target="_blank" rel="noopener"><code>pscl</code> package</a> -
<ul>
<li>often time in modeling counts, they maybe more zero counts in the data than anticipated, which the regular Poisson model doesn’t account for</li>
<li>the function <code>zeroinfl</code> fits zero inflated Poisson (ziP) models to such data</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="fitting-functions">Fitting Functions</h2>
<ul>
<li><strong>scatterplot smoothing</strong> = fitting functions (multiple linear models, piece-wise zig-zag lines) to data in the form <span class="math inline">\(Y_i = f(X_i) + \epsilon_i\)</span></li>
<li>consider the model <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \sum_{k=1}^d (x_i - \xi_k)_+ \gamma_k + \epsilon_{i}\]</span> where <span class="math inline">\((a)_+ = a\)</span> if <span class="math inline">\(a &gt; 0\)</span> and <span class="math inline">\(0\)</span> otherwise and <span class="math inline">\(\xi_1 \leq ... \leq \xi_d\)</span> are known <strong>knot points</strong></li>
<li>the mean function <span class="math display">\[E[Y_i] = \beta_0 + \beta_1 X_i + \sum_{k=1}^d (x_i - \xi_k)_+ \gamma_k\]</span> is continuous at the knot points
<ul>
<li>for <span class="math inline">\(\xi_k = 5\)</span>, the expected value for <span class="math inline">\(Y_i\)</span> as <span class="math inline">\(x_i\)</span> approaches 5 from the left is <span class="math display">\[\begin{aligned}
E[Y_i]_{\xi = 5 | left} &amp; = \beta_0 + \beta_1 x_i + (x_i - 5)_+ \gamma_k \\
(since~x_i&lt;5)~ E[Y_i]_{\xi = 5 | left} &amp; = \beta_0 + \beta_1 x_i \\
\end{aligned}\]</span></li>
<li>the expected value for <span class="math inline">\(Y_i\)</span> as <span class="math inline">\(x_i\)</span> approaches 5 from the right is <span class="math display">\[\begin{aligned}
E[Y_i]_{\xi = 5 | right} &amp; = \beta_0 + \beta_1 x_i + (x_i - 5)_+ \gamma_k \\
(since~x_i&gt;5)~ E[Y_i]_{\xi = 5 | right} &amp; = \beta_0 + \beta_1 x_i + (x_i - 5) \gamma_k\\
(simplify)~ E[Y_i]_{\xi = 5 | right} &amp; = \beta_0 - 5 \gamma_k + (\beta_1 + \gamma_k) x_i \\
\end{aligned}\]</span></li>
<li>as we can see from above, the right side is just another line with different intercept (<span class="math inline">\(\beta_0 - 5 \gamma_k\)</span>) and slope (<span class="math inline">\(\beta_1 + \gamma_k\)</span>)</li>
<li>so as <span class="math inline">\(x\)</span> approaches 5, both sides converge</li>
</ul></li>
</ul>
<h3 id="considerations">Considerations</h3>
<ul>
<li><strong>basis</strong> = the collection of regressors</li>
<li>single knot point terms can fit <em>hockey-stick-like</em> processes</li>
<li>these bases can be used in GLMs (as an additional term/predictor) as well</li>
<li>issue with these approaches is the <strong>large</strong> number of parameters introduced
<ul>
<li>requires some method of <strong><em>regularization</em></strong>, or penalize for large number of parameters (see <strong>Practical Machine Learning</strong> course)</li>
<li>introducing large number of knots have significant consequences</li>
</ul></li>
</ul>
<h3 id="example---fitting-piecewise-linear-function">Example - Fitting Piecewise Linear Function</h3>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 500; x &lt;- seq(0, 4 * pi, length = n); y &lt;- sin(x) + rnorm(n, sd = .3)</span><br><span class="line"># define 20 knot points</span><br><span class="line">knots &lt;- seq(0, 8 * pi, length = 20);</span><br><span class="line"># define the ()+ function to only take the values that are positive after the knot pt</span><br><span class="line">splineTerms &lt;- sapply(knots, function(knot) (x &gt; knot) * (x - knot))</span><br><span class="line"># define the predictors as X and spline term</span><br><span class="line">xMat &lt;- cbind(x, splineTerms)</span><br><span class="line"># fit linear models for y vs predictors</span><br><span class="line">yhat &lt;- predict(lm(y ~ xMat))</span><br><span class="line"># plot data points (x, y)</span><br><span class="line">plot(x, y, frame = FALSE, pch = 21, bg = &quot;lightblue&quot;)</span><br><span class="line"># plot fitted values</span><br><span class="line">lines(x, yhat, col = &quot;red&quot;, lwd = 2)</span><br></pre></td></tr></table></figure>
<h3 id="example---fitting-piecewise-quadratic-function">Example - Fitting Piecewise Quadratic Function</h3>
<ul>
<li>adding squared terms makes it <strong><em>continuous</em></strong> AND <strong><em>differentiable</em></strong> at the knot points, and the model becomes <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \sum_{k=1}^d (x_i - \xi_k)_+^2 \gamma_k + \epsilon_{i}\]</span> where <span class="math inline">\((a)^2_+ = a^2\)</span> if <span class="math inline">\(a &gt; 0\)</span> and <span class="math inline">\(0\)</span> otherwise</li>
<li>adding cubic terms makes it twice continuously differentiable at the knot points, etcetera</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># define the knot terms in the model</span><br><span class="line">splineTerms &lt;- sapply(knots, function(knot) (x &gt; knot) * (x - knot)^2)</span><br><span class="line"># define the predictors as x, x^2 and knot terms</span><br><span class="line">xMat &lt;- cbind(x, x^2, splineTerms)</span><br><span class="line"># fit linear models for y vs predictors</span><br><span class="line">yhat &lt;- predict(lm(y ~ xMat))</span><br><span class="line"># plot data points (x, y)</span><br><span class="line">plot(x, y, frame = FALSE, pch = 21, bg = &quot;lightblue&quot;)</span><br><span class="line"># plot fitted values</span><br><span class="line">lines(x, yhat, col = &quot;red&quot;, lwd = 2)</span><br></pre></td></tr></table></figure>
<h3 id="example---harmonics-using-linear-models">Example - Harmonics using Linear Models</h3>
<ul>
<li><strong>discrete Fourier transforms</strong> = instance of linear regression model, use sin and cosine functions as basis to fit data</li>
<li>to demonstrate this, we will generate 2 seconds of sound data using sin waves, simulate a chord, and apply linear regression to find out which notes are playing</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># frequencies for white keys from c4 to c5</span><br><span class="line">notes4 &lt;- c(261.63, 293.66, 329.63, 349.23, 392.00, 440.00, 493.88, 523.25)</span><br><span class="line"># generate sequence for 2 seconds</span><br><span class="line">t &lt;- seq(0, 2, by = .001); n &lt;- length(t)</span><br><span class="line"># define data for c4 e4 g4 using sine waves with their frequencies</span><br><span class="line">c4 &lt;- sin(2 * pi * notes4[1] * t); e4 &lt;- sin(2 * pi * notes4[3] * t);</span><br><span class="line">g4 &lt;- sin(2 * pi * notes4[5] * t)</span><br><span class="line"># define data for a chord and add a bit of noise</span><br><span class="line">chord &lt;- c4 + e4 + g4 + rnorm(n, 0, 0.3)</span><br><span class="line"># generate profile data for all notes</span><br><span class="line">x &lt;- sapply(notes4, function(freq) sin(2 * pi * freq * t))</span><br><span class="line"># fit the chord using the profiles for all notes</span><br><span class="line">fit &lt;- lm(chord ~ x - 1)</span><br></pre></td></tr></table></figure>
<ul>
<li>after generating the data and running the linear regression, we can plot the results to see if the notes are correctly identified</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># set up plot</span><br><span class="line">plot(c(0, 9), c(0, 1.5), xlab = &quot;Note&quot;, ylab = &quot;Coef^2&quot;, axes = FALSE, frame = TRUE, type = &quot;n&quot;)</span><br><span class="line"># set up axes</span><br><span class="line">axis(2)</span><br><span class="line">axis(1, at = 1 : 8, labels = c(&quot;c4&quot;, &quot;d4&quot;, &quot;e4&quot;, &quot;f4&quot;, &quot;g4&quot;, &quot;a4&quot;, &quot;b4&quot;, &quot;c5&quot;))</span><br><span class="line"># add vertical lines for each note</span><br><span class="line">for (i in 1 : 8) abline(v = i, lwd = 3, col = grey(.8))</span><br><span class="line"># plot the linear regression fits</span><br><span class="line">lines(c(0, 1 : 8, 9), c(0, coef(fit)^2, 0), type = &quot;l&quot;, lwd = 3, col = &quot;red&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the correct notes were identified</li>
<li>we can also use the <a href="http://en.wikipedia.org/wiki/Fast_Fourier_transform" target="_blank" rel="noopener"><strong>Fast Fourier Transforms</strong></a> to identify the notes
<ul>
<li><code>fft(data)</code> = performs fast Fourier transforms on provided data</li>
<li><code>Re(data)</code> = subset to only the real components of the complex data</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># perform fast fourier transforms on the chord matrix</span><br><span class="line">a &lt;- fft(chord)</span><br><span class="line"># plot only the real components of the fft</span><br><span class="line">plot(Re(a)^2, type = &quot;l&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: the algorithm checks for all possible notes at all frequencies it can detect, which is why the peaks are very high in magnitude </em></li>
<li><em><strong>Note</strong>: the symmetric display of the notes are due to periodic symmetries of the sine functions </em></li>
</ul>

          
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Statistical-Inference/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/07/Statistical-Inference/" itemprop="url">
                  Statistical Inference
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 14:07:49 / Modified: 14:21:25" itemprop="dateCreated datePublished" datetime="2018-08-07T14:07:49+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">70k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">1:04</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="overview">Overview</h2>
<ul>
<li><strong>Statistical Inference</strong> = generating conclusions about a population from a noisy sample</li>
<li>Goal = extend beyond data to population</li>
<li>Statistical Inference = only formal system of inference we have</li>
<li>many different modes, but <strong>two</strong> broad flavors of inference (inferential paradigms): <strong><em>Bayesian</em></strong> vs <strong><em>Frequencist</em></strong>
<ul>
<li><strong>Frequencist</strong> = uses long run proportion of times an event occurs independent identically distributed repetitions
<ul>
<li>frequentist is what this class is focused on</li>
<li>believes if an experiment is repeated many many times, the resultant percentage of success/something happening defines that population parameter</li>
</ul></li>
<li><strong>Bayesian</strong> = probability estimate for a hypothesis is updated as additional evidence is acquired</li>
</ul></li>
<li><strong>statistic</strong> = number computed from a sample of data
<ul>
<li>statistics are used to infer information about a population</li>
</ul></li>
<li><strong>random variable</strong> = outcome from an experiment
<ul>
<li>deterministic processes (variance/means) produce additional random variables when applied to random variables, and they have their own distributions</li>
</ul></li>
</ul>
<h2 id="probability">Probability</h2>
<ul>
<li><strong>Probability</strong> = the study of quantifying the likelihood of particular events occurring
<ul>
<li>given a random experiment, <strong><em>probability</em></strong> = population quantity that summarizes the randomness
<ul>
<li>not in the data at hand, but a conceptual quantity that exist in the population that we want to estimate</li>
</ul></li>
</ul></li>
</ul>
<h3 id="general-probability-rules">General Probability Rules</h3>
<ul>
<li>discovered by Russian mathematician Kolmogorov, also known as “Probability Calculus”</li>
<li>probability = function of any set of outcomes and assigns it a number between 0 and 1
<ul>
<li><span class="math inline">\(0 \le P(E) \le 1\)</span>, where <span class="math inline">\(E\)</span> = event</li>
</ul></li>
<li>probability that nothing occurs = 0 (impossible, have to roll dice to create outcome), that something occurs is 1 (certain)</li>
<li>probability of outcome or event <span class="math inline">\(E\)</span>, <span class="math inline">\(P(E)\)</span> = ratio of ways that <span class="math inline">\(E\)</span> could occur to number of all possible outcomes or events</li>
<li>probability of something = 1 - probability of the opposite occurring</li>
<li>probability of the <strong>union</strong> of any two sets of outcomes that have nothing in common (mutually exclusive) = sum of respective probabilities</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library(grid);library(png)</span><br><span class="line">grid.raster(readPNG(&quot;figures/1.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>if <span class="math inline">\(A\)</span> implies occurrence of <span class="math inline">\(B\)</span>, then <span class="math inline">\(P(A)\)</span> occurring <span class="math inline">\(&lt; P(B)\)</span> occurring</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/2.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>for any two events, probability of at least one occurs = the sum of their probabilities - their intersection (in other words, probabilities can not be added simply if they have non-trivial intersection)</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/3.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>for independent events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, <span class="math inline">\(P(A \:\cup\: B) = P(A) \times P(B)\)</span></li>
<li>for outcomes that can occur with different combination of events and these combinations are mutually exclusive, the <span class="math inline">\(P(E_{total}) = \sum P(E_{part})\)</span></li>
</ul>
<h3 id="conditional-probability">Conditional Probability</h3>
<ul>
<li>let B = an event so that <span class="math inline">\(P(B) &gt; 0\)</span></li>
<li><strong>conditional probability</strong> of an event A, given B is defined as the probability that BOTH A and B occurring divided by the probability of B occurring <span class="math display">\[P(A\:|\:B) = \frac{P(A \:\cap\: B)}{P(B)}\]</span></li>
<li>if A and B are <strong><em>independent</em></strong>, then <span class="math display">\[P(A\:|\:B) = \frac{P(A)P(B)}{P(B)} = P(A)\]</span></li>
<li><strong><em>example</em></strong>
<ul>
<li>for die roll, <span class="math inline">\(A = \{1\}\)</span>, <span class="math inline">\(B = \{1, 3, 5\}\)</span>, then <span class="math display">\[P(1~|~Odd) = P(A\:|\:B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A)}{P(B)} = \frac{1/6}{3/6} = \frac{1}{3}\]</span></li>
</ul></li>
</ul>
<h3 id="bayes-rule">Baye’s Rule</h3>
<ul>
<li>definition <span class="math display">\[P(B\:|\:A) = \frac{P(A\:|\:B)P(B)}{P(A\:|\:B)P(B)+P(A\:|\:B^c)P(B^c)}\]</span> where <span class="math inline">\(B^c\)</span> = corresponding probability of event <span class="math inline">\(B\)</span>, <span class="math inline">\(P(B^c) = 1 - P(B)\)</span></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="random-variables">Random Variables</h2>
<ul>
<li><strong>random variable</strong> = numeric outcome of experiment</li>
<li><strong>discrete</strong> (what you can count/categories) = assign probabilities to every number/value the variable can take
<ul>
<li>coin flip, rolling a die, web traffic in a day</li>
</ul></li>
<li><strong>continuous</strong> (any number within a continuum) = assign probabilities to the range the variable can take
<ul>
<li>BMI index, intelligence quotients</li>
<li><em><strong>Note</strong>: limitations of precision in taking the measurements may imply that the values are discrete, but we in fact consider them continuous </em></li>
</ul></li>
<li><code>rbinom()</code>, <code>rnorm()</code>, <code>rgamma()</code>, <code>rpois()</code>, <code>runif()</code> = functions to generate random variables from the binomial, normal, Gamma, Poisson, and uniform distributions</li>
<li>density and mass functions (population quantities, not what occurs in data) for random variables = best starting point to model/think about probabilities for numeric outcome of experiments (variables)
<ul>
<li>use data to estimate properties of population <span class="math inline">\(\rightarrow\)</span> linking sample to population</li>
</ul></li>
</ul>
<h3 id="probability-mass-function-pmf">Probability Mass Function (PMF)</h3>
<ul>
<li>evaluates the probability that the <strong>discrete random variable</strong> takes on a specific value
<ul>
<li>measures the chance of a particular outcome happening</li>
<li>always <span class="math inline">\(\ge\)</span> 0 for every possible outcome</li>
<li><span class="math inline">\(\sum\)</span> possible values that the variable can take = 1</li>
</ul></li>
<li><strong><em>Bernoulli distribution example</em></strong>
<ul>
<li>X = 0 <span class="math inline">\(\rightarrow\)</span> tails, X = 1 <span class="math inline">\(\rightarrow\)</span> heads
<ul>
<li>X here represents potential outcome</li>
</ul></li>
<li><span class="math inline">\(P(X = x) = (\frac{1}{2})^x(\frac{1}{2})^{1-x}\)</span> for <span class="math inline">\(X = 0, 1\)</span>
<ul>
<li><span class="math inline">\(x\)</span> here represents a value we can plug into the PMF</li>
<li>general form <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(p(x) = (\theta)^x(1-\theta)^{1-x}\)</span></li>
</ul></li>
</ul></li>
<li><code>dbinom(k, n, p)</code> = return the probability of getting <code>k</code> successes out of <code>n</code> trials, given probability of success is <code>p</code></li>
</ul>
<h3 id="probability-density-function-pdf">Probability Density Function (PDF)</h3>
<ul>
<li>evaluates the probability that the <strong>continuous random variable</strong> takes on a specific value
<ul>
<li>always <span class="math inline">\(\ge\)</span> 0 everywhere</li>
<li>total area under curve must = 1</li>
</ul></li>
<li><strong>areas under PDFs</strong> correspond to the probabilities for that random variable taking on that range of values (PMF)</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/4-1.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>but the probability of the variable taking a specific value = 0 (area of a line is 0)</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/4-2.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><p><em><strong>Note</strong>: the above is true because it is modeling random variables as if they have infinite precision, when in reality they do not </em></p></li>
<li><p><code>dnorm()</code>, <code>dgamma()</code>, <code>dpois()</code>, <code>dunif()</code> = return probability of a certain value from the normal, Gamma, Poisson, and uniform distributions</p></li>
</ul>
<h3 id="cumulative-distribution-function-cdf">Cumulative Distribution Function (CDF)</h3>
<ul>
<li>CDF of a random variable <span class="math inline">\(X\)</span> = probability that the random variable is <span class="math inline">\(\le\)</span> value <span class="math inline">\(x\)</span>
<ul>
<li><span class="math inline">\(F(x) = P(X \le x)\)</span> = applies when <span class="math inline">\(X\)</span> is discrete/continuous</li>
</ul></li>
<li>PDF = derivative of CDF
<ul>
<li>integrate PDF <span class="math inline">\(\rightarrow\)</span> CDF
<ul>
<li><code>integrate(function, lower=0, upper=1)</code> <span class="math inline">\(\rightarrow\)</span> can be used to evaluate integrals for a specified range</li>
</ul></li>
</ul></li>
<li><code>pbinom()</code>, <code>pnorm()</code>, <code>pgamma()</code>, <code>ppois()</code>, <code>punif()</code> = returns the cumulative probabilities from 0 up to a specified value from the binomial, normal, Gamma, Poisson, and uniform distributions</li>
</ul>
<h3 id="survival-function">Survival Function</h3>
<ul>
<li>survival function of a random variable <span class="math inline">\(X\)</span> = probability the random variable <span class="math inline">\(&gt; x\)</span>, complement of CDF
<ul>
<li><span class="math inline">\(S(x) = P(X &gt; x) = 1 - F(x)\)</span>, where <span class="math inline">\(F(x) =\)</span> CDF</li>
</ul></li>
</ul>
<h3 id="quantile">Quantile</h3>
<ul>
<li>the <span class="math inline">\(\alpha^{th}\)</span> quantile of a distribution with distribution function F = point <span class="math inline">\(x_{\alpha}\)</span>
<ul>
<li><span class="math inline">\(F(x_{\alpha}) = \alpha\)</span></li>
<li>percentile = quantile with <span class="math inline">\(\alpha\)</span> expressed as a percent</li>
<li>median = 50<sup>th</sup> percentile</li>
<li><span class="math inline">\(\alpha\)</span>% of the possible outcomes lie below it</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/5.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>qbeta(quantileInDecimals, 2, 1)</code> = returns quantiles for beta distribution
<ul>
<li>works for <code>qnorm()</code>, <code>qbinom()</code>, <code>qgamma()</code>, <code>qpois()</code>, etc.</li>
</ul></li>
<li>median estimated in this fashion = a population median</li>
<li>probability model connects data to population using assumptions
<ul>
<li>population median = <strong><em>estimand</em></strong>, sample median = <strong><em>estimator</em></strong></li>
</ul></li>
</ul>
<h3 id="independence">Independence</h3>
<ul>
<li>two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong><em>independent</em></strong> if the following is true
<ul>
<li><span class="math inline">\(P(A\:\cap\:B) = P(A)P(B)\)</span></li>
<li><span class="math inline">\(P(A\:|\:B) = P(A)\)</span></li>
</ul></li>
<li>two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong><em>independent</em></strong>, if for any two sets, <strong>A</strong> and <strong>B</strong>, the following is true
<ul>
<li><span class="math inline">\(P([X \in A]\cap[Y \in B]) = P(X \in A)P(Y \in B)\)</span></li>
</ul></li>
<li><strong>independence</strong> = statistically unrelated from one another</li>
<li>if <span class="math inline">\(A\)</span> is <strong><em>independent</em></strong> of <span class="math inline">\(B\)</span>, then the following are true
<ul>
<li><span class="math inline">\(A^c\)</span> is independent of <span class="math inline">\(B\)</span></li>
<li><span class="math inline">\(A\)</span> is independent of <span class="math inline">\(B^c\)</span></li>
<li><span class="math inline">\(A^c\)</span> is independent of <span class="math inline">\(B^c\)</span></li>
</ul></li>
</ul>
<h3 id="iid-random-variables">IID Random Variables</h3>
<ul>
<li>random variables are said to be <strong>IID</strong> if they are <strong><em>independent and identically distributed</em></strong>
<ul>
<li><strong>independent</strong> = statistically unrelated from each other</li>
<li><strong>identically distributed</strong> = all having been drawn from the same population distribution</li>
</ul></li>
<li>IID random variables = default model for random samples = default starting point of inference</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="diagnostic-test">Diagnostic Test</h2>
<ul>
<li>Let <span class="math inline">\(+\)</span> and <span class="math inline">\(-\)</span> be the results, positive and negative respectively, of a diagnostic test</li>
<li>Let <span class="math inline">\(D\)</span> = subject of the test has the disease, <span class="math inline">\(D^c\)</span> = subject does not</li>
<li><strong>sensitivity</strong> = <span class="math inline">\(P(+\:|\:D)\)</span> = probability that the test is positive given that the subject has the disease (the higher the better)</li>
<li><strong>specificity</strong> = <span class="math inline">\(P(-\:|\:D^c)\)</span> = probability that the test is negative given that the subject does not have the disease (the higher the better)</li>
<li><strong>positive predictive value</strong> = <span class="math inline">\(P(D\:|\:+)\)</span> = probability that that subject has the disease given that the test is positive</li>
<li><strong>negative predictive value</strong> = <span class="math inline">\(P(D^c\:|\:-)\)</span> = probability that the subject does not have the disease given the test is negative</li>
<li><strong>prevalence of disease</strong> = <span class="math inline">\(P(D)\)</span> = marginal probability of disease</li>
</ul>
<h3 id="example">Example</h3>
<ul>
<li>specificity of 98.5%, sensitivity = 99.7%, prevalence of disease = .1% <span class="math display">\[\begin{aligned}
P(D ~|~ +) &amp; = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}\\
&amp; = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + \{1-P(-~|~D^c)\}\{1 - P(D)\}} \\
&amp; = \frac{.997\times .001}{.997 \times .001 + .015 \times .999}\\
&amp; =  .062
\end{aligned}\]</span></li>
<li>low positive predictive value <span class="math inline">\(\rightarrow\)</span> due to low prevalence of disease and somewhat modest specificity
<ul>
<li>suppose it was know that the subject uses drugs and has regular intercourse with an HIV infect partner (his probability of being + is higher than suspected)</li>
<li>evidence implied by a positive test result</li>
</ul></li>
</ul>
<h3 id="likelihood-ratios">Likelihood Ratios</h3>
<ul>
<li><strong>diagnostic likelihood ratio</strong> of a <strong>positive</strong> test result is defined as <span class="math display">\[DLR_+ = \frac{sensitivity}{1-specificity} =  \frac{P(+\:|\:D)}{P(+\:|\:D^c)}\]</span></li>
<li><strong>diagnostic likelihood ratio</strong> of a <strong>negative</strong> test result is defined as <span class="math display">\[DLR_- = \frac{1 - sensitivity}{specificity} =  \frac{P(-\:|\:D)}{P(-\:|\:D^c)}\]</span></li>
<li>from Baye’s Rules, we can derive the <em>positive predictive value</em> and <em>false positive value</em> <span class="math display">\[P(D\:|\:+) = \frac{P(+\:|\:D)P(D)}{P(+\:|\:D)P(D)+P(+\:|\:D^c)P(D^c)}~~~~~~\mbox{(1)}\]</span> <span class="math display">\[P(D^c\:|\:+) = \frac{P(+\:|\:D^c)P(D^c)}{P(+\:|\:D)P(D)+P(+\:|\:D^c)P(D^c)}~~~~~~\mbox{(2)}\]</span></li>
<li>if we divide equation <span class="math inline">\((1)\)</span> over <span class="math inline">\((2)\)</span>, the quantities over have the same denominator so we get the following <span class="math display">\[\frac{P(D\:|\:+)}{P(D^c\:|\:+)} = \frac{P(+\:|\:D)}{P(+\:|\:D^c)} \times \frac{P(D)}{P(D^c)}\]</span> which can also be written as <span class="math display">\[\mbox{post-test odds of D} = DLR_+ \times \mbox{pre-test odds of D}\]</span>
<ul>
<li><strong>odds</strong> = <span class="math inline">\(p/(1-p)\)</span></li>
<li><span class="math inline">\(\frac{P(D)}{P(D^c)}\)</span> = <strong>pre-test odds</strong>, or odds of disease in absence of test</li>
<li><span class="math inline">\(\frac{P(D\:|\:+)}{P(+\:|\:D^c)}\)</span> = <strong>post-test odds</strong>, or odds of disease given a positive test result</li>
<li><span class="math inline">\(DLR_+\)</span> = factor by which the odds in the presence of a positive test can be multiplied to obtain the post-test odds</li>
<li><span class="math inline">\(DLR_-\)</span> = relates the decrease in odds of disease after a negative result</li>
</ul></li>
<li>following the previous example, for sensitivity of 0.997 and specificity of 0.985, so the diagnostic likelihood ratios are as follows <span class="math display">\[DLR_+ = .997/(1-.985) = 66 ~~~~~~ DLR_- =(1-.997)/.985 = 0.003\]</span>
<ul>
<li>this indicates that the result of the positive test is the odds of disease is 66 times the pretest odds</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="expected-valuesmean">Expected Values/Mean</h2>
<ul>
<li>useful for characterizing a distribution (properties of distributions)</li>
<li><strong>mean</strong> = characterization of the center of the distribution = <em>expected value</em></li>
<li>expected value operation = <strong><em>linear</em></strong> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(E(aX +bY) = aE(X) + bE(Y)\)</span></li>
<li><strong>variance/standard deviation</strong> = characterization of how spread out the distribution is</li>
<li><p><em>sample</em> expected values for sample mean and variance will estimate the <em>population</em> counterparts</p></li>
<li><strong>population mean</strong>
<ul>
<li>expected value/mean of a random variable = center of its distribution (center of mass)</li>
<li><strong><em>discrete variables</em></strong>
<ul>
<li>for <span class="math inline">\(X\)</span> with PMF <span class="math inline">\(p(x)\)</span>, the population mean is defined as <span class="math display">\[E[X] = \sum_{x} xp(x)\]</span> where the sum is taken over <strong><em>all</em></strong> possible values of <span class="math inline">\(x\)</span></li>
<li><span class="math inline">\(E[X]\)</span> = center of mass of a collection of location and weights <span class="math inline">\({x,~p(x)}\)</span></li>
<li><em>coin flip example</em>: <span class="math inline">\(E[X] = 0 \times (1-p) + 1 \times p = p\)</span></li>
</ul></li>
<li><strong><em>continuous variable</em></strong>
<ul>
<li>for <span class="math inline">\(X\)</span> with PDF <span class="math inline">\(f(x)\)</span>, the expected value = the center of mass of the density</li>
<li>instead of summing over discrete values, the expectation <strong><em>integrates</em></strong> over a continuous function
<ul>
<li>PDF = <span class="math inline">\(f(x)\)</span></li>
<li><span class="math inline">\(\int xf(x)\)</span> = area under the PDF curve = mean/expected value of <span class="math inline">\(X\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>sample mean</strong>
<ul>
<li>sample mean estimates the population mean
<ul>
<li>sample mean = center of mass of observed data = empirical mean <span class="math display">\[\bar X = \sum_{x}^n x_i p(x_i)\]</span> where <span class="math inline">\(p(x_i) = 1/n\)</span></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># load relevant packages</span><br><span class="line">library(UsingR); data(galton); library(ggplot2)</span><br><span class="line"># plot galton data</span><br><span class="line">g &lt;- ggplot(galton, aes(x = child))</span><br><span class="line"># add histogram for children data</span><br><span class="line">g &lt;- g + geom_histogram(fill = &quot;salmon&quot;, binwidth=1, aes(y=..density..), colour=&quot;black&quot;)</span><br><span class="line"># add density smooth</span><br><span class="line">g &lt;- g + geom_density(size = 2)</span><br><span class="line"># add vertical line</span><br><span class="line">g &lt;- g + geom_vline(xintercept = mean(galton$child), size = 2)</span><br><span class="line"># print graph</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>average of random variables</strong> = a new random variable where its distribution has an expected value that is the <strong>same</strong> as the original distribution (centers are the same)
<ul>
<li>the mean of the averages = average of the original data <span class="math inline">\(\rightarrow\)</span> estimates average of the population</li>
<li>if <span class="math inline">\(E[sample~mean]\)</span> = population mean, then estimator for the sample mean is <strong>unbiased</strong>
<ul>
<li>[<strong>derivation</strong>] let <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span>, … <span class="math inline">\(X_n\)</span> be a collection of <span class="math inline">\(n\)</span> samples from the population with mean <span class="math inline">\(\mu\)</span></li>
<li>mean of this sample <span class="math display">\[\bar X = \frac{X_1 + X_2 + X_3 +  .  + X_n}{n}\]</span></li>
<li>since <span class="math inline">\(E(aX) = aE(X)\)</span>, the expected value of the mean is can be written as <span class="math display">\[E\left[\frac{X_1 + X_2 + X_3 + ... + X_n}{n}\right] = \frac{1}{n} \times [E(X_1) + E(X_2) + E(X_3) + ... + E(X_n)]\]</span></li>
<li>since each of the <span class="math inline">\(E(X_i)\)</span> is drawn from the population with mean <span class="math inline">\(\mu\)</span>, the expected value of each sample should be <span class="math display">\[E(X_i) = \mu\]</span></li>
<li>therefore <span class="math display">\[\begin{aligned}
E\left[\frac{X_1 + X_2 + X_3 + ... + X_n}{n}\right] &amp; = \frac{1}{n} \times [E(X_1) + E(X_2) + E(X_3) + ... + E(X_n)]\\
&amp; = \frac{1}{n} \times [\mu + \mu + \mu + ... + \mu]\\
&amp; = \frac{1}{n} \times n \times \mu\\
&amp; = \mu\\
\end{aligned}\]</span></li>
</ul></li>
</ul></li>
<li><em><strong>Note</strong>: the more data that goes into the sample mean, the more concentrated its density/mass functions are around the population mean </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">nosim &lt;- 1000</span><br><span class="line"># simulate data for sample size 1 to 4</span><br><span class="line">dat &lt;- data.frame(</span><br><span class="line">	x = c(sample(1 : 6, nosim, replace = TRUE),</span><br><span class="line">        apply(matrix(sample(1 : 6, nosim * 2, replace = TRUE), nosim), 1, mean),</span><br><span class="line">        apply(matrix(sample(1 : 6, nosim * 3, replace = TRUE), nosim), 1, mean),</span><br><span class="line">        apply(matrix(sample(1 : 6, nosim * 4, replace = TRUE), nosim), 1, mean)),</span><br><span class="line">	size = factor(rep(1 : 4, rep(nosim, 4))))</span><br><span class="line"># plot histograms of means by sample size</span><br><span class="line">g &lt;- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.25, colour = &quot;black&quot;)</span><br><span class="line">g + facet_grid(. ~ size)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="variance">Variance</h2>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># generate x value ranges</span><br><span class="line">xvals &lt;- seq(-10, 10, by = .01)</span><br><span class="line"># generate data from normal distribution for sd of 1 to 4</span><br><span class="line">dat &lt;- data.frame(</span><br><span class="line">    y = c(dnorm(xvals, mean = 0, sd = 1),</span><br><span class="line">        dnorm(xvals, mean = 0, sd = 2),</span><br><span class="line">        dnorm(xvals, mean = 0, sd = 3),</span><br><span class="line">        dnorm(xvals, mean = 0, sd = 4)),</span><br><span class="line">    x = rep(xvals, 4),</span><br><span class="line">    factor = factor(rep(1 : 4, rep(length(xvals), 4)))</span><br><span class="line">)</span><br><span class="line"># plot 4 lines for the different standard deviations</span><br><span class="line">ggplot(dat, aes(x = x, y = y, color = factor)) + geom_line(size = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>variance</strong> = measure of spread or dispersion, the expected squared distance of the variable from its mean (expressed in <span class="math inline">\(X\)</span>’s units<span class="math inline">\(^2\)</span>)
<ul>
<li>as we can see from above, higher variances <span class="math inline">\(\rightarrow\)</span> more spread, lower <span class="math inline">\(\rightarrow\)</span> smaller spread</li>
<li><span class="math inline">\(Var(X) = E[(X-\mu)^2] = E[X^2] - E[X]^2\)</span></li>
<li><strong>standard deviation</strong> <span class="math inline">\(= \sqrt{Var(X)}\)</span> <span class="math inline">\(\rightarrow\)</span> has same units as X</li>
<li><strong><em>example</em></strong>
<ul>
<li>for die roll, <span class="math inline">\(E[X] = 3.5\)</span></li>
<li><span class="math inline">\(E[X^2] = 1^2 \times 1/6 + 2^2 \times 1/6 + 3^2 \times 1/6 + 4^2 \times 1/6 + 5^2 \times 1/6 + 6^2 \times 1/6 = 15.17\)</span></li>
<li><span class="math inline">\(Var(X) = E[X^2] - E[X]^2 \approx 2.92\)</span></li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>for coin flip, <span class="math inline">\(E[X] = p\)</span></li>
<li><span class="math inline">\(E[X^2] = 0^2 \times (1 - p) + 1^2 \times p= p\)</span></li>
<li><span class="math inline">\(Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1-p)\)</span></li>
</ul></li>
</ul></li>
</ul>
<h3 id="sample-variance">Sample Variance</h3>
<ul>
<li>the <strong>sample variance</strong> is defined as <span class="math display">\[S^2 = \frac{\sum_{i=1} (X_i - \bar X)^2}{n-1}\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/6.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>on the above line representing the population (in magenta), any subset of data (3 of 14 selected, marked in blue) will most likely have a variance that is <strong><em>lower than</em></strong> the population variance</li>
<li>dividing by <span class="math inline">\(n - 1\)</span> will make the variance estimator <strong><em>larger</em></strong> to adjust for this fact <span class="math inline">\(\rightarrow\)</span> leads to more accurate estimation <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(S^2\)</span> = so called <strong><em>unbiased estimate of population variance</em></strong>
<ul>
<li><span class="math inline">\(S^2\)</span> is a random variable, and therefore has an associated population distribution
<ul>
<li><span class="math inline">\(E[S^2]\)</span> = population variance, where <span class="math inline">\(S\)</span> = sample standard deviation</li>
<li>as we see from the simulation results below, with more data, the distribution for <span class="math inline">\(S^2\)</span> gets more concentrated around population variance</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># specify number of simulations</span><br><span class="line">nosim &lt;- 10000;</span><br><span class="line"># simulate data for various sample sizes</span><br><span class="line">dat &lt;- data.frame(</span><br><span class="line">    x = c(apply(matrix(rnorm(nosim * 10), nosim), 1, var),</span><br><span class="line">          apply(matrix(rnorm(nosim * 20), nosim), 1, var),</span><br><span class="line">          apply(matrix(rnorm(nosim * 30), nosim), 1, var)),</span><br><span class="line">    n = factor(rep(c(&quot;10&quot;, &quot;20&quot;, &quot;30&quot;), c(nosim, nosim, nosim))) )</span><br><span class="line"># plot density function for different sample size data</span><br><span class="line">ggplot(dat, aes(x = x, fill = n)) + geom_density(size = 1, alpha = .2) +</span><br><span class="line">	geom_vline(xintercept = 1, size = 1)</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: for any variable, properties of the population = <strong>parameter</strong>, estimates of properties for samples = <strong>statistic</strong> </em>
<ul>
<li>below is a summary for the mean and variance for population and sample</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/8.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>distribution for mean of random samples</strong>
<ul>
<li>expected value of the <strong>mean</strong> of distribution of means = expected value of the sample mean = population mean
<ul>
<li><span class="math inline">\(E[\bar X]=\mu\)</span></li>
</ul></li>
<li>expected value of the variance of distribution of means
<ul>
<li><span class="math inline">\(Var(\bar X) = \sigma^2/n\)</span></li>
<li>as <strong>n</strong> becomes larger, the mean of random sample <span class="math inline">\(\rightarrow\)</span> more concentrated around the population mean <span class="math inline">\(\rightarrow\)</span> variance approaches 0
<ul>
<li>this again confirms that sample mean estimates population mean</li>
</ul></li>
</ul></li>
<li><em><strong>Note</strong>: normally we only have 1 sample mean (from collected sample) and can estimate the variance <span class="math inline">\(\sigma^2\)</span> <span class="math inline">\(\rightarrow\)</span> so we know a lot about the <strong>distribution of the means</strong> from the data observed </em></li>
</ul></li>
<li><strong>standard error (SE)</strong>
<ul>
<li>the standard error of the mean is defined as <span class="math display">\[SE_{mean} = \sigma/\sqrt{n}\]</span></li>
<li>this quantity is effectively the standard deviation of the distribution of a statistic (i.e. mean)</li>
<li>represents variability of means</li>
</ul></li>
</ul>
<h3 id="entire-estimator-estimation-relationship">Entire Estimator-Estimation Relationship</h3>
<ul>
<li>Start with a sample</li>
<li><span class="math inline">\(S^2\)</span> = sample variance
<ul>
<li>estimates how variable the population is</li>
<li>estimates population variance <span class="math inline">\(\sigma^2\)</span></li>
<li><span class="math inline">\(S^2\)</span> = a random variable and has its own distribution centered around <span class="math inline">\(\sigma^2\)</span>
<ul>
<li>more concentrated around <span class="math inline">\(\sigma^2\)</span> as <span class="math inline">\(n\)</span> increases</li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\bar X\)</span> = sample mean
<ul>
<li>estimates population mean <span class="math inline">\(\mu\)</span></li>
<li><span class="math inline">\(\bar X\)</span> = a random variable and has its own distribution centered around <span class="math inline">\(\mu\)</span>
<ul>
<li>more concentrated around <span class="math inline">\(\mu\)</span> as <span class="math inline">\(n\)</span> increases</li>
<li>variance of distribution of <span class="math inline">\(\bar X = \sigma^2/n\)</span></li>
<li>estimate of variance = <span class="math inline">\(S^2/n\)</span></li>
<li>estimate of standard error = <span class="math inline">\(S/\sqrt{n}\)</span> <span class="math inline">\(\rightarrow\)</span> “sample standard error of the mean”
<ul>
<li>estimates how variable sample means (<span class="math inline">\(n\)</span> size) from the population are</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="example---standard-normal">Example - Standard Normal</h3>
<ul>
<li>variance = 1</li>
<li>means of <strong>n</strong> standard normals (sample) have standard deviation = <span class="math inline">\(1/\sqrt{n}\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># specify number of simulations with 10 as number of observations per sample</span><br><span class="line">nosim &lt;- 1000; n &lt;-10</span><br><span class="line"># estimated standard deviation of mean</span><br><span class="line">sd(apply(matrix(rnorm(nosim * n), nosim), 1, mean))</span><br><span class="line"># actual standard deviation of mean of standard normals</span><br><span class="line">1 / sqrt(n)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>rnorm()</code> = generate samples from the standard normal</li>
<li><code>matrix()</code> = puts all samples into a nosim by <span class="math inline">\(n\)</span> matrix, so that each row represents a simulation with <code>nosim</code> observations</li>
<li><code>apply()</code> = calculates the mean of the <span class="math inline">\(n\)</span> samples</li>
<li><code>sd()</code> = returns standard deviation</li>
</ul>
<h3 id="example---standard-uniform">Example - Standard Uniform</h3>
<ul>
<li>standard uniform <span class="math inline">\(\rightarrow\)</span> triangle straight line distribution <span class="math inline">\(\rightarrow\)</span> mean = 1/2 and variance = 1/12</li>
<li>means of random samples of <span class="math inline">\(n\)</span> uniforms have have standard deviation of <span class="math inline">\(1/\sqrt{12 \times n}\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># estimated standard deviation of the sample means</span><br><span class="line">sd(apply(matrix(runif(nosim * n), nosim), 1, mean))</span><br><span class="line"># actual standard deviation of the means</span><br><span class="line">1/sqrt(12*n)</span><br></pre></td></tr></table></figure>
<h3 id="example---poisson">Example - Poisson</h3>
<ul>
<li><span class="math inline">\(Poisson(x^2)\)</span> have variance of <span class="math inline">\(x^2\)</span></li>
<li>means of random samples of <span class="math inline">\(n~ Poisson(4)\)</span> have standard deviation of <span class="math inline">\(2/\sqrt{n}\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># estimated standard deviation of the sample means</span><br><span class="line">sd(apply(matrix(rpois(nosim * n, lambda=4), nosim), 1, mean))</span><br><span class="line"># actual standard deviation of the means</span><br><span class="line">2/sqrt(n)</span><br></pre></td></tr></table></figure>
<h3 id="example---bernoulli">Example - Bernoulli</h3>
<ul>
<li>for <span class="math inline">\(p = 0.5\)</span>, the Bernoulli distribution has variance of 0.25</li>
<li>means of random samples of <span class="math inline">\(n\)</span> coin flips have standard deviations of <span class="math inline">\(1 / (2 \sqrt{n})\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># estimated standard deviation of the sample means</span><br><span class="line">sd(apply(matrix(sample(0 : 1, nosim * n, replace = TRUE), nosim), 1, mean))</span><br><span class="line"># actual standard deviation of the means</span><br><span class="line">1/(2*sqrt(n))</span><br></pre></td></tr></table></figure>
<h3 id="example---fatherson">Example - Father/Son</h3>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">library(UsingR); data(father.son);</span><br><span class="line"># define son height as the x variable</span><br><span class="line">x &lt;- father.son$sheight</span><br><span class="line"># n is the length</span><br><span class="line">n&lt;-length(x)</span><br><span class="line"># plot histogram for son&apos;s heights</span><br><span class="line">g &lt;- ggplot(data = father.son, aes(x = sheight))</span><br><span class="line">g &lt;- g + geom_histogram(aes(y = ..density..), fill = &quot;lightblue&quot;, binwidth=1, colour = &quot;black&quot;)</span><br><span class="line">g &lt;- g + geom_density(size = 2, colour = &quot;black&quot;)</span><br><span class="line">g</span><br><span class="line"># we calculate the parameters for variance of distribution and sample mean,</span><br><span class="line">round(c(sampleVar = var(x),</span><br><span class="line">	sampleMeanVar = var(x) / n,</span><br><span class="line">	# as well as standard deviation of distribution and sample mean</span><br><span class="line">	sampleSd = sd(x),</span><br><span class="line">	sampleMeanSd = sd(x) / sqrt(n)),2)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="binomial-distribution">Binomial Distribution</h2>
<ul>
<li><strong>binomial random variable</strong> = sum of <strong>n</strong> Bernoulli variables <span class="math display">\[X = \sum_{i=1}^n X_i\]</span> where <span class="math inline">\(X_1,\ldots,X_n = Bernoulli(p)\)</span>
<ul>
<li>PMF is defined as <span class="math display">\[P(X=x) = {n \choose x}p^x(1-p)^{n-x}\]</span> where <span class="math inline">\({n \choose x}\)</span> = number of ways selecting <span class="math inline">\(x\)</span> items out of <span class="math inline">\(n\)</span> options without replacement or regard to order and for <span class="math inline">\(x=0,\ldots,n\)</span></li>
<li><strong>combination</strong> or “<span class="math inline">\(n\)</span> choose <span class="math inline">\(x\)</span>” is defined as <span class="math display">\[{n \choose x} = \frac{n!}{x!(n-x)!}\]</span></li>
<li>the base cases are <span class="math display">\[{n \choose n} = {n \choose 0} = 1\]</span></li>
</ul></li>
<li><strong>Bernoulli distribution</strong> = binary outcome
<ul>
<li>only possible outcomes
<ul>
<li>1 = “success” with probability of <span class="math inline">\(p\)</span></li>
<li>0 = “failure” with probability of <span class="math inline">\(1 - p\)</span></li>
</ul></li>
<li>PMF is defined as <span class="math display">\[P(X=x) = p^x(1 - p)^{1-x}\]</span></li>
<li>mean = <span class="math inline">\(p\)</span></li>
<li>variance = <span class="math inline">\(p(1 - p)\)</span></li>
</ul></li>
</ul>
<h3 id="example-1">Example</h3>
<ul>
<li>of 8 children, whats the probability of 7 or more girls (50/50 chance)? <span class="math display">\[{8 \choose 7}.5^7(1-.5)^{1} + {8 \choose 8}.5^8(1-.5)^{0} \approx 0.04\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># calculate probability using PMF</span><br><span class="line">choose(8, 7) * .5 ^ 8 + choose(8, 8) * .5 ^ 8</span><br><span class="line"># calculate probability using CMF from distribution</span><br><span class="line">pbinom(6, size = 8, prob = .5, lower.tail = FALSE)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>choose(8, 7)</code> = R function to calculate n choose x</li>
<li><code>pbinom(6, size=8, prob =0.5, lower.tail=TRUE)</code> = probability of 6 or less successes out of 8 samples with probability of 0.5 (CMF)
<ul>
<li><code>lower.tail=FALSE</code> = returns the complement, in this case it’s the probability of greater than 6 successes out of 8 samples with probability of 0.5</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="normal-distribution">Normal Distribution</h2>
<ul>
<li>normal/Gaussian distribution for random variable X
<ul>
<li>notation = <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span></li>
<li>mean = <span class="math inline">\(E[X] = \mu\)</span></li>
<li>variance = <span class="math inline">\(Var(X) = \sigma^2\)</span></li>
<li>PMF is defined as <span class="math display">\[f(x)=(2\pi\sigma^2)^{-1/2}e^{-(x-\mu)^2/2\sigma^2}\]</span></li>
</ul></li>
<li><span class="math inline">\(X \sim N(0, 1)\)</span> = <strong>standard normal distribution</strong> (standard normal random variables often denoted using <span class="math inline">\(Z_1, Z_2, \ldots\)</span>)
<ul>
<li><em><strong>Note</strong>: see below graph for reference for the following observations </em></li>
<li>~68% of data/normal density <span class="math inline">\(\rightarrow\)</span> between <span class="math inline">\(\pm\)</span> 1 standard deviation from <span class="math inline">\(\mu\)</span></li>
<li>~95% of data/normal density <span class="math inline">\(\rightarrow\)</span> between <span class="math inline">\(\pm\)</span> 2 standard deviation from <span class="math inline">\(\mu\)</span></li>
<li>~99% of data/normal density <span class="math inline">\(\rightarrow\)</span> between <span class="math inline">\(\pm\)</span> 3 standard deviation from <span class="math inline">\(\mu\)</span></li>
<li><span class="math inline">\(\pm\)</span> 1.28 standard deviations from <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> 10<span class="math inline">\(^{th}\)</span> (-) and 90<span class="math inline">\(^{th}\)</span> (+) percentiles</li>
<li><span class="math inline">\(\pm\)</span> 1.645 standard deviations from <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> 5<span class="math inline">\(^{th}\)</span> (-) and 95<span class="math inline">\(^{th}\)</span> (+) percentiles</li>
<li><span class="math inline">\(\pm\)</span> 1.96 standard deviations from <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> 2.5<span class="math inline">\(^{th}\)</span> (-) and 97.5<span class="math inline">\(^{th}\)</span> (+) percentiles</li>
<li><span class="math inline">\(\pm\)</span> 2.33 standard deviations from <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> 1<span class="math inline">\(^{st}\)</span> (-) and 99<span class="math inline">\(^{th}\)</span> (+) percentiles</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># plot standard normal</span><br><span class="line">x &lt;- seq(-3, 3, length = 1000)</span><br><span class="line">g &lt;- ggplot(data.frame(x = x, y = dnorm(x)),</span><br><span class="line">            aes(x = x, y = y)) + geom_line(size = 2)</span><br><span class="line">g &lt;- g + geom_vline(xintercept = -3 : 3, size = 2)</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<ul>
<li>for any <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, calculating the number of standard deviations each observation is from the mean <strong><em>converts</em></strong> the random variable to a <strong><em>standard normal</em></strong> (denoted as <span class="math inline">\(Z\)</span> below) <span class="math display">\[Z=\frac{X-\mu}{\sigma} \sim N(0,1)\]</span></li>
<li>conversely, a standard normal can then be converted to <strong><em>any normal distribution</em></strong> by multiplying by standard deviation and adding the mean <span class="math display">\[X = \mu + \sigma Z \sim N(\mu, \sigma^2)\]</span></li>
<li><code>qnorm(n, mean=mu, sd=sd)</code> = returns the <span class="math inline">\(n^{th}\)</span> percentiles for the given normal distribution</li>
<li><code>pnorm(x, mean=mu, sd=sd, lower.tail=F)</code> = returns the probability of an observation drawn from the given distribution is larger in value than the specified threshold <span class="math inline">\(x\)</span></li>
</ul>
<h3 id="example-2">Example</h3>
<ul>
<li>the number of daily ad clicks for a company is (approximately) normally distributed with a mean of 1020 and a standard deviation of 50</li>
<li>What’s the probability of getting more than 1,160 clicks in a day?</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># calculate number of standard deviations from the mean</span><br><span class="line">(1160 - 1020) / 50</span><br><span class="line"># calculate probability using given distribution</span><br><span class="line">pnorm(1160, mean = 1020, sd = 50, lower.tail = FALSE)</span><br><span class="line"># calculate probability using standard normal</span><br><span class="line">pnorm(2.8, lower.tail = FALSE)</span><br></pre></td></tr></table></figure>
<ul>
<li>therefore, it is not very likely (<code>r pnorm(2.8, lower.tail = FALSE)*100</code>% chance), since 1,160 is <code>r (1160 - 1020) / 50</code> standard deviations from the mean</li>
<li>What number of daily ad clicks would represent the one where 75% of days have fewer clicks (assuming days are independent and identically distributed)?</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qnorm(0.75, mean = 1020, sd = 50)</span><br></pre></td></tr></table></figure>
<ul>
<li>therefore, <code>r qnorm(0.75, mean = 1020, sd = 50)</code> would represent the threshold that has more clicks than 75% of days</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="poisson-distribution">Poisson Distribution</h2>
<ul>
<li>used to model counts
<ul>
<li>mean = <span class="math inline">\(\lambda\)</span></li>
<li>variance = <span class="math inline">\(\lambda\)</span></li>
<li>PMF is defined as <span class="math display">\[P(X = x; \lambda)=\frac{\lambda^xe^{-\lambda}}{x!}\]</span> where <span class="math inline">\(X = 0, 1, 2, ... \infty\)</span></li>
</ul></li>
<li>modeling uses for Poisson distribution
<ul>
<li>count data</li>
<li>event-time/survival <span class="math inline">\(\rightarrow\)</span> cancer trials, some patients never develop and some do, dealing with the data for both (“censoring”)</li>
<li>contingency tables <span class="math inline">\(\rightarrow\)</span> record results for different characteristic measurements</li>
<li>approximating binomials <span class="math inline">\(\rightarrow\)</span> instances where <strong>n</strong> is large and <strong>p</strong> is small (i.e. pollution on lung disease)
<ul>
<li><span class="math inline">\(X \sim Binomial(n, p)\)</span></li>
<li><span class="math inline">\(\lambda = np\)</span></li>
</ul></li>
<li>rates <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(X \sim Poisson(\lambda t)\)</span>
<ul>
<li><span class="math inline">\(\lambda = E[X/t]\)</span> <span class="math inline">\(\rightarrow\)</span> expected count per unit of time</li>
<li><span class="math inline">\(t\)</span> = total monitoring time</li>
</ul></li>
</ul></li>
<li><code>ppois(n, lambda = lambda*t)</code> = returns probability of <span class="math inline">\(n\)</span> or fewer events happening given the rate <span class="math inline">\(\lambda\)</span> and time <span class="math inline">\(t\)</span></li>
</ul>
<h3 id="example-3">Example</h3>
<ul>
<li>number of people that show up at a bus stop can be modeled with Poisson distribution with a mean of 2.5 per hour</li>
<li>after watching the bus stop for 4 hours, what is the probability that 3 or fewer people show up for the whole time?</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># calculate using distribution</span><br><span class="line">ppois(3, lambda = 2.5 * 4)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, there is a <code>r ppois(3, lambda = 2.5 * 4)*100</code>% chance for 3 or fewer people show up total at the bus stop during 4 hours of monitoring</li>
</ul>
<h3 id="example---approximating-binomial-distribution">Example - Approximating Binomial Distribution</h3>
<ul>
<li>flip a coin with success probability of 0.01 a total 500 times (low <span class="math inline">\(p\)</span>, large <span class="math inline">\(n\)</span>)</li>
<li>what’s the probability of 2 or fewer successes?</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># calculate correct probability from Binomial distribution</span><br><span class="line">pbinom(2, size = 500, prob = .01)</span><br><span class="line"># estimate probability using Poisson distribution</span><br><span class="line">ppois(2, lambda=500 * .01)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the two probabilities (<code>r pbinom(2, size = 500, prob = .01)*100</code>% vs <code>r pbinom(2, size = 500, prob = .01)*100</code>%) are extremely close</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="asymptotics">Asymptotics</h2>
<ul>
<li><strong>asymptotics</strong> = behavior of statistics as sample size <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\infty\)</span></li>
<li>useful for simple statistical inference/approximations</li>
<li>form basis for frequentist interpretation of probabilities (“Law of Large Numbers”)</li>
</ul>
<h3 id="law-of-large-numbers-lln">Law of Large Numbers (LLN)</h3>
<ul>
<li>IID sample statistic that estimates property of the sample (i.e. mean, variance) <strong><em>becomes</em></strong> the population statistic (i.e. population mean, population variance) as <span class="math inline">\(n\)</span> increases</li>
<li><em><strong>Note</strong>: an estimator is <strong>consistent</strong> if it converges to what it is estimating </em></li>
<li>sample mean/variance/standard deviation are all <strong><em>consistent estimators</em></strong> for their population counterparts
<ul>
<li><span class="math inline">\(\bar X_n\)</span> is average of the result of <span class="math inline">\(n\)</span> coin flips (i.e. the sample proportion of heads)</li>
<li>as we flip a fair coin over and over, it <strong><em>eventually converges</em></strong> to the true probability of a head</li>
</ul></li>
</ul>
<h3 id="example---lln-for-normal-and-bernoulli-distribution">Example - LLN for Normal and Bernoulli Distribution</h3>
<ul>
<li>for this example, we will simulate 10000 samples from the normal and Bernoulli distributions respectively</li>
<li>we will plot the distribution of sample means as <span class="math inline">\(n\)</span> increases and compare it to the population means</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># load library</span><br><span class="line">library(gridExtra)</span><br><span class="line"># specify number of trials</span><br><span class="line">n &lt;- 10000</span><br><span class="line"># calculate sample (from normal distribution) means for different size of n</span><br><span class="line">means &lt;- cumsum(rnorm(n)) / (1  : n)</span><br><span class="line"># plot sample size vs sample mean</span><br><span class="line">g &lt;- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y))</span><br><span class="line">g &lt;- g + geom_hline(yintercept = 0) + geom_line(size = 2)</span><br><span class="line">g &lt;- g + labs(x = &quot;Number of obs&quot;, y = &quot;Cumulative mean&quot;)</span><br><span class="line">g &lt;- g + ggtitle(&quot;Normal Distribution&quot;)</span><br><span class="line"># calculate sample (coin flips) means for different size of n</span><br><span class="line">means &lt;- cumsum(sample(0 : 1, n , replace = TRUE)) / (1  : n)</span><br><span class="line"># plot sample size vs sample mean</span><br><span class="line">p &lt;- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y))</span><br><span class="line">p &lt;- p + geom_hline(yintercept = 0.5) + geom_line(size = 2)</span><br><span class="line">p &lt;- p + labs(x = &quot;Number of obs&quot;, y = &quot;Cumulative mean&quot;)</span><br><span class="line">p &lt;- p + ggtitle(&quot;Bernoulli Distribution (Coin Flip)&quot;)</span><br><span class="line"># combine plots</span><br><span class="line">grid.arrange(g, p, ncol = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, for both distributions the sample means undeniably approach the respective population means as <span class="math inline">\(n\)</span> increases</li>
</ul>
<h3 id="central-limit-theorem">Central Limit Theorem</h3>
<ul>
<li>one of the most important theorems in statistics</li>
<li>distribution of means of IID variables approaches the standard normal as sample size <span class="math inline">\(n\)</span> increases</li>
<li>in other words, for large values of <span class="math inline">\(n\)</span>, <span class="math display">\[\frac{\mbox{Estimate} - \mbox{Mean of Estimate}}{\mbox{Std. Err. of Estimate}} = \frac{\bar X_n - \mu}{\sigma / \sqrt{n}}=\frac{\sqrt n (\bar X_n - \mu)}{\sigma} \longrightarrow N(0, 1)\]</span></li>
<li>this translates to the distribution of the sample mean <span class="math inline">\(\bar X_n\)</span> is approximately <span class="math inline">\(N(\mu, \sigma^2/n)\)</span>
<ul>
<li>distribution is centered at the population mean</li>
<li>with standard deviation = standard error of the mean</li>
</ul></li>
<li>typically the Central Limit Theorem can be applied when <span class="math inline">\(n \geq 30\)</span></li>
</ul>
<h3 id="example---clt-with-bernoulli-trials-coin-flips">Example - CLT with Bernoulli Trials (Coin Flips)</h3>
<ul>
<li>for this example, we will simulate <span class="math inline">\(n\)</span> flips of a possibly unfair coin
<ul>
<li>let <span class="math inline">\(X_i\)</span> be the 0 or 1 result of the <span class="math inline">\(i^{th}\)</span> flip of a possibly unfair coin</li>
<li>sample proportion , <span class="math inline">\(\hat p\)</span>, is the average of the coin flips</li>
<li><span class="math inline">\(E[X_i] = p\)</span> and <span class="math inline">\(Var(X_i) = p(1-p)\)</span></li>
<li>standard error of the mean is <span class="math inline">\(SE = \sqrt{p(1-p)/n}\)</span></li>
</ul></li>
<li>in principle, normalizing the random variable <span class="math inline">\(X_i\)</span>, we should get an approximately standard normal distribution <span class="math display">\[\frac{\hat p - p}{\sqrt{p(1-p)/n}} \sim N(0,~1)\]</span></li>
<li>therefore, we will flip a coin <span class="math inline">\(n\)</span> times, take the sample proportion of heads (successes with probability <span class="math inline">\(p\)</span>), subtract off 0.5 (ideal sample proportion) and multiply the result by <span class="math inline">\(\frac{1}{2 \sqrt{n}}\)</span> and compare it to the standard normal</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># specify number of simulations</span><br><span class="line">nosim &lt;- 1000</span><br><span class="line"># convert to standard normal</span><br><span class="line">cfunc &lt;- function(x, n) 2 * sqrt(n) * (mean(x) - 0.5)</span><br><span class="line"># simulate data for sample sizes 10, 20, and 30</span><br><span class="line">dat &lt;- data.frame(</span><br><span class="line">	x = c(apply(matrix(sample(0:1, nosim*10, replace=TRUE), nosim), 1, cfunc, 10),</span><br><span class="line">        apply(matrix(sample(0:1, nosim*20, replace=TRUE), nosim), 1, cfunc, 20),</span><br><span class="line">        apply(matrix(sample(0:1, nosim*30, replace=TRUE), nosim), 1, cfunc, 30)),</span><br><span class="line">	size = factor(rep(c(10, 20, 30), rep(nosim, 3))))</span><br><span class="line"># plot histograms for the trials</span><br><span class="line">g &lt;- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3,</span><br><span class="line">	colour = &quot;black&quot;, aes(y = ..density..))</span><br><span class="line"># plot standard normal distribution for reference</span><br><span class="line">g &lt;- g + stat_function(fun = dnorm, size = 2)</span><br><span class="line"># plot panel plots by sample size</span><br><span class="line">g + facet_grid(. ~ size)</span><br></pre></td></tr></table></figure>
<ul>
<li>now, we can run the same simulation trials for an extremely unfair coin with <span class="math inline">\(p\)</span> = 0.9</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># specify number of simulations</span><br><span class="line">nosim &lt;- 1000</span><br><span class="line"># convert to standard normal</span><br><span class="line">cfunc &lt;- function(x, n) sqrt(n) * (mean(x) - 0.9) / sqrt(.1 * .9)</span><br><span class="line"># simulate data for sample sizes 10, 20, and 30</span><br><span class="line">dat &lt;- data.frame(</span><br><span class="line">	x = c(apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 10, replace = TRUE),</span><br><span class="line">                     nosim), 1, cfunc, 10),</span><br><span class="line">        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 20, replace = TRUE),</span><br><span class="line">                     nosim), 1, cfunc, 20),</span><br><span class="line">        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 30, replace = TRUE),</span><br><span class="line">                     nosim), 1, cfunc, 30)),</span><br><span class="line">	size = factor(rep(c(10, 20, 30), rep(nosim, 3))))</span><br><span class="line"># plot histograms for the trials</span><br><span class="line">g &lt;- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3,</span><br><span class="line">	colour = &quot;black&quot;, aes(y = ..density..))</span><br><span class="line"># plot standard normal distribution for reference</span><br><span class="line">g &lt;- g + stat_function(fun = dnorm, size = 2)</span><br><span class="line"># plot panel plots by sample size</span><br><span class="line">g + facet_grid(. ~ size)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from both simulations, the converted/standardized distribution of the samples convert to the standard normal distribution</li>
<li><em><strong>Note</strong>: speed at which the normalized coin flips converge to normal distribution depends on how biased the coin is (value of <span class="math inline">\(p\)</span>) </em></li>
<li><em><strong>Note</strong>: does not guarantee that the normal distribution will be a good approximation, but just that eventually it will be a good approximation as n <span class="math inline">\(\rightarrow \infty\)</span> </em></li>
</ul>
<h3 id="confidence-intervals---normal-distributionz-intervals">Confidence Intervals - Normal Distribution/Z Intervals</h3>
<ul>
<li><strong>Z confidence interval</strong> is defined as <span class="math display">\[Estimate \pm ZQ \times SE_{Estimate}\]</span> where <span class="math inline">\(ZQ\)</span> = quantile from the standard normal distribution</li>
<li>according to CLT, the sample mean, <span class="math inline">\(\bar X\)</span>, is approximately normal with mean <span class="math inline">\(\mu\)</span> and sd <span class="math inline">\(\sigma / \sqrt{n}\)</span></li>
<li><strong>95% confidence interval for the population mean <span class="math inline">\(\mu\)</span></strong> is defined as <span class="math display">\[\bar X \pm 2\sigma/\sqrt{n}\]</span> for the sample mean <span class="math inline">\(\bar X \sim N(\mu, \sigma^2/n)\)</span>
<ul>
<li>you can choose to use 1.96 to be more accurate for the confidence interval</li>
<li><span class="math inline">\(P(\bar{X} &gt; \mu + 2\sigma/\sqrt{n}~or~\bar{X} &lt; \mu - 2\sigma/\sqrt{n}) = 5\%\)</span></li>
<li><strong>interpretation</strong>: if we were to repeatedly draw samples of size <span class="math inline">\(n\)</span> from the population and construct this confidence interval for each case, approximately 95% of the intervals will contain <span class="math inline">\(\mu\)</span></li>
</ul></li>
<li>confidence intervals get <strong>narrower</strong> with less variability or larger sample sizes</li>
<li><em><strong>Note</strong>: Poisson and binomial distributions have exact intervals that don’t require CLT </em></li>
<li><strong><em>example</em></strong>
<ul>
<li>for this example, we will compute the 95% confidence interval for sons height data in inches</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># load son height data</span><br><span class="line">data(father.son); x &lt;- father.son$sheight</span><br><span class="line"># calculate confidence interval for sons height in inches</span><br><span class="line">mean(x) + c(-1, 1) * qnorm(0.975) * sd(x)/sqrt(length(x))</span><br></pre></td></tr></table></figure>
<h3 id="confidence-interval---bernoulli-distributionwald-interval">Confidence Interval - Bernoulli Distribution/Wald Interval</h3>
<ul>
<li>for Bernoulli distributions, <span class="math inline">\(X_i\)</span> is 0 or 1 with success probability <span class="math inline">\(p\)</span> and the variance is <span class="math inline">\(\sigma^2 = p(1 - p)\)</span></li>
<li>the confidence interval takes the form of <span class="math display">\[\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}\]</span></li>
<li>since the population proportion <span class="math inline">\(p\)</span> is unknown, we can use the sampled proportion of success <span class="math inline">\(\hat{p} = X/n\)</span> as estimate</li>
<li><span class="math inline">\(p(1-p)\)</span> is largest when <span class="math inline">\(p = 1/2\)</span>, so 95% confidence interval can be calculated by <span class="math display">\[\begin{aligned}
\hat{p} \pm Z_{0.95} \sqrt{\frac{0.5(1-0.5)}{n}} &amp; = \hat{p} \pm qnorm(.975) \sqrt{\frac{1}{4n}}\\
&amp; = \hat{p} \pm 1.96 \sqrt{\frac{1}{4n}}\\
&amp; = \hat{p} \pm \frac{1.96}{2} \sqrt{\frac{1}{n}}\\
&amp; \approx \hat{p} \pm \frac{1}{\sqrt{n}}\\
\end{aligned}\]</span>
<ul>
<li>this is known as the <strong>Wald Confidence Interval</strong> and is useful in <strong><em>roughly estimating</em></strong> confidence intervals</li>
<li>generally need <span class="math inline">\(n\)</span> = 100 for 1 decimal place, 10,000 for 2, and 1,000,000 for 3</li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>suppose a random sample of 100 likely voters, 56 intent to vote for you, can you secure a victory?</li>
<li>we can use the Wald interval to quickly estimate the 95% confidence interval</li>
<li>as we can see below, because the interval [<code>r 0.56 + c(-1, 1) * 1/sqrt(100)</code>] contains values below 50%, victory is not guaranteed</li>
<li><code>binom.test(k, n)$conf</code> = returns confidence interval binomial distribution (collection of Bernoulli trial) with <code>k</code> successes in <code>n</code> draws</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># define sample probability and size</span><br><span class="line">p = 0.56; n = 100</span><br><span class="line"># Wald interval</span><br><span class="line">c(&quot;WaldInterval&quot; = p + c(-1, 1) * 1/sqrt(n))</span><br><span class="line"># 95% confidence interval</span><br><span class="line">c(&quot;95CI&quot; = p + c(-1, 1) * qnorm(.975) * sqrt(p * (1-p)/n))</span><br><span class="line"># perform binomial test</span><br><span class="line">binom.test(p*100, n*100)$conf.int</span><br></pre></td></tr></table></figure>
<h3 id="confidence-interval---binomial-distributionagresti-coull-interval">Confidence Interval - Binomial Distribution/Agresti-Coull Interval</h3>
<ul>
<li>for a binomial distribution with smaller values of <span class="math inline">\(n\)</span> (when <span class="math inline">\(n\)</span> &lt; 30, thus not large enough for CLT), often time the normal confidence intervals, as defined by <span class="math display">\[\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}\]</span> <strong>do not</strong> provide accurate estimates</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># simulate 1000 samples of size 20 each</span><br><span class="line">n &lt;- 20; nosim &lt;- 1000</span><br><span class="line"># simulate for p values from 0.1 to 0.9</span><br><span class="line">pvals &lt;- seq(.1, .9, by = .05)</span><br><span class="line"># calculate the confidence intervals</span><br><span class="line">coverage &lt;- sapply(pvals, function(p)&#123;</span><br><span class="line">	# simulate binomial data</span><br><span class="line">	phats &lt;- rbinom(nosim, prob = p, size = n) / n</span><br><span class="line">	# calculate lower 95% CI bound</span><br><span class="line">	ll &lt;- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)</span><br><span class="line">	# calculate upper 95% CI bound</span><br><span class="line">	ul &lt;- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)</span><br><span class="line">	# calculate percent of intervals that contain p</span><br><span class="line">	mean(ll &lt; p &amp; ul &gt; p)</span><br><span class="line">&#125;)</span><br><span class="line"># plot CI results vs 95%</span><br><span class="line">ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the interval do not provide adequate coverage as 95% confidence intervals (frequently only provide 80 to 90% coverage)</li>
<li>we can construct the <strong>Agresti-Coull Interval</strong>, which is defined uses the adjustment <span class="math display">\[\hat{p} = \frac{X+2}{n+4}\]</span> where we effectively <strong><em>add 2</em></strong> to number of successes, <span class="math inline">\(X\)</span>, and <strong><em>add 2</em></strong> to number of failure</li>
<li>therefore the interval becomes <span class="math display">\[\frac{X+2}{n+4} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}\]</span></li>
<li><em><strong>Note</strong>: interval tend to be <strong>conservative</strong> </em></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># simulate 1000 samples of size 20 each</span><br><span class="line">n &lt;- 20; nosim &lt;- 1000</span><br><span class="line"># simulate for p values from 0.1 to 0.9</span><br><span class="line">pvals &lt;- seq(.1, .9, by = .05)</span><br><span class="line"># calculate the confidence intervals</span><br><span class="line">coverage &lt;- sapply(pvals, function(p)&#123;</span><br><span class="line">	# simulate binomial data with Agresti/Coull Interval adjustment</span><br><span class="line">	phats &lt;- (rbinom(nosim, prob = p, size = n) + 2) / (n + 4)</span><br><span class="line">		# calculate lower 95% CI bound</span><br><span class="line">	ll &lt;- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)</span><br><span class="line">	# calculate upper 95% CI bound</span><br><span class="line">	ul &lt;- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)</span><br><span class="line">	# calculate percent of intervals that contain p</span><br><span class="line">	mean(ll &lt; p &amp; ul &gt; p)</span><br><span class="line">&#125;)</span><br><span class="line"># plot CI results vs 95%</span><br><span class="line">ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)</span><br><span class="line">`</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the coverage is much better for the 95% interval</li>
<li>in fact, all of the estimates are more conservative as we previously discussed, indicating the Agresti-Coull intervals are <strong><em>wider</em></strong> than the regular confidence intervals</li>
</ul>
<h3 id="confidence-interval---poisson-interval">Confidence Interval - Poisson Interval</h3>
<ul>
<li>for <span class="math inline">\(X \sim Poisson(\lambda t)\)</span>
<ul>
<li>estimate rate <span class="math inline">\(\hat{\lambda} = X/t\)</span></li>
<li><span class="math inline">\(var(\hat{\lambda}) = \lambda/t\)</span></li>
<li>variance estimate <span class="math inline">\(= \hat{\lambda}/t\)</span></li>
</ul></li>
<li>so the confidence interval is defined as <span class="math display">\[\hat \lambda \pm z_{1-\alpha/2}\sqrt{\frac{\lambda}{t}}\]</span>
<ul>
<li>however, for small values of <span class="math inline">\(\lambda\)</span> (few events larger time interval), we <strong>should not</strong> use the asymptotic interval estimated</li>
<li><strong><em>example</em></strong>
<ul>
<li>for this example, we will go through a specific scenario as well as a simulation exercise to demonstrate the ineffectiveness of asymptotic intervals for small values of <span class="math inline">\(\lambda\)</span></li>
<li>nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate per day?</li>
<li><code>poisson.test(x, T)$conf</code> = returns Poisson 95% confidence interval for given <code>x</code> occurrence over <code>T</code> time period</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># define parameters</span><br><span class="line">x &lt;- 5; t &lt;- 94.32; lambda &lt;- x / t</span><br><span class="line"># calculate confidence interval</span><br><span class="line">round(lambda + c(-1, 1) * qnorm(.975) * sqrt(lambda / t), 3)</span><br><span class="line"># return accurate confidence interval from poisson.test</span><br><span class="line">poisson.test(x, T = 94.32)$conf</span><br><span class="line"># small lambda simulations</span><br><span class="line">lambdavals &lt;- seq(0.005, 0.10, by = .01); nosim &lt;- 1000; t &lt;- 100</span><br><span class="line"># calculate coverage using Poisson intervals</span><br><span class="line">coverage &lt;- sapply(lambdavals, function(lambda)&#123;</span><br><span class="line">	# calculate Poisson rates</span><br><span class="line">	lhats &lt;- rpois(nosim, lambda = lambda * t) / t</span><br><span class="line">	# lower bound of 95% CI</span><br><span class="line">	ll &lt;- lhats - qnorm(.975) * sqrt(lhats / t)</span><br><span class="line">	# upper bound of 95% CI</span><br><span class="line">	ul &lt;- lhats + qnorm(.975) * sqrt(lhats / t)</span><br><span class="line">	# calculate percent of intervals that contain lambda</span><br><span class="line">	mean(ll &lt; lambda &amp; ul &gt; lambda)</span><br><span class="line">&#125;)</span><br><span class="line"># plot CI results vs 95%</span><br><span class="line">ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ylim(0, 1.0)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see above, for small values of <span class="math inline">\(\lambda = X/t\)</span>, the confidence interval produced by the asymptotic interval is <strong><em>not</em></strong> an accurate estimate of the actual 95% interval (not enough coverage)</li>
<li>however, as <span class="math inline">\(t \to \infty\)</span>, the interval becomes the <strong><em>true 95% interval</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># small lambda simulations</span><br><span class="line">lambdavals &lt;- seq(0.005, 0.10, by = .01); nosim &lt;- 1000; t &lt;- 1000</span><br><span class="line"># calculate coverage using Poisson intervals</span><br><span class="line">coverage &lt;- sapply(lambdavals, function(lambda)&#123;</span><br><span class="line">	# calculate Poisson rates</span><br><span class="line">	lhats &lt;- rpois(nosim, lambda = lambda * t) / t</span><br><span class="line">	# lower bound of 95% CI</span><br><span class="line">	ll &lt;- lhats - qnorm(.975) * sqrt(lhats / t)</span><br><span class="line">	# upper bound of 95% CI</span><br><span class="line">	ul &lt;- lhats + qnorm(.975) * sqrt(lhats / t)</span><br><span class="line">	# calculate percent of intervals that contain lambda</span><br><span class="line">	mean(ll &lt; lambda &amp; ul &gt; lambda)</span><br><span class="line">&#125;)</span><br><span class="line"># plot CI results vs 95%</span><br><span class="line">ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(0, 1.0)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, as <span class="math inline">\(t\)</span> increases, the Poisson intervals become closer to the actual 95% confidence intervals</li>
</ul>
<h3 id="confidence-intervals---t-distributionsmall-samples">Confidence Intervals - T Distribution(Small Samples)</h3>
<ul>
<li><strong>t</strong> confidence interval is defined as <span class="math display">\[Estimate \pm TQ \times SE_{Estimate} = \bar X \pm \frac{t_{n-1} S}{\sqrt{n}}\]</span>
<ul>
<li><span class="math inline">\(TQ\)</span> = quantile from T distribution</li>
<li><span class="math inline">\(t_{n-1}\)</span> = relevant quantile</li>
<li><span class="math inline">\(t\)</span> interval assumes data is IID normal so that <span class="math display">\[\frac{\bar X - \mu}{S/\sqrt{n}}\]</span> follows Gosset’s <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom</li>
<li>works well with data distributions that are roughly symmetric/mound shaped, and <strong><em>does not</em></strong> work with skewed distributions
<ul>
<li>skewed distribution <span class="math inline">\(\rightarrow\)</span> meaningless to center interval around the mean <span class="math inline">\(\bar X\)</span></li>
<li>logs/median can be used instead</li>
</ul></li>
<li>paired observations (multiple measurements from same subjects) can be analyzed by t interval of differences</li>
<li>as more data collected (large degrees of freedom), t interval <span class="math inline">\(\rightarrow\)</span> z interval</li>
<li><code>qt(0.975, df=n-1)</code> = calculate the relevant quantile using t distribution</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Plot normal vs t distributions</span><br><span class="line">k &lt;- 1000; xvals &lt;- seq(-5, 5, length = k); df &lt;- 10</span><br><span class="line">d &lt;- data.frame(y = c(dnorm(xvals), dt(xvals, df)),x = xvals,</span><br><span class="line">              dist = factor(rep(c(&quot;Normal&quot;, &quot;T&quot;), c(k,k))))</span><br><span class="line">g &lt;- ggplot(d, aes(x = x, y = y))</span><br><span class="line">g &lt;- g + geom_line(size = 2, aes(colour = dist)) + ggtitle(&quot;Normal vs T Distribution&quot;)</span><br><span class="line"># plot normal vs t quantiles</span><br><span class="line">d &lt;- data.frame(n= qnorm(pvals),t=qt(pvals, df),p = pvals)</span><br><span class="line">h &lt;- ggplot(d, aes(x= n, y = t))</span><br><span class="line">h &lt;- h + geom_abline(size = 2, col = &quot;lightblue&quot;)</span><br><span class="line">h &lt;- h + geom_line(size = 2, col = &quot;black&quot;)</span><br><span class="line">h &lt;- h + geom_vline(xintercept = qnorm(0.975))</span><br><span class="line">h &lt;- h + geom_hline(yintercept = qt(0.975, df)) + ggtitle(&quot;Normal vs T Quantiles&quot;)</span><br><span class="line"># plot 2 graphs together</span><br><span class="line">grid.arrange(g, h, ncol = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>William Gosset’s <strong>t</strong> Distribution (“Student’s T distribution”)
<ul>
<li>test = Gosset’s pseudoname which he published under</li>
<li>indexed/defined by <strong><em>degrees of freedom</em></strong>, and becomes more like standard normal as degrees of freedom gets larger</li>
<li>thicker tails centered around 0, thus confidence interval = <strong><em>wider</em></strong> than Z interval (more mass concentrated away from the center)</li>
<li>for <strong><em>small</em></strong> sample size (value of n), normalizing the distribution by <span class="math inline">\(\frac{\bar X - \mu}{S/\sqrt{n}}\)</span> <span class="math inline">\(\rightarrow\)</span> t distribution, <strong><em>not</em></strong> the standard normal distribution
<ul>
<li><span class="math inline">\(S\)</span> = standard deviation may be inaccurate, as the std of the data sample may not be truly representative of the population std</li>
<li>using the Z interval here thus may produce an interval that is too <strong><em>narrow</em></strong></li>
</ul></li>
</ul></li>
</ul>
<h3 id="confidence-interval---paired-t-tests">Confidence Interval - Paired T Tests</h3>
<ul>
<li>compare observations for the same subjects over two different sets of data (i.e. different times, different treatments)</li>
<li>the confidence interval is defined by <span class="math display">\[ \bar X_1 - \bar X_2 \pm \frac{t_{n-1} S}{\sqrt{n}}\]</span> where <span class="math inline">\(\bar X_1\)</span> represents the first observations and <span class="math inline">\(\bar X_2\)</span> the second set of observations</li>
<li><code>t.test(difference)</code> = performs group mean t test and returns metrics as results, which includes the confidence intervals
<ul>
<li><code>t.test(g2, g1, paired = TRUE)</code> = performs the same paired t test with data directly</li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>the data used here is for a study of the effects of two soporific drugs (increase in hours of sleep compared to control) on 10 patients</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">data(sleep)</span><br><span class="line"># plot the first and second observations</span><br><span class="line">g &lt;- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))</span><br><span class="line">g &lt;- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = &quot;salmon&quot;, alpha = .5)</span><br><span class="line">g</span><br><span class="line"># define groups</span><br><span class="line">g1 &lt;- sleep$extra[1 : 10]; g2 &lt;- sleep$extra[11 : 20]</span><br><span class="line"># define difference</span><br><span class="line">difference &lt;- g2 - g1</span><br><span class="line"># calculate mean and sd of differences</span><br><span class="line">mn &lt;- mean(difference); s &lt;- sd(difference); n &lt;- 10</span><br><span class="line"># calculate intervals manually</span><br><span class="line">mn + c(-1, 1) * qt(.975, n-1) * s / sqrt(n)</span><br><span class="line"># perform the same test to get confidence intervals</span><br><span class="line">t.test(difference)</span><br><span class="line">t.test(g2, g1, paired = TRUE)</span><br></pre></td></tr></table></figure>
<h3 id="independent-group-t-intervals---same-variance">Independent Group <strong>t</strong> Intervals - Same Variance</h3>
<ul>
<li>compare two groups in randomized trial (“A/B Testing”)</li>
<li>cannot use the paired t test because the groups are independent and may have different sample sizes</li>
<li>perform randomization to balance unobserved covariance that may otherwise affect the result</li>
<li><span class="math inline">\(t\)</span> confidence interval for <span class="math inline">\(\mu_y - \mu_x\)</span> is defined as <span class="math display">\[\bar Y - \bar X \pm t_{n_x + n_y - 2, 1 - \alpha/2}S_p\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2}\]</span>
<ul>
<li><span class="math inline">\(t_{n_x + n_y - 2, 1 - \alpha/2}\)</span> = relevant quantile</li>
<li><span class="math inline">\(n_x + n_y - 2\)</span> = degrees of freedom</li>
<li><span class="math inline">\(S_p\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2}\)</span> = standard error</li>
<li><span class="math inline">\(S_p^2 = \{(n_x - 1) S_x^2 + (n_y - 1) S_y^2\}/(n_x + n_y - 2)\)</span> = pooled variance estimator
<ul>
<li>this is effectively a weighted average between the two variances, such that different sample sizes are taken in to account</li>
<li>For equal sample sizes, <span class="math inline">\(n_x = n_y\)</span>, <span class="math inline">\(S_p^2 = \frac{S_x^2 + S_y^2}{2}\)</span> (average of variance of two groups)</li>
</ul></li>
<li><em><strong>Note:</strong> this interval assumes <strong>constant variance</strong> across two groups; if variance is different, use the next interval </em></li>
</ul></li>
</ul>
<h3 id="independent-group-t-intervals---different-variance">Independent Group t Intervals - Different Variance</h3>
<ul>
<li>confidence interval for <span class="math inline">\(\mu_y - \mu_x\)</span> is defined as <span class="math display">\[\bar Y - \bar X \pm t_{df} \times \left(\frac{s_x^2}{n_x} + \frac{s_y^2}{n_y}\right)^{1/2}\]</span>
<ul>
<li><span class="math inline">\(t_{df}\)</span> = relevant quantile with df as defined below</li>
<li><em><strong>Note</strong>: normalized statistic does not follow t distribution but can be approximated through the formula with df defined below </em> <span class="math display">\[df = \frac{\left(S_x^2 / n_x + S_y^2/n_y\right)^2}
  {\left(\frac{S_x^2}{n_x}\right)^2 / (n_x - 1) +
  \left(\frac{S_y^2}{n_y}\right)^2 / (n_y - 1)}\]</span>
<ul>
<li><span class="math inline">\(\left(\frac{s_x^2}{n_x} + \frac{s_y^2}{n_y}\right)^{1/2}\)</span> = standard error</li>
</ul></li>
</ul></li>
<li>Comparing other kinds of data
<ul>
<li>binomial <span class="math inline">\(\rightarrow\)</span> relative risk, risk difference, odds ratio</li>
<li>binomial <span class="math inline">\(\rightarrow\)</span> Chi-squared test, normal approximations, exact tests</li>
<li>count <span class="math inline">\(\rightarrow\)</span> Chi-squared test, exact tests</li>
</ul></li>
<li>R commands
<ul>
<li>t Confidence Intervals
<ul>
<li><code>mean + c(-1, 1) * qt(0.975, n - 1) * std / sqrt(n)</code>
<ul>
<li><strong><em>c(-1, 1)</em></strong> = plus and minus, <span class="math inline">\(\pm\)</span></li>
</ul></li>
</ul></li>
<li>Difference Intervals (all equivalent)
<ul>
<li><code>mean2 - mean1 + c(-1, 1) * qt(0.975, n - 1) * std / sqrt(n)</code>
<ul>
<li><strong><em>n</em></strong> = number of paired observations</li>
<li><strong><em>qt(0.975, n - 1)</em></strong> = relevant quantile for paired</li>
<li><strong><em>qt(0.975, n<span class="math inline">\(_x\)</span> + n<span class="math inline">\(_y\)</span> - 2)</em></strong> = relevant quantile for independent</li>
</ul></li>
<li><code>t.test(mean2 - mean1)</code></li>
<li><code>t.test(data2, data1, paired = TRUE, var.equal = TRUE)</code>
<ul>
<li><strong><em>paired</em></strong> = whether or not the two sets of data are paired (same subjects different observations for treatment) <span class="math inline">\(\rightarrow\)</span> <code>TRUE</code> for paired, <code>FALSE</code> for independent</li>
<li><strong><em>var.equal</em></strong> = whether or not the variance of the datasets should be treated as equal <span class="math inline">\(\rightarrow\)</span> <code>TRUE</code> for same variance, <code>FALSE</code> for unequal variances</li>
</ul></li>
<li><code>t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)</code>
<ul>
<li><strong><em>relevel(factor, ref)</em></strong> = reorders the levels in the factor so that “ref” is changed to the first level <span class="math inline">\(\rightarrow\)</span> doing this here is so that the second set of measurements come first (1, 2 <span class="math inline">\(\rightarrow\)</span> 2, 1) in order to perform mean<span class="math inline">\(_2\)</span> - mean<span class="math inline">\(_1\)</span></li>
<li><strong><em>I(object)</em></strong> = prepend the class “AsIs” to the object</li>
<li><em><strong>Note</strong>: I(relevel(group, 2)) = explanatory variable, must be <strong>factor</strong> and have <strong>two levels</strong> </em></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="hypothesis-testing">Hypothesis Testing</h2>
<ul>
<li>Hypothesis testing = making decisions using data
<ul>
<li><strong>null</strong> hypothesis (<strong>H<span class="math inline">\(_0\)</span></strong>) = status quo</li>
<li>assumed to be <strong><em>true</em></strong> <span class="math inline">\(\rightarrow\)</span> statistical evidence required to reject it for <strong>alternative</strong> or “research” hypothesis (<strong>H<span class="math inline">\(_a\)</span></strong>)
<ul>
<li>alternative hypothesis typically take form of &gt;, &lt; or <span class="math inline">\(\ne\)</span></li>
</ul></li>
<li><strong>Results</strong></li>
</ul></li>
</ul>
<table>
<thead>
<tr class="header">
<th>Truth</th>
<th>Decide</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(H_0\)</span></td>
<td><span class="math inline">\(H_0\)</span></td>
<td>Correctly accept null</td>
</tr>
<tr class="even">
<td><span class="math inline">\(H_0\)</span></td>
<td><span class="math inline">\(H_a\)</span></td>
<td>Type I error</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(H_a\)</span></td>
<td><span class="math inline">\(H_a\)</span></td>
<td>Correctly reject null</td>
</tr>
<tr class="even">
<td><span class="math inline">\(H_a\)</span></td>
<td><span class="math inline">\(H_0\)</span></td>
<td>Type II error</td>
</tr>
</tbody>
</table>
<ul>
<li><strong><span class="math inline">\(\alpha\)</span></strong> = Type I error rate
<ul>
<li>probability of <strong><em>rejecting</em></strong> the null hypothesis when the hypothesis is <strong><em>correct</em></strong></li>
<li><span class="math inline">\(\alpha\)</span> = 0.05 <span class="math inline">\(\rightarrow\)</span> standard for hypothesis testing</li>
<li><em><strong>Note</strong>: as Type I error rate increases, Type II error rate decreases and vice versa </em></li>
</ul></li>
<li>for large samples (large n), use the <strong>Z Test</strong> for <span class="math inline">\(H_0:\mu = \mu_0\)</span>
<ul>
<li><strong><span class="math inline">\(H_a\)</span>:</strong>
<ul>
<li><span class="math inline">\(H_1: \mu &lt; \mu_0\)</span></li>
<li><span class="math inline">\(H_2: \mu \neq \mu_0\)</span></li>
<li><span class="math inline">\(H_3: \mu &gt; \mu_0\)</span></li>
</ul></li>
<li>Test statistic <span class="math inline">\(TS = \frac{\bar{X} - \mu_0}{S / \sqrt{n}}\)</span></li>
<li>Reject the null hypothesis <span class="math inline">\(H_0\)</span> when
<ul>
<li><span class="math inline">\(H_1: TS \leq Z_{\alpha}\)</span> OR <span class="math inline">\(-Z_{1 - \alpha}\)</span></li>
<li><span class="math inline">\(H_2: |TS| \geq Z_{1 - \alpha / 2}\)</span></li>
<li><span class="math inline">\(H_3: TS \geq Z_{1 - \alpha}\)</span></li>
</ul></li>
<li><em><strong>Note</strong>: In case of <span class="math inline">\(\alpha\)</span> = 0.05 (most common), <span class="math inline">\(Z_{1-\alpha}\)</span> = 1.645 (95 percentile) </em></li>
<li><span class="math inline">\(\alpha\)</span> = low, so that when <span class="math inline">\(H_0\)</span> is rejected, original model <span class="math inline">\(\rightarrow\)</span> wrong or made an error (low probability)</li>
</ul></li>
<li>For small samples (small n), use the <strong>T Test</strong> for <span class="math inline">\(H_0:\mu = \mu_0\)</span>
<ul>
<li><strong><span class="math inline">\(H_a\)</span>:</strong>
<ul>
<li><span class="math inline">\(H_1: \mu &lt; \mu_0\)</span></li>
<li><span class="math inline">\(H_2: \mu \neq \mu_0\)</span></li>
<li><span class="math inline">\(H_3: \mu &gt; \mu_0\)</span></li>
</ul></li>
<li>Test statistic <span class="math inline">\(TS = \frac{\bar{X} - \mu_0}{S / \sqrt{n}}\)</span></li>
<li>Reject the null hypothesis <span class="math inline">\(H_0\)</span> when
<ul>
<li><span class="math inline">\(H_1: TS \leq T_{\alpha}\)</span> OR <span class="math inline">\(-T_{1 - \alpha}\)</span></li>
<li><span class="math inline">\(H_2: |TS| \geq T_{1 - \alpha / 2}\)</span></li>
<li><span class="math inline">\(H_3: TS \geq T_{1 - \alpha}\)</span></li>
</ul></li>
<li><em><strong>Note</strong>: In case of <span class="math inline">\(\alpha\)</span> = 0.05 (most common), <span class="math inline">\(T_{1-\alpha}\)</span> = <code>qt(.95, df = n-1)</code> </em></li>
<li>R commands for T test:
<ul>
<li><code>t.test(vector1 - vector2)</code></li>
<li><code>t.test(vector1, vector2, paired = TRUE)</code>
<ul>
<li><code>alternative</code> argument can be used to specify one-sided tests: <code>less</code> or <code>greater</code></li>
<li><code>alternative</code> default = <code>two-sided</code></li>
</ul></li>
<li>prints test statistic (<code>t</code>), degrees of freedom (<code>df</code>), <code>p-value</code>, 95% confidence interval, and mean of sample
<ul>
<li>confidence interval in units of data, and can be used to intepret the practical significance of the results</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>rejection region</strong> = region of TS values for which you reject <span class="math inline">\(H_0\)</span></li>
<li><strong>power</strong> = probability of rejecting <span class="math inline">\(H_0\)</span>
<ul>
<li>power is used to calculate sample size for experiments</li>
</ul></li>
<li><strong>two-sided tests</strong> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(H_a: \mu \neq \mu_0\)</span>
<ul>
<li>reject <span class="math inline">\(H_0\)</span> only if test statistic is too larger/small</li>
<li>for <span class="math inline">\(\alpha\)</span> = 0.05, split equally to 2.5% for upper and 2.5% for lower tails
<ul>
<li>equivalent to <span class="math inline">\(|TS| \geq T_{1 - \alpha / 2}\)</span></li>
<li>example: for T test, <code>qt(.975, df)</code> and <code>qt(.025, df)</code></li>
</ul></li>
<li><em><strong>Note</strong>: failing to reject one-sided test = fail to reject two-sided</em></li>
</ul></li>
<li><strong>tests vs confidence intervals</strong>
<ul>
<li>(<span class="math inline">\(1-\alpha\)</span>)% confidence interval for <span class="math inline">\(\mu\)</span> = set of all possible values that fail to reject <span class="math inline">\(H_0\)</span></li>
<li>if (<span class="math inline">\(1-\alpha\)</span>)% confidence interval contains <span class="math inline">\(\mu_0\)</span>, fail to reject <span class="math inline">\(H_0\)</span></li>
</ul></li>
<li><strong>two-group intervals/test</strong>
<ul>
<li>Rejection rules the same</li>
<li>Test <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_1 = \mu_2\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\mu_1 - \mu_2 = 0\)</span></li>
<li>Test statistic: <span class="math display">\[\frac{Estimate - H_0 Value}{SE_{Estimate}} = \frac{\bar X_1 - \bar X_2 - 0}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}\]</span></li>
<li>R Command
<ul>
<li><code>t.test(values ~ factor, paired = FALSE, var.equal = TRUE, data = data)</code>
<ul>
<li><code>paired = FALSE</code> = independent values</li>
<li><code>factor</code> argument must have only two levels</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>p values</strong>
<ul>
<li>most common measure of statistical significance</li>
<li><strong>p-value</strong> = probability under the null hypothesis of obtaining evidence as extreme or more than that of the obtained
<ul>
<li>Given that <span class="math inline">\(H_0\)</span> is true, how likely is it to obtain the result (test statistic)?</li>
</ul></li>
<li><strong>attained significance level</strong> = smallest value for <span class="math inline">\(\alpha\)</span> for which <span class="math inline">\(H_0\)</span> is rejected <span class="math inline">\(\rightarrow\)</span> equivalent to p-value
<ul>
<li>if p-value &lt; <span class="math inline">\(\alpha\)</span>, reject <span class="math inline">\(H_0\)</span></li>
<li>for two-sided tests, double the p-values</li>
</ul></li>
<li>if p-value is small, either <span class="math inline">\(H_0\)</span> is true AND the obeserved is a rare event <strong>OR</strong> <span class="math inline">\(H_0\)</span> is false</li>
<li>R Command
<ul>
<li>p-value = <code>pt(statistic, df, lower.tail = FALSE)</code>
<ul>
<li><code>lower.tail = FALSE</code> = returns the probability of getting a value from the t distribution that is larger than the test statistic</li>
</ul></li>
<li>Binomial (coin flips)
<ul>
<li>probability of getting x results out of n trials and event probability of p = <code>pbinom(x, size = n, prob = p, lower.tail = FALSE)</code></li>
<li>two-sided interval (testing for <span class="math inline">\(\ne\)</span>): find the smaller of two one-sided intervals (X &lt; value, X &gt; value), and double the result</li>
<li><em><strong>Note</strong>: <code>lower.tail = FALSE</code> = strictly greater </em></li>
</ul></li>
<li>Poisson
<ul>
<li>probability of getting x results given the rate r = <code>ppois(x - 1, r, lower.tail = FALSE)</code></li>
<li><code>x - 1</code> is used here because the upper tail includes the specified number (since we want greater than x, we start at x - 1)</li>
<li><code>r</code> = events that should occur given the rate (multiplied by 100 to yield an integer)</li>
<li><em><strong>Note</strong>: <code>lower.tail = FALSE</code> = strictly greater </em></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="power">Power</h2>
<ul>
<li><strong>Power</strong> = probability of rejecting the null hypothesis when it is false (the more power the better)
<ul>
<li>most often used in designing studies so that there’s a reasonable chance to detect the alternative hypothesis if the alternative hypothesis is true</li>
</ul></li>
<li><span class="math inline">\(\beta\)</span> = probability of type II error = failing to reject the null hypothesis when it’s false</li>
<li>power = <span class="math inline">\(1 - \beta\)</span></li>
<li><strong><em>example</em></strong>
<ul>
<li><span class="math inline">\(H_0: \mu = 30 \to \bar X \sim N(\mu_0, \sigma^2/n)\)</span></li>
<li><span class="math inline">\(H_a: \mu &gt; 30 \to \bar X \sim N(\mu_a, \sigma^2/n)\)</span></li>
<li>Power: <span class="math display">\[Power = P\left(\frac{\bar X - 30}{s /\sqrt{n}} &gt; t_{1-\alpha,n-1} ~;~ \mu = \mu_a \right)\]</span>
<ul>
<li><em><strong>Note</strong>: the above function depends on value of <span class="math inline">\(\mu_a\)</span> </em></li>
<li><em><strong>Note</strong>: as <span class="math inline">\(\mu_a\)</span> approaches 30, power approaches <span class="math inline">\(\alpha\)</span> </em></li>
</ul></li>
<li>assuming the sample mean is normally distributed, <span class="math inline">\(H_0\)</span> is rejected when <span class="math inline">\(\frac{\bar X - 30}{\sigma /\sqrt{n}} &gt; Z_{1-\alpha}\)</span></li>
<li>or, <span class="math inline">\(\bar X &gt; 30 + Z_{1-\alpha} \frac{\sigma}{\sqrt{n}}\)</span></li>
</ul></li>
<li>R commands:
<ul>
<li><code>alpha = 0.05; z = qnorm(1-alpha)</code> <span class="math inline">\(\rightarrow\)</span> calculates <span class="math inline">\(Z_{1-\alpha}\)</span></li>
<li><code>pnorm(mu0 + z * sigma/sqrt(n), mean = mua, sd = sigma/sqrt(n), lower.tail = FALSE)</code> <span class="math inline">\(\rightarrow\)</span> calculates the probability of getting a sample mean that is larger than <span class="math inline">\(Z_{1-\alpha} \frac{\sigma}{\sqrt{n}}\)</span> given that the population mean is <span class="math inline">\(\mu_a\)</span>
<ul>
<li><em><strong>Note</strong>: using <code>mean = mu0</code> in the function would = <span class="math inline">\(\alpha\)</span> </em></li>
</ul></li>
<li>Power curve behavior
<ul>
<li>Power increases as <span class="math inline">\(mu_a\)</span> increases <span class="math inline">\(\rightarrow\)</span> we are more likely to detect the difference in <span class="math inline">\(mu_a\)</span> and <span class="math inline">\(mu_0\)</span></li>
<li>Power increases as <strong>n</strong> increases <span class="math inline">\(\rightarrow\)</span> with more data, more likely to detect any alternative <span class="math inline">\(mu_a\)</span></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.align</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">mu0 = 30; mua = 32; sigma = 4; n = 16</span><br><span class="line">alpha = 0.05</span><br><span class="line">z = qnorm(1 - alpha)</span><br><span class="line">nseq = c(8, 16, 32, 64, 128)</span><br><span class="line">mu_a = seq(30, 35, by = 0.1)</span><br><span class="line">power = sapply(nseq, function(n)</span><br><span class="line">    pnorm(mu0 + z * sigma / sqrt(n), mean = mu_a, sd = sigma / sqrt(n),</span><br><span class="line">          lower.tail = FALSE)</span><br><span class="line">    )</span><br><span class="line">colnames(power) &lt;- paste(&quot;n&quot;, nseq, sep = &quot;&quot;)</span><br><span class="line">d &lt;- data.frame(mu_a, power)</span><br><span class="line">library(reshape2)</span><br><span class="line">d2 &lt;- melt(d, id.vars = &quot;mu_a&quot;)</span><br><span class="line">names(d2) &lt;- c(&quot;mu_a&quot;, &quot;n&quot;, &quot;power&quot;)</span><br><span class="line">g &lt;- ggplot(d2,</span><br><span class="line">            aes(x = mu_a, y = power, col = n)) + geom_line(size = 2)</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Solving for Power</strong>
<ul>
<li>When testing <span class="math inline">\(H_a : \mu &gt; \mu_0\)</span> (or <span class="math inline">\(&lt;\)</span> or <span class="math inline">\(\neq\)</span>) <span class="math display">\[Power = 1 - \beta = P\left(\bar X &gt; \mu_0 + Z_{1-\alpha} \frac{\sigma}{\sqrt{n}} ; \mu = \mu_a \right)\]</span> where <span class="math inline">\(\bar X \sim N(\mu_a, \sigma^2 / n)\)</span></li>
<li>Unknowns = <span class="math inline">\(\mu_a\)</span>, <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(n\)</span>, <span class="math inline">\(\beta\)</span></li>
<li>Knowns = <span class="math inline">\(\mu_0\)</span>, <span class="math inline">\(\alpha\)</span></li>
<li>Specify any 3 of the unknowns and you can solve for the remainder; most common are two cases
<ol type="1">
<li>Given power desired, mean to detect, variance that we can tolerate, find the <strong>n</strong> to produce desired power (designing experiment/trial)</li>
<li>Given the size <strong>n</strong> of the sample, find the power that is achievable (finding the utility of experiment)</li>
</ol></li>
<li><em><strong>Note</strong>: for <span class="math inline">\(H_a: \mu \neq mu_0\)</span>, calculated one-sided power using <span class="math inline">\(z_{1-\alpha / 2}\)</span>; however, the power calculation here exclusdes the probability of getting a large TS in the opposite direction of the truth, but this is only applicable when <span class="math inline">\(\mu_a\)</span> and <span class="math inline">\(\mu_0\)</span> are close together</em></li>
</ul></li>
<li><strong>Power Behavior</strong>
<ul>
<li>Power increases as <span class="math inline">\(\alpha\)</span> becomes larger</li>
<li>Power of one-sided test <span class="math inline">\(&gt;\)</span> power of associated two-sided test</li>
<li>Power increases as <span class="math inline">\(\mu_a\)</span> gets further away from <span class="math inline">\(\mu_0\)</span></li>
<li>Power increases as <strong>n</strong> increases (sample mean has less variability)</li>
<li>Power increases as <span class="math inline">\(\sigma\)</span> decreases (again less variability)</li>
<li>Power usually depends only <span class="math inline">\(\frac{\sqrt{n}(\mu_a - \mu_0)}{\sigma}\)</span>, and not <span class="math inline">\(\mu_a\)</span>, <span class="math inline">\(\sigma\)</span>, and <span class="math inline">\(n\)</span>
<ul>
<li><strong>effect size</strong> = <span class="math inline">\(\frac{\mu_a - \mu_0}{\sigma}\)</span> <span class="math inline">\(\rightarrow\)</span> unit free, can be interpretted across settings</li>
</ul></li>
</ul></li>
<li><strong>T-test Power</strong>
<ul>
<li>for Gossett’s T test, <span class="math display">\[Power = P\left(\frac{\bar X - \mu_0}{S/\sqrt{n}} &gt; t_{1-\alpha, n-1} ; \mu = \mu_a \right)\]</span>
<ul>
<li><span class="math inline">\(\frac{\bar X - \mu_0}{S/\sqrt{n}}\)</span> does not follow a t distribution if the true mean is <span class="math inline">\(\mu_a\)</span> and NOT <span class="math inline">\(\mu_0\)</span> <span class="math inline">\(\rightarrow\)</span> follows a non-central t distribution instead</li>
</ul></li>
<li><code>power.t.test</code> = evaluates the non-central t distribution and solves for a parameter given all others are specified
<ul>
<li><code>power.t.test(n = 16, delta = 0.5, sd = 1, type = &quot;one.sample&quot;, alt = &quot;one.sided&quot;)$power</code> = calculates power with inputs of n, difference in means, and standard deviation
<ul>
<li><code>delta</code> = argument for difference in means</li>
<li><em><strong>Note</strong>: since effect size = <code>delta/sd</code>, as <code>n</code>, <code>type</code>, and <code>alt</code> are held constant, any distribution with the same effect size will have the same power </em></li>
</ul></li>
<li><code>power.t.test(power = 0.8, delta = 0.5, sd = 1, type = &quot;one.sample&quot;, alt = &quot;one.sided&quot;)$n</code> = calculates size n with inputs of power, difference in means, and standard deviation
<ul>
<li><em><strong>Note</strong>: n should always be rounded up (ceiling) </em></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="multiple-testing">Multiple Testing</h2>
<ul>
<li>Hypothesis testing/significant analysis commonly overused</li>
<li>correct for multiple testing to avoid false positives/conclusions (two key components)
<ol type="1">
<li>error measure</li>
<li>correction</li>
</ol></li>
<li>multiple testing is needed because of the increase in ubiquitous data collection technology and analysis
<ul>
<li>DNA sequencing machines</li>
<li>imaging patients in clinical studies</li>
<li>electronic medical records</li>
<li>individualized movement data (fitbit)</li>
</ul></li>
</ul>
<h3 id="type-of-errors">Type of Errors</h3>
<table>
<thead>
<tr class="header">
<th>Actual <span class="math inline">\(H_0\)</span> = True</th>
<th>Actual <span class="math inline">\(H_a\)</span> = True</th>
<th>Total</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Conclude <span class="math inline">\(H_0\)</span> = True (non-significant)</td>
<td><span class="math inline">\(U\)</span></td>
<td><span class="math inline">\(T\)</span></td>
<td><span class="math inline">\(m-R\)</span></td>
</tr>
<tr class="even">
<td>Conclude <span class="math inline">\(H_a\)</span> = True (significant)</td>
<td><span class="math inline">\(V\)</span></td>
<td><span class="math inline">\(S\)</span></td>
<td><span class="math inline">\(R\)</span></td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><span class="math inline">\(m_0\)</span></td>
<td><span class="math inline">\(m-m_0\)</span></td>
<td><span class="math inline">\(m\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li><strong><span class="math inline">\(m_0\)</span></strong> = number of true null hypotheses, or cases where <span class="math inline">\(H_0\)</span> = actually true (unknown)</li>
<li><strong><span class="math inline">\(m - m_0\)</span></strong> = number of true alternative hypotheses, or cases where <span class="math inline">\(H_a\)</span> = actually true (unknown)</li>
<li><strong><span class="math inline">\(R\)</span></strong> = number of null hypotheses rejected, or cases where <span class="math inline">\(H_a\)</span> = concluded to be true (measurable)</li>
<li><strong><span class="math inline">\(m - R\)</span></strong> = number of null hypotheses that failed to be rejected, or cases where <span class="math inline">\(H_0\)</span> = concluded to be true (measurable)</li>
<li><strong><span class="math inline">\(V\)</span></strong> = Type I Error / false positives, concludes <span class="math inline">\(H_a\)</span> = True when <span class="math inline">\(H_0\)</span> = actually True</li>
<li><strong><span class="math inline">\(T\)</span></strong> = Type II Error / false negatives, concludes <span class="math inline">\(H_0\)</span> = True when <span class="math inline">\(H_a\)</span> = actually True</li>
<li><strong><span class="math inline">\(S\)</span></strong> = true positives, concludes <span class="math inline">\(H_a\)</span> = True when <span class="math inline">\(H_a\)</span> = actually True</li>
<li><strong><span class="math inline">\(U\)</span></strong> = true negatives, concludes <span class="math inline">\(H_0\)</span> = True when <span class="math inline">\(H_0\)</span> = actually True</li>
</ul>
<h3 id="error-rates">Error Rates</h3>
<ul>
<li><strong><em>false positive rate</em></strong> = rate at which false results are called significant <span class="math inline">\(E[\frac{V}{m_0}]\)</span> <span class="math inline">\(\rightarrow\)</span> average fraction of times that <span class="math inline">\(H_a\)</span> is claimed to be true when <span class="math inline">\(H_0\)</span> is actually true
<ul>
<li><em><strong>Note</strong>: mathematically equal to type I error rate <span class="math inline">\(\rightarrow\)</span> false positive rate is associated with a post-prior result, which is the expected number of false positives divided by the total number of hypotheses under the real combination of true and non-true null hypotheses (disregarding the “global null” hypothesis). Since the false positive rate is a parameter that is not controlled by the researcher, it cannot be identified with the significance level, which is what determines the type I error rate. </em></li>
</ul></li>
<li><strong><em>family wise error rate (FWER)</em></strong> = probability of at least one false positive <span class="math inline">\(Pr(V \ge 1)\)</span></li>
<li><p><strong><em>false discovery rate (FDR)</em></strong> = rate at which claims of significance are false <span class="math inline">\(E[\frac{V}{R}]\)</span></p></li>
<li><strong>controlling error rates (adjusting <span class="math inline">\(\alpha\)</span>)</strong>
<ul>
<li>false positive rate
<ul>
<li>if we call all <span class="math inline">\(P&lt;\alpha\)</span> significant (reject <span class="math inline">\(H_0\)</span>), we are expected to get <span class="math inline">\(\alpha \times m\)</span> false positives, where <span class="math inline">\(m\)</span> = total number of hypothesis test performed</li>
<li>with high values of <span class="math inline">\(m\)</span>, false positive rate is very large as well</li>
</ul></li>
<li>family-wise error rate (FWER)
<ul>
<li>controlling FWER = controlling the probability of even one false positive</li>
<li><em>bonferroni</em> correction (oldest multiple testing correction)
<ul>
<li>for <span class="math inline">\(m\)</span> tests, we want <span class="math inline">\(Pr(V \ge 1) &lt; \alpha\)</span></li>
<li>calculate P-values normally, and deem them significant if and only if <span class="math inline">\(P &lt; \alpha_{fewer} = \alpha / m\)</span></li>
</ul></li>
<li>easy to calculate, but tend to be very <strong><em>conservative</em></strong></li>
</ul></li>
<li>false discovery rate (FDR)
<ul>
<li>most popular correction = controlling FDR</li>
<li>for <span class="math inline">\(m\)</span> tests, we want <span class="math inline">\(E[\frac{V}{R}] \le \alpha\)</span></li>
<li>calculate P-values normally and sort some from smallest to largest <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(P_{(1)},P_{(1)}, ... , P_{(m)}\)</span></li>
<li>deem the P-values significant if <span class="math inline">\(P_{(i)} \le \alpha \times \frac{i}{m}\)</span></li>
<li>easy to calculate, less conservative, but allows for more false positives and may behave strangely under dependence (related hypothesis tests/regression with different variables)</li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>10 P-values with <span class="math inline">\(\alpha = 0.20\)</span></li>
</ul>
<img src="figures/9.png" title="fig:" alt="alt text"></li>
</ul></li>
<li><strong>adjusting for p-values</strong>
<ul>
<li><em><strong>Note</strong>: changing P-values will fundamentally change their properties but they can be used directly without adjusting <span class="math inline">\(/alpha\)</span> </em></li>
<li><em>bonferroni</em> (FWER)
<ul>
<li><span class="math inline">\(P_i^{fewer} = max(mP_i, 1)\)</span> <span class="math inline">\(\rightarrow\)</span> since p cannot exceed value of 1</li>
<li>deem P-values significant if <span class="math inline">\(P_i^{fewer} &lt; \alpha\)</span></li>
<li>similar to controlling FWER</li>
</ul></li>
</ul></li>
</ul>
<h3 id="example-4">Example</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">set.seed(1010093)</span><br><span class="line">pValues &lt;- rep(NA,1000)</span><br><span class="line">for(i in 1:1000)&#123;</span><br><span class="line">  x &lt;- rnorm(20)</span><br><span class="line">  # First 500 beta=0, last 500 beta=2</span><br><span class="line">  if(i &lt;= 500)&#123;y &lt;- rnorm(20)&#125;else&#123; y &lt;- rnorm(20,mean=2*x)&#125;</span><br><span class="line">  # calculating p-values by using linear model; the [2, 4] coeff in result = pvalue</span><br><span class="line">  pValues[i] &lt;- summary(lm(y ~ x))$coeff[2,4]</span><br><span class="line">&#125;</span><br><span class="line"># Controls false positive rate</span><br><span class="line">trueStatus &lt;- rep(c(&quot;zero&quot;,&quot;not zero&quot;),each=500)</span><br><span class="line">table(pValues &lt; 0.05, trueStatus)</span><br><span class="line"># Controls FWER</span><br><span class="line">table(p.adjust(pValues,method=&quot;bonferroni&quot;) &lt; 0.05,trueStatus)</span><br><span class="line"># Controls FDR (Benjamin Hochberg)</span><br><span class="line">table(p.adjust(pValues,method=&quot;BH&quot;) &lt; 0.05,trueStatus)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="resample-inference">Resample Inference</h2>
<ul>
<li><strong>Bootstrap</strong> = useful tool for constructing confidence intervals and caclulating standard errors for difficult statistics
<ul>
<li><strong><em>principle</em></strong> = if a statistic’s (i.e. median) sampling distribution is unknown, then use distribution defined by the data to approximate it</li>
<li><strong><em>procedures</em></strong>
<ol type="1">
<li>simulate <span class="math inline">\(n\)</span> observations <strong>with replacement</strong> from the observed data <span class="math inline">\(\rightarrow\)</span> results in <span class="math inline">\(1\)</span> simulated complete data set</li>
<li>calculate desired statistic (i.e. median) for each simulated data set</li>
<li>repeat the above steps <span class="math inline">\(B\)</span> times, resulting in <span class="math inline">\(B\)</span> simulated statistics</li>
<li>these statistics are approximately drawn from the sampling distribution of the true statistic of <span class="math inline">\(n\)</span> observations</li>
<li>perform one of the following
<ul>
<li>plot a histogram</li>
<li>calculate standard deviation of the statistic to estimate its standard error</li>
<li>take quantiles (2.5<sup>th</sup> and 97.5<sup>th</sup>) as a confidence interval for the statistic (“<em>bootstrap CI</em>”)</li>
</ul></li>
</ol></li>
<li><strong><em>example</em></strong>
<ul>
<li>Bootstrap procedure for calculating confidence interval for the median from a data set of <span class="math inline">\(n\)</span> observations <span class="math inline">\(\rightarrow\)</span> approximate sampling distribution</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.align</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">library(UsingR); data(father.son)</span><br><span class="line"># observed dataset</span><br><span class="line">x &lt;- father.son$sheight</span><br><span class="line"># number of simulated statistic</span><br><span class="line">B &lt;- 1000</span><br><span class="line"># generate samples</span><br><span class="line">resamples &lt;- matrix(</span><br><span class="line">    sample(x,               # sample to draw frome</span><br><span class="line">           n * B,           # draw B datasets with n observations each</span><br><span class="line">           replace = TRUE), # cannot draw n*B elements from x (has n elements) without replacement</span><br><span class="line">    B, n)                   # arrange results into n x B matrix</span><br><span class="line">                            # (every row = bootstrap sample with n observations)</span><br><span class="line"># take median for each row/generated sample</span><br><span class="line">medians &lt;- apply(resamples, 1, median)</span><br><span class="line"># estimated standard error of median</span><br><span class="line">sd(medians)</span><br><span class="line"># confidence interval of median</span><br><span class="line">quantile(medians, c(.025, .975))</span><br><span class="line"># histogram of bootstraped samples</span><br><span class="line">hist(medians)</span><br></pre></td></tr></table></figure>
<ul>
<li><p><em><strong>Note:</strong> better percentile bootstrap confidence interval = “bias corrected and accelerated interval” in <code>bootstrap</code> package</em></p></li>
<li><strong>Permutation Tests</strong>
<ul>
<li><strong><em>procedures</em></strong>
<ul>
<li>compare groups of data and test the null hypothesis that the distribution of the observations from each group = same
<ul>
<li><em><strong>Note</strong>: if this is true, then group labels/divisions are irrelevant </em></li>
</ul></li>
<li>permute the labels for the groups</li>
<li>recalculate the statistic
<ul>
<li>Mean difference in counts</li>
<li>Geometric means</li>
<li>T statistic</li>
</ul></li>
<li>Calculate the percentage of simulations where the simulated statistic was more extreme (toward the alternative) than the observed</li>
</ul></li>
<li><p><strong><em>variations</em></strong></p>
<table>
<thead>
<tr class="header">
<th>Data type</th>
<th>Statistic</th>
<th>Test name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ranks</td>
<td>rank sum</td>
<td>rank sum test</td>
</tr>
<tr class="even">
<td>Binary</td>
<td>hypergeometric prob</td>
<td>Fisher’s exact test</td>
</tr>
<tr class="odd">
<td>Raw data</td>
<td></td>
<td>ordinary permutation test</td>
</tr>
</tbody>
</table>
<ul>
<li><em><strong>Note</strong>: randomization tests are exactly permutation tests, with a different motivation </em></li>
<li>For matched data, one can randomize the signs</li>
<li>For ranks, this results in the <strong>signed rank test</strong></li>
<li>Permutation strategies work for regression by permuting a regressor of interest</li>
<li>Permutation tests work very well in multivariate settings</li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>we will compare groups <strong>B</strong> and <strong>C</strong> in this dataset for null hypothesis <span class="math inline">\(H_0:\)</span> there are no difference between the groups</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">data(InsectSprays)</span><br><span class="line"># plot boxplot of dataset</span><br><span class="line">ggplot(InsectSprays, aes(spray, count, fill = spray)) + geom_boxplot()</span><br></pre></td></tr></table></figure>
<ul>
<li>we will compare groups <strong>B</strong> and <strong>C</strong> in this dataset for null hypothesis <span class="math inline">\(H_0:\)</span> there are no difference between the groups</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># subset to only &quot;B&quot; and &quot;C&quot; groups</span><br><span class="line">subdata &lt;- InsectSprays[InsectSprays$spray %in% c(&quot;B&quot;, &quot;C&quot;),]</span><br><span class="line"># values</span><br><span class="line">y &lt;- subdata$count</span><br><span class="line"># labels</span><br><span class="line">group &lt;- as.character(subdata$spray)</span><br><span class="line"># find mean difference between the groups</span><br><span class="line">testStat &lt;- function(w, g) mean(w[g == &quot;B&quot;]) - mean(w[g == &quot;C&quot;])</span><br><span class="line">observedStat &lt;- testStat(y, group)</span><br><span class="line">observedStat</span><br></pre></td></tr></table></figure>
<ul>
<li>the observed difference between the groups is <code>r observedStat</code></li>
<li>now we changed the resample the lables for groups <strong>B</strong> and <strong>C</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create 10000 permutations of the data with the labels&apos; changed</span><br><span class="line">permutations &lt;- sapply(1 : 10000, function(i) testStat(y, sample(group)))</span><br><span class="line"># find the number of permutations whose difference that is bigger than the observed</span><br><span class="line">mean(permutations &gt; observedStat)</span><br></pre></td></tr></table></figure>
<ul>
<li>we created 1000 permutations from the observed dataset, and found <strong><em>no datasets</em></strong> with mean differences between groups <strong>B</strong> and <strong>C</strong> larger than the original data</li>
<li>therefore, p-value is very small and we can <strong><em>reject the null</em></strong> hypothesis with any reasonable <span class="math inline">\(\alpha\)</span> levels</li>
<li>below is the plot for the null distribution/permutations</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># plot distribution of permutations</span><br><span class="line">ggplot(data.frame(permutations = permutations),</span><br><span class="line">           aes(permutations)) + geom_histogram(fill = &quot;lightblue&quot;, color = &quot;black&quot;, binwidth = 1) + geom_vline(xintercept = observedStat, size = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from the black line, the observed difference/statistic is very far from the mean <span class="math inline">\(\rightarrow\)</span> likely 0 is <strong><em>not</em></strong> the true difference
<ul>
<li>with this information, formal confidence intervals can be constructed and p-values can be calculated</li>
</ul></li>
</ul>

          
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Reproducible-Research/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/07/Reproducible-Research/" itemprop="url">
                  Reproducible Research
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 14:05:25 / Modified: 14:06:57" itemprop="dateCreated datePublished" datetime="2018-08-07T14:05:25+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">38k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">34 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="replication">Replication</h2>
<ul>
<li>ultimate standard for strengthening of scientific evidence</li>
<li>reproduce findings and conduct studies with independent investigators/data/analytical methods/laboratories/instruments</li>
<li>particularly important in studies that affect policy</li>
<li>HOWEVER, some studies can’t be replicated (time, opportunity, money, unique)
<ul>
<li>Make research reproducible (making available data/code so others may reproduce analysis)</li>
</ul></li>
</ul>
<h2 id="reproducibility">Reproducibility</h2>
<ul>
<li>bridges gap between replication and letting the study stand alone</li>
<li>take data/code and replicate findings <span class="math inline">\(\rightarrow\)</span> validate their findings</li>
<li>why we need reproducible research
<ul>
<li>new technologies increase data throughput while adding complexities and dimension to data</li>
<li>existing databases merged into bigger collection of data</li>
<li>computational power increased</li>
</ul></li>
<li><strong>Research Pipeline</strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># install grid and png packages if not present</span><br><span class="line">library(png)</span><br><span class="line">library(grid)</span><br><span class="line">grid.raster(readPNG(&quot;figures/1.jpg&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>Institute of Medicine report - Evolution of Translational Omics
<ul>
<li>Data/meta data should be made available</li>
<li>Computer code/fully specified computational procedures need to be available</li>
<li>Data processing and computational analysis steps described</li>
</ul></li>
<li><strong>Necessities for Reproducible Research</strong>
<ul>
<li>analytical data (not raw data)</li>
<li>analytical code (applied to data, i.e. regression model etc.)</li>
<li>documentation of data/code</li>
<li>standard means of distribution</li>
</ul></li>
<li><strong>Challenges</strong>
<ul>
<li>authors make considerable effort to make data/results available</li>
<li>readers must download data/results individually and piece it all together</li>
<li>readers may not have the same resources as authors (computing clusters, etc.)</li>
<li>few tools for readers/authors</li>
<li>In reality, authors just put their content online (often disorganized, but there are some central databases for various fields) and readers have to find the data and piece it together</li>
</ul></li>
</ul>
<h2 id="literatestatistical-programming">Literate/Statistical Programming</h2>
<ul>
<li>Notion of an article is a stream of <strong>text</strong> and <strong>code</strong></li>
<li>original idea came from <em>Don Knuth</em></li>
<li>statistical analysis is divided into text and code chunks
<ul>
<li>text explains whats going on</li>
<li>code loads data/computes results</li>
<li>presentation code formats results (tables, graphics, etc)</li>
</ul></li>
<li>Literate Programming requires
<ul>
<li><strong>documentation language</strong> - [<strong><em>weave</em></strong>] produce human-readable docs (HTML/PDF)</li>
<li><strong>programming language</strong> - [<strong><em>tangle</em></strong>] produce machine-readable code</li>
</ul></li>
<li><strong>Sweave</strong> - original program in R designed to do literate programming
<ul>
<li>LaTeX for documentation, R for programming</li>
<li>developed by <em>Friedrich Leisch</em> (member of R core)</li>
<li>still maintained by R core</li>
<li><strong><em>limitations</em></strong>
<ul>
<li>focus on LaTeX, difficult to learn markup languages</li>
<li>lacks features like caching, multiple plots per chunk, mixing programming languages with many other technical terms</li>
<li>not frequently updated/actively developed</li>
</ul></li>
</ul></li>
<li><strong><code>knitr</code></strong> = alternative literate programming package
<ul>
<li>R for programming (can use others as well)</li>
<li>LaTeX/HTML/Markdown for documentation</li>
<li>developed by Iowa grad student <em>Yihui Xie</em></li>
</ul></li>
<li><strong>reproducible research</strong> = minimum standard for non-replicable/computationally intensive studies
<ul>
<li>infrastructure still need for creating/distributing reproducible documents</li>
</ul></li>
</ul>
<h2 id="structure-of-data-analysis">Structure of Data Analysis</h2>
<h3 id="key-challenge-in-data-analysis">Key Challenge in Data Analysis</h3>
<blockquote>
<p>“Ask yourselves, what problem have you solved, ever, that was worth solving, where you knew all of the given information in advance? Where you didn’t have a surplus of information and have to filter it out, or you had insufficient information and have to go find some?” - <strong><em>Dan Myer</em></strong></p>
</blockquote>
<h3 id="define-the-question">Define The Question</h3>
<ul>
<li>most powerful dimension reduction tool, narrowing down question reduces noise/simply problem</li>
<li>Science <span class="math inline">\(\rightarrow\)</span> Data <span class="math inline">\(\rightarrow\)</span> Applied Statistics <span class="math inline">\(\rightarrow\)</span> Theoretical Statistics</li>
<li>properly using the combination of science/data/applied statistics = proper data analysis</li>
<li><strong><em>example</em></strong>
<ul>
<li>general question: Can I automatically detect emails that are SPAM from those that are not?</li>
<li>concrete question: Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?</li>
</ul></li>
</ul>
<h3 id="determine-the-ideal-data-set">Determine the Ideal Data Set</h3>
<ul>
<li>after having the concrete question, find the data set that is suitable for your goal:
<ul>
<li><strong><em>Descriptive</em></strong> = whole population</li>
<li><strong><em>Exploratory</em></strong> = random sample with multiple variables measured</li>
<li><strong><em>Inferential</em></strong> = drawing conclusion from a sample for a larger population, so choosing the right population, and carefully/randomly sampled</li>
<li><strong><em>Predictive</em></strong> = need training and test data sets from the same population to build a model and classifier</li>
<li><strong><em>Causal</em></strong> = (if this is modified, then that happens) experimental data from randomized study</li>
<li><strong><em>Mechanistic</em></strong> = data from all components of system that you want to describe</li>
</ul></li>
</ul>
<h3 id="determine-what-data-you-can-access">Determine What Data You Can Access</h3>
<ul>
<li>often won’t have access to the ideal data set</li>
<li>free data from the web, paid data from provider (must respect terms of use)</li>
<li>if data doesn’t exist, you may need to generate some of the data</li>
</ul>
<h3 id="obtain-the-data">Obtain The Data</h3>
<ul>
<li>try to obtain the raw data and reference the source</li>
<li>if loading data from internet source, record at the minimum url and time of access</li>
</ul>
<h3 id="clean-the-data">Clean The Data</h3>
<ul>
<li>raw data need to be processed to be fed into modeling program</li>
<li>if already processed, need to understand how and understand documentation on how the pre-processing was done</li>
<li>understanding source of data (i.e. survey - how survey was done?)</li>
<li>may need to reformat/subsample data <span class="math inline">\(\rightarrow\)</span> need to record steps</li>
<li>determine if data is good enough to solve the problem <span class="math inline">\(\rightarrow\)</span> if not, find other data/quit/change question to avoid misleading conclusions</li>
</ul>
<h3 id="exploratory-data-analysis">Exploratory Data Analysis</h3>
<ul>
<li>for the purpose of the SPAM question, data needs to be split into training and test test (Predictive)</li>
</ul>
<figure class="highlight plain"><figcaption><span>warning </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># If it isn&apos;t installed, install the kernlab package with install.packages()</span><br><span class="line">library(kernlab)</span><br><span class="line">data(spam)</span><br><span class="line">set.seed(3435)</span><br><span class="line">trainIndicator = rbinom(4601, size = 1, prob = 0.5)</span><br><span class="line">table(trainIndicator)</span><br><span class="line">trainSpam = spam[trainIndicator == 1, ]</span><br><span class="line">testSpam = spam[trainIndicator == 0, ]</span><br></pre></td></tr></table></figure>
<ul>
<li>look at one/two-dimensional summaries of data, what the distribution looks like, relationships between variables
<ul>
<li><code>names()</code>, <code>summary()</code>, <code>head()</code></li>
<li><code>table(trainSpam$type)</code></li>
</ul></li>
<li>check for missing data/why there is missing data</li>
<li>create exploratory plots
<ul>
<li><code>plot(trainSpam$capitalAve ~ trainSpam$type)</code></li>
<li><code>plot(log10(trainSpam$capitalAve + 1) ~ trainSpam$type)</code>
<ul>
<li>because data is skewed <span class="math inline">\(\rightarrow\)</span> use <code>log</code></li>
<li>a lot of zeroes <span class="math inline">\(\rightarrow\)</span> <code>+1</code></li>
</ul></li>
</ul></li>
<li>perform exploratory analysis (clustering)
<ul>
<li>relationships between predictors</li>
<li><code>plot(log10(trainSpam[, 1:4] + 1))</code>
<ul>
<li>log transformations diagrams between pairs of variables</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/2.jpg&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>hierarchical cluster analysis (<strong>Dendrogram</strong>) = what words/characteristics tend to cluster together
<ul>
<li><code>hCluster = hclust(dist(t(trainSpam[, 1:57])))</code>
<ul>
<li><em><strong>Note</strong>: Dendrograms can be sensitive to skewness in distribution of individual variables, so maybe useful to transform predictor (see below) </em></li>
</ul></li>
<li><code>hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:55] + 1))))</code></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/3.jpg&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="statistical-modellingprediction">Statistical Modelling/Prediction</h3>
<ul>
<li>should be guided by exploratory analysis</li>
<li>exact methods depend on the question of interest</li>
<li>account for data transformation/processing
<ul>
<li>go through each variable in the dataset and fit logistic regression to see if we can predict if an email is spam or not by using a single variable</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>cache </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">trainSpam$numType = as.numeric(trainSpam$type) - 1</span><br><span class="line">costFunction = function(x, y) sum(x != (y &gt; 0.5))</span><br><span class="line">cvError = rep(NA, 55)</span><br><span class="line">library(boot)</span><br><span class="line">for (i in 1:55) &#123;</span><br><span class="line">    # creates formula with one variable and the result</span><br><span class="line">    lmFormula = reformulate(names(trainSpam)[i], response = &quot;numType&quot;)</span><br><span class="line">    glmFit = glm(lmFormula, family = &quot;binomial&quot;, data = trainSpam)</span><br><span class="line">    # cross validated error</span><br><span class="line">    cvError[i] = cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]</span><br><span class="line">&#125;</span><br><span class="line"># Which predictor has minimum cross-validated error?</span><br><span class="line">names(trainSpam)[which.min(cvError)]</span><br></pre></td></tr></table></figure>
<ul>
<li>think about measures/sources of uncertainty</li>
</ul>
<figure class="highlight plain"><figcaption><span>warning </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Use the best model from the group</span><br><span class="line">predictionModel = glm(numType ~ charDollar,family=&quot;binomial&quot;,data=trainSpam)</span><br><span class="line"># Get predictions on the test set</span><br><span class="line">predictionTest = predict(predictionModel,testSpam)</span><br><span class="line">predictedSpam = rep(&quot;nonspam&quot;,dim(testSpam)[1])</span><br><span class="line"># Classify as &apos;spam&apos; for those with prob &gt; 0.5</span><br><span class="line">predictedSpam[predictionModel$fitted &gt; 0.5] = &quot;spam&quot;</span><br><span class="line"># Classification table</span><br><span class="line">table(predictedSpam, testSpam$type)</span><br><span class="line"># Error rate</span><br><span class="line">(61 + 458)/(1346 + 458 + 61 + 449)</span><br></pre></td></tr></table></figure>
<h3 id="interpret-results">Interpret Results</h3>
<ul>
<li>use appropriate language and not go over what you performed
<ul>
<li>describes, correlates/associated with, lead to/causes, predicts</li>
</ul></li>
<li>give explanation of the results (why certain models predict better than others)</li>
<li>interpret coefficients
<ul>
<li>The fraction of characters that are dollar signs can be used to predict if an email is Spam</li>
<li>Anything with more than 6.6% dollar signs is classified as Spam</li>
<li>More dollar signs always means more Spam under our prediction</li>
</ul></li>
<li>interpret measures of uncertainty to calibrate your interpretation of results
<ul>
<li>Our test set error rate was 22.4%</li>
</ul></li>
</ul>
<h3 id="challenge-results">Challenge Results</h3>
<ul>
<li>good to challenge the whole process/all results you found (because somebody else will)
<ul>
<li>question <span class="math inline">\(\rightarrow\)</span> data source <span class="math inline">\(\rightarrow\)</span> processing <span class="math inline">\(\rightarrow\)</span> analysis <span class="math inline">\(\rightarrow\)</span> conclusion</li>
</ul></li>
<li>challenge measures of uncertainty and choices in what to include in the model</li>
<li>think about alternative analyses, maybe useful to try in case they produce interesting/better models/predictions</li>
</ul>
<h3 id="synthesizewrite-up-and-results">Synthesize/Write Up and Results</h3>
<ul>
<li>tell coherent story of the most important part of analysis</li>
<li>lead with question <span class="math inline">\(\rightarrow\)</span> provide context to better understand framework of analysis
<ul>
<li>Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?</li>
</ul></li>
<li>summarize the analysis into the story
<ul>
<li>Collected data from UCI <span class="math inline">\(\rightarrow\)</span> created training/test sets</li>
<li>Explored relationships</li>
<li>Choose logistic model on training set by cross validation</li>
<li>Applied to test, 78% test set accuracy</li>
</ul></li>
<li>don’t need every analysis performed, only include the ones that are needed for the story or address a challenge
<ul>
<li>Number of dollar signs seems to indicate SPAM, “Make money with Viagra $ $ $ $!”</li>
<li>78% isn’t that great</li>
<li>I could use more variables</li>
<li>Why logistic regression?</li>
</ul></li>
<li>order analysis according to story rather than chronologically</li>
<li>include well defined figures to help understanding</li>
</ul>
<h3 id="create-reproducible-code">Create Reproducible Code</h3>
<ul>
<li>document your analysis as you do them using markdown or <code>knitr</code>
<ul>
<li>preserve any R code or written summary in a single document using <code>knitr</code></li>
</ul></li>
<li>ensure all analysis is reproducible (standard that most data analysis are judged on)</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="organizing-data-analysis">Organizing Data Analysis</h2>
<ul>
<li>key data analysis files
<ul>
<li><strong>data</strong>: raw
<ul>
<li>should be store in analysis folder</li>
<li>if accessed from the web, include in README file the URL, where the data is from, what the data is, brief description of what it is for, date of access</li>
<li>if data is store in Git repository, can use the log to talk about the above info</li>
</ul></li>
<li><strong>data</strong>: processed
<ul>
<li>should be names so that its easy to see what script generated what data</li>
<li>In README, important to document what code files were used to transform raw data to processed data</li>
<li>processed data should be tidy/organized</li>
</ul></li>
<li><strong>figures</strong>: exploratory
<ul>
<li>made during process of analysis, not necessarily all part of final report</li>
<li>don’t need to be well formatted/annotated</li>
<li>need to be usable enough so that the author can easily understand what is being presented and how to reproduce it</li>
</ul></li>
<li><strong>figures</strong>: final
<ul>
<li>more polished, better organized, more readable, possibly multiple panels</li>
<li>usually a small subset or original exploratory figures (typical journal article contains 5-6 figures)</li>
<li>clearly labeled and well annotated, axes/color set to make figure clear</li>
</ul></li>
<li><strong>R code</strong>: raw/unused scripts
<ul>
<li>could be less commented, multiple versions</li>
<li>may include analyses that are later discarded</li>
<li>important to name/document appropriately to understand what was performed</li>
<li>placed in a separate part of the data analysis directory (separate from final)</li>
</ul></li>
<li><strong>R code</strong>: final scripts
<ul>
<li>clearly commented - small comments for what/when/why/how, big comment blocks for whole sections to explain what is being done</li>
<li>include processing details for the raw data</li>
<li>should pertain to only analyses that appear in in the final write up</li>
</ul></li>
<li><strong>R code</strong>: R markdown files
<ul>
<li>not necessary or require but useful to summarize parts of analysis</li>
<li>can be used to generate reproducible reports, as it can embed code and text into single document and processing the document into readable webpage/pdf doc</li>
<li>easy to create in Rstudio</li>
</ul></li>
<li><strong>text</strong>: README file
<ul>
<li>explain what is going on in project directory</li>
<li>not necessary if you have R markdown files (don’t separate analysis and code - literate programming principle)</li>
<li>should contain step-by-step instructions for how analysis was conducted, what code files are called first, what are used to process the data, what are used to fit models, what are used to generate figures, etc.</li>
</ul></li>
<li><strong>text</strong>: text of analysis/report
<ul>
<li>should include title, introduction/motivation for problem, the method (statistical method), the results (including measures of uncertainty) and conclusions (including pitfalls/problems)</li>
<li>coherent story from all analysis performed, but does not need to include all analysis performed but only the most important and relevant parts</li>
<li>should include references for statistical methods, software packages, implementation that were used (for reproducibility)</li>
</ul></li>
</ul></li>
<li><strong>Resources</strong>
<ul>
<li><a href="http://projecttemplate.net/" target="_blank" rel="noopener">Project template</a> = a external R package</li>
<li><a href="http://www.r-statistics.com/2010/09/managing-a-statistical-analysis-project-guidelines-and-best-practices/" target="_blank" rel="noopener">Managing a statistical analysis project guidelines and best practices</a></li>
</ul></li>
</ul>
<h2 id="coding-standards-in-r">Coding Standards in R</h2>
<ul>
<li>write code in text editor and save as text file</li>
<li>indenting (4 space minimum)</li>
<li>limit the width of your code (80 columns)</li>
<li>limit the length of functions</li>
</ul>
<h2 id="markdown-documentation">Markdown (<a href="http://daringfireball.net/projects/markdown/syntax" target="_blank" rel="noopener">documentation</a>)</h2>
<ul>
<li>Markdown = text-to-HTML conversion tool for web writers.
<ul>
<li>simplified version of markup language</li>
<li>write using easy-to-read, easy-to-write plain text format</li>
<li>any text editor can create markdown document</li>
<li>convert the text to structurally valid XHTML/HTML</li>
<li>created by John Gruber</li>
</ul></li>
<li><strong>italics</strong> = <code>*text*</code></li>
<li><strong>bold</strong> = <code>**text**</code></li>
<li><strong>main heading</strong> = <code>#Heading</code></li>
<li><strong>secondary heading</strong> = <code>##Heading</code></li>
<li><strong>tertiary main heading</strong> = <code>###Heading</code></li>
<li><strong>unordered list</strong> = <code>- first element</code></li>
<li><strong>ordered list</strong> = <code>1. first element</code>
<ul>
<li>the number in front actually doesn’t matter, as long as there is a number there, markdown will be compiled to an ordered list (no need for renumbering)</li>
</ul></li>
<li><strong>links</strong> = <code>[text](url)</code>
<ul>
<li>OR <code>[text][1]</code> <span class="math inline">\(\rightarrow\)</span> later in the document, define all of the links in this format: <code>[1]: url &quot;text&quot;</code></li>
</ul></li>
<li><strong>new lines</strong> = requires a <strong><em>double space</em></strong> after the end of a line</li>
</ul>
<h2 id="r-markdown">R Markdown</h2>
<ul>
<li>integration of R code and markdown
<ul>
<li>allows creating documents containing “live” R code</li>
<li>R code is evaluated as a part of the processing of the markdown</li>
<li>results from R code inserted into markdown document</li>
<li>core tool for literate statistical programming</li>
<li><strong>pros</strong>
<ul>
<li>text/code all in one place in logical order</li>
<li>data/results automatically updated to reflect external changes</li>
<li>code is live = automatic test when building document</li>
</ul></li>
<li><strong>cons</strong>
<ul>
<li>text/code all in one place, can be difficult to read especially if there’s a lot of code</li>
<li>can substantially slowdown processing of documents</li>
</ul></li>
</ul></li>
<li><strong><code>knitr</code> package</strong>
<ul>
<li>written by YihuiXie, built into RStudio</li>
<li>support Markdown, LaTeX, HTML as documentation languages</li>
<li>Exports PDF/HTML</li>
<li>good for manuals, short/medium technical documents, tutorials, periodic reports, data preprocessing documents/summaries</li>
<li>not good for long research articles, complex time-consuming computations, precisely formatted documents</li>
<li>evaluates R markdown documents and return/records the results, and write out a Markdown files</li>
<li>Markdown file can then be converted into HTML using markdown package</li>
<li>solidify package converts R markdown into presentation slides</li>
</ul></li>
<li>In RStudio, create new R Markdown files by clicking New <span class="math inline">\(\rightarrow\)</span> R Markdown
<ul>
<li><code>========</code> = indicates title of document (large text)</li>
<li><code>$expression$</code> = indicates LaTeX expression/formatting</li>
<li>`<code>text</code>` = changes text to code format (typewriter font)</li>
<li>```<code>{r name, echo = FALSE, results = hide}...</code>``` = R code chunk
<ul>
<li><code>name</code> = name of the code chunk</li>
<li><code>echo = FALSE</code> = turns off the echo of the R code chunk, which means display only the result</li>
<li><em><strong>Note</strong>: by default code in code chunk is echoed = print code AND results </em></li>
<li><code>results = hide</code> = hides the results from being placed in the markdown document</li>
</ul></li>
<li>inline text computations
<ul>
<li>`<code>r</code> <code>variable</code>` = prints the value of that variable directly inline with the text</li>
</ul></li>
<li>incorporating graphics
<ul>
<li>```<code>{r scatterplot, fig.height = 4, fig.width = 6} ... plot() ...</code>``` = inserts a plot into markdown document
<ul>
<li><code>scatterplot</code> = name of this code chunk (can be anything)</li>
<li><code>fig.height = 4</code> = adjusts height of the figure, specifying this alone will produce a rectangular plot rather than a square one by default</li>
<li><code>fig.width = 6</code> = adjusts width of the figure</li>
</ul></li>
<li>knitr produces HTML, with the image embedded in HTML using base64 encoding
<ul>
<li>does not depend on external image files</li>
<li>not efficient but highly transportable</li>
</ul></li>
</ul></li>
<li>incorporating tables (xtable package: <code>install.packages(&quot;xtable&quot;)</code>)
<ul>
<li><code>xtable</code>prints the table in html format, which is better presented than plain text normally</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>showtable, results </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">library(datasets)</span><br><span class="line">library(xtable)</span><br><span class="line">fit &lt;- lm(Ozone ~ Wind + Temp + Solar.R, data = airquality)</span><br><span class="line">xt &lt;- xtable(summary(fit))</span><br><span class="line">print(xt, &quot;html&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>setting global options</strong>
<ul>
<li>```<code>{r setoptions, echo = FALSE} opts_chunk$set(echo = FALSE, results = &quot;hide&quot;)</code>``` = sets the default option to not print the code/results unless otherwise specified</li>
</ul></li>
<li><strong>common options</strong>
<ul>
<li><strong>output</strong>: <code>results = &quot;asis&quot;</code> OR <code>&quot;hide&quot;</code>
<ul>
<li><code>&quot;asis&quot;</code> = output to stay in original format and not compiled into HTML</li>
</ul></li>
<li><strong>output</strong>: <code>echo = TRUE</code> OR <code>FALSE</code></li>
<li><strong>figures</strong>: <code>fig.height = numeric</code></li>
<li><strong>figures</strong>: <code>fig.width = numeric</code></li>
</ul></li>
<li><strong>caching computations</strong>
<ul>
<li>add argument to code chunk: <code>cache = TRUE</code></li>
<li>computes and stores result of code the first time it is run, and calls the stored result directly from file for each subsequent call</li>
<li>useful for complex computations</li>
<li>caveats:
<ul>
<li>if data/code changes, you will need to re-run cached code chunks</li>
<li>dependencies not checked explicitly (changes in other parts of the code <span class="math inline">\(\rightarrow\)</span> need to re-run the cached code)</li>
<li>if code does something outside of the document (i.e. produce a png file), the operation cannot be cached</li>
</ul></li>
</ul></li>
<li><strong>“Knit HTML”</strong> button to process the document
<ul>
<li>alternatively, when not in RStudio, the process can be accomplished through the following</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(knitr)</span><br><span class="line">setwd(&lt;working directory&gt;)</span><br><span class="line">knit2html(<span class="string">"document.Rmd"</span>)</span><br><span class="line">browseURL(<span class="string">"document.html"</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>processing of <code>knitr</code> documents
<ul>
<li>author drafts R Markdown (.Rmd) <span class="math inline">\(\rightarrow\)</span> <code>knitr</code> processes file to Markdown (.md) <span class="math inline">\(\rightarrow\)</span> <code>knitr</code> converts file to HTML</li>
<li><em><strong>Note</strong>: author should NOT edit/save the .md or .html document until you are done with the document </em></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="communicating-results">Communicating Results</h2>
<ul>
<li>when presenting information, it is important to breakdown results of an analysis into different levels of granularity/detail</li>
<li>below are listed in order of <strong><em>least <span class="math inline">\(\rightarrow\)</span> most specific</em></strong></li>
</ul>
<h3 id="hierarchy-of-information---research-paper">Hierarchy of Information - Research Paper</h3>
<ul>
<li><strong>title/author list</strong> = description of topic covered</li>
<li><strong>abstract</strong> = a few hundred words about the problem, motivation, and solution</li>
<li><strong>body/results</strong> = detailed methods, results, sensitivity analysis, discussion/implication of findings</li>
<li><strong>supplementary material</strong> = granular details about analysis performed and method used</li>
<li><strong>code/detail</strong> = material to reproduce analysis/finding</li>
</ul>
<h3 id="hierarchy-of-information---email-presentation">Hierarchy of Information - Email Presentation</h3>
<ul>
<li><strong>subject line/sender</strong> = must have, should be concise/descriptive, summarize findings in one sentence if possible</li>
<li><strong>email body</strong> = should be 1-2 paragraphs, brief description of problem/context, summarize findings/results
<ul>
<li>If action needs to be taken, suggest some options and make them as concrete as possible</li>
<li>If questions need to be addressed, try to make them yes/no</li>
</ul></li>
<li><strong>attachment(s)</strong> = R Markdown/report containing more details, should still stay concise (not pages of code)</li>
<li><strong>links to supplementary materials</strong> = GitHub/web link to code, software, data</li>
</ul>
<h3 id="rpubs-link">RPubs (<a href="http://rpubs.com/" target="_blank" rel="noopener">link</a>)</h3>
<ul>
<li>built-in service with RStudio and works seamlessly with <code>knitr</code></li>
<li>after kniting the R Markdown file in HTML, you can click on <strong>Publish</strong> to publish the HTML document on RPubs</li>
<li><em><strong>Note</strong>: all publications to RPups are publicly available immediately</em></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="reproducible-research-checklist">Reproducible Research Checklist</h2>
<ul>
<li><strong>Checklist</strong>
<ul>
<li>Are we doing good science?</li>
<li>Was any part of this analysis done by hand?
<ul>
<li>If so, are those parts <em>precisely</em> document?</li>
<li>Does the documentation match reality?</li>
</ul></li>
<li>Have we taught a computer to do as much as possible (i.e. coded)?</li>
<li>Are we using a version control system?</li>
<li>Have we documented our software environment?</li>
<li>Have we saved any output that we cannot reconstruct from original data + code?</li>
<li>How far back in the analysis pipeline can we go before our results are no longer (automatically) reproducible?</li>
</ul></li>
</ul>
<h3 id="dos">Do’s</h3>
<ul>
<li><strong><em>start with good science</em></strong>
<ul>
<li>work on interesting problem (to you &amp; other people)</li>
<li>form coherent/focused question to simplify problem</li>
<li>collaborate with others to reinforce good practices and habits</li>
</ul></li>
<li><strong><em>teach a computer</em></strong>
<ul>
<li>worthwhile to automate all tasks through script/programming</li>
<li>code = precise instructions to process/analyze data</li>
<li>teaching the computer almost guarantee’s reproducibility</li>
<li><code>download.file(&quot;url&quot;, &quot;filename&quot;)</code> = convenient way to download file
<ul>
<li>full URL specified (instead series of links/clicks)</li>
<li>name of file specified</li>
<li>directory specified</li>
<li>code can be executed in R (as long as link is available)</li>
</ul></li>
</ul></li>
<li><strong><em>use version control</em></strong>
<ul>
<li>GitHub/BitBucket are good tools</li>
<li>helps to slow down
<ul>
<li>forces the author to think about changes made and commit changes and keep track of analysis performed</li>
</ul></li>
<li>helps to keep track of history/snapshots</li>
<li>allows reverting to old versions</li>
</ul></li>
<li><strong><em>keep track of software environment</em></strong>
<ul>
<li>some tools/datasets may only work on certain software/environment
<ul>
<li>software and computing environment are critical to reproducing analysis</li>
<li>everything should be documented</li>
</ul></li>
<li><strong>computer architecture</strong>: CPU (Intel, AMD, ARM), GPUs, 32 vs 64bit</li>
<li><strong>operating system</strong>: Windows, Mac OS, Linux/Unix</li>
<li><strong>software toolchain</strong>: compilers, interpreters, command shell, programming languages (C, Perl, Python, etc.), database backends, data analysis software</li>
<li><strong>supporting software/infrastructure</strong>: Libraries, R packages, dependencies</li>
<li><strong>external dependencies</strong>: web sites, data repositories (data source), remote databases, software repositories</li>
<li><strong>version numbers</strong>: ideally, for everything (if available)</li>
<li><code>sessionInfo()</code> = prints R version, operating system, local, base/attached/utilized packages</li>
</ul></li>
<li><strong><em>set random number generator seed</em></strong>
<ul>
<li>random number generators produce pseudo-random numbers based on initial seed (usually number/set of numbers)
<ul>
<li><code>set.seed()</code> can be used to specify seet for random generator in R</li>
</ul></li>
<li>setting seed allows stream of random numbers to be reproducible</li>
<li>whenever you need to generate steam of random numbers for non-trivial purpose (i.e. simulations, Markov Chain Monte Carlo analysis), <strong><em>always</em></strong> set the seed.</li>
</ul></li>
<li><strong><em>think about entire pipeline</em></strong>
<ul>
<li>data analysis is length process, important to ensure each piece is reproducible
<ul>
<li>final product is important, but the process is just as important</li>
</ul></li>
<li>raw data <span class="math inline">\(\rightarrow\)</span> processed data <span class="math inline">\(\rightarrow\)</span> analysis <span class="math inline">\(\rightarrow\)</span> report</li>
<li>the more of the pipeline that is made reproducible, the more credible the results are</li>
</ul></li>
</ul>
<h3 id="donts">Don’ts</h3>
<ul>
<li><strong><em>do things by hand</em></strong>
<ul>
<li>may lead to unreproducible results
<ul>
<li>edit spreadsheets using Excel</li>
<li>remove outliers (without noting criteria)</li>
<li>edit tables/figures</li>
<li>validate/quality control for data</li>
</ul></li>
<li>downloading data from website (clicking on link)
<ul>
<li>need lengthy set of instructions to obtain the same data set</li>
</ul></li>
<li>moving/split/reformat data (no record of what was done)</li>
<li>if necessary, manual tasks must be documented precisely (account for people with different background/context)</li>
</ul></li>
<li><strong><em>point and click</em></strong>
<ul>
<li>graphical user interfaces (GUI) make it easy to process/analyze data
<ul>
<li>GUIs are intuitive to use but actions are <strong>difficult</strong> to track and for others to reproduce</li>
<li>some GUIs include log files that can be utilized for review</li>
</ul></li>
<li>any interactive software should be carefully used to ensure all results can be reproduced</li>
<li>text editors are usually ok, but when in doubt, document it</li>
</ul></li>
<li><strong><em>save output for convenience</em></strong>
<ul>
<li>avoid saving data analysis output
<ul>
<li>tables, summaries, figures, processed, data</li>
</ul></li>
<li>output saved stand-alone without code/steps on how it is produced is not reproducible</li>
<li>when data changes or error is detected in parts of analysis the output is dependent on, the original graph/table will not be updated</li>
<li>intermediate files (processed data) are ok to keep but clear/precise documentation must be created</li>
<li>should save the <strong>data/code</strong> instead of the output</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="replication-vs-reproducibility">Replication vs Reproducibility</h2>
<ul>
<li><strong>Replication</strong>
<ul>
<li>focuses on validity of scientific claim</li>
<li>if a study states “if X is related to Y” then we ask “is the claim true?”</li>
<li>need to reproduce results with new investigators, data, analytical methods, laboratories, instruments, etc.</li>
<li>particularly important in studies that can impact broad policy or regulatory decisions</li>
<li>stands as <strong>ultimate standard</strong> for strengthening scientific evidence</li>
</ul></li>
<li><strong>Reproducibility</strong>
<ul>
<li>focuses on validity of data analysis</li>
<li>“Can we trust this analysis?”</li>
<li>reproduce results new investigators, same data, same methods</li>
<li>important when replication is impossible</li>
<li>arguably a <strong>minimum standard</strong> for any scientific study</li>
</ul></li>
</ul>
<h3 id="background-and-underlying-trend-for-reproducibility">Background and Underlying Trend for Reproducibility</h3>
<ul>
<li>some studies cannot be replicated (money/time/unique/opportunistic) <span class="math inline">\(\rightarrow\)</span> 25 year studies in epidemiology</li>
<li>technology increases rate of collection/complexity of data</li>
<li>existing databases merged to become bigger databases (sometimes used off-label) <span class="math inline">\(\rightarrow\)</span> administrative data used in health studies</li>
<li>computing power allows for more sophisitcated analyses</li>
<li>“computational” fields are ubiquitous</li>
<li>all of the above leads to the following
<ul>
<li>analysis are difficult to describe</li>
<li>inadequate training in statistics/computing and cause errors to be more easily produced through the pipeline</li>
<li>large data throughput, complex analysis, and failure to explain the process can inhibit knowledge transfer</li>
<li>results are difficult to replicate (knowledge/cost)</li>
<li>complicated analyses are not as easily trusted</li>
</ul></li>
</ul>
<h3 id="problems-with-reproducibility">Problems with Reproducibility</h3>
<ul>
<li>aiming for reproducibility provides for
<ul>
<li>transparency of data analysis
<ul>
<li>check validity of analysis <span class="math inline">\(\rightarrow\)</span> people check each other <span class="math inline">\(\rightarrow\)</span> “self-correcting” community</li>
<li>re-run the analysis</li>
<li>check the code for bugs/errors/sensitivity</li>
<li>try alternate approaches</li>
</ul></li>
<li>data/software/methods availability</li>
<li>improved transfer of knowledge
<ul>
<li>focuses on “downstream” aspect of research dissemination (does solve problems with erroneous analysis)</li>
</ul></li>
</ul></li>
<li>reproducibility does not guarantee
<ul>
<li>correctness of analysis (reproducible <span class="math inline">\(\neq\)</span> true)</li>
<li>trustworthiness of analysis</li>
<li>deterrence of bad analysis</li>
</ul></li>
</ul>
<h3 id="evidence-based-data-analysis">Evidence-based Data Analysis</h3>
<ul>
<li>create analytic pipelines from evidence-based components – standardize it (<a href="http://goo.gl/Qvlhuv" target="_blank" rel="noopener">“Deterministic Statistical Machine”</a>)
<ul>
<li>apply thoroughly-studied (via statistical research), <strong><em>mutually-agreed-upon methods</em></strong> to analyze data whenever possible</li>
<li>once pipeline is established, shouldn’t change it (“transparent box”)</li>
<li>this reduces the “researcher degrees of freedom” (restricting the ability to alter different parts of analysis pipeline)
<ul>
<li>analogous to a pre-specified clinical trial protocol</li>
</ul></li>
<li><strong><em>example</em></strong>: time series of air pollution/health
<ul>
<li><em>key question</em>: “Are short-term changes in pollution associated with short-term changes in a population health outcome?”</li>
<li>long history of statistical research investigating proper methods of analysis</li>
<li>pipeline
<ol type="1">
<li>check for outliers, high leverage, overdispersion <span class="math inline">\(\rightarrow\)</span> skewedness of data</li>
<li>Fill in missing data? <span class="math inline">\(\rightarrow\)</span> NO! (systematic missing data, imputing would add noise)</li>
<li>model selection: estimate degrees of freedom to adjust for unmeasured confounding variables</li>
<li>multiple lag analysis</li>
<li>sensitivity analysis with respect to unmeasured confounder adjustment/influential points</li>
</ol></li>
</ul></li>
</ul></li>
<li>this provides standardized, best practices for given scientific areas and questions</li>
<li>gives reviewers an important tool without dramatically increasing the burden on them</li>
<li>allows more effort to be focused on improving the quality of “upstream” aspects of scientific research</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="caching-computations">Caching Computations</h2>
<ul>
<li>Literate (Statistical) Programming
<ul>
<li>paper/code chunks <span class="math inline">\(\rightarrow\)</span> database/cached computations <span class="math inline">\(\rightarrow\)</span> published report</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/4.png&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="cacher-package"><code>cacher</code> Package</h3>
<ul>
<li>evaluates code in files and stores intermediate results in a key-value database</li>
<li>R expressions are given <strong><em>SHA-1 hash values</em></strong> so changes can be tracked and code can be re-evaluated if necessary</li>
<li><strong>cacher packages</strong>
<ul>
<li>used by authors to create data analyses packages for distribution</li>
<li>used by readers to clone analysis and inspect/evaluate subset of code/data objects <span class="math inline">\(\rightarrow\)</span> readers may not have resources/want to see entire analysis</li>
</ul></li>
<li><strong>conceptual model</strong>
<ul>
<li>dataset/code <span class="math inline">\(\rightarrow\)</span> source file <span class="math inline">\(\rightarrow\)</span> result</li>
</ul></li>
<li><strong>process for authors</strong>
<ul>
<li><code>cacher</code> package parses R source files and creates necessary cache directories/subdirectories</li>
<li>if expression = never evaluated
<ul>
<li>evaluate it and store any results R objects in cached data (similar to <code>cache = TRUE</code> function for <code>knitr</code>)</li>
</ul></li>
<li>else if cached result exists
<ul>
<li>lazy load the results from cache database and move on to next expression</li>
</ul></li>
<li>if expression <em>does not</em> create any R objects (nothing to cache)
<ul>
<li>add expression to list where evaluation needs to be forced</li>
</ul></li>
<li>write out metadata for this expression to metadata file</li>
<li><code>cachepackage</code> function creates <code>cacher</code> package storing
<ul>
<li>source file</li>
<li>cached data objects</li>
<li>metadata</li>
</ul></li>
<li>package file is zipped and can be distributed</li>
</ul></li>
<li><strong>process for readers</strong>
<ul>
<li>a journal article can say that “the code and data for this analysis can be found in the cacher package 092dcc7dda4b93e42f23e038a60e1d44dbec7b3f”
<ul>
<li>the code is the SHA-1 hash code and can be loaded/cloned using the <code>cacher</code> package</li>
</ul></li>
<li>when analysis is cloned, local directories are created, source files/metadata are downloaded
<ul>
<li>data objects are <strong><em>NOT</em></strong> downloaded by default</li>
<li>references to data objects are loaded and corresponding data can be lazy-loaded on demand
<ul>
<li>when user examines it, then it is downloaded/calculated</li>
</ul></li>
</ul></li>
<li>readers are free to examine and evaluate the code
<ul>
<li><code>clone(id = &quot;####&quot;)</code> = loads data from cache
<ul>
<li>using the first 4 characters is generally enough to identify</li>
</ul></li>
<li><code>showfiles()</code> = lists R scripts available in cache</li>
<li><code>sourcefile(&quot;name.R&quot;)</code> = loads cached R file</li>
<li><code>code()</code> = prints the content of the R file line by line</li>
<li><code>graphcode()</code> = plots a graph to demonstrate dependencies/structure of code</li>
<li><code>objectcode(&quot;object&quot;)</code> = shows lines of code that were used to generate that specific object (tracing all the way back to reading data)</li>
<li><code>runcode()</code> = executes code by loading data from cached database (much faster than regular)
<ul>
<li>by default, expressions that result in object being created are <strong><em>NOT</em></strong> run and are loaded from cached databases</li>
<li>expressions not resulting in object <strong><em>ARE</em></strong> evaluated (i.e. plots)</li>
</ul></li>
<li><code>checkcode()</code> = evaluates all expressions from scratch
<ul>
<li>results of evaluation are checked against stored results to see if they are the same</li>
<li>setting random number generators are critical for this to work</li>
</ul></li>
<li><code>checkobjects()</code> = check for integrity of data objects (i.e. see if there are possible data corruption)</li>
<li><code>loadcache()</code> = loads pointers to data objects in the data base
<ul>
<li>when a specific object is called, cache is then transferred and the data is downloaded</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(cacher)</span><br><span class="line">clonecache(id = <span class="string">"092dcc7dda4b93e42f23e038a60e1d44dbec7b3f"</span>)</span><br><span class="line">clonecache(id = <span class="string">"092d"</span>)  <span class="comment">## effectively the same as above</span></span><br><span class="line"><span class="comment"># output: created cache directory '.cache'</span></span><br><span class="line">showfiles() <span class="comment"># show files stored in cache</span></span><br><span class="line"><span class="comment"># output: [1] "top20.R"</span></span><br><span class="line">sourcefile(<span class="string">"top20.R"</span>) <span class="comment"># load R script</span></span><br><span class="line"></span><br><span class="line">code() <span class="comment"># examine the content of the code</span></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># source file: top20.R</span></span><br><span class="line"><span class="comment"># 1  cities &lt;- readLines("citylist.txt")</span></span><br><span class="line"><span class="comment"># 2  classes &lt;- readLines("colClasses.txt")</span></span><br><span class="line"><span class="comment"># 3  vars &lt;- c("date", "dow", "death",</span></span><br><span class="line"><span class="comment"># 4  data &lt;- lapply(cities, function(city) &#123;</span></span><br><span class="line"><span class="comment"># 5  names(data) &lt;- cities</span></span><br><span class="line"><span class="comment"># 6  estimates &lt;- sapply(data, function(city) &#123;</span></span><br><span class="line"><span class="comment"># 7  effect &lt;- weighted.mean(estimates[1,</span></span><br><span class="line"><span class="comment"># 8  stderr &lt;- sqrt(1/sum(1/estimates[2,</span></span><br><span class="line"></span><br><span class="line">graphcode() <span class="comment"># generate graph showing structure of code</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/5.png&quot;))</span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">objectcode(“data”)</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># source file: top20.R</span></span><br><span class="line"><span class="comment"># 1  cities &lt;- readLines("citylist.txt")</span></span><br><span class="line"><span class="comment"># 2  classes &lt;- readLines("colClasses.txt")</span></span><br><span class="line"><span class="comment"># 3  vars &lt;- c("date", "dow", "death", "tmpd", "rmtmpd", "dptp", "rmdptp", "l1pm10tmean")</span></span><br><span class="line"><span class="comment"># 4  data &lt;- lapply(cities, function(city) &#123;</span></span><br><span class="line"><span class="comment">#    		filename &lt;- file.path("data", paste(city, "csv", sep = "."))</span></span><br><span class="line"><span class="comment">#         d0 &lt;- read.csv(filename, colClasses = classes, nrow = 5200)</span></span><br><span class="line"><span class="comment"># 		d0[, vars]</span></span><br><span class="line"><span class="comment">#    &#125;)</span></span><br><span class="line"><span class="comment"># 5  names(data) &lt;- cities</span></span><br><span class="line"></span><br><span class="line">loadcache()</span><br><span class="line">ls()</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># [1] "cities"    "classes" 	"data"      "effect"</span></span><br><span class="line"><span class="comment"># [5] "estimates" "stderr" 		"vars"</span></span><br><span class="line"></span><br><span class="line">cities</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># / transferring cache db file b8fd490bcf1d48cd06...</span></span><br><span class="line"><span class="comment">#  [1] "la"   "ny"   "chic" "dlft" "hous" "phoe"</span></span><br><span class="line"><span class="comment">#  [7] "staa" "sand" "miam" "det"  "seat" "sanb"</span></span><br><span class="line"><span class="comment"># [13] "sanj" "minn" "rive" "phil" "atla" "oakl"</span></span><br><span class="line"><span class="comment"># [19] "denv" "clev"</span></span><br><span class="line"></span><br><span class="line">effect</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># / transferring cache db file 584115c69e5e2a4ae5...</span></span><br><span class="line"><span class="comment"># [1] 0.0002313219</span></span><br><span class="line"></span><br><span class="line">stderr</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># / transferring cache db file 81b6dc23736f3d72c6...</span></span><br><span class="line"><span class="comment"># [1] 0.000052457</span></span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="case-study-air-pollution">Case Study: Air Pollution</h2>
<ul>
<li>reanalysis of data from MMAPS and link with PM chemical constituent data
<ul>
<li>Lippmann <em>et al.</em> found strong evidence that Ni modified the short-term effect of <span class="math inline">\(PM_{10}\)</span> across 60 US communities</li>
<li>National Morbidity, Mortality, and Air Pollution Study (NMMAPS) = national study of the short-term health effects of ambient air pollution
<ul>
<li>focused primarily on particulate matter (<span class="math inline">\(PM_{10}\)</span>) and ozone (<span class="math inline">\(O_3\)</span>)</li>
<li>health outcomes included mortality from all causes and hospitalizations for cardiovascular and respiratory diseases</li>
<li>Data made available at the Internet-based Health and Air Pollution Surveillance System (iHAPSS)</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/6.png&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="analysis-does-nickel-make-pm-toxic">Analysis: Does Nickel Make PM Toxic?</h3>
<ul>
<li>Long-term average nickel concentrations appear correlated with PM risk (<span class="math inline">\(p &lt; 0.01\)</span> <span class="math inline">\(\rightarrow\)</span> statistically significant)</li>
<li>there appear to be some outliers on the right-hand side (New York City)
<ul>
<li>adjusting the data by removing the New York counties altered the regression line (shown in blue)</li>
<li>though the relationship is still positive, regression line no longer statistically significant (<span class="math inline">\(p &lt; 0.31\)</span>)</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/7.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>sensitivity analysis shows that the regression line is particularly sensative to the New York data</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/8.png&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="conclusions-and-lessons-learnt">Conclusions and Lessons Learnt</h3>
<ul>
<li>New York havs very high levels of nickel and vanadium, much higher than any other US community</li>
<li>there is evidence of a positive relationship between Ni concentrations and <span class="math inline">\(PM_{10}\)</span> risk</li>
<li>strength of this relationship is highly sensitive to the observations from New York City</li>
<li>most of the information in the data is derived from just 3 observations</li>
<li>reproducibility of NMMAPS allowed for a secondary analysis (and linking with PM chemical constituent data) investigating a novel hypothesis (Lippmann <em>et al.</em>), as well as a critique of that analysis and new findings (Dominici <em>et al.</em>)</li>
<li>original hypothesis not necessarily invalidated, but evidence not as strong as originally suggested (more work should be done)</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="case-study-high-throughput-biology">Case Study: High Throughput Biology</h2>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/9.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>Potti <em>et al.</em> published in 2006 that microarray data from cell lines (the NCI60) can be used to define drug response “signatures”, which can be used to predict whether patients will respond</li>
<li>however, analysis performed were fundamentally flawed as there exist much misclassification and mishandling of data and were unreproducible</li>
<li>the results of their study, even after being corrected twice, still contained many errors and were unfortunately used as guidelines for clinical trials</li>
<li>the fiasco with this paper and associated research, though spectacular in its own light, was by no means an unique occurrence</li>
</ul>
<h3 id="conclusions-and-lessons-learnt-1">Conclusions and Lessons Learnt</h3>
<ul>
<li>most common mistakes are simple
<ul>
<li>confounding in the experimental design</li>
<li>mixing up the sample labels</li>
<li>mixing up the gene labels</li>
<li>mixing up the group labels</li>
</ul></li>
<li>most mixups involve simple switches or offsets
<ul>
<li>this simplicity is often hidden due to incomplete documentation</li>
</ul></li>
<li>research papers should include the following (particularly those that would potentially lead to clinical trials)
<ul>
<li>data (with column names)</li>
<li>provenance (who owns what data/which sample is which)</li>
<li>code</li>
<li>descriptions of non-scriptable steps</li>
<li>descriptions of planned design (if used)</li>
</ul></li>
<li><strong>reproducible research</strong> is the key to minimize these errors
<ul>
<li>literate programming</li>
<li>reusing templates</li>
<li>report structure</li>
<li>executive summaries</li>
<li>appendices (sessionInfo, saves, file location)</li>
</ul></li>
</ul>

          
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Exploratory-Data-Analysis/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/07/Exploratory-Data-Analysis/" itemprop="url">
                  Exploratory Data Analysis
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 14:01:01 / Modified: 14:02:45" itemprop="dateCreated datePublished" datetime="2018-08-07T14:01:01+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">52k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">48 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="principle-of-analytic-graphics">Principle of Analytic Graphics</h2>
<ul>
<li><strong>Principle 1: Show Comparisons</strong>
<ul>
<li>always comparative (compared to what)</li>
<li>randomized trial - compare control group to test group</li>
<li>evidence for a hypothesis is always relative to another competing hypothesis</li>
</ul></li>
<li><strong>Principle 2: Show causality/mechanism/explanation/systematic structure</strong>
<ul>
<li>form hypothesis to evidence showing a relationship (causal framework, why something happened)</li>
</ul></li>
<li><strong>Principle 3: Show multivariate data</strong>
<ul>
<li>more than 2 variables because the real world is multivariate</li>
<li>show as much data on a plot as you can</li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># install grid and png packages if not present</span><br><span class="line">library(png)</span><br><span class="line">library(grid)</span><br><span class="line">grid.raster(readPNG(&quot;figures/1.jpg&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>slightly negative relationship between pollution and mortality
<ul>
<li>when split up by season, the relationships are all positive <span class="math inline">\(\rightarrow\)</span> season = confounding variable</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/2.jpg&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Principle 4: Integration of evidence</strong>
<ul>
<li>use as many modes of evidence/displaying evidence as possible (modes of data presentation)</li>
<li>integrate words/numbers/images/diagrams (information rich)</li>
<li>analysis should drive the tool</li>
</ul></li>
<li><strong>Principle 5: Describe/document evidence with appropriate labels/scales/sources</strong>
<ul>
<li>add credibility to that data graphic</li>
</ul></li>
<li><strong>Principle 6: Content is the most important</strong>
<ul>
<li>analytical presentations ultimately stand/fall depending on quality/relevance/integrity of content</li>
</ul></li>
</ul>
<h2 id="exploratory-graphs-examples">Exploratory Graphs (<a href="http://gallery.r-enthusiasts.com/" target="_blank" rel="noopener">examples</a>)</h2>
<ul>
<li><strong>Purpose</strong>: understand data properties, find pattern in data, suggest modeling strategies, debug</li>
<li><strong>Characteristics</strong>: made quickly, large number produced, gain personal understanding, appearances and presentation are aren’t as important</li>
</ul>
<h3 id="one-dimension-summary-of-data">One Dimension Summary of Data</h3>
<ul>
<li><code>summary(data)</code> = returns min, 1st quartile, median, mean, 3rd quartile, max</li>
<li><code>boxplot(data, col = “blue”)</code> = produces a box with middles 50% highlighted in the specified color
<ul>
<li><code>whiskers</code> = <span class="math inline">\(\pm 1.58IQR/\sqrt{n}\)</span>
<ul>
<li>IQR = interquartile range, Q<span class="math inline">\(_3\)</span> - Q<span class="math inline">\(_1\)</span></li>
</ul></li>
<li><code>box</code> = 25%, median, 75%</li>
</ul></li>
<li><code>histograms(data, col = “green”)</code> = produces a histogram with specified breaks and color
<ul>
<li><code>breaks = 100</code> = the higher the number is the smaller/narrower the histogram columns are</li>
</ul></li>
<li><code>rug(data)</code> = density plot, add a strip under the histogram indicating location of each data point</li>
<li><p><code>barplot(data, col = wheat)</code> = produces a bar graph, usually for categorical data</p></li>
<li><strong>Overlaying Features</strong></li>
<li><code>abline(h/v = 12)</code> = overlays horizontal/vertical line at specified location
<ul>
<li><code>col = “red”</code> = specifies color</li>
<li><code>lwd = 4</code> = line width</li>
<li><code>lty = 2</code> = line type</li>
</ul></li>
</ul>
<h3 id="two-dimensional-summaries">Two Dimensional Summaries</h3>
<ul>
<li>multiple/overlay 1D plots (using lattice/ggplot2)</li>
<li><strong>box plots</strong>: <code>boxplot(pm25 ~ region, data = pollution, col = “red”)</code></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/3.jpg&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>histogram</strong>:
<ul>
<li><code>par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))</code> = set margin</li>
<li><code>hist(subset(pollution, region == &quot;east&quot;)$pm25, col = &quot;green&quot;)</code> = first histogram</li>
<li><code>hist(subset(pollution, region == &quot;west&quot;)$pm25, col = &quot;green&quot;)</code> = second histogram</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/4.jpg&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>scatterplot</strong>
<ul>
<li><code>with(pollution, plot(latitude, pm25, col = region))</code></li>
<li><code>abline(h = 12, lwd = 2, lty = 2)</code> = plots horizontal dotted line</li>
<li><code>plot(jitter(child, 4)~parent, galton)</code> = spreads out data points at the same position to simulate measurement error/make high frequency more visibble</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/5.jpg&quot;))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library(UsingR); data(galton)</span><br><span class="line">plot(jitter(child, 4)~parent, galton)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>multiple scatter plots</strong>
<ul>
<li><code>par(mfrow = c(1, 2), mar = c(5, 4, 2, 1))</code> = sets margins</li>
<li><code>with(subset(pollution, region == &quot;west&quot;), plot(latitude, pm25, main = &quot;West&quot;))</code> = left scatterplot</li>
<li><code>with(subset(pollution, region == &quot;east&quot;), plot(latitude,  pm25, main = &quot;East&quot;))</code> = right scatterplot</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/6.jpg&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="process-of-making-a-plotconsiderations">Process of Making a Plot/Considerations</h3>
<ul>
<li>where will plot be made? screen or file?</li>
<li>how will plot be used? viewing on screen/web browser/print/presentation?</li>
<li>large amount of data vs few points?</li>
<li>need to be able to dynamically resize?</li>
<li><strong>plotting system</strong>: base, lattice, ggplot2?</li>
</ul>
<h2 id="base-plotting">Base Plotting</h2>
<ul>
<li>blank canvas, “artist’s palette”, start with plot function</li>
<li>annotations - text, lines, points, axis</li>
<li>convenient, but cannot go back when started (need to plan ahead)</li>
<li>everything need to be manually set carefully to be able to achieve the desired effect (margins)</li>
<li>core plotting/graphics engine in R encapsulated in the following
<ul>
<li><strong><em>graphics</em></strong>: plotting functions for vase graphing system (plot, hist, boxplot, text)</li>
<li><strong><em>grDevices</em></strong>: contains all the code implementing the various graphics devices (x11, PDF, PostScript, PNG, etc)</li>
</ul></li>
<li><strong><em>Two phase</em></strong>: initialize, annotate</li>
<li>calling <code>plot(x, y)</code> or <code>hist(x)</code> will launch a graphics device and draw a plot on device
<ul>
<li>if no argument specified, default called</li>
<li>parameters documented in “<code>?par</code>”</li>
<li><em><strong>Note</strong>: it is some times necessary to convert column/variable to factor to make plotting easier </em>
<ul>
<li><code>airquality &lt;- transform(airquality, Month = factor(month))</code></li>
</ul></li>
</ul></li>
</ul>
<h3 id="base-graphics-functions-and-parameters">Base Graphics Functions and Parameters</h3>
<ul>
<li><strong>arguments</strong>
<ul>
<li><code>pch</code>: plotting symbol (default = open circle)</li>
<li><code>lty</code>: line type (default is solid)
<ul>
<li>0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash</li>
</ul></li>
<li><code>lwd</code>: line width (integer)</li>
<li><code>col</code>: plotting color (number string or hexcode, colors() returns vector of colors)</li>
<li><code>xlab</code>, <code>ylab</code>: x-y label character strings</li>
<li><code>cex</code>: numerical value giving the amount by which plotting text/symbols should be magnified relative to the default
<ul>
<li><code>cex = 0.15 * variable</code>: plot size as an additional variable</li>
</ul></li>
</ul></li>
<li><code>par()</code> function = specifies global graphics parameters, affects all plots in an R session (can be overridden)
<ul>
<li><code>las</code>: orientation of axis labels</li>
<li><code>bg</code>: background color</li>
<li><code>mar</code>: margin size (order = bottom left top right)</li>
<li><code>oma</code>: outer margin size (default = 0 for all sides)</li>
<li><code>mfrow</code>: number of plots per row, column (plots are filled row-wise)</li>
<li><code>mfcol</code>: number of plots per row, column (plots are filled column-wise)</li>
<li>can verify all above parameters by calling <code>par(&quot;parameter&quot;)</code></li>
</ul></li>
<li><strong>plotting functions</strong>
<ul>
<li><code>lines</code>: adds liens to a plot, given a vector of x values and corresponding vector of y values</li>
<li><code>points</code>: adds a point to the plot</li>
<li><code>text</code>: add text labels to a plot using specified x,y coordinates</li>
<li><code>title</code>: add annotations to x,y axis labels, title, subtitles, outer margin</li>
<li><code>mtext</code>: add arbitrary text to margins (inner or outer) of plot</li>
<li><code>axis</code>: specify axis ticks</li>
</ul></li>
</ul>
<h3 id="base-plot-example">Base Plot Example</h3>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">library(datasets)</span><br><span class="line"># type =“n” sets up the plot and does not fill it with data</span><br><span class="line">with(airquality, plot(Wind, Ozone, main = &quot;Ozone and Wind in New York City&quot;, type = &quot;n&quot;))</span><br><span class="line"># subsets of data are plotted here using different colors</span><br><span class="line">with(subset(airquality, Month == 5), points(Wind, Ozone, col = &quot;blue&quot;))</span><br><span class="line">with(subset(airquality, Month != 5), points(Wind, Ozone, col = &quot;red&quot;))</span><br><span class="line">legend(&quot;topright&quot;, pch = 1, col = c(&quot;blue&quot;, &quot;red&quot;), legend = c(&quot;May&quot;, &quot;Other Months&quot;))</span><br><span class="line">model &lt;- lm(Ozone ~ Wind, airquality)</span><br><span class="line"># regression line is produced here</span><br><span class="line">abline(model, lwd = 2)</span><br></pre></td></tr></table></figure>
<h3 id="multiple-plot-example">Multiple Plot Example</h3>
<ul>
<li><em><strong>Note</strong>: typing <code>example(points)</code> in R will launch a demo of base plotting system and may provide some helpful tips on graphing </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># this expression sets up a plot with 1 row 3 columns, sets the margin and outer margins</span><br><span class="line">par(mfrow = c(1, 3), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))</span><br><span class="line">with(airquality, &#123;</span><br><span class="line">	# here three plots are filled in with their respective titles</span><br><span class="line">	plot(Wind, Ozone, main = &quot;Ozone and Wind&quot;)</span><br><span class="line">	plot(Solar.R, Ozone, main = &quot;Ozone and Solar Radiation&quot;)</span><br><span class="line">	plot(Temp, Ozone, main = &quot;Ozone and Temperature&quot;)</span><br><span class="line">	# this adds a line of text in the outer margin*</span><br><span class="line">	mtext(&quot;Ozone and Weather in New York City&quot;, outer = TRUE)&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="graphics-device">Graphics Device</h2>
<ul>
<li>A graphics device is something where you can make a plot appear
<ul>
<li><strong>window on screen</strong> (screen device) = quick visualizations and exploratory analysis</li>
<li><strong>pdf</strong> (file device) = plots that may be printed out or incorporated in to document</li>
<li><strong>PNG/JPEG</strong> (file device) = plots that may be printed out or incorporated in to document</li>
<li><strong>scalable vector graphics</strong> (SVG)</li>
</ul></li>
<li>When a plot is created in R, it has to be sent to a graphics device</li>
<li><strong>Most common is screen device</strong>
<ul>
<li><code>quartz()</code> on Mac, <code>windows()</code> on Windows, <code>x11()</code> on Unix/Linux</li>
<li><code>?Devices</code> = lists devices found</li>
</ul></li>
<li><strong>Plot creation</strong>
<ul>
<li>screen device
<ul>
<li>call plot/xplot/qplot <span class="math inline">\(\rightarrow\)</span> plot appears on screen device <span class="math inline">\(\rightarrow\)</span> annotate as necessary <span class="math inline">\(\rightarrow\)</span> use</li>
</ul></li>
<li>file devices
<ul>
<li>explicitly call graphics device <span class="math inline">\(\rightarrow\)</span> plotting function to make plot (write to file) <span class="math inline">\(\rightarrow\)</span> annotate as necessary <span class="math inline">\(\rightarrow\)</span> explicitly close graphics device with <code>dev.off()</code></li>
</ul></li>
</ul></li>
<li><strong>Graphics File Devices</strong>
<ul>
<li><strong><em>Vector Formats</em></strong> (good for line drawings/plots w/ solid colors, a modest number of points)
<ul>
<li><code>pdf</code>: useful for line type graphics, resizes well, usually portable, not efficient if too many points</li>
<li><code>svg</code>: XML based scalable vector graphics, support animation and interactivity, web based</li>
<li><code>win.metafile</code>: Windows metafile format</li>
<li><code>postscript</code>: older format, resizes well, usually portable, can create encapsulated postscript file, Windows often don’t have postscript viewer (postscript = predecessor of PDF)</li>
</ul></li>
<li><strong><em>Bitmap Formats</em></strong> (good for plots w/ large number of points, natural scenes/webbased plots)
<ul>
<li><code>png</code>: Portable Network Graphics, good for line drawings/image with solid colors, uses lossless compression, most web browsers read this natively, good for plotting a lot of data points, does not resize well</li>
<li><code>JPEG</code>: good for photographs/natural scenes/gradient colors, size efficient, uses lossy compression, good for plotting many points, does not resize well, can be read by almost any computer/browser, not great for line drawings (aliasing on edges)</li>
<li><code>tiff</code>: common bitmap format supports lossless compression</li>
<li><code>bmp</code>: native Windows bitmapped format</li>
</ul></li>
</ul></li>
<li><strong>Multiple Open Graphics Devices</strong>
<ul>
<li>possible to open multiple graphics devices (screen, file, or both)</li>
<li>plotting occurs only one device at a time</li>
<li><code>dev.cur()</code> = returns the currently active device</li>
<li>every open graphics device is assigned an integer &gt;= 2</li>
<li><code>dev.set(&lt;integer&gt;)</code> = change the active graphics device <code>&lt;integer&gt;</code> = number associated with the graphics device you want to switch to</li>
</ul></li>
<li><strong>Copying plots</strong>
<ul>
<li><code>dev.copy()</code> = copy a plot from one device to another</li>
<li><code>dev.copy2pdf()</code> = specifically for copying to PDF files</li>
<li><em><strong>Note</strong>: copying a plot is not an exact operation, so the result may not be identical to the original </em></li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">## Create plot on screen device</span><br><span class="line">with(faithful, plot(eruptions, waiting))</span><br><span class="line">## Add a main title</span><br><span class="line">title(main = &quot;Old Faithful Geyser data&quot;)</span><br><span class="line">## Copy my plot to a PNG file</span><br><span class="line">dev.copy(png, file = &quot;geyserplot.png&quot;)</span><br><span class="line">## Don&apos;t forget to close the PNG device!</span><br><span class="line">dev.off()</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="lattice-plotting-system"><code>lattice</code> Plotting System</h2>
<ul>
<li><code>library(lattice)</code> = load lattice system</li>
<li>implemented using the <code>lattice</code> and <code>grid</code> packages
<ul>
<li><code>lattice</code> package = contains code for producing <strong><em>Trellis</em></strong> graphics (independent from base graphics system)</li>
<li><code>grid</code> package = implements the graphing system; lattice build on top of grid</li>
</ul></li>
<li>all plotting and annotation is done with <strong><em>single function call</em></strong>
<ul>
<li>margins/spacing/labels set automatically for entire plot, good for putting multiple on the screen</li>
<li>good for conditioning plots <span class="math inline">\(\rightarrow\)</span> examining same plots over different conditions how y changes vs x across different levels of z</li>
<li><code>panel</code> functions can be specified/customized to modify the subplots</li>
</ul></li>
<li>lattice graphics functions return an object of class “trellis”, where as base graphics functions plot data directly to graphics device
<ul>
<li>print methods for lattice functions actually plots the data on graphics device</li>
<li>trellis objects are auto-printed</li>
<li><code>trellis.par.set()</code> <span class="math inline">\(\rightarrow\)</span> can be used to set global graphic parameters for all trellis objects</li>
</ul></li>
<li>hard to annotate, awkward to specify entire plot in one function call</li>
<li>cannot add to plot once created, panel/subscript functions hard to prepare</li>
</ul>
<h3 id="lattice-functions-and-parameters"><code>lattice</code> Functions and Parameters</h3>
<ul>
<li><strong>Funtions</strong>
<ul>
<li><code>xyplot()</code> = main function for creating scatterplots</li>
<li><code>bwplot()</code> = box and whiskers plots (box plots)</li>
<li><code>histogram()</code> = histograms</li>
<li><code>stripplot()</code> = box plot with actual points</li>
<li><code>dotplot()</code> = plot dots on “violin strings”</li>
<li><code>splom()</code> = scatterplot matrix (like pairs() in base plotting system)</li>
<li><code>levelplot()</code>/<code>contourplot()</code> = plotting image data</li>
</ul></li>
<li><strong>Arguments</strong> for <code>xyplot(y ~ x | f * g, data, layout, panel)</code>
<ul>
<li>default blue open circles for data points</li>
<li>formula notation is used here (<code>~</code>) = left hand side is the y-axis variable, and the right hand side is the x-axis variable</li>
<li><code>f</code>/<code>g</code> = conditioning/categorical variables (optional)
<ul>
<li>basically creates multi-panelled plots (for different factor levels)</li>
<li><code>*</code> indicates interaction between two variables</li>
<li>intuitively, the xyplot displays a graph between x and y for every level of <code>f</code> and <code>g</code></li>
</ul></li>
<li><code>data</code> = the data frame/list from which the variables should be looked up
<ul>
<li>if nothing is passed, the parent frame is used (searching for variables in the workspace)</li>
<li>if no other arguments are passed, defaults will be used</li>
</ul></li>
<li><code>layout</code> = specifies how the different plots will appear
<ul>
<li><code>layout = c(5, 1)</code> = produces 5 subplots in a horizontal fashion</li>
<li>padding/spacing/margin automatically set</li>
</ul></li>
<li>[optional] <code>panel</code> function can be added to control what is plotted inside each panel of the plot
<ul>
<li><code>panel</code> functions receive x/y coordinates of the data points in their panel (along with any additional arguments)</li>
<li><code>?panel.xyplot</code> = brings up documentation for the panel functions</li>
<li><em><strong>Note</strong>: no base plot functions can be used for lattice plots </em></li>
</ul></li>
</ul></li>
</ul>
<h3 id="lattice-example"><code>lattice</code> Example</h3>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">library(lattice)</span><br><span class="line">set.seed(10)</span><br><span class="line">x &lt;- rnorm(100)</span><br><span class="line">f &lt;- rep(0:1, each = 50)</span><br><span class="line">y &lt;- x + f - f * x+ rnorm(100, sd = 0.5)</span><br><span class="line">f &lt;- factor(f, labels = c(&quot;Group 1&quot;, &quot;Group 2&quot;))</span><br><span class="line">## Plot with 2 panels with custom panel function</span><br><span class="line">xyplot(y ~ x | f, panel = function(x, y, ...) &#123;</span><br><span class="line">	# call the default panel function for xyplot</span><br><span class="line">	panel.xyplot(x, y, ...)</span><br><span class="line">	# adds a horizontal line at the median</span><br><span class="line">	panel.abline(h = median(y), lty = 2)</span><br><span class="line">	# overlays a simple linear regression line</span><br><span class="line">	panel.lmline(x, y, col = 2)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="ggplot2-plotting-system"><code>ggplot2</code> Plotting System</h2>
<ul>
<li><code>library(ggplot2)</code> = loads ggplot2 package</li>
<li>implementation of Grammar of Graphics by Leland Wilkinson, written by Hadley Wickham (created RStudio)</li>
</ul>
<blockquote>
<p>“In brief, the grammar tells us that a statistical graphic is a mapping from data to aesthetic attributes (color, shape, size) of geometric objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinate system”</p>
</blockquote>
<ul>
<li>grammar graphics plot, splits the different between base and lattice systems</li>
<li>automatically sets spacings/text/tiles but also allows annotations to be added</li>
<li>default makes a lot of choices, but still customizable</li>
</ul>
<h3 id="ggplot2-functions-and-parameters"><code>ggplot2</code> Functions and Parameters</h3>
<ul>
<li><strong>basic components</strong> of a <code>ggplot2</code> graphic
<ul>
<li><strong>data frame</strong> = source of data</li>
<li><strong>aesthetic mappings</strong> = how data are mappped to color/size (x vs y)</li>
<li><strong>geoms</strong> = geometric objects like points/lines/shapes to put on page</li>
<li><strong>facets</strong> = conditional plots using factor variables/multiple panels</li>
<li><strong>stats</strong> = statistical transformations like binning/quantiles/smoothing</li>
<li><strong>scales</strong> = scale aesthetic map uses (i.e. male = red, female = blue)</li>
<li><strong>coordinate system</strong> = system in which data are plotted</li>
</ul></li>
<li><code>qplot(x, y, data , color, geom)</code> = quick plot, analogous to base system’s <code>plot()</code> function
<ul>
<li><strong>default style</strong>: gray background, white gridlines, x and y labels automatic, and solid black circles for data points</li>
<li>data always comes from data frame (in unspecified, function will look for data in workspace)</li>
<li>plots are made up of aesthetics (size, shape, color) and geoms (points, lines)</li>
<li><em><strong>Note</strong>: capable of producing quick graphics, but difficult to customize in detail</em></li>
</ul></li>
<li><strong>factor variables</strong>: important for graphing subsets of data = they should be labelled with specific information, and not just 1, 2, 3
<ul>
<li><code>color = factor1</code> = use the factor variable to display subsets of data in different colors on the same plot (legend automatically generated)</li>
<li><code>shape = factor2</code> = use the factor variable to display subsets of data in different shapes on the same plot (legend automatically generated)</li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">qplot(displ, hwy, data = mpg, color = drv, shape = drv)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>adding statistics</strong>: <code>geom = c(&quot;points&quot;, &quot;smooth&quot;)</code> = add a smoother/“low S”
<ul>
<li>“points” plots the data themselves, “smooth” plots a smooth mean line in blue with an area of 95% confidence interval shaded in dark gray</li>
<li><code>method = &quot;lm&quot;</code> = additional argument method can be specified to create different lines/confidence intervals
<ul>
<li><code>lm</code> = linear regression</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qplot(displ, hwy, data = mpg, geom = c(&quot;point&quot;, &quot;smooth&quot;), method=&quot;lm&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>histograms</strong>: if only one value is specified, a histogram is produced
<ul>
<li><code>fill = factor1</code> = can be used to fill the histogram with different colors for the subsets (legend automatically generated)</li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qplot(hwy, data = mpg, fill = drv)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>facets</strong>: similar to panels in lattice, split data according to factor variables
<ul>
<li><code>facets = rows ~ columns</code> = produce different subplots by factor variables specified (rows/columns)</li>
<li><code>&quot;.&quot;</code> indicates there are no addition row or column</li>
<li><code>facets = . ~ columns</code> = creates 1 by col subplots</li>
<li><code>facets = row ~ .</code> = creates row row by 1 subplots</li>
<li>labels get generated automatically based on factor variable values</li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">qplot(displ, hwy, data = mpg, facets = . ~ drv)</span><br><span class="line">qplot(hwy, data = mpg, facets = drv ~ ., binwidth = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>density smooth</strong>: smooths the histograms into a line tracing its shape
<ul>
<li><code>geom = &quot;density&quot;</code> = replaces the default scatterplot with density smooth curve</li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/16.jpg&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>ggplot()</code></strong>
<ul>
<li>built up in layers/modularly (similar to base plotting system)
<ul>
<li>data <span class="math inline">\(\rightarrow\)</span> overlay summary <span class="math inline">\(\rightarrow\)</span> metadata/annotation</li>
</ul></li>
<li><code>g &lt;- ggplot(data, aes(var1, var2))</code>
<ul>
<li>initiates call to <code>ggplot</code> and specifies the data frame that will be used</li>
<li><code>aes(var1, var2)</code> = specifies aesthetic mapping, or var1 = x variable, and var2 = y variable</li>
<li><code>summary(g)</code> = displays summary of ggplot object</li>
<li><code>print(g)</code> = returns error (“no layer on plot”) which means the plot does know how to draw the data yet</li>
</ul></li>
<li><code>g + geom_point()</code> = takes information from g object and produces scatter plot</li>
<li><code>+ geom_smooth()</code> = adds low S mean curve with confidence interval
<ul>
<li><code>method = &quot;lm&quot;</code> = changes the smooth curve to be linear regression</li>
<li><code>size = 4</code>, <code>linetype = 3</code> = can be specified to change the size/style of the line</li>
<li><code>se = FALSE</code> = turns off confidence interval</li>
</ul></li>
<li><code>+ facet_grid(row ~ col)</code> = splits data into subplots by factor variables (see facets from <code>qplot()</code>)
<ul>
<li>conditioning on continous variables is possible through cutting/making a new categorical variable</li>
<li><code>cutPts &lt;- quantiles(df$cVar, seq(0, 1, length=4), na.rm = TRUE)</code> = creates quantiles where the continuous variable will be cut
<ul>
<li><code>seq(0, 1, length=4)</code> = creates 4 quantile points</li>
<li><code>na.rm = TRUE</code> = removes all NA values</li>
</ul></li>
<li><code>df$newFactor &lt;- cut(df$cVar, cutPts)</code> = creates new categorical/factor variable by using the cutpoints
<ul>
<li>creates n-1 ranges from n points = in this case 3</li>
</ul></li>
</ul></li>
<li><strong><em>annotations</em></strong>:
<ul>
<li><code>xlab()</code>, <code>ylab()</code>, <code>labs()</code>, <code>ggtitle()</code> = for labels and titles
<ul>
<li><code>labs(x = expression(&quot;log &quot; * PM[2.5]), y = &quot;Nocturnal&quot;)</code> = specifies x and y labels</li>
<li><code>expression()</code> = used to produce mathematical expressions</li>
</ul></li>
<li><code>geom</code> functions = many options to modify</li>
<li><code>theme()</code> = for global changes in presentation
<ul>
<li><strong><em>example</em></strong>: <code>theme(legend.position = &quot;none&quot;)</code></li>
</ul></li>
<li>two standard themes defined: <code>theme_gray()</code> and <code>theme_bw()</code></li>
<li><code>base_family = &quot;Times&quot;</code> = changes font to Times</li>
</ul></li>
<li><strong><em>aesthetics</em></strong>
<ul>
<li><code>+ geom_point(color, size, alpha)</code> = specifies how the points are supposed to be plotted on the graph (style)
<ul>
<li><em><strong>Note</strong>: this translates to geom_line()/other forms of plots </em></li>
<li><code>color = &quot;steelblue&quot;</code> = specifies color of the data points</li>
<li><code>aes(color = var1)</code> = wrapping color argument this way allows a factor variable to be assigned to the data points, thus subsetting it with different colors based on factor variable values</li>
<li><code>size = 4</code> = specifies size of the data points</li>
<li><code>alpha = 0.5</code> = specifies transparency of the data points</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure>
<img src="figures/17.jpg" alt="Alpha Level"><figcaption>Alpha Level</figcaption>
</figure>
<ul>
<li><strong><em>axis limits</em></strong>
<ul>
<li><code>+ ylim(-3, 3)</code> = limits the range of y variable to a specific range
<ul>
<li><em><strong>Note</strong>: ggplot will exclude (not plot) points that fall outside of this range (outliers), potentially leaving gaps in plot </em></li>
</ul></li>
<li><code>+ coord_cartesian(ylim(-3, 3))</code> = this will limit the visible range but plot all points of the data</li>
</ul></li>
</ul></li>
</ul>
<h3 id="ggplot2-comprehensive-example"><code>ggplot2</code> Comprehensive Example</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># initiates ggplot</span></span><br><span class="line">g &lt;- ggplot(maacs, aes(logpm25, NocturnalSympt))</span><br><span class="line">g + geom_point(alpha = <span class="number">1</span>/<span class="number">3</span>) 					        <span class="comment"># adds points</span></span><br><span class="line">  + facet_wrap(bmicat ~ no2dec, nrow = <span class="number">2</span>, ncol = <span class="number">4</span>)     <span class="comment"># make panels</span></span><br><span class="line">  + geom_smooth(method=<span class="string">"lm"</span>, se=<span class="literal">FALSE</span>, col=<span class="string">"steelblue"</span>) <span class="comment"># adds smoother</span></span><br><span class="line">  + theme_bw(base_family = <span class="string">"Avenir"</span>, base_size = <span class="number">10</span>)    <span class="comment"># change theme</span></span><br><span class="line">  + labs(x = expression(<span class="string">"log "</span> * PM[<span class="number">2.5</span>])   		    <span class="comment"># add labels</span></span><br><span class="line">  + labs(y = <span class="string">"Nocturnal Symptoms”)</span></span><br><span class="line"><span class="string">  + labs(title = "</span>MAACS Cohort”)</span><br></pre></td></tr></table></figure>
<figure>
<img src="figures/18.jpg" alt="Final Plot"><figcaption>Final Plot</figcaption>
</figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="hierarchical-clustering">Hierarchical Clustering</h2>
<ul>
<li>useful for visualizing high dimensional data, organizes things that are close into groups</li>
<li><strong>agglomerative approach</strong> (most common) — bottom up
<ol type="1">
<li>start with data</li>
<li>find closest pairs, put them together (create “super point” and remove original data)</li>
<li>find the next closest</li>
<li>repeat = yields a tree showing order of merging (dendrogram)</li>
</ol>
<ul>
<li>requires
<ul>
<li><strong><em>merging approach</em></strong>: how to merge two points</li>
<li><strong><em>distance metric</em></strong>: calculating distance between two points</li>
<li><strong>continuous</strong> - <em>Euclidean distance</em> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\sqrt{(A_1 - A_2)^2 + (B_1 - B_2)^2 + \dots + (Z_1 -Z_2)^2 }\)</span></li>
<li><strong>continuous</strong> - <em>correlation similarity</em> <span class="math inline">\(\rightarrow\)</span> how correlated two data points are</li>
<li><strong>binary</strong> - <em>Manhattan distance</em> (“city block distance”) <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(|A_1 - A_2| + |B_1 - B_2| + \dots + |Z_1 -Z_2|\)</span></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/25.png&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="procedure-for-constructing-hierarchical-clusters-hclust-function">Procedure for Constructing Hierarchical Clusters (<code>hclust</code> function)</h3>
<ol type="1">
<li>calculate all pair wise distances between all points to see which points are closest together
<ul>
<li><code>dist(data.frame(x=x, y=y)</code> = returns pair wise distances for all of the (x,y) coordinates</li>
<li><em><strong>Note</strong>: <code>dist()</code> function uses Euclidean distance by default </em></li>
</ul></li>
<li>group two closest points from the calculated distances and merge them to a single point</li>
<li>find the next two closest points and merge them, and repeat</li>
<li>order of clustering is shown in the dendrogram</li>
</ol>
<h3 id="approaches-for-merging-pointsclusters">Approaches for Merging Points/Clusters</h3>
<ul>
<li>the approach is specified in the argument <code>method = &quot;complete&quot;</code> or <code>&quot;average&quot;</code> in <code>hclust()</code> function</li>
<li><strong><em>average linkage</em></strong> = taking average of the x and y coordinates for both points/clusters (center of mass effectively)</li>
<li><strong><em>complete linkage</em></strong> = to measure distance of two clusters, take the two points in the clusters that are the furthest apart</li>
<li><em><strong>Note</strong>: two approaches may produce different results so it’s a good idea to use both approaches to validate results </em></li>
</ul>
<h3 id="characteristics-of-hierarchical-clustering-algorithms">Characteristics of Hierarchical Clustering Algorithms</h3>
<ul>
<li>clustering result/plot maybe <strong><em>unstable</em></strong>
<ul>
<li>changing few points/outliers could lead to large changes</li>
<li>change different distance metrics to see how sensitive the clustering is</li>
<li>change merging strategy</li>
<li>scaling of variables could affect the clustering (if one unit/measurement is much larger than another)</li>
</ul></li>
<li><strong><em>deterministic</em></strong> = running the <code>hclust</code> function with same parameters and the same data will produce the same plot</li>
<li>determining how many clusters there are (where to cut) may <strong><em>not always be clear</em></strong></li>
<li><strong><em>primarily used for exploratory data analysis</em></strong>, to see over all pattern in data if there is any at all</li>
</ul>
<h3 id="hclust-function-and-example"><code>hclust</code> Function and Example</h3>
<ul>
<li><code>hh &lt;- hclust(dist(dataFrame))</code> function = produces a hierarchical cluster object based on pair wise distances from a data frame of x and y values
<ul>
<li><code>dist()</code> = defaults to Euclidean, calculates the distance/similarity between two observations; when applied to a data frame, the function applies the <span class="math inline">\(\sqrt{(A_1 - A_2)^2 + (B_1 - B_2)^2 + ... + (Z_1 -Z_2)^2 }\)</span> formula to every pair of rows of data to construct a matrix of distances between the roes
<ul>
<li>order of the hierarchical cluster is derived from the distance</li>
</ul></li>
<li><code>plot(hh)</code> = plots the dendrogram</li>
<li>automatically sorts column and row according to cluster</li>
<li><code>names(hh)</code> = returns all parameters of the <code>hclust</code> object
<ul>
<li><code>hh$order</code> = returns the order of the rows/clusters from the dendrogram</li>
<li><code>hh$dist.method</code> = returns method for calculating distance/similarity</li>
</ul></li>
</ul></li>
<li><em><strong>Note</strong>: dendrogram that gets generated <strong>DOES NOT</strong> show how many clusters there are, so cutting (at 2.0 level for example) must be done to determine number of clusters — must be a convenient and sensible point </em></li>
<li><strong><code>hclust</code> Example</strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set.seed(1234)</span><br><span class="line">x &lt;- rnorm(12,mean=rep(1:3,each=4),sd=0.2)</span><br><span class="line">y &lt;- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)</span><br><span class="line">dataFrame &lt;- data.frame(x=x,y=y)</span><br><span class="line">distxy &lt;- dist(dataFrame)</span><br><span class="line">hClustering &lt;- hclust(distxy)</span><br><span class="line">plot(hClustering)</span><br></pre></td></tr></table></figure>
<h3 id="myplcclust-function-and-example"><code>myplcclust</code> Function and Example</h3>
<ul>
<li><em><strong>Note</strong>: <code>myplcclust</code> = a function to plot <code>hclust</code> objects in color (clusters labeled 1 2 3 etc.), but must know how many clusters there are initially </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">myplclust &lt;- function(hclust, lab = hclust$labels,</span><br><span class="line">	lab.col = rep(1, length(hclust$labels)), hang = 0.1, ...) &#123;</span><br><span class="line">	## modifiction of plclust for plotting hclust objects *in colour*! Copyright</span><br><span class="line">	## Eva KF Chan 2009 Arguments: hclust: hclust object lab: a character vector</span><br><span class="line">	## of labels of the leaves of the tree lab.col: colour for the labels;</span><br><span class="line">	## NA=default device foreground colour hang: as in hclust &amp; plclust Side</span><br><span class="line">	## effect: A display of hierarchical cluster with coloured leaf labels.</span><br><span class="line">	y &lt;- rep(hclust$height, 2)</span><br><span class="line">	x &lt;- as.numeric(hclust$merge)</span><br><span class="line">	y &lt;- y[which(x &lt; 0)]</span><br><span class="line">	x &lt;- x[which(x &lt; 0)]</span><br><span class="line">	x &lt;- abs(x)</span><br><span class="line">	y &lt;- y[order(x)]</span><br><span class="line">	x &lt;- x[order(x)]</span><br><span class="line">	plot(hclust, labels = FALSE, hang = hang, ...)</span><br><span class="line">	text(x = x, y = y[hclust$order] - (max(hclust$height) * hang), labels = lab[hclust$order],</span><br><span class="line"> col = lab.col[hclust$order], srt = 90, adj = c(1, 0.5), xpd = NA, ...)</span><br><span class="line">&#125;</span><br><span class="line"># example</span><br><span class="line">dataFrame &lt;- data.frame(x = x, y = y)</span><br><span class="line">distxy &lt;- dist(dataFrame)</span><br><span class="line">hClustering &lt;- hclust(distxy)</span><br><span class="line">myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))</span><br></pre></td></tr></table></figure>
<h3 id="heatmap-function-and-example"><code>heatmap</code> Function and Example</h3>
<ul>
<li><code>heatmap(data.matrix)</code> function = similar to <code>image(t(x))</code>
<ul>
<li>good for visualizing high-dimension matrix data, runs hierarchical analysis on rows and columns of table</li>
<li>yellow = high value, red = low value</li>
<li><em><strong>Note</strong>: the input must be a numeric matrix, so as.matrix(data.frame) can be used to convert if necessary </em></li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set.seed(12345)</span><br><span class="line">data &lt;- matrix(rnorm(400), nrow = 40)</span><br><span class="line">heatmap(data)</span><br></pre></td></tr></table></figure>
<h3 id="image-function-and-example"><code>image</code> Function and Example</h3>
<ul>
<li><code>image(x, y, t(dataMatrix)[, nrow(dataMatrix):1])</code> = produces similar color grid plot as the <code>heatmap()</code> without the dendrograms
<ul>
<li><code>t(dataMatrix)[, nrow(dataMatrix)]</code>
<ul>
<li><code>t(dataMatrix)</code> = transpose of dataMatrix, this is such that the plot will be displayed in the same fashion as the matrix (rows as values on the y axis and columns as values on the x axis)
<ul>
<li><strong><em>example</em></strong> 40 x 10 matrix will have graph the 10 columns as x values and 40 rows as y values</li>
</ul></li>
<li><code>[, nrow(dataMatrix)]</code> = subsets the data frame in reverse column order; when combined with the <code>t()</code> function, it reorders the rows of data from 40 to 1, such that the data from the matrix is displayed in order from top to bottom
<ul>
<li><em><strong>Note</strong>: without this statement the rows will be displayed in order from bottom to top, as that is in line with the positive y axis </em></li>
</ul></li>
</ul></li>
<li><code>x</code>, <code>y</code> = used to specify the values displayed on the x and y axis
<ul>
<li><em><strong>Note</strong>: must be in increasing order </em></li>
</ul></li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image(1:10, 1:40, t(data)[, nrow(data):1])</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="k-means-clustering">K-means Clustering</h2>
<ul>
<li>similar to hierarchical clustering, focuses on finding things that are close together
<ul>
<li>define close, groups, visualizing/interpreting grouping</li>
</ul></li>
<li><strong>partitioning approach</strong>
<ol type="1">
<li>set number of clusters initially</li>
<li>find centroids for each cluster</li>
<li>assign points to the closest centroid</li>
<li>recalculate centroid</li>
<li>repeat = yields estimate of cluster centroids and which cluster each point belongs to</li>
</ol>
<ul>
<li><strong>requires</strong>
<ul>
<li>distance metric</li>
<li>initial number of clusters</li>
<li>initial guess as to where the cluster centroids are</li>
</ul></li>
</ul></li>
</ul>
<h3 id="procedure-for-constructing-k-means-clusters-kmeans-function">Procedure for Constructing K-means Clusters (<code>kmeans</code> function)</h3>
<ol type="1">
<li>choose three random points as the starting centroids</li>
<li>take each of the data points and assign it to the closest centroid (creating a cluster around each starting point)</li>
<li>take each cluster and recalculate the centroid (taking the mean) with its enclosed data points</li>
<li>repeat step 2 and 3 (reassign points to centroids and update centroid locations) until a stable result is achieved</li>
</ol>
<ul>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">set.seed(1234)</span><br><span class="line">x &lt;- rnorm(12,mean=rep(1:3,each=4),sd=0.2)</span><br><span class="line">y &lt;- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)</span><br><span class="line">dataFrame &lt;- data.frame(x=x,y=y)</span><br><span class="line"># specifies initial number of clusters to be 3</span><br><span class="line">kmeansObj &lt;- kmeans(dataFrame,centers=3)</span><br><span class="line">names(kmeansObj)</span><br><span class="line"># returns cluster assignments</span><br><span class="line">kmeansObj$cluster</span><br><span class="line">par(mar=rep(0.2,4))</span><br><span class="line">plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)</span><br><span class="line">points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)</span><br></pre></td></tr></table></figure>
<h3 id="characteristics-of-k-means-clustering-algorithms">Characteristics of K-means Clustering Algorithms</h3>
<ul>
<li>requires number of clusters initially
<ul>
<li>pick by eye/intuition</li>
<li>pick by cross validation/information theory, etc. [link]</li>
</ul></li>
<li>not deterministic (starting points chosen at random)
<ul>
<li>useful to run the algorithms a few times with different starting points to validate results</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="dimension-reduction">Dimension Reduction</h2>
<ul>
<li>two kinds of problems that relate to high-dimension dataset/matrix with many variables
<ol type="1">
<li>find a new set (smaller) of variables that are uncorrelated and explain as much variance of data as possible
<ul>
<li>normally many variables are not independent (i.e. height vs weight)</li>
<li>statistical problem, commonly solved with PCA</li>
</ul></li>
<li>find a lower rank matrix (best matrix created with fewer variables) that still explains the data
<ul>
<li>data compression problem, commonly solved SVD</li>
</ul></li>
</ol></li>
<li><strong><em>example</em></strong>
<ul>
<li><em><strong>Note</strong>: we are arbitrarily introduced pattern in data: we flip a coin and if the it is heads, we replace the row with [0, 0, 0, 0, 0, 3, 3, 3, 3, 3] </em></li>
<li>here we plot the patterns in rows and columns (already sorted)</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">for(i in 1:40)&#123;</span><br><span class="line">  # flip a coin</span><br><span class="line">  coinFlip &lt;- rbinom(1,size=1,prob=0.5)</span><br><span class="line">  # if coin is heads add a common pattern to that row</span><br><span class="line">  if(coinFlip)&#123;</span><br><span class="line">    data[i,] &lt;- data[i,] + rep(c(0,3),each=5)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"># hierarchical clustering</span><br><span class="line">hh &lt;- hclust(dist(data))</span><br><span class="line">dataOrdered &lt;- data[hh$order,]</span><br><span class="line"># create 1 x 3 panel plot</span><br><span class="line">par(mfrow=c(1,3))</span><br><span class="line"># heat map (sorted)</span><br><span class="line">image(t(dataOrdered)[,nrow(dataOrdered):1])</span><br><span class="line"># row means (40 rows)</span><br><span class="line">plot(rowMeans(dataOrdered),40:1,,xlab=&quot;Row Mean&quot;,ylab=&quot;Row&quot;,pch=19)</span><br><span class="line"># column means (10 columns)</span><br><span class="line">plot(colMeans(dataOrdered),xlab=&quot;Column&quot;,ylab=&quot;Column Mean&quot;,pch=19)</span><br></pre></td></tr></table></figure>
<h3 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h3>
<ul>
<li>Let <span class="math inline">\(X\)</span> = matrix which each variable in column (measurement) and each observation in row (subject)</li>
<li>SVD in this case is a <strong>matrix decomposition</strong> process, in which X is divided into <strong><em>three</em></strong> separate matrices as follows: <span class="math display">\[X = UDV^T\]</span>
<ul>
<li><span class="math inline">\(U\)</span> = left singular vector, orthogonal matrix (columns independent of each other)</li>
<li><span class="math inline">\(D\)</span> = singular values, diagonal matrix</li>
<li><span class="math inline">\(V\)</span> = right singular vector, orthogonal matrix (columns independent of each other)</li>
<li><em><strong>Note</strong>: orthogonal implies that a matrix is always invertible [<span class="math inline">\(A^{-1} = A^T\)</span>] and that the product of the matrix and its transpose equals the identity matrix [<span class="math inline">\(AA^T = I\)</span>] </em>
<ul>
<li>when a orthogonal matrices, <span class="math inline">\(A\)</span>, is multiplied by another matrix, <span class="math inline">\(B\)</span>, it is effectively a linear transformation in that the length and angles of <span class="math inline">\(B\)</span> are preserved</li>
</ul></li>
<li><em><strong>Note</strong>: diagonal implies that any value outside of the main diagonal (<span class="math inline">\(\searrow\)</span>) = 0 </em>
<ul>
<li>example <span class="math display">\[A = \begin{bmatrix}
 1 &amp; 0 &amp; 0 \\
 0 &amp; 2 &amp; 0 \\
 0 &amp; 0 &amp; 3 \end{bmatrix}\]</span></li>
</ul></li>
</ul></li>
<li><em><strong>Note</strong>: scale of data matters for SVD/PCA (scaling the data may help), patterns detected maybe mixed together, and computation is intensive for these operations </em></li>
</ul>
<h3 id="principal-components-analysis-pca">Principal Components Analysis (PCA)</h3>
<ul>
<li>first scale the variables and run SVD on normalized matrix
<ul>
<li><strong>scaling</strong> = subtract each column by its mean and divide by its standard deviation</li>
</ul></li>
<li><strong>principal components</strong> = the right singular values or the <span class="math inline">\(V\)</span> matrix</li>
</ul>
<h3 id="svd-and-pca-example">SVD and PCA Example</h3>
<ul>
<li><strong><span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> Matrices</strong>
<ul>
<li><code>s &lt;- svd(data)</code> = performs SVD on data (<span class="math inline">\(n \times m\)</span> matrix) and splits it into <span class="math inline">\(u\)</span>, <span class="math inline">\(v\)</span>, and <span class="math inline">\(d\)</span> matrices
<ul>
<li><code>s$u</code> = <span class="math inline">\(n \times m\)</span> matrix <span class="math inline">\(\rightarrow\)</span> horizontal variation</li>
<li><code>s$d</code> = <span class="math inline">\(1 \times m\)</span> vector <span class="math inline">\(\rightarrow\)</span> vector of the singular/diagonal values
<ul>
<li><code>diag(s$d)</code> = <span class="math inline">\(m \times m\)</span> diagonal matrix</li>
</ul></li>
<li><code>s$v</code> = <span class="math inline">\(m \times m\)</span> matrix <span class="math inline">\(\rightarrow\)</span> vertical variation</li>
<li><code>s$u %*% diag(s$d) %*% t(s$v)</code> = returns the original data <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(X = UDV^T\)</span></li>
</ul></li>
<li><code>scale(data)</code> = scales the original data by subtracting each data point by its column mean and dividing by its column standard deviation</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># running svd</span><br><span class="line">svd1 &lt;- svd(scale(dataOrdered))</span><br><span class="line"># create 1 by 3 panel plot</span><br><span class="line">par(mfrow=c(1,3))</span><br><span class="line"># data heatmap (sorted)</span><br><span class="line">image(t(dataOrdered)[,nrow(dataOrdered):1])</span><br><span class="line"># U Matrix - first column</span><br><span class="line">plot(svd1$u[,1],40:1,,xlab=&quot;Row&quot;,ylab=&quot;First left singular vector&quot;,pch=19)</span><br><span class="line"># V vector - first column</span><br><span class="line">plot(svd1$v[,1],xlab=&quot;Column&quot;,ylab=&quot;First right singular vector&quot;,pch=19)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><span class="math inline">\(D\)</span> Matrix and Variance Explained</strong>
<ul>
<li><span class="math inline">\(d\)</span> matrix (<code>s$d</code> vector) captures the singular values, or <strong><em>variation in data that is explained by that particular component</em></strong> (variable/column/dimension)</li>
<li><strong>proportion of variance Explained</strong> = converting the singular values to variance (square the values) and divide by the total variance (sum of the squared singular values)
<ul>
<li>effectively the same pattern as the singular values, just converted to percentage</li>
<li>in this case, the first component/dimension, which captures the shift in means (see previous plot) of SVD captures about 40% of the variation</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># create 1 x 2 panel plot</span><br><span class="line">par(mfrow=c(1,2))</span><br><span class="line"># plot singular values</span><br><span class="line">plot(svd1$d,xlab=&quot;Column&quot;,ylab=&quot;Singular value&quot;,pch=19)</span><br><span class="line"># plot proportion of variance explained</span><br><span class="line">plot(svd1$d^2/sum(svd1$d^2),xlab=&quot;Column&quot;,ylab=&quot;Prop. of variance explained&quot;,pch=19)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Relationship to PCA</strong>
<ul>
<li><code>p &lt;- prcomp(data, scale = TRUE)</code> = performs PCA on data specified
<ul>
<li><code>scale = TRUE</code> = scales the data before performing PCA</li>
<li>returns <code>prcomp</code> object</li>
<li><code>summary(p)</code> = prints out the principal component’s standard deviation, proportion of variance, and cumulative proportion</li>
</ul></li>
<li>PCA’s rotation vectors are equivalent to their counterparts in the V matrix from the SVD</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># SVD</span><br><span class="line">svd1 &lt;- svd(scale(dataOrdered))</span><br><span class="line"># PCA</span><br><span class="line">pca1 &lt;- prcomp(dataOrdered,scale=TRUE)</span><br><span class="line"># Plot the rotation from PCA (Principal Components) vs v vector from SVD</span><br><span class="line">plot(pca1$rotation[,1],svd1$v[,1],pch=19,xlab=&quot;Principal Component 1&quot;,</span><br><span class="line">	ylab=&quot;Right Singular Vector 1&quot;)</span><br><span class="line">abline(c(0,1))</span><br><span class="line"># summarize PCA</span><br><span class="line">summary(pca1)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>More Complex Patterns</strong>
<ul>
<li>SVD can be used to <strong><em>detect unknown patterns</em></strong> within the data (we rarely know the true distribution/pattern about the population we’re analyzing)</li>
<li>however, it may be hard to pinpoint exact patterns as the principal components may confound each other
<ul>
<li>in the example below, you can see that the two principal components that capture the most variation have both horizontal shifts and alternating patterns captured in them</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>,fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">set.seed(678910)</span><br><span class="line"># setting pattern</span><br><span class="line">data &lt;- matrix(rnorm(400), nrow = 40)</span><br><span class="line">for(i in 1:40)&#123;</span><br><span class="line">  # flip a coin</span><br><span class="line">  coinFlip1 &lt;- rbinom(1,size=1,prob=0.5)</span><br><span class="line">  coinFlip2 &lt;- rbinom(1,size=1,prob=0.5)</span><br><span class="line">  # if coin is heads add a common pattern to that row</span><br><span class="line">  if(coinFlip1)&#123;</span><br><span class="line">    data[i,] &lt;- data[i,] + rep(c(0,5),each=5)</span><br><span class="line">  &#125;</span><br><span class="line">  if(coinFlip2)&#123;</span><br><span class="line">    data[i,] &lt;- data[i,] + rep(c(0,5),5)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">hh &lt;- hclust(dist(data)); dataOrdered &lt;- data[hh$order,]</span><br><span class="line"></span><br><span class="line"># perform SVD</span><br><span class="line">svd2 &lt;- svd(scale(dataOrdered))</span><br><span class="line">par(mfrow=c(2,3))</span><br><span class="line">image(t(dataOrdered)[,nrow(dataOrdered):1])</span><br><span class="line">plot(rep(c(0,1),each=5),pch=19,xlab=&quot;Column&quot;, main=&quot;True Pattern 1&quot;)</span><br><span class="line">plot(rep(c(0,1),5),pch=19,xlab=&quot;Column&quot;,main=&quot;True Pattern 2&quot;)</span><br><span class="line">image(t(dataOrdered)[,nrow(dataOrdered):1])</span><br><span class="line">plot(svd2$v[,1],pch=19,xlab=&quot;Column&quot;,ylab=&quot;First right singular vector&quot;,</span><br><span class="line">	main=&quot;Detected Pattern 1&quot;)</span><br><span class="line">plot(svd2$v[,2],pch=19,xlab=&quot;Column&quot;,ylab=&quot;Second right singular vector&quot;,</span><br><span class="line">	main=&quot;Detected Pattern 2&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Missing Data</strong>
<ul>
<li>SVD cannot be performed on dataset with <code>NA</code> values</li>
<li><code>impute</code> package from <a href="http://bioconductor.org" target="_blank" rel="noopener">Bioconductor</a> can help approximate missing values from surrounding values
<ul>
<li><code>impute.knn</code> function takes the missing row and imputes the data using the <code>k</code> nearest neighbors to that row
<ul>
<li><code>k=10</code> = default value (take the nearest 10 rows)</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">library(impute)  ## Available from http://bioconductor.org</span><br><span class="line">data2 &lt;- dataOrdered</span><br><span class="line"># set random samples = NA</span><br><span class="line">data2[sample(1:100,size=40,replace=FALSE)] &lt;- NA</span><br><span class="line">data2 &lt;- impute.knn(data2)$data</span><br><span class="line">svd1 &lt;- svd(scale(dataOrdered)); svd2 &lt;- svd(scale(data2))</span><br><span class="line">par(mfrow=c(1,2))</span><br><span class="line">plot(svd1$v[,1],pch=19, main=&quot;Original&quot;)</span><br><span class="line">plot(svd2$v[,1],pch=19, main=&quot;Imputed&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="create-approximationsdata-compression">Create Approximations/Data Compression</h3>
<ul>
<li>SVD can be used to create lower rank representation, or compressed representation of data</li>
<li>if we look at the variance explained plot below, <strong><em>most of the variation</em></strong> is explained by the <strong><em>first few principal components</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># load faceData</span><br><span class="line">load(&quot;figures/face.rda&quot;)</span><br><span class="line"># perform SVD</span><br><span class="line">svd3 &lt;- svd(scale(faceData))</span><br><span class="line">plot(svd3$d^2/sum(svd3$d^2),pch=19,xlab=&quot;Singular vector&quot;,ylab=&quot;Variance explained&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>approximations can thus be created by taking the first few components and using matrix multiplication with the corresponding <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span>, and <span class="math inline">\(D\)</span> components</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">approx1 &lt;- svd3$u[,1] %*% t(svd3$v[,1]) * svd3$d[1]</span><br><span class="line">approx5 &lt;- svd3$u[,1:5] %*% diag(svd3$d[1:5])%*% t(svd3$v[,1:5])</span><br><span class="line">approx10 &lt;- svd3$u[,1:10] %*% diag(svd3$d[1:10])%*% t(svd3$v[,1:10])</span><br><span class="line"># create 1 x 4 panel plot</span><br><span class="line">par(mfrow=c(1,4))</span><br><span class="line"># plot original facedata</span><br><span class="line">image(t(approx1)[,nrow(approx1):1], main = &quot;1 Component&quot;)</span><br><span class="line">image(t(approx5)[,nrow(approx5):1], main = &quot;5 Component&quot;)</span><br><span class="line">image(t(approx10)[,nrow(approx10):1], main = &quot;10 Component&quot;)</span><br><span class="line">image(t(faceData)[,nrow(faceData):1], main = &quot;Original&quot;)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="color-packages-in-r-plots">Color Packages in R Plots</h2>
<ul>
<li>proper use of color can help convey the message by improving clarity/contrast of data presented</li>
<li>default color schemes for most plots in R are fairly terrible, so some external packages are helpful</li>
</ul>
<h3 id="grdevices-package"><code>grDevices</code> Package</h3>
<ul>
<li><code>colors()</code> function = lists names of colors available in any plotting function</li>
<li><strong><code>colorRamp</code> function</strong>
<ul>
<li>takes any set of colors and return a function that takes values between 0 and 1, indicating the extremes of the color palette (e.g. see the <code>gray</code> function)</li>
<li><code>pal &lt;- colorRamp(c(&quot;red&quot;, &quot;blue&quot;))</code> = defines a <code>colorRamp</code> function</li>
<li><code>pal(0)</code> returns a 1 x 3 matrix containing values for RED, GREEN, and BLUE values that range from 0 to 255</li>
<li><code>pal(seq(0, 1, len = 10))</code> returns a 10 x 3 matrix of 10 colors that range from RED to BLUE (two ends of spectrum defined in the object)</li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># define colorRamp function</span><br><span class="line">pal &lt;- colorRamp(c(&quot;red&quot;, &quot;blue&quot;))</span><br><span class="line"># create a color</span><br><span class="line">pal(0.67)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>colorRampPalette</code> function</strong>
<ul>
<li>takes any set of colors and return a function that takes integer arguments and returns a vector of colors interpolating the palette (like <code>heat.colors</code> or <code>topo.colors</code>)</li>
<li><code>pal &lt;- colorRampPalette(c(&quot;red&quot;, &quot;yellow&quot;))</code> defines a <code>colorRampPalette</code> function</li>
<li><code>pal(10)</code> returns 10 interpolated colors in hexadecimal format that range between the defined ends of spectrum</li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># define colorRampPalette function</span><br><span class="line">pal &lt;- colorRampPalette(c(&quot;red&quot;, &quot;yellow&quot;))</span><br><span class="line"># create 10 colors</span><br><span class="line">pal(10)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>rgb</code> function</strong>
<ul>
<li><code>red</code>, <code>green</code>, and <code>blue</code> arguments = values between 0 and 1</li>
<li><code>alpha = 0.5</code> = transparency control, values between 0 and 1</li>
<li>returns hexadecimal string for color that can be used in <code>plot</code>/<code>image</code> commands</li>
<li><code>colorspace</code> package <code>cna</code> be used for different control over colors</li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- rnorm(200); y &lt;- rnorm(200)</span><br><span class="line">par(mfrow=c(1,2))</span><br><span class="line"># normal scatter plot</span><br><span class="line">plot(x, y, pch = 19, main = &quot;Default&quot;)</span><br><span class="line"># using transparency shows data much better</span><br><span class="line">plot(x, y, col = rgb(0, 0, 0, 0.2), main = &quot;With Transparency&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="rcolorbrewer-package"><code>RColorBrewer</code> Package</h3>
<ul>
<li>can be found on CRAN that has predefined color palettes
<ul>
<li><code>library(RColorBrewer)</code></li>
</ul></li>
<li><strong>types of palettes</strong>
<ul>
<li><em>Sequential</em> = numerical/continuous data that is ordered from low to high</li>
<li><em>Diverging</em> = data that deviate from a value, increasing in two directions (i.e. standard deviations from the mean)</li>
<li><em>Qualitative</em> = categorical data/factor variables</li>
</ul></li>
<li>palette information from the <code>RColorBrewer</code> package can be used by <code>colorRamp</code> and <code>colorRampPalette</code> functions</li>
<li><strong>available colors palettes</strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/color.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>brewer.pal(n, &quot;BuGn&quot;)</code> function</strong>
<ul>
<li><code>n</code> = number of colors to generated</li>
<li><code>&quot;BuGn&quot;</code> = name of palette
<ul>
<li><code>?brewer.pal</code> list all available palettes to use</li>
</ul></li>
<li>returns list of <code>n</code> hexadecimal colors</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">library(RColorBrewer)</span><br><span class="line"># generate 3 colors using brewer.pal function</span><br><span class="line">cols &lt;- brewer.pal(3, &quot;BuGn&quot;)</span><br><span class="line">pal &lt;- colorRampPalette(cols)</span><br><span class="line">par(mfrow=c(1,3))</span><br><span class="line"># heat.colors/default</span><br><span class="line">image(volcano, main = &quot;Heat.colors/Default&quot;)</span><br><span class="line"># topographical colors</span><br><span class="line">image(volcano, col = topo.colors(20), main = &quot;Topographical Colors&quot;)</span><br><span class="line"># RColorBrewer colors</span><br><span class="line">image(volcano, col = pal(20), main = &quot;RColorBrewer Colors&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><code>smoothScatter</code> function</strong>
<ul>
<li>used to plot large quantities of data points</li>
<li>creates 2D histogram of points and plots the histogram</li>
<li>default color scheme = “Blues” palette from <code>RColorBrewer</code> package</li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- rnorm(10000); y &lt;- rnorm(10000)</span><br><span class="line">smoothScatter(x, y)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="case-study-human-activity-tracking-with-smart-phones">Case Study: Human Activity Tracking with Smart Phones</h2>
<p><strong>Loading Training Set of Samsung S2 Data from <a href="http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones" target="_blank" rel="noopener">UCI Repository</a></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># load data frame provided</span><br><span class="line">load(&quot;samsungData.rda&quot;)</span><br><span class="line"># table of 6 types of activities</span><br><span class="line">table(samsungData$activity)</span><br></pre></td></tr></table></figure>
<p><strong>Plotting Average Acceleration for First Subject</strong></p>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># set up 1 x 2 panel plot</span><br><span class="line">par(mfrow=c(1, 2), mar = c(5, 4, 1, 1))</span><br><span class="line"># converts activity to a factor variable</span><br><span class="line">samsungData &lt;- transform(samsungData, activity = factor(activity))</span><br><span class="line"># find only the subject 1 data</span><br><span class="line">sub1 &lt;- subset(samsungData, subject == 1)</span><br><span class="line"># plot mean body acceleration in X direction</span><br><span class="line">plot(sub1[, 1], col = sub1$activity, ylab = names(sub1)[1],</span><br><span class="line">	main = &quot;Mean Body Acceleration for X&quot;)</span><br><span class="line"># plot mean body acceleration in Y direction</span><br><span class="line">plot(sub1[, 2], col = sub1$activity, ylab = names(sub1)[2],</span><br><span class="line">	main = &quot;Mean Body Acceleration for Y&quot;)</span><br><span class="line"># add legend</span><br><span class="line">legend(&quot;bottomright&quot;,legend=unique(sub1$activity),col=unique(sub1$activity), pch = 1)</span><br></pre></td></tr></table></figure>
<p><strong>Clustering Based on Only Average Acceleration</strong></p>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># load myplclust function</span><br><span class="line">source(&quot;myplclust.R&quot;)</span><br><span class="line"># calculate distance matrix</span><br><span class="line">distanceMatrix &lt;- dist(sub1[,1:3])</span><br><span class="line"># form hclust object</span><br><span class="line">hclustering &lt;- hclust(distanceMatrix)</span><br><span class="line"># run myplclust on data</span><br><span class="line">myplclust(hclustering, lab.col = unclass(sub1$activity))</span><br></pre></td></tr></table></figure>
<p><strong>Plotting Max Acceleration for the First Subject</strong></p>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># create 1 x 2 panel</span><br><span class="line">par(mfrow=c(1,2))</span><br><span class="line"># plot max accelecrations in x and y direction</span><br><span class="line">plot(sub1[,10],pch=19,col=sub1$activity,ylab=names(sub1)[10],</span><br><span class="line">	main = &quot;Max Body Acceleration for X&quot;)</span><br><span class="line">plot(sub1[,11],pch=19,col = sub1$activity,ylab=names(sub1)[11],</span><br><span class="line">	main = &quot;Max Body Acceleration for Y&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>Clustering Based on Maximum Acceleration</strong></p>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># calculate distance matrix for max distances</span><br><span class="line">distanceMatrix &lt;- dist(sub1[,10:12])</span><br><span class="line">hclustering &lt;- hclust(distanceMatrix)</span><br><span class="line">myplclust(hclustering,lab.col=unclass(sub1$activity))</span><br></pre></td></tr></table></figure>
<p><strong>Singular Value Decomposition</strong></p>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># perform SVD minus last two columns (subject and activity)</span><br><span class="line">svd1 = svd(scale(sub1[,-c(562,563)]))</span><br><span class="line"># create 1 x 2 panel plot</span><br><span class="line">par(mfrow=c(1,2))</span><br><span class="line"># plot first two left singular vector</span><br><span class="line"># separate moving from non moving</span><br><span class="line">plot(svd1$u[,1],col=sub1$activity,pch=19, main = &quot;First Left Singular Vector&quot;)</span><br><span class="line">plot(svd1$u[,2],col=sub1$activity,pch=19, main = &quot;Second Left Singular Vector&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>New Clustering with Maximum Contributers</strong></p>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># find the max contributing feature</span><br><span class="line">maxContrib &lt;- which.max(svd1$v[,2])</span><br><span class="line"># recalculate distance matrix</span><br><span class="line">distanceMatrix &lt;- dist(sub1[, c(10:12,maxContrib)])</span><br><span class="line">hclustering &lt;- hclust(distanceMatrix)</span><br><span class="line">myplclust(hclustering,lab.col=unclass(sub1$activity))</span><br><span class="line"># name of max contributing factor</span><br><span class="line">names(samsungData)[maxContrib]</span><br></pre></td></tr></table></figure>
<p><strong>K-means Clustering (nstart=1, first try)</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># specify 6 centers for data</span><br><span class="line">kClust &lt;- kmeans(sub1[,-c(562,563)],centers=6)</span><br><span class="line"># tabulate 6 clusteres against 6 activity but many clusters contain multiple activities</span><br><span class="line">table(kClust$cluster,sub1$activity)</span><br></pre></td></tr></table></figure>
<p><strong>K-means clustering (nstart=100, first try)</strong></p>
<figure class="highlight plain"><figcaption><span>cache</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># run k-means algorithm 100 times</span><br><span class="line">kClust &lt;- kmeans(sub1[,-c(562,563)],centers=6,nstart=100)</span><br><span class="line"># tabulate results</span><br><span class="line">table(kClust$cluster,sub1$activity)</span><br></pre></td></tr></table></figure>
<p><strong>K-means clustering (nstart=100, second try)</strong></p>
<figure class="highlight plain"><figcaption><span>cache</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># run k-means algorithm 100 times</span><br><span class="line">kClust &lt;- kmeans(sub1[,-c(562,563)],centers=6,nstart=100)</span><br><span class="line"># tabulate results</span><br><span class="line">table(kClust$cluster,sub1$activity)</span><br></pre></td></tr></table></figure>
<p><strong>Cluster 1 Variable Centers (Laying)</strong></p>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot first 10 centers of k-means for laying to understand which features drive the activity</span><br><span class="line">plot(kClust$center[1,1:10],pch=19,ylab=&quot;Cluster Center&quot;,xlab=&quot;&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>Cluster 2 Variable Centers (Walking)</strong></p>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot first 10 centers of k-means for laying to understand which features drive the activity</span><br><span class="line">plot(kClust$center[4,1:10],pch=19,ylab=&quot;Cluster Center&quot;,xlab=&quot;&quot;)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="case-study-fine-particle-pollution-in-the-u.s.-from-1999-to-2012">Case Study: Fine Particle Pollution in the U.S. from 1999 to 2012</h2>
<p><strong>Read Raw Data from 1999 and 2012</strong></p>
<figure class="highlight plain"><figcaption><span>cache </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># read in raw data from 1999</span><br><span class="line">pm0 &lt;- read.table(&quot;pm25_data/RD_501_88101_1999-0.txt&quot;, comment.char = &quot;#&quot;, header = FALSE, sep = &quot;|&quot;, na.strings = &quot;&quot;)</span><br><span class="line"># read in headers/column lables</span><br><span class="line">cnames &lt;- readLines(&quot;pm25_data/RD_501_88101_1999-0.txt&quot;, 1)</span><br><span class="line"># convert string into vector</span><br><span class="line">cnames &lt;- strsplit(substring(cnames, 3), &quot;|&quot;, fixed = TRUE)</span><br><span class="line"># make vector the column names</span><br><span class="line">names(pm0) &lt;- make.names(cnames[[1]])</span><br><span class="line"># we are interested in the pm2.5 readings in the &quot;Sample.Value&quot; column</span><br><span class="line">x0 &lt;- pm0$Sample.Value</span><br><span class="line"># read in the data from 2012</span><br><span class="line">pm1 &lt;- read.table(&quot;pm25_data/RD_501_88101_2012-0.txt&quot;, comment.char = &quot;#&quot;, header = FALSE, sep = &quot;|&quot;,</span><br><span class="line">	na.strings = &quot;&quot;, nrow = 1304290)</span><br><span class="line"># make vector the column names</span><br><span class="line">names(pm1) &lt;- make.names(cnames[[1]])</span><br><span class="line"># take the 2012 data for pm2.5 readings</span><br><span class="line">x1 &lt;- pm1$Sample.Value</span><br></pre></td></tr></table></figure>
<p><strong>Summaries for Both Periods</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># generate 6 number summaries</span><br><span class="line">summary(x1)</span><br><span class="line">summary(x0)</span><br><span class="line"># calculate % of missing values, Are missing values important here?</span><br><span class="line">data.frame(NA.1990 = mean(is.na(x0)), NA.2012 = mean(is.na(x1)))</span><br></pre></td></tr></table></figure>
<p><strong>Make a boxplot of both 1999 and 2012</strong></p>
<figure class="highlight plain"><figcaption><span>warning</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">par(mfrow = c(1,2))</span><br><span class="line"># regular boxplot, data too right skewed</span><br><span class="line">boxplot(x0, x1, main = &quot;Regular Boxplot&quot;)</span><br><span class="line"># log boxplot, significant difference in means, but more spread</span><br><span class="line">boxplot(log10(x0), log10(x1), main = &quot;log Boxplot&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>Check for Negative Values in ‘x1’</strong></p>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># summary again</span><br><span class="line">summary(x1)</span><br><span class="line"># create logical vector for</span><br><span class="line">negative &lt;- x1 &lt; 0</span><br><span class="line"># count number of negatives</span><br><span class="line">sum(negative, na.rm = T)</span><br><span class="line"># calculate percentage of negatives</span><br><span class="line">mean(negative, na.rm = T)</span><br><span class="line"># capture the date data</span><br><span class="line">dates &lt;- pm1$Date</span><br><span class="line">dates &lt;- as.Date(as.character(dates), &quot;%Y%m%d&quot;)</span><br><span class="line"># plot the histogram</span><br><span class="line">hist(dates, &quot;month&quot;)  ## Check what&apos;s going on in months 1--6</span><br></pre></td></tr></table></figure>
<p><strong>Check Same New York Monitors at 1999 and 2012</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># find unique monitors in New York in 1999</span><br><span class="line">site0 &lt;- unique(subset(pm0, State.Code == 36, c(County.Code, Site.ID)))</span><br><span class="line"># find unique monitors in New York in 2012</span><br><span class="line">site1 &lt;- unique(subset(pm1, State.Code == 36, c(County.Code, Site.ID)))</span><br><span class="line"># combine country codes and siteIDs of the monitors</span><br><span class="line">site0 &lt;- paste(site0[,1], site0[,2], sep = &quot;.&quot;)</span><br><span class="line">site1 &lt;- paste(site1[,1], site1[,2], sep = &quot;.&quot;)</span><br><span class="line"># find common monitors in both</span><br><span class="line">both &lt;- intersect(site0, site1)</span><br><span class="line"># print common monitors in 1999 and 2012</span><br><span class="line">print(both)</span><br></pre></td></tr></table></figure>
<p><strong>Find how many observations available at each monitor</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># add columns for combined county/site for the original data</span><br><span class="line">pm0$county.site &lt;- with(pm0, paste(County.Code, Site.ID, sep = &quot;.&quot;))</span><br><span class="line">pm1$county.site &lt;- with(pm1, paste(County.Code, Site.ID, sep = &quot;.&quot;))</span><br><span class="line"># find subsets where state = NY and county/site = what we found previously</span><br><span class="line">cnt0 &lt;- subset(pm0, State.Code == 36 &amp; county.site %in% both)</span><br><span class="line">cnt1 &lt;- subset(pm1, State.Code == 36 &amp; county.site %in% both)</span><br><span class="line"># split data by the county/size values and count oberservations</span><br><span class="line">sapply(split(cnt0, cnt0$county.site), nrow)</span><br><span class="line">sapply(split(cnt1, cnt1$county.site), nrow)</span><br></pre></td></tr></table></figure>
<p><strong>Choose Monitor where County = 63 and Side ID = 2008</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># filter data by state/county/siteID</span><br><span class="line">pm1sub &lt;- subset(pm1, State.Code == 36 &amp; County.Code == 63 &amp; Site.ID == 2008)</span><br><span class="line">pm0sub &lt;- subset(pm0, State.Code == 36 &amp; County.Code == 63 &amp; Site.ID == 2008)</span><br><span class="line"># there are 30 observations from 2012, and 122 from 1999</span><br><span class="line">dim(pm1sub)</span><br><span class="line">dim(pm0sub)</span><br></pre></td></tr></table></figure>
<p><strong>Plot Data for 2012</strong></p>
<figure class="highlight plain"><figcaption><span>fig.width</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># capture the dates of the subset of data</span><br><span class="line">dates1 &lt;- pm1sub$Date</span><br><span class="line"># capture measurements for the subset of data</span><br><span class="line">x1sub &lt;- pm1sub$Sample.Value</span><br><span class="line"># convert dates to appropriate format</span><br><span class="line">dates1 &lt;- as.Date(as.character(dates1), &quot;%Y%m%d&quot;)</span><br><span class="line"># plot pm2.5 value vs time</span><br><span class="line">plot(dates1, x1sub, main = &quot;PM2.5 Polution Level in 2012&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>Plot data for 1999</strong></p>
<figure class="highlight plain"><figcaption><span>fig.width</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># capture the dates of the subset of data</span><br><span class="line">dates0 &lt;- pm0sub$Date</span><br><span class="line"># convert dates to appropriate format</span><br><span class="line">dates0 &lt;- as.Date(as.character(dates0), &quot;%Y%m%d&quot;)</span><br><span class="line"># capture measurements for the subset of data</span><br><span class="line">x0sub &lt;- pm0sub$Sample.Value</span><br><span class="line"># plot pm2.5 value vs time</span><br><span class="line">plot(dates0, x0sub, main = &quot;PM2.5 Polution Level in 1999&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>Panel Plot for Both Years</strong></p>
<figure class="highlight plain"><figcaption><span>fig.width</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># find max range for data</span><br><span class="line">rng &lt;- range(x0sub, x1sub, na.rm = T)</span><br><span class="line"># create 1 x 2 panel plot</span><br><span class="line">par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))</span><br><span class="line"># plot time series plot for 1999</span><br><span class="line">plot(dates0, x0sub, pch = 20, ylim = rng, main=&quot;Pollution in 1999&quot;)</span><br><span class="line"># plot the median</span><br><span class="line">abline(h = median(x0sub, na.rm = T))</span><br><span class="line"># plot time series plot for 2012</span><br><span class="line">plot(dates1, x1sub, pch = 20, ylim = rng, main=&quot;Pollution in 2012&quot;)</span><br><span class="line"># plot the median</span><br><span class="line">abline(h = median(x1sub, na.rm = T))</span><br></pre></td></tr></table></figure>
<p><strong>Find State-wide Means and Trend</strong></p>
<figure class="highlight plain"><figcaption><span>fig.width</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># divide data by state and find tne mean of pollution level for 1999</span><br><span class="line">mn0 &lt;- with(pm0, tapply(Sample.Value, State.Code, mean, na.rm = T))</span><br><span class="line"># divide data by state and find tne mean of pollution level for 1999</span><br><span class="line">mn1 &lt;- with(pm1, tapply(Sample.Value, State.Code, mean, na.rm = T))</span><br><span class="line"># convert to data frames while preserving state names</span><br><span class="line">d0 &lt;- data.frame(state = names(mn0), mean = mn0)</span><br><span class="line">d1 &lt;- data.frame(state = names(mn1), mean = mn1)</span><br><span class="line"># merge the 1999 and 2012 means by state</span><br><span class="line">mrg &lt;- merge(d0, d1, by = &quot;state&quot;)</span><br><span class="line"># dimension of combined data frame</span><br><span class="line">dim(mrg)</span><br><span class="line"># first few lines of data</span><br><span class="line">head(mrg)</span><br><span class="line"></span><br><span class="line"># plot the pollution levels data points for 1999</span><br><span class="line">with(mrg, plot(rep(1, 52), mrg[, 2], xlim = c(.8, 2.2), ylim = c(3, 20),</span><br><span class="line">	main = &quot;PM2.5 Pollution Level by State for 1999 &amp; 2012&quot;,</span><br><span class="line">	xlab = &quot;&quot;, ylab = &quot;State-wide Mean PM&quot;))</span><br><span class="line"># plot the pollution levels data points for 2012</span><br><span class="line">with(mrg, points(rep(2, 52), mrg[, 3]))</span><br><span class="line"># connected the dots</span><br><span class="line">segments(rep(1, 52), mrg[, 2], rep(2, 52), mrg[, 3])</span><br><span class="line"># add 1999 and 2012 labels</span><br><span class="line">axis(1, c(1, 2), c(&quot;1999&quot;, &quot;2012&quot;))</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/R-Programming-Course-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/07/R-Programming-Course-Notes/" itemprop="url">
                  R Programming Course Notes
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 11:48:47 / Modified: 11:53:36" itemprop="dateCreated datePublished" datetime="2018-08-07T11:48:47+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">40k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">37 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="overview-and-history-of-r">Overview and History of R</h2>
<ul>
<li><strong>R</strong> = dialect of the <strong>S</strong> language
<ul>
<li>S was developed by John Chambers @ Bell Labs</li>
<li>initiated in 1976 as internal tool, originally FORTRAN libraries</li>
<li>1988 rewritten in C (version 3 of language)</li>
<li>1998 version 4 (what we use today)</li>
</ul></li>
<li><strong>History of S</strong>
<ul>
<li>Bell labs <span class="math inline">\(\rightarrow\)</span> insightful <span class="math inline">\(\rightarrow\)</span> Lucent <span class="math inline">\(\rightarrow\)</span> Alcatel-Lucent</li>
<li>in 1998, S won the Association for computing machinery’s software system award</li>
</ul></li>
<li><strong>History of R</strong>
<ul>
<li>1991 created in New Zealand by Ross Ihaka &amp; RobertGentleman</li>
<li>1993 first announcement of R to public</li>
<li>1995 Martin Machler convinces founders to use GNU General Public license to make R free</li>
<li>1996 public mailing list created R-help and R-devel</li>
<li>1997 R Core Group formed</li>
<li>2000 R v1.0.0 released</li>
</ul></li>
<li><strong>R Features</strong>
<ul>
<li>Syntax similar to S, semantics similar to S, runs on any platforms, frequent releasees</li>
<li>lean software, functionalities in modular packages, sophisticated graphics capabilities</li>
<li>useful for interactive work, powerful programming language</li>
<li>active user community and <strong><em>FREE</em></strong> (4 freedoms)
<ul>
<li>freedom to run the program</li>
<li>freedom to study how the program works and adapt it</li>
<li>freedom to redistribute copies</li>
<li>freedom to improve the program</li>
</ul></li>
</ul></li>
<li><strong>R Drawbacks</strong>
<ul>
<li>40 year-old technology</li>
<li>little built-in support for dynamic/3D graphics</li>
<li>functionality based on consumer demand</li>
<li>objects generally stored in physical memory (limited by hardware)</li>
</ul></li>
<li><strong>Design of the R system</strong>
<ul>
<li>2 conceptual parts: base R from CRAN vs. everything else</li>
<li>functionality divided into different packages
<ul>
<li><strong>base R</strong> contains core functionality and fundamental functions</li>
<li>other utility packages included in the base install: <code>util</code>, <code>stats</code>, <code>datasets</code>, …</li>
<li>Recommended packages: <code>bootclass</code>, <code>KernSmooth</code>, etc</li>
</ul></li>
<li>5000+ packages available</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="coding-standards">Coding Standards</h2>
<ul>
<li>Always use text files/editor</li>
<li>Indent code (4 space minimum)</li>
<li>limit the width of code (80 columns)</li>
<li>limit the length of individual functions</li>
</ul>
<h2 id="workspace-and-files">Workspace and Files</h2>
<ul>
<li><code>getwd()</code> = return current working directory</li>
<li><code>setwd()</code> = set current working directory</li>
<li><code>?function</code> = brings up help for that function</li>
<li><code>dir.create(&quot;path/foldername&quot;, recursive = TRUE)</code> = create directories/subdirectories</li>
<li><code>unlink(directory, recursive = TRUE)</code> = delete directory and subdirectories</li>
<li><code>ls()</code> = list all objects in the local workspace</li>
<li><code>list.files(recursive = TRUE)</code> = list all, including subdirectories</li>
<li><code>args(function)</code> = returns arguments for the function</li>
<li><code>file.create(&quot;name&quot;)</code> = create file
<ul>
<li><code>.exists(&quot;name&quot;)</code> = return true/false exists in working directory</li>
<li><code>.info(&quot;name&quot;)</code> = return file info</li>
<li><code>.info(&quot;name&quot;)$property</code> = returns value for the specific attribute</li>
<li><code>.rename(&quot;name1&quot;, &quot;name2&quot;)</code> = rename file</li>
<li><code>.copy(&quot;name1&quot;, &quot;name2&quot;)</code> = copy file</li>
<li><code>.path(&quot;name1&quot;)</code> = return path of file</li>
</ul></li>
</ul>
<h2 id="r-console-and-evaluation">R Console and Evaluation</h2>
<ul>
<li><code>&lt;-</code> = assignment operator</li>
<li><code>#</code> = comment</li>
<li>expression is evaluated after hitting <code>enter</code> and result is returned</li>
<li>autoprinting occurs when you call a variable
<ul>
<li><code>print(x)</code> = explicitly printing</li>
</ul></li>
<li><code>[1]</code> at the beginning of the output = which element of the vector is being shown</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="r-objects-and-data-structures">R Objects and Data Structures</h2>
<ul>
<li>5 basic/<strong>atomic classes</strong> of objects:
<ol type="1">
<li>character</li>
<li>numeric</li>
<li>integer</li>
<li>complex</li>
<li>logical</li>
</ol></li>
<li><strong>Numbers</strong>
<ul>
<li>numbers generally treated as <code>numeric</code> objects (double precision real numbers - decimals)</li>
<li>Integer objects can be created by adding <code>L</code> to the end of a number(ex. <code>1L</code>)</li>
<li><code>Inf</code> = infinity, can be used in calculations</li>
<li><code>NaN</code> = not a number/undefined</li>
<li><code>sqrt(value)</code> = square root of value</li>
</ul></li>
<li><strong>Variables</strong>
<ul>
<li><code>variable &lt;- value</code> = assignment of a value to a variable name</li>
</ul></li>
</ul>
<h3 id="vectors-and-lists">Vectors and Lists</h3>
<ul>
<li><strong>atomic vector</strong> = contains one data type, most basic object
<ul>
<li><code>vector &lt;- c(value1, value2, ...)</code> = creates a vector with specified values</li>
<li><code>vector1*vector2</code> = element by element multiplication (rather than matrix multiplication)
<ul>
<li>if the vectors are of different lengths, shorter vector will be recycled until the longer runs out</li>
<li>computation on vectors/between vectors (<code>+</code>, <code>-</code>, <code>==</code>, <code>/</code>, etc.) are done element by element by default</li>
<li><code>%*%</code> = force matrix multiplication between vectors/matrices</li>
</ul></li>
<li><code>vector(&quot;class&quot;, n)</code> = creates empty vector of length n and specified class
<ul>
<li><code>vector(&quot;numeric&quot;, 3)</code> = creates 0 0 0</li>
</ul></li>
</ul></li>
<li><code>c()</code> = concatenate
<ul>
<li><code>T, F</code> = shorthand for <code>TRUE</code> and <code>FALSE</code></li>
<li><code>1+0i</code> = complex numbers</li>
</ul></li>
<li><strong>explicit coercion</strong>
<ul>
<li><code>as.numeric(x)</code>, <code>as.logical(x)</code>, <code>as.character(x)</code>, <code>as.complex(x)</code> = convert object from one class to another</li>
<li>nonsensible coercion will result in NA (ex. <code>as.numeric(c(&quot;a&quot;, &quot;b&quot;)</code>)</li>
<li><code>as.list(data.frame)</code> = converts a <code>data.frame</code> object into a <code>list</code> object</li>
<li><code>as.character(list)</code> = converts list into a character vector</li>
</ul></li>
<li><strong>implicit coercion</strong>
<ul>
<li>matrix/vector can only contain one data type, so when attempting to create matrix/vector with different classes, forced coercion occurs to make every element to same class
<ul>
<li><em>least common denominator</em> is the approach used (basically everything is converted to a class that all values can take, numbers <span class="math inline">\(\rightarrow\)</span> characters) and <em>no errors generated</em></li>
<li>coercion occurs to make every element to same class (implicit)</li>
<li><code>x &lt;- c(NA, 2, &quot;D&quot;)</code> will create a vector of character class</li>
</ul></li>
</ul></li>
<li><code>list()</code> = special vector wit different classes of elements
<ul>
<li><code>list</code> = vector of objects of different classes</li>
<li>elements of list use <code>[[]]</code>, elements of other vectors use <code>[]</code></li>
</ul></li>
<li><strong>logical vectors</strong> = contain values <code>TRUE</code>, <code>FALSE</code>, and <code>NA</code>, values are generated as result of logical conditions comparing two objects/values</li>
<li><code>paste(characterVector, collapse = &quot; &quot;)</code> = join together elements of the vector and separating with the <code>collapse</code> parameter</li>
<li><code>paste(vec1, vec2, sep = &quot; &quot;)</code> = join together different vectors and separating with the <code>sep</code> parameter
<ul>
<li><em><strong>Note</strong>: vector recycling applies here too </em></li>
<li><code>LETTERS</code>, <code>letters</code>= predefined vectors for all 26 upper and lower letters</li>
</ul></li>
<li><code>unique(values)</code> = returns vector with all duplicates removed</li>
</ul>
<h3 id="matrices-and-data-frames">Matrices and Data Frames</h3>
<ul>
<li><code>matrix</code> can contain <strong>only 1</strong> type of data</li>
<li><code>data.frame</code> can contain <strong>multiple</strong></li>
<li><code>matrix(values, nrow = n, ncol = m)</code> = creates a n by m matrix
<ul>
<li>constructed <strong>COLUMN WISE</strong> <span class="math inline">\(\rightarrow\)</span> the elements are placed into the matrix from top to bottom for each column, and by column from left to right</li>
<li>matrices can also be created by adding the dimension attribute to vector
<ul>
<li><code>dim(m) &lt;- c(2, 5)</code></li>
</ul></li>
<li>matrices can also be created by binding columns and rows
<ul>
<li><code>rbind(x, y)</code>, <code>cbind(x, y)</code> = combine rows/columns; can be used on vectors or matrices</li>
</ul></li>
<li><code>*</code> and <code>/</code> = element by element computation between two matrices
<ul>
<li><code>%*%</code> = matrix multiplication</li>
</ul></li>
</ul></li>
<li><code>dim(obj)</code> = dimensions of an object (returns <code>NULL</code> if a vector)
<ul>
<li><code>dim(obj) &lt;- c(4, 5)</code> = assign <code>dim</code> attribute to an object
<ul>
<li>if object is a vector, R converts the vector to a n by m matrix (i.e. 4 rows by 5 column from the example command)
<ul>
<li><em><strong>Note</strong>: if n by m is larger than length of vector, then an error is returned</em></li>
</ul></li>
<li><strong><em>example</em></strong> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># initiate a vector</span><br><span class="line">x &lt;-c(NA, 1, &quot;cx&quot;, NA, 2, &quot;dsa&quot;)</span><br><span class="line">class(x)</span><br><span class="line">x</span><br><span class="line"></span><br><span class="line"># convert to matrix</span><br><span class="line">dim(x) &lt;- c(3, 2)</span><br><span class="line">class(x)</span><br><span class="line">x</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
<li><code>data.frame(var = 1:4, var2 = c(….))</code> = creates a data frame
<ul>
<li><code>nrow()</code>, <code>ncol()</code> = returns row and column numbers</li>
<li><code>data.frame(vector, matrix)</code> = takes any number of arguments and returns a single object of class “data.frame” composed of original objects</li>
<li><code>as.data.frame(obj)</code> = converts object to data frame</li>
<li>data frames store tabular data</li>
<li>special type of list where every list has the same length (can be of different type)</li>
<li>data frames are usually created through <code>read.table()</code> and <code>read.csv()</code></li>
<li><code>data.matrix()</code> = converts a data frame to matrix.</li>
</ul></li>
<li><code>colMeans(matrix)</code> or <code>rowMeans(matrix)</code> = returns means of the columns/rows of a matrix/dataframe in a vector</li>
<li><code>as.numeric(rownames(df))</code> = returns row indices for rows of a data frame with unnamed rows</li>
<li><strong>attributes</strong>
<ul>
<li>objects can have attributes: <code>names</code>, <code>dimnames</code>, <code>row.names</code>, <code>dim</code> (matrices, arrays), <code>class</code>, <code>length</code>, or any user-defined ones</li>
<li><code>attributes(obj)</code>, <code>class(obj)</code> = return attributes/class for an R object</li>
<li><code>attr(object, &quot;attribute&quot;) &lt;- &quot;value&quot;</code> = creates/assigns a value to a new/existing attribute for the object</li>
<li><code>names</code> attribute
<ul>
<li>all objects can have names</li>
<li><code>names(x)</code> = returns names (<code>NULL</code> if no name exists)
<ul>
<li><code>names(x) &lt;- c(&quot;a&quot;, …)</code> = can be used to assign names to vectors</li>
</ul></li>
<li><code>list(a = 1, b = 2, …)</code> = <code>a</code>, <code>b</code> are names</li>
<li><code>dimnames(matrix) &lt;- list(c(&quot;a&quot;, &quot;b&quot;), c(&quot;c&quot; , &quot;d&quot;))</code> = assign names to matrices
<ul>
<li>use list of two vectors: row, column in that order</li>
</ul></li>
</ul></li>
<li><code>colnames(data.frame)</code> = return column names (can be used to set column names as well, similar to <code>dim()</code>)</li>
<li><code>row.names</code> = names of rows in the data frame (attribute)</li>
</ul></li>
</ul>
<h3 id="arrays">Arrays</h3>
<ul>
<li>multi-dimensional collection of data with <span class="math inline">\(k\)</span> dimensions
<ul>
<li>matrix = 2 dimensional array</li>
</ul></li>
<li><code>array(data, dim, dimnames)</code>
<ul>
<li><code>data</code> = data to be stored in array</li>
<li><code>dim</code> = dimensions of the array
<ul>
<li><code>dim = c(2, 2, 5)</code> = 3 dimensional array <span class="math inline">\(\rightarrow\)</span> creates 5 2x2 array</li>
</ul></li>
<li><code>dimnames</code> = add names to the dimensions
<ul>
<li>input must be a <code>list</code></li>
<li>every element of the <code>list</code> must correspond in length to the dimensions of the array</li>
<li><code>dimnames(x) &lt;- list(c(&quot;a&quot;, &quot;b&quot;), c(&quot;c&quot;, &quot;d&quot;), c(&quot;e&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;, &quot;i&quot;))</code> = set the names for row, column, and third dimension respectively (2 x 2 x 5 in this case)</li>
</ul></li>
</ul></li>
<li><code>dim()</code> function can be used to create arrays from vectors or matrices
<ul>
<li><code>x &lt;- rnorm(20); dim(x) &lt;- c(2, 2, 5)</code> = converts a 20 element vector to a 2x2x5 array</li>
</ul></li>
</ul>
<h3 id="factors">Factors</h3>
<ul>
<li>factors are used to represent <em>categorical data</em> (integer vector where each value has a label)</li>
<li>2 types: <strong>unordered</strong> vs <strong>ordered</strong></li>
<li>treated specially by <code>lm()</code>, <code>glm()</code></li>
<li>Factors easier to understand because they self describe (vs. 1 and 2)</li>
<li><code>factor(c(&quot;a&quot;, &quot;b&quot;), levels = c(&quot;1&quot;, &quot;2&quot;))</code> = creates factor
<ul>
<li><code>levels()</code> argument can be used to specify baseline levels vs other levels
<ul>
<li><em><strong>Note</strong>:without explicit specification, R uses alphabetical order</em></li>
</ul></li>
<li><code>table(factorVar)</code> = how many of each are in the factor</li>
</ul></li>
</ul>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/08/07/R-Programming-Course-Notes/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Data-Scientists-Toolbox-Course-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/07/Data-Scientists-Toolbox-Course-Notes/" itemprop="url">
                  Data Scientists Toolbox Course Notes
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 11:43:09 / Modified: 11:53:23" itemprop="dateCreated datePublished" datetime="2018-08-07T11:43:09+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">4.8k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">4 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="cli-command-line-interface">CLI (Command Line Interface)</h2>
<ul>
<li><code>/</code> = root directory</li>
<li><code>~</code> = home directory</li>
<li><code>pwd</code> = print working directory (current directory)</li>
<li><code>clear</code> = clear screen</li>
<li><code>ls</code> = list stuff
<ul>
<li><code>-a</code> = see all (hidden)</li>
<li><code>-l</code> = details</li>
</ul></li>
<li><code>cd</code> = change directory</li>
<li><code>mkdir</code> = make directory</li>
<li><code>touch</code> = creates an empty file</li>
<li><code>cp</code> = copy
<ul>
<li><code>cp &lt;file&gt; &lt;directory&gt;</code> = copy a file to a directory</li>
<li><code>cp -r &lt;directory&gt; &lt;newDirectory&gt;</code> = copy all documents from directory to new Directory * <code>-r</code> = recursive</li>
</ul></li>
<li><code>rm</code> = remove
<ul>
<li><code>-r</code> = remove entire directories (no undo)</li>
</ul></li>
<li><code>mv</code> = move
<ul>
<li><code>move &lt;file&gt; &lt;directory&gt;</code> = move file to directory</li>
<li><code>move &lt;fileName&gt; &lt;newName&gt;</code> = rename file</li>
</ul></li>
<li><code>echo</code> = print arguments you give/variables</li>
<li><code>date</code> = print current date</li>
</ul>
<h2 id="github">GitHub</h2>
<ul>
<li><strong>Workflow</strong>
<ol type="1">
<li>make edits in workspace</li>
<li>update index/add files</li>
<li>commit to local repo</li>
<li>push to remote repository</li>
</ol></li>
<li><code>git add .</code> = add all new files to be tracked</li>
<li><code>git add -u</code> = updates tracking for files that are renamed or deleted</li>
<li><code>git add -A</code> = both of the above
<ul>
<li><em><strong>Note</strong>: <code>add</code> is performed before committing </em></li>
</ul></li>
<li><code>git commit -m &quot;message&quot;</code> = commit the changes you want to be saved to the local copy</li>
<li><code>git checkout -b branchname</code> = create new branch</li>
<li><code>git branch</code> = tells you what branch you are on</li>
<li><code>git checkout master</code> = move back to the master branch</li>
<li><code>git pull</code> = merge you changes into other branch/repo (pull request, sent to owner of the repo)</li>
<li><code>git push</code> = commit local changes to remote (GitHub)</li>
</ul>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/08/07/Data-Scientists-Toolbox-Course-Notes/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/06/JHU-Coursera-Rprogramming-Course-Project-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/06/JHU-Coursera-Rprogramming-Course-Project-3/" itemprop="url">
                  JHU Coursera Rprogramming Course Project 3
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-06 22:08:38" itemprop="dateCreated datePublished" datetime="2018-08-06T22:08:38+08:00">2018-08-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-08-07 11:53:15" itemprop="dateModified" datetime="2018-08-07T11:53:15+08:00">2018-08-07</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">4.6k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">4 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <blockquote>
<p>JHU DataScience Specialization/Cousers Rprogramming/Week3/Course Project 3</p>
</blockquote>
<h1 id="医院病例数据分析">医院病例数据分析</h1>
<p>作业练习目标:通过分析医院数据,编写函数,通过函数分析各州医院指定的病例排名</p>
<p><a href="https://spark-public.s3.amazonaws.com/compdata/data/ProgAssignment3-data.zip" target="_blank" rel="noopener">数据文档打包</a></p>
<ul>
<li>[x] 数据源:outcome-of-care-measures.csv</li>
</ul>
<p>Contains information about THIRTY(30)-day mortality and readmission rates for heart attacks,heart failure, and pneumonia for over FOUR THOUSAND (4,000) hospitals;</p>
<ul>
<li>[x] 说明书:Hospital_Revised_Flatfiles.pdf</li>
</ul>
<h2 id="天死亡最率最低的医院">30天死亡最率最低的医院</h2>
<p>输出指定州(例子德州TX)函数(best)</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">best &lt;- <span class="keyword">function</span>(state, outcome) &#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">## Read the outcome data</span></span><br><span class="line">  dat &lt;- read.csv(<span class="string">"outcome-of-care-measures.csv"</span>, colClasses = <span class="string">"character"</span>)</span><br><span class="line">  <span class="comment">## Check that state and outcome are valid 病例分类，判断州名称</span></span><br><span class="line">  <span class="keyword">if</span> (!state %<span class="keyword">in</span>% unique(dat[, <span class="number">7</span>])) &#123;</span><br><span class="line">    <span class="keyword">stop</span>(<span class="string">"invalid state"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">switch</span>(outcome, `heart attack` = &#123;</span><br><span class="line">    col = <span class="number">11</span></span><br><span class="line">  &#125;, `heart failure` = &#123;</span><br><span class="line">    col = <span class="number">17</span></span><br><span class="line">  &#125;, pneumonia = &#123;</span><br><span class="line">    col = <span class="number">23</span></span><br><span class="line">  &#125;, <span class="keyword">stop</span>(<span class="string">"invalid outcome"</span>))</span><br><span class="line">  <span class="comment">## Return hospital name in that state with lowest 30-day death rate</span></span><br><span class="line">  df = dat[dat$State == state, c(<span class="number">2</span>, col)]</span><br><span class="line">  df[which.min(df[, <span class="number">2</span>]), <span class="number">1</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>例子:输出指定州(德州TX)30天死亡最率最低的医院</p>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## Warning <span class="keyword">in</span> which.min(df[, <span class="number">2</span>]): 强制改变过程中产生了NA</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## [<span class="number">1</span>] <span class="string">"Hospital with lowest 30-day death rate is: CYPRESS FAIRBANKS MEDICAL CENTER"</span></span><br></pre></td></tr></table></figure>
<h2 id="心脏病死亡率最低医院">心脏病死亡率最低医院</h2>
<p>输出前10字母排名州的心脏病死亡最低医院(rankall)</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/08/06/JHU-Coursera-Rprogramming-Course-Project-3/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/06/JHU-Coursera-Rprogramming-Assignment-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/06/JHU-Coursera-Rprogramming-Assignment-2/" itemprop="url">
                  JHU Coursera Rprogramming Assignment 2
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-06 18:26:18" itemprop="dateCreated datePublished" datetime="2018-08-06T18:26:18+08:00">2018-08-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-08-07 11:53:02" itemprop="dateModified" datetime="2018-08-07T11:53:02+08:00">2018-08-07</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">2.8k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">3 mins.</span>
              
            </div>
          

          
              <div class="post-description">JHU Coursera Rprogramming Assignment 2</div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <blockquote>
<p>JHU DataScience Specialization/Cousers R Programming/Week2/Programming Assignment 2</p>
</blockquote>
<h1 id="programming-assignment-2-函数与缓存">Programming Assignment 2 函数与缓存</h1>
<p>This two functions below are used to create a special object that stores a numeric matrix and cache’s its inverse</p>
<h2 id="第一步编写一个函数存储四个函数">第一步编写一个函数存储四个函数</h2>
<p>makeCacheMatrix creates a list containing a function to 1. set the value of the matrix 2. get the value of the matrix 3. set the value of inverse of the matrix 4. get the value of inverse of the matrix</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">makeCacheMatrix &lt;- <span class="keyword">function</span>(x = matrix()) &#123;</span><br><span class="line">    inv &lt;- <span class="literal">NULL</span></span><br><span class="line">    set &lt;- <span class="keyword">function</span>(y) &#123;</span><br><span class="line">        x &lt;&lt;- y</span><br><span class="line">        inv &lt;&lt;- <span class="literal">NULL</span></span><br><span class="line">    &#125;</span><br><span class="line">    get &lt;- <span class="keyword">function</span>() x</span><br><span class="line">    setinverse &lt;- <span class="keyword">function</span>(inverse) inv &lt;&lt;- inverse</span><br><span class="line">    getinverse &lt;- <span class="keyword">function</span>() inv</span><br><span class="line">    list(set=set, get=get, setinverse=setinverse, getinverse=getinverse)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/08/06/JHU-Coursera-Rprogramming-Assignment-2/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/06/JHU-Coursera-Datatools-Course-Project/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/06/JHU-Coursera-Datatools-Course-Project/" itemprop="url">
                  JHU Coursera Datatools Course Project
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-06 17:28:52" itemprop="dateCreated datePublished" datetime="2018-08-06T17:28:52+08:00">2018-08-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-08-07 11:52:54" itemprop="dateModified" datetime="2018-08-07T11:52:54+08:00">2018-08-07</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">6k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">5 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <blockquote>
<p>JHU DataScience Specialization/Cousers The Data Scientist’s Toolbox/Week??/Course Project</p>
</blockquote>
<h1 id="datatools-course-project-数据科学工具">Datatools Course Project 数据科学工具</h1>
<p>由于这个非常入门的课程，共4Week每周一个小quiz，这个应该是最后的Project忘记了 主要是熟练掌握一些R语言的数据处理工具例如 xlsx,XML等格式，以及readr这些有用的R包用法 以下代码下载资源比较大暂不执行有兴趣的读者可以自己尝试</p>
<h2 id="读取csv格式-2006年美国社区调查acs">读取csv格式 2006年美国社区调查（ACS）</h2>
<p>The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:</p>
<ul>
<li>(pid, Population CSV file)</li>
<li>(hid, Household CSV file)</li>
</ul>
<p><a href="https://www.nber.org/acs/PUMS/README" target="_blank" rel="noopener">说明书 PUMS</a> <a href="https://www.nber.org/acs/PUMS/2006/PUMSDataDict06.txt" target="_blank" rel="noopener">说明书 DATA DICTIONARY - 2006 HOUSING</a> <a href="https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf" target="_blank" rel="noopener">PDF</a></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fileUrl &lt;- <span class="string">"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"</span></span><br><span class="line">download.file(fileUrl,destfile = <span class="string">"Fss06pid.csv"</span>,method = <span class="string">"libcurl"</span>)</span><br><span class="line">fileUrl &lt;- <span class="string">"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"</span></span><br><span class="line">download.file(fileUrl,destfile = <span class="string">"Fss06hid.csv"</span>,method = <span class="string">"libcurl"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="读取几十上百m的数据算大文件运算应该考虑花销">读取几十上百m的数据算大文件，运算应该考虑花销</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Unit: milliseconds</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                           expr       min        lq</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  PID &lt;- fread(file = "Fss06pid.csv", fill = T)  13.40045  13.40045</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                read_csv(file = "Fss06pid.csv") 627.03159 627.03159</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                read.csv(file = "Fss06pid.csv") 422.56456 422.56456</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#       mean    median        uq       max neval</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   13.40045  13.40045  13.40045  13.40045     1</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  627.03159 627.03159 627.03159 627.03159     1</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  422.56456 422.56456 422.56456 422.56456     1</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Unit: milliseconds</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                           expr       min        lq</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  HID &lt;- fread(file = "Fss06hid.csv", fill = T)  17.08404  17.08404</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                read_csv(file = "Fss06hid.csv") 519.85311 519.85311</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                read.csv(file = "Fss06hid.csv") 971.56116 971.56116</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#       mean    median        uq       max neval</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   17.08404  17.08404  17.08404  17.08404     1</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  519.85311 519.85311 519.85311 519.85311     1</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  971.56116 971.56116 971.56116 971.56116     1</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Unit: microseconds</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                       expr      min        lq       mean</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#         tapply(PID$pwgtp15, PID$SEX, mean)  159.973  203.6820  262.39322</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  sapply(split(PID$pwgtp15, PID$SEX), mean)  137.322  162.9805  250.35640</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          mean(PID[PID$SEX == 1, ]$pwgtp15) 1533.888 1615.2890 2570.51404</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#            mean(PID$pwgtp15, by = PID$SEX)   11.326   16.1045   20.48577</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          median(PID$pwgtp15, by = PID$SEX)   49.550   64.7685   91.37941</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#         mean(PID[PID$SEX == 1, ]$SERIALNO) 1548.398 1624.1370 3693.22785</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#       median(PID[PID$SEX == 1, ]$SERIALNO) 1630.508 1709.9625 2321.75111</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     median        uq        max neval cld</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   267.5640  289.3300    441.692   100  a </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   215.8915  246.1520   3571.401   100  a </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  1694.3900 4332.1510   6311.799   100   b</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    18.2280   22.4745     78.571   100  a </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    85.2955  109.8930    249.160   100  a </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  1740.4000 2707.6610 127876.038   100   b</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  1751.5480 2145.2830   6184.035   100   b</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pander(arrange(A,mean))</span><br></pre></td></tr></table></figure>
<table>
<caption>Table continues below</caption>
<thead>
<tr class="header">
<th style="text-align: center;">expr</th>
<th style="text-align: center;">min</th>
<th style="text-align: center;">lq</th>
<th style="text-align: center;">mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">PID &lt;- fread(file = “Fss06pid.csv”, fill = T)</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">13.4</td>
</tr>
<tr class="even">
<td style="text-align: center;">read.csv(file = “Fss06pid.csv”)</td>
<td style="text-align: center;">422.6</td>
<td style="text-align: center;">422.6</td>
<td style="text-align: center;">422.6</td>
</tr>
<tr class="odd">
<td style="text-align: center;">read_csv(file = “Fss06pid.csv”)</td>
<td style="text-align: center;">627</td>
<td style="text-align: center;">627</td>
<td style="text-align: center;">627</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">median</th>
<th style="text-align: center;">uq</th>
<th style="text-align: center;">max</th>
<th style="text-align: center;">neval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">422.6</td>
<td style="text-align: center;">422.6</td>
<td style="text-align: center;">422.6</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">627</td>
<td style="text-align: center;">627</td>
<td style="text-align: center;">627</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pander(arrange(B,mean))</span><br></pre></td></tr></table></figure>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/08/06/JHU-Coursera-Datatools-Course-Project/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/03/jhu-coursera-regression-model-quizes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/08/03/jhu-coursera-regression-model-quizes/" itemprop="url">
                  JHU Coursera Regression Model Quizes
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-03 00:00:00" itemprop="dateCreated datePublished" datetime="2018-08-03T00:00:00+08:00">2018-08-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-08-06 10:30:55" itemprop="dateModified" datetime="2018-08-06T10:30:55+08:00">2018-08-06</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/DataScience/" itemprop="url" rel="index"><span itemprop="name">DataScience</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">8.9k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">8 mins.</span>
              
            </div>
          

          
              <div class="post-description">JHU Coursera Regression Model Quizes.</div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <blockquote>
<p>JHU DataScience Specialization/Cousers Reproducible Data/Week1-4/Regression Model Quizes</p>
</blockquote>
<h1 id="jhu-coursera-regression-model-quizes">JHU Coursera Regression Model Quizes</h1>
<p>主要练习手工计算回归模型的基础方法</p>
<h2 id="week-2">Week 2</h2>
<h3 id="quiz-1">Quiz 1</h3>
<p>手算均值</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- c(<span class="number">0.18</span>, -<span class="number">1.54</span>, <span class="number">0.42</span>, <span class="number">0.95</span>)</span><br><span class="line">w &lt;- c(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">mu.y &lt;- sum(w * x) / sum(w)</span><br><span class="line">sprintf(<span class="string">"mean of y is : %f"</span>,mu.y)</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## [<span class="number">1</span>] <span class="string">"mean of y is : 0.147143"</span></span><br></pre></td></tr></table></figure>
<h3 id="quiz-2">Quiz 2</h3>
<p>线性回归</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- c(<span class="number">0.8</span>, <span class="number">0.47</span>, <span class="number">0.51</span>, <span class="number">0.73</span>, <span class="number">0.36</span>, <span class="number">0.58</span>, <span class="number">0.57</span>, <span class="number">0.85</span>, <span class="number">0.44</span>, <span class="number">0.42</span>)</span><br><span class="line">y &lt;- c(<span class="number">1.39</span>, <span class="number">0.72</span>, <span class="number">1.55</span>, <span class="number">0.48</span>, <span class="number">1.19</span>, -<span class="number">1.59</span>, <span class="number">1.23</span>, -<span class="number">0.65</span>, <span class="number">1.49</span>, <span class="number">0.05</span>)</span><br><span class="line">pander(lm(y~x)) <span class="comment">#THROUGH THE ORIGIN</span></span><br></pre></td></tr></table></figure>
<table>
<caption>Fitting linear model: y ~ x</caption>
<thead>
<tr class="header">
<th style="text-align: center;"> </th>
<th style="text-align: center;">Estimate</th>
<th style="text-align: center;">Std. Error</th>
<th style="text-align: center;">t value</th>
<th style="text-align: center;">Pr(&gt;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>(Intercept)</strong></td>
<td style="text-align: center;">1.567</td>
<td style="text-align: center;">1.252</td>
<td style="text-align: center;">1.252</td>
<td style="text-align: center;">0.246</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>x</strong></td>
<td style="text-align: center;">-1.713</td>
<td style="text-align: center;">2.105</td>
<td style="text-align: center;">-0.8136</td>
<td style="text-align: center;">0.4394</td>
</tr>
</tbody>
</table>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pander(lm(y~x-<span class="number">1</span>)) <span class="comment">#去除截距</span></span><br></pre></td></tr></table></figure>
<table>
<caption>Fitting linear model: y ~ x - 1</caption>
<thead>
<tr class="header">
<th style="text-align: center;"> </th>
<th style="text-align: center;">Estimate</th>
<th style="text-align: center;">Std. Error</th>
<th style="text-align: center;">t value</th>
<th style="text-align: center;">Pr(&gt;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>x</strong></td>
<td style="text-align: center;">0.8263</td>
<td style="text-align: center;">0.5817</td>
<td style="text-align: center;">1.421</td>
<td style="text-align: center;">0.1892</td>
</tr>
</tbody>
</table>
<h3 id="quiz-3">Quiz 3</h3>
<p>mtcars 回归系数</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">(Intercept)</th>
<th style="text-align: center;">wt</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">37.29</td>
<td style="text-align: center;">-5.344</td>
</tr>
</tbody>
</table>
<h3 id="quiz-4">Quiz 4</h3>
<p>练习求b1</p>
<p><span class="math display">\[\begin{align}
Cor(Y,X) &amp;= 0.5 \qquad Sd(Y) = 1 \qquad Sd(X) = 0.5 \\
 \beta_1 &amp;= Cor(Y,X) * \frac{Sd(Y)}{Sd(X)}
\end{align}\]</span></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">B1 = <span class="number">0.5</span> * <span class="number">1</span> / <span class="number">0.5</span></span><br></pre></td></tr></table></figure>
<h3 id="quiz-5">Quiz 5</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">corr &lt;- <span class="number">.4</span>; emean &lt;- <span class="number">0</span>; varr1 &lt;- <span class="number">1</span></span><br><span class="line">varr2 &lt;- <span class="number">1</span>; b0 &lt;- <span class="number">0</span>; x &lt;- <span class="number">1.5</span></span><br><span class="line">b1 &lt;- corr * sqrt(varr1) / sqrt(varr2)</span><br><span class="line">(y &lt;- b0 + b1 * x)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [1] 0.6</span></span></span><br></pre></td></tr></table></figure>
<h3 id="quiz-6">Quiz 6</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- c(<span class="number">8.58</span>, <span class="number">10.46</span>, <span class="number">9.01</span>, <span class="number">9.64</span>, <span class="number">8.86</span>)</span><br><span class="line">(x - mean(x)) / sd(x) <span class="comment"># Choose No.1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## <span class="selector-attr">[1]</span> <span class="selector-tag">-0</span><span class="selector-class">.9718658</span>  1<span class="selector-class">.5310215</span> <span class="selector-tag">-0</span><span class="selector-class">.3993969</span>  0<span class="selector-class">.4393366</span> <span class="selector-tag">-0</span><span class="selector-class">.5990954</span></span><br></pre></td></tr></table></figure>
<h3 id="quiz-7">Quiz 7</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- c(<span class="number">0.8</span>, <span class="number">0.47</span>, <span class="number">0.51</span>, <span class="number">0.73</span>, <span class="number">0.36</span>, <span class="number">0.58</span>, <span class="number">0.57</span>, <span class="number">0.85</span>, <span class="number">0.44</span>, <span class="number">0.42</span>)</span><br><span class="line">y &lt;- c(<span class="number">1.39</span>, <span class="number">0.72</span>, <span class="number">1.55</span>, <span class="number">0.48</span>, <span class="number">1.19</span>, -<span class="number">1.59</span>, <span class="number">1.23</span>, -<span class="number">0.65</span>, <span class="number">1.49</span>, <span class="number">0.05</span>)</span><br><span class="line">pander(lm(y~x))</span><br></pre></td></tr></table></figure>
<table>
<caption>Fitting linear model: y ~ x</caption>
<thead>
<tr class="header">
<th style="text-align: center;"> </th>
<th style="text-align: center;">Estimate</th>
<th style="text-align: center;">Std. Error</th>
<th style="text-align: center;">t value</th>
<th style="text-align: center;">Pr(&gt;</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>(Intercept)</strong></td>
<td style="text-align: center;">1.567</td>
<td style="text-align: center;">1.252</td>
<td style="text-align: center;">1.252</td>
<td style="text-align: center;">0.246</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>x</strong></td>
<td style="text-align: center;">-1.713</td>
<td style="text-align: center;">2.105</td>
<td style="text-align: center;">-0.8136</td>
<td style="text-align: center;">0.4394</td>
</tr>
</tbody>
</table>
<h3 id="quiz-8">Quiz 8</h3>
<p>It must be identically 0.</p>
<h3 id="quiz-9">Quiz 9</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- c(<span class="number">0.8</span>, <span class="number">0.47</span>, <span class="number">0.51</span>, <span class="number">0.73</span>, <span class="number">0.36</span>, <span class="number">0.58</span>, <span class="number">0.57</span>, <span class="number">0.85</span>, <span class="number">0.44</span>, <span class="number">0.42</span>)</span><br><span class="line">mean(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [1] 0.573</span></span></span><br></pre></td></tr></table></figure>
<h3 id="quiz-10">Quiz 10</h3>
<p><span class="math display">\[\begin{align}
\beta_1 &amp;= Cor(Y,X)*Sd(Y)/Sd(X) \\
    Y_1 &amp;= Cor(Y,X)*Sd(X)/Sd(Y) \\
\beta_1/Y_1 &amp;= Sd(Y)^2/Sd(X)^2 \notag \\ &amp;= Var(Y)/Var(X)
\end{align}\]</span></p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/08/03/jhu-coursera-regression-model-quizes/#more" rel="contents">
              Read more &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    
      


    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avator.png"
                alt="Autoz" />
            
              <p class="site-author-name" itemprop="name">Autoz</p>
              <p class="site-description motion-element" itemprop="description">Fidelty.</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/autolordz" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:autolordz@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Autoz</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Symbols count total: </span>
    
    <span title="Symbols count total">514k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    
    <span title="Reading time total">7:47</span>
  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.5.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Gemini</a> v6.3.0</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="Total Visitors">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="Total Views">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.3.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  










  





  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=6.3.0"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=6.3.0"></script>


  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('Copy').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('Copied')
          else $(this).text('Copy failed')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('Copy')
        }, 300)
      }).append(e)
    })
  </script>


</body>
</html>
