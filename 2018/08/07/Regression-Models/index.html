<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.3.0',
    sidebar: {"position":"right","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="\(\pagebreak\) Introduction to Regression  linear regression/linear models \(\rightarrow\) go to procedure to analyze data Francis Galton invented the term and concepts of regression and correlati">
<meta name="keywords" content="R,DataScience,R Markdown">
<meta property="og:type" content="article">
<meta property="og:title" content="Regression Models">
<meta property="og:url" content="http://yoursite.com/2018/08/07/Regression-Models/index.html">
<meta property="og:site_name" content="AutozLand">
<meta property="og:description" content="\(\pagebreak\) Introduction to Regression  linear regression/linear models \(\rightarrow\) go to procedure to analyze data Francis Galton invented the term and concepts of regression and correlati">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2018-08-07T06:21:13.493Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Regression Models">
<meta name="twitter:description" content="\(\pagebreak\) Introduction to Regression  linear regression/linear models \(\rightarrow\) go to procedure to analyze data Francis Galton invented the term and concepts of regression and correlati">






  <link rel="canonical" href="http://yoursite.com/2018/08/07/Regression-Models/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Regression Models | AutozLand</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AutozLand</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Autoz's Learning Blogs</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives<span class="badge">12</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags<span class="badge">3</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories<span class="badge">2</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />About</a>
  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Regression-Models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Regression Models<a href="https://github.com/theme-next/theme-next.org/_posts/tree/master/Regression-Models.md" class="post-edit-link" title="Edit this post" target="_blank">
                    <i class="fa fa-pencil"></i>
                  </a>
                
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 14:16:32 / Modified: 14:21:13" itemprop="dateCreated datePublished" datetime="2018-08-07T14:16:32+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon"
            >
            <i class="fa fa-eye"></i>
             Views:  
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">161k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">2:27</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="introduction-to-regression">Introduction to Regression</h2>
<ul>
<li>linear regression/linear models <span class="math inline">\(\rightarrow\)</span> go to procedure to analyze data</li>
<li><em>Francis Galton</em> invented the term and concepts of regression and correlation
<ul>
<li>he predicted child’s height from parents height</li>
</ul></li>
<li>questions that regression can help answer
<ul>
<li>prediction of one thing from another</li>
<li>find simple, interpretable, meaningful model to predict the data</li>
<li>quantify and investigate variations that are unexplained or unrelated to the predictor <span class="math inline">\(\rightarrow\)</span> <strong>residual variation</strong></li>
<li>quantify the effects of other factors may have on the outcome</li>
<li>assumptions to generalize findings beyond data we have <span class="math inline">\(\rightarrow\)</span> <strong>statistical inference</strong></li>
<li><strong>regression to the mean</strong> (see below)</li>
</ul></li>
</ul>
<h2 id="notation">Notation</h2>
<ul>
<li>regular letters (i.e. <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>) = generally used to denote <strong>observed</strong> variables</li>
<li>Greek letters (i.e. <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma\)</span>) = generally used to denote <strong>unknown</strong> variables that we are trying to estimate</li>
<li><span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> describes <span class="math inline">\(n\)</span> data points</li>
<li><span class="math inline">\(\bar X\)</span>, <span class="math inline">\(\bar Y\)</span> = observed means for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></li>
<li><span class="math inline">\(\hat \beta_0\)</span>, <span class="math inline">\(\hat \beta_1\)</span> = estimators for true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></li>
</ul>
<h3 id="empiricalsample-mean">Empirical/Sample Mean</h3>
<ul>
<li><strong>empirical mean</strong> is defined as <span class="math display">\[\bar X = \frac{1}{n}\sum_{i=1}^n X_i\]</span></li>
<li><strong>centering</strong> the random variable is defined as <span class="math display">\[\tilde X_i = X_i - \bar X\]</span>
<ul>
<li>mean of <span class="math inline">\(\tilde X_i\)</span> = 0</li>
</ul></li>
</ul>
<h3 id="empiricalsample-standard-deviation-variance">Empirical/Sample Standard Deviation &amp; Variance</h3>
<ul>
<li><strong>empirical variance</strong> is defined as <span class="math display">\[
S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X)^2
= \frac{1}{n-1} \left( \sum_{i=1}^n X_i^2 - n \bar X ^ 2 \right)
\Leftarrow \mbox{shortcut for calculation}\]</span></li>
<li><strong>empirical standard deviation</strong> is defined as <span class="math inline">\(S = \sqrt{S^2}\)</span>
<ul>
<li>average squared distances between the observations and the mean</li>
<li>has same units as the data</li>
</ul></li>
<li><strong>scaling</strong> the random variables is defined as <span class="math inline">\(X_i / S\)</span>
<ul>
<li>standard deviation of <span class="math inline">\(X_i / S\)</span> = 1</li>
</ul></li>
</ul>
<h3 id="normalization">Normalization</h3>
<ul>
<li><strong>normalizing</strong> the data/random variable is defined <span class="math display">\[Z_i = \frac{X_i - \bar X}{s}\]</span>
<ul>
<li>empirical mean = 0, empirical standard deviation = 1</li>
<li>distribution centered around 0 and data have units = # of standard deviations away from the original mean
<ul>
<li><strong><em>example</em></strong>: <span class="math inline">\(Z_1 = 2\)</span> means that the data point is 2 standard deviations larger than the original mean</li>
</ul></li>
</ul></li>
<li>normalization makes non-comparable data <strong>comparable</strong></li>
</ul>
<h3 id="empirical-covariance-correlation">Empirical Covariance &amp; Correlation</h3>
<ul>
<li>Let <span class="math inline">\((X_i, Y_i)\)</span> = pairs of data</li>
<li><strong>empirical covariance</strong> is defined as <span class="math display">\[
Cov(X, Y) = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar X) (Y_i - \bar Y) = \frac{1}{n-1}\left( \sum_{i=1}^n X_i Y_i - n \bar X \bar Y\right)
\]</span>
<ul>
<li>has units of <span class="math inline">\(X \times\)</span> units of <span class="math inline">\(Y\)</span></li>
</ul></li>
<li><strong>correlation</strong> is defined as <span class="math display">\[Cor(X, Y) = \frac{Cov(X, Y)}{S_x S_y}\]</span> where <span class="math inline">\(S_x\)</span> and <span class="math inline">\(S_y\)</span> are the estimates of standard deviations for the <span class="math inline">\(X\)</span> observations and <span class="math inline">\(Y\)</span> observations, respectively
<ul>
<li>the value is effectively the covariance standardized into a unit-less quantity</li>
<li><span class="math inline">\(Cor(X, Y) = Cor(Y, X)\)</span></li>
<li><span class="math inline">\(-1 \leq Cor(X, Y) \leq 1\)</span></li>
<li><span class="math inline">\(Cor(X,Y) = 1\)</span> and <span class="math inline">\(Cor(X, Y) = -1\)</span> only when the <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> observations fall perfectly on a positive or negative sloped line, respectively</li>
<li><span class="math inline">\(Cor(X, Y)\)</span> measures the strength of the linear relationship between the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> data, with stronger relationships as <span class="math inline">\(Cor(X,Y)\)</span> heads towards -1 or 1</li>
<li><span class="math inline">\(Cor(X, Y) = 0\)</span> implies no linear relationship</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="daltons-data-and-least-squares">Dalton’s Data and Least Squares</h2>
<ul>
<li>collected data from 1885 in <code>UsingR</code> package</li>
<li>predicting children’s heights from parents’ height</li>
<li>observations from the marginal/individual parent/children distributions</li>
<li>looking only at the children’s dataset to find the best predictor
<ul>
<li>“middle” of children’s dataset <span class="math inline">\(\rightarrow\)</span> best predictor</li>
<li>“middle” <span class="math inline">\(\rightarrow\)</span> center of mass <span class="math inline">\(\rightarrow\)</span> mean of the dataset
<ul>
<li>Let <span class="math inline">\(Y_i\)</span> = height of child <span class="math inline">\(i\)</span> for <span class="math inline">\(i = 1, \ldots, n = 928\)</span>, the “middle” = <span class="math inline">\(\mu\)</span> such that <span class="math display">\[\sum_{i=1}^n (Y_i - \mu)^2\]</span></li>
<li><span class="math inline">\(\mu = \bar Y\)</span> for the above sum to be the smallest <span class="math inline">\(\rightarrow\)</span> <strong>least squares = empirical mean</strong></li>
</ul></li>
<li><em><strong>Note</strong>: <code>manipulate</code> function can help to show this </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># load necessary packages/install if needed</span><br><span class="line">library(ggplot2); library(UsingR); data(galton)</span><br><span class="line"># function to plot the histograms</span><br><span class="line">myHist &lt;- function(mu)&#123;</span><br><span class="line">	# calculate the mean squares</span><br><span class="line">    mse &lt;- mean((galton$child - mu)^2)</span><br><span class="line">    # plot histogram</span><br><span class="line">    g &lt;- ggplot(galton, aes(x = child)) + geom_histogram(fill = &quot;salmon&quot;,</span><br><span class="line">    	colour = &quot;black&quot;, binwidth=1)</span><br><span class="line">    # add vertical line marking the center value mu</span><br><span class="line">    g &lt;- g + geom_vline(xintercept = mu, size = 2)</span><br><span class="line">    g &lt;- g + ggtitle(paste(&quot;mu = &quot;, mu, &quot;, MSE = &quot;, round(mse, 2), sep = &quot;&quot;))</span><br><span class="line">    g</span><br><span class="line">&#125;</span><br><span class="line"># manipulate allows the user to change the variable mu to see how the mean squares changes</span><br><span class="line">#   library(manipulate); manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))]</span><br><span class="line"># plot the correct graph</span><br><span class="line">myHist(mean(galton$child))</span><br></pre></td></tr></table></figure>
<ul>
<li><p>in order to visualize the parent-child height relationship, a scatter plot can be used</p></li>
<li><p><em><strong>Note</strong>: because there are multiple data points for the same parent/child combination, a third dimension (size of point) should be used when constructing the scatter plot </em></p></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">library(dplyr)</span><br><span class="line"># constructs table for different combination of parent-child height</span><br><span class="line">freqData &lt;- as.data.frame(table(galton$child, galton$parent))</span><br><span class="line">names(freqData) &lt;- c(&quot;child (in)&quot;, &quot;parent (in)&quot;, &quot;freq&quot;)</span><br><span class="line"># convert to numeric values</span><br><span class="line">freqData$child &lt;- as.numeric(as.character(freqData$child))</span><br><span class="line">freqData$parent &lt;- as.numeric(as.character(freqData$parent))</span><br><span class="line"># filter to only meaningful combinations</span><br><span class="line">g &lt;- ggplot(filter(freqData, freq &gt; 0), aes(x = parent, y = child))</span><br><span class="line">g &lt;- g + scale_size(range = c(2, 20), guide = &quot;none&quot; )</span><br><span class="line"># plot grey circles slightly larger than data as base (achieve an outline effect)</span><br><span class="line">g &lt;- g + geom_point(colour=&quot;grey50&quot;, aes(size = freq+10, show_guide = FALSE))</span><br><span class="line"># plot the accurate data points</span><br><span class="line">g &lt;- g + geom_point(aes(colour=freq, size = freq))</span><br><span class="line"># change the color gradient from default to lightblue -&gt; $white</span><br><span class="line">g &lt;- g + scale_colour_gradient(low = &quot;lightblue&quot;, high=&quot;white&quot;)</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h3 id="derivation-for-least-squares-empirical-mean-finding-the-minimum">Derivation for Least Squares = Empirical Mean (Finding the Minimum)</h3>
<ul>
<li>Let <span class="math inline">\(X_i =\)</span> <strong>regressor</strong>/<strong>predictor</strong>, and <span class="math inline">\(Y_i =\)</span> <strong>outcome</strong>/<strong>result</strong> so we want to minimize the the squares: <span class="math display">\[\sum_{i=1}^n (Y_i - \mu)^2\]</span></li>
<li>Proof is as follows <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \mu)^2 &amp; = \sum_{i=1}^n (Y_i - \bar Y + \bar Y - \mu)^2 \Leftarrow \mbox{added}  \pm \bar Y \mbox{which is adding 0 to the original equation}\\
(expanding~the~terms)&amp; = \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 \sum_{i=1}^n (Y_i - \bar Y)  (\bar Y - \mu) + \sum_{i=1}^n (\bar Y - \mu)^2 \Leftarrow (Y_i - \bar Y), (\bar Y - \mu) \mbox{ are the terms}\\
(simplifying) &amp; = \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 (\bar Y - \mu) \sum_{i=1}^n (Y_i - \bar Y)  +\sum_{i=1}^n (\bar Y - \mu)^2 \Leftarrow (\bar Y - \mu) \mbox{ does not depend on } i \\
(simplifying) &amp; = \sum_{i=1}^n (Y_i - \bar Y)^2 + 2 (\bar Y - \mu)  (\sum_{i=1}^n Y_i - n \bar Y) +\sum_{i=1}^n (\bar Y - \mu)^2 \Leftarrow \sum_{i=1}^n \bar Y \mbox{ is equivalent to }n \bar Y\\
(simplifying) &amp; = \sum_{i=1}^n (Y_i - \bar Y)^2 + \sum_{i=1}^n (\bar Y - \mu)^2 \Leftarrow \sum_{i=1}^n Y_i - n \bar Y = 0 \mbox{ since } \sum_{i=1}^n Y_i = n \bar Y  \\
\sum_{i=1}^n (Y_i - \mu)^2 &amp; \geq \sum_{i=1}^n (Y_i - \bar Y)^2  \Leftarrow \sum_{i=1}^n (\bar Y - \mu)^2 \mbox{ is always} \geq 0 \mbox{ so we can take it out to form the inequality}
\end{aligned}
\]</span>
<ul>
<li>because of the inequality above, to minimize the sum of the squares <span class="math inline">\(\sum_{i=1}^n (Y_i - \mu)^2\)</span>, <strong><span class="math inline">\(\bar Y\)</span> must be equal to <span class="math inline">\(\mu\)</span></strong></li>
</ul></li>
<li>An alternative approach to finding the minimum is taking the <strong><em>derivative</em></strong> with respect to <span class="math inline">\(\mu\)</span> <span class="math display">\[
\begin{aligned}
\frac{d(\sum_{i=1}^n (Y_i - \mu)^2)}{d\mu} &amp; = 0 \Leftarrow \mbox{setting this equal to 0 to find minimum}\\
-2\sum_{i=1}^n (Y_i - \mu) &amp; = 0 \Leftarrow \mbox{divide by -2 on both sides and move } \mu \mbox{ term over to the right}\\
\sum_{i=1}^n Y_i &amp; = \sum_{i=1}^n \mu \Leftarrow \mbox{for the two sums to be equal, all the terms must be equal}\\
Y_i &amp; = \mu \\
\end{aligned}
\]</span></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="regression-through-the-origin">Regression through the Origin</h2>
<ul>
<li>Let <span class="math inline">\(X_i =\)</span> parents’ heights (<strong>regressor</strong>) and <span class="math inline">\(Y_i =\)</span> children’s heights (<strong>outcome</strong>)</li>
<li>find a line with slope <span class="math inline">\(\beta\)</span> that passes through the origin at (0,0) <span class="math display">\[Y_i = X_i \beta\]</span> such that it minimizes <span class="math display">\[\sum_{i=1}^n (Y_i - X_i \beta)^2\]</span></li>
<li><em><strong>Note</strong>: it is generally a bad practice forcing the line through (0, 0) </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># install grid and png packages if not present</span><br><span class="line">library(png); library(grid); doMC::registerDoMC(cores = 4)</span><br><span class="line">grid.raster(readPNG(&quot;figures/1.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em>Centering Data/Gaussian Elimination</em></strong>
<ul>
<li><em><strong>Note</strong>: this is <strong>different</strong> from regression through the origin, because it is effectively moving the regression line </em></li>
<li>subtracting the means from the <span class="math inline">\(X_i\)</span>s and <span class="math inline">\(Y_i\)</span>s moves the origin (reorienting the axes) to the center of the data set so that a regression line can be constructed</li>
<li><em><strong>Note</strong>: the line constructed here has an <strong>equivalent slope</strong> as the result from linear regression with intercept </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/2.png&quot;))</span><br></pre></td></tr></table></figure>
<h3 id="derivation-for-beta">Derivation for <span class="math inline">\(\beta\)</span></h3>
<ul>
<li><p>Let <span class="math inline">\(Y = \beta X\)</span>, and <span class="math inline">\(\hat \beta\)</span> = estimate of <span class="math inline">\(\beta\)</span>, the slope of the least square regression line <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - X_i \beta)^2 &amp; = \sum_{i=1}^n \left[ (Y_i - X_i \hat \beta) + (X_i \hat \beta - X_i \beta) \right]^2 \Leftarrow \mbox{added }  \pm X_i \hat \beta \mbox{ is effectively adding zero}\\
(expanding~the~terms)&amp; = \sum_{i=1}^n (Y_i - X_i \hat \beta)^2 + 2 \sum_{i=1}^n (Y_i - X_i \hat \beta)(X_i \hat \beta - X_i \beta) + \sum_{i=1}^n (X_i \hat \beta - X_i \beta)^2 \\
\sum_{i=1}^n (Y_i - X_i \beta)^2 &amp; \geq \sum_{i=1}^n (Y_i - X_i \hat \beta)^2 + 2 \sum_{i=1}^n (Y_i - X_i \hat \beta)(X_i \hat \beta - X_i \beta) \Leftarrow \sum_{i=1}^n (X_i \hat \beta - X_i \beta)^2 \mbox{ is always positive}\\
&amp; (ignoring~the~second~term~for~now,~for~\hat \beta ~to~be~the~minimizer~of~the~squares, \\
&amp; the~following~must~be~true)\\
\sum_{i=1}^n (Y_i - X_i \beta)^2  &amp; \geq \sum_{i=1}^n (Y_i - X_i \hat \beta)^2 \Leftarrow \mbox{every other } \beta \mbox{ value creates a least square criteria that is }\geq \hat \beta\\
(this~means)&amp; \Rightarrow 2 \sum_{i=1}^n (Y_i - X_i \hat \beta)(X_i \hat \beta - X_i \beta) = 0\\
(simplifying)&amp; \Rightarrow \sum_{i=1}^n (Y_i - X_i \hat \beta) X_i (\hat \beta - \beta) = 0 \Leftarrow (\hat \beta - \beta) \mbox{ does not depend on }i\\
(simplifying)&amp; \Rightarrow \sum_{i=1}^n (Y_i - X_i \hat \beta) X_i = 0 \\
(solving~for~\hat \beta) &amp; \Rightarrow \hat \beta = \frac{\sum_{i=1}^n Y_i X_i}{\sum_{i=1}^n X_i^2} = \beta\\
\end{aligned}
\]</span></p></li>
<li><strong><em>example</em></strong>
<ul>
<li>Let <span class="math inline">\(X_1, X_2, \ldots , X_n = 1\)</span> <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - X_i \beta)^2 = \sum_{i=1}^n (Y_i - \beta)^2 \\
\Rightarrow \hat \beta = \frac{\sum_{i=1}^n Y_i X_i}{\sum_{i=1}^n X_i^2} = \frac{\sum_{i=1}^n Y_i}{\sum_{i=1}^n 1} = \frac{\sum_{i=1}^n Y_i}{n} = \bar Y
\end{aligned}
\]</span></li>
<li><em><strong>Note</strong>: this is the result from our previous derivation for least squares = empirical mean </em></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="finding-the-best-fit-line-ordinary-least-squares">Finding the Best Fit Line (Ordinary Least Squares)</h2>
<ul>
<li>best fitted line for predictor, <span class="math inline">\(X\)</span>, and outcome, <span class="math inline">\(Y\)</span> is derived from the <strong>least squares</strong> <span class="math display">\[\sum_{i=1}^n \{Y_i - (\beta_0 + \beta_1 X_i)\}^2\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/3.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>each of the data point contributes equally to the error between the their locations and the regression line <span class="math inline">\(\rightarrow\)</span> goal of regression is to <strong><em>minimize</em></strong> this error</li>
</ul>
<h3 id="least-squares-model-fit">Least Squares Model Fit</h3>
<ul>
<li>model fit = <span class="math inline">\(Y = \beta_0 + \beta_1 X\)</span> through the data pairs <span class="math inline">\((X_i, Y_i)\)</span> where <span class="math inline">\(Y_i\)</span> as the outcome
<ul>
<li><em><strong>Note</strong>: this is the model that we use to guide our <strong>estimated</strong> best fit (see below) </em></li>
</ul></li>
<li>best fit line with estimated slope and intercept (<span class="math inline">\(X\)</span> as predictor, <span class="math inline">\(Y\)</span> as outcome) <span class="math inline">\(\rightarrow\)</span> <span class="math display">\[Y = \hat \beta_0 + \hat \beta_1 X\]</span> where <span class="math display">\[\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X\]</span>
<ul>
<li>[<strong><em>slope</em></strong>] <span class="math inline">\(\hat \beta_1\)</span> has the units of <span class="math inline">\(Y / X\)</span>
<ul>
<li><span class="math inline">\(Cor(Y, X)\)</span> = unit-less</li>
<li><span class="math inline">\(Sd(Y)\)</span> = has units of <span class="math inline">\(Y\)</span></li>
<li><span class="math inline">\(Sd(X)\)</span> = has units of <span class="math inline">\(X\)</span></li>
</ul></li>
<li>[<strong><em>intercept</em></strong>] <span class="math inline">\(\hat \beta_0\)</span> has the units of <span class="math inline">\(Y\)</span></li>
<li>the line passes through the point <span class="math inline">\((\bar X, \bar Y\)</span>)
<ul>
<li>this is evident from equation for <span class="math inline">\(\beta_0\)</span> (rearrange equation)</li>
</ul></li>
</ul></li>
<li>best fit line with <span class="math inline">\(X\)</span> as outcome and <span class="math inline">\(Y\)</span> as predictor has slope, <span class="math inline">\(\hat \beta_1 = Cor(Y, X) Sd(X)/ Sd(Y)\)</span>.</li>
<li>slope of best fit line = slope of best fit line through the origin for centered data <span class="math inline">\((X_i - \bar X, Y_i - \bar Y)\)</span></li>
<li>slope of best fit line for normalized the data, <span class="math inline">\(\{ \frac{X_i - \bar X}{Sd(X)}, \frac{Y_i - \bar Y}{Sd(Y)}\}\)</span> = <span class="math inline">\(Cor(Y, X)\)</span></li>
</ul>
<h3 id="derivation-for-beta_0-and-beta_1">Derivation for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span></h3>
<ul>
<li>Let <span class="math inline">\(Y = \beta_0 + \beta_1 X\)</span>, and <span class="math inline">\(\hat \beta_0\)</span>/<span class="math inline">\(\hat \beta_1 =\)</span> estimates <span class="math inline">\(\beta_0\)</span>/<span class="math inline">\(\beta_1\)</span>, the intercept and slope of the least square regression line, respectively <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2 &amp; = \sum_{i=1}^n (Y_i^* - \beta_0)^2 ~~~where~ Y_i^* = Y_i - \beta_1 X_i\\
solution~for~\sum_{i=1}^n (Y_i^* - \beta_0)^2 &amp; \Rightarrow \hat \beta_0 = \frac{\sum_{i=1}^n Y_i^*}{n} = \frac{\sum_{i=1}^n Y_i - \beta_1 X_i}{n}\\
&amp; \Rightarrow \hat \beta_0 = \frac{\sum_{i=1}^n Y_i}{n} - \beta_1 \frac{\sum_{i=1}^n X_i}{n}\\
&amp; \Rightarrow \hat \beta_0 = \bar Y - \beta_1 \bar X\\
\Longrightarrow \sum_{i=1}^n (Y_i - \beta_0 - \beta_1 X_i)^2 &amp; = \sum_{i=1}^n \left[Y_i - (\bar Y - \beta_1 \bar X) - \beta_1 X_i \right]^2\\
&amp; = \sum_{i=1}^n \left[(Y_i - \bar Y) - (X_i - \bar X)\beta_1 \right]^2\\
&amp; = \sum_{i=1}^n \left[\tilde Y_i - \tilde X_i \beta_1 \right]^2 ~~~ where~ \tilde Y_i = Y_i - \bar Y, \tilde X_i = X_i - \bar X\\
&amp; \Rightarrow \hat \beta_1 = \frac{\sum_{i=1}^n \tilde Y_i \tilde X_i}{\sum_{i=1}^n \tilde{X_i}^2} = \frac{(Y_i - \bar Y)(X_i - \bar X)}{\sum_{i=1}^n (X_i - \bar X)^2}\\
&amp; \Rightarrow \hat \beta_1 = \frac{(Y_i - \bar Y)(X_i - \bar X)/(n-1)}{\sum_{i=1}^n (X_i - \bar X)^2/(n-1)} = \frac{Cov(Y, X)}{Var(X)}\\
&amp; \Rightarrow \hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)}\\
&amp; \Rightarrow \hat \beta_0 = \bar Y - \hat \beta_1 \bar X\\
\end{aligned}
\]</span></li>
</ul>
<h3 id="examples-and-r-commands">Examples and R Commands</h3>
<ul>
<li><span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span> can be manually calculated through the above formulas</li>
<li><code>coef(lm(y ~ x)))</code> = R command to run the least square regression model on the data with <code>y</code> as the outcome, and <code>x</code> as the regressor
<ul>
<li><code>coef()</code> = returns the slope and intercept coefficients of the <code>lm</code> results</li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># outcome</span><br><span class="line">y &lt;- galton$child</span><br><span class="line"># regressor</span><br><span class="line">x &lt;- galton$parent</span><br><span class="line"># slope</span><br><span class="line">beta1 &lt;- cor(y, x) * sd(y) / sd(x)</span><br><span class="line"># intercept</span><br><span class="line">beta0 &lt;- mean(y) - beta1 * mean(x)</span><br><span class="line"># results are the same as using the lm command</span><br><span class="line">results &lt;- rbind(&quot;manual&quot; = c(beta0, beta1), &quot;lm(y ~ x)&quot; = coef(lm(y ~ x)))</span><br><span class="line"># set column names</span><br><span class="line">colnames(results) &lt;- c(&quot;intercept&quot;, &quot;slope&quot;)</span><br><span class="line"># print results</span><br><span class="line">results</span><br></pre></td></tr></table></figure>
<ul>
<li>slope of the best fit line = slope of best fit line through the origin for centered data</li>
<li><code>lm(y ~ x - 1)</code> = forces a regression line to go through the origin (0, 0)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># centering y</span><br><span class="line">yc &lt;- y - mean(y)</span><br><span class="line"># centering x</span><br><span class="line">xc &lt;- x - mean(x)</span><br><span class="line"># slope</span><br><span class="line">beta1 &lt;- sum(yc * xc) / sum(xc ^ 2)</span><br><span class="line"># results are the same as using the lm command</span><br><span class="line">results &lt;- rbind(&quot;centered data (manual)&quot; = beta1, &quot;lm(y ~ x)&quot; = coef(lm(y ~ x))[2],</span><br><span class="line">	&quot;lm(yc ~ xc - 1)&quot; = coef(lm(yc ~ xc - 1))[1])</span><br><span class="line"># set column names</span><br><span class="line">colnames(results) &lt;- c(&quot;slope&quot;)</span><br><span class="line"># print results</span><br><span class="line">results</span><br></pre></td></tr></table></figure>
<ul>
<li>slope of best fit line for normalized the data = <span class="math inline">\(Cor(Y, X)\)</span></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># normalize y</span><br><span class="line">yn &lt;- (y - mean(y))/sd(y)</span><br><span class="line"># normalize x</span><br><span class="line">xn &lt;- (x - mean(x))/sd(x)</span><br><span class="line"># compare correlations</span><br><span class="line">results &lt;- rbind(&quot;cor(y, x)&quot; = cor(y, x), &quot;cor(yn, xn)&quot; = cor(yn, xn),</span><br><span class="line">	&quot;slope&quot; = coef(lm(yn ~ xn))[2])</span><br><span class="line"># print results</span><br><span class="line">results</span><br></pre></td></tr></table></figure>
<ul>
<li><code>geom_smooth(method = &quot;lm&quot;, formula = y~x)</code> function in <code>ggplot2</code> = adds regression line and confidence interval to graph
<ul>
<li><code>formula = y~x</code> = default for the line (argument can be eliminated if <code>y~x</code> produces the line you want)</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># constructs table for different combination of parent-child height</span><br><span class="line">freqData &lt;- as.data.frame(table(galton$child, galton$parent))</span><br><span class="line">names(freqData) &lt;- c(&quot;child (in)&quot;, &quot;parent (in)&quot;, &quot;freq&quot;)</span><br><span class="line"># convert to numeric values</span><br><span class="line">freqData$child &lt;- as.numeric(as.character(freqData$child))</span><br><span class="line">freqData$parent &lt;- as.numeric(as.character(freqData$parent))</span><br><span class="line">g &lt;- ggplot(filter(freqData, freq &gt; 0), aes(x = parent, y = child))</span><br><span class="line">g &lt;- g + scale_size(range = c(2, 20), guide = &quot;none&quot; )</span><br><span class="line">g &lt;- g + geom_point(colour=&quot;grey50&quot;, aes(size = freq+10, show_guide = FALSE))</span><br><span class="line">g &lt;- g + geom_point(aes(colour=freq, size = freq))</span><br><span class="line">g &lt;- g + scale_colour_gradient(low = &quot;lightblue&quot;, high=&quot;white&quot;)</span><br><span class="line">g &lt;- g + geom_smooth(method=&quot;lm&quot;, formula=y~x)</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="regression-to-the-mean">Regression to the Mean</h2>
<ul>
<li>first investigated by Francis Galton in the paper <em>“Regression towards mediocrity in hereditary stature” The Journal of the Anthropological Institute of Great Britain and Ireland , Vol. 15, (1886)</em></li>
<li><strong>regression to the mean</strong> was invented by Fancis Galton to capture the following phenomena
<ul>
<li>children of tall parents tend to be tall, but not as tall as their parents</li>
<li>children of short parents tend to be short, but not as short as their parents</li>
<li>parents of very short children, tend to be short, but not as short as their child</li>
<li>parents of very tall children, tend to be tall, but not as tall as their children</li>
</ul></li>
<li>in thinking of the extremes, the following are true
<ul>
<li><span class="math inline">\(P(Y &lt; x | X = x)\)</span> gets bigger as <span class="math inline">\(x\)</span> heads to very large values
<ul>
<li>in other words, given that the value of X is already very large (extreme), the chance that the value of Y is as large or larger than that of X is small (unlikely)</li>
</ul></li>
<li>similarly, <span class="math inline">\(P(Y &gt; x | X = x)\)</span> gets bigger as <span class="math inline">\(x\)</span> heads to very small values
<ul>
<li>in other words, given that the value of X is already very small (extreme), the chance that the value of Y is as small or smaller than that of X is small (unlikely)</li>
</ul></li>
</ul></li>
<li>when constructing regression lines between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the line represents the intrinsic relationship (“mean”) between the variables, but does not capture the extremes (“noise”)
<ul>
<li>unless <span class="math inline">\(Cor(Y, X) = 1\)</span>, the regression line or the intrinsic part of the relationship between variables won’t capture all of the variation (some noise exists)</li>
</ul></li>
</ul>
<h3 id="daltons-investigation-on-regression-to-the-mean">Dalton’s Investigation on Regression to the Mean</h3>
<ul>
<li>both <span class="math inline">\(X\)</span>, child’s heights, and <span class="math inline">\(Y\)</span>, parent’s heights, are <strong>normalized</strong> so that they mean of 0 and variance of 1</li>
<li>regression lines must pass <span class="math inline">\((\bar X, \bar Y)\)</span> or <span class="math inline">\((0, 0)\)</span> in this case</li>
<li>slope of regression line = <span class="math inline">\(Cor(Y,X)\)</span> regardless of which variable is the outcome/regressor (because standard deviations of both variables = 1)
<ul>
<li><em><strong>Note</strong>: however, for both regression lines to be plotted on the same graph, the second line’s slope must be <span class="math inline">\(1/Cor(Y, X)\)</span> because the two relationships have flipped axes </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># load father.son data</span><br><span class="line">data(father.son)</span><br><span class="line"># normalize son&apos;s height</span><br><span class="line">y &lt;- (father.son$sheight - mean(father.son$sheight)) / sd(father.son$sheight)</span><br><span class="line"># normalize father&apos;s height</span><br><span class="line">x &lt;- (father.son$fheight - mean(father.son$fheight)) / sd(father.son$fheight)</span><br><span class="line"># calculate correlation</span><br><span class="line">rho &lt;- cor(x, y)</span><br><span class="line"># plot the relationship between the two</span><br><span class="line">g = ggplot(data.frame(x = x, y = y), aes(x = x, y = y))</span><br><span class="line">g = g + geom_point(size = 3, alpha = .2, colour = &quot;black&quot;)</span><br><span class="line">g = g + geom_point(size = 2, alpha = .2, colour = &quot;red&quot;)</span><br><span class="line">g = g + xlim(-4, 4) + ylim(-4, 4)</span><br><span class="line"># reference line for perfect correlation between</span><br><span class="line"># variables (data points will like on diagonal line)</span><br><span class="line">g = g + geom_abline(position = &quot;identity&quot;)</span><br><span class="line"># if there is no correlation between the two variables,</span><br><span class="line"># the data points will lie on horizontal/vertical lines</span><br><span class="line">g = g + geom_vline(xintercept = 0)</span><br><span class="line">g = g + geom_hline(yintercept = 0)</span><br><span class="line"># plot the actual correlation for both</span><br><span class="line">g = g + geom_abline(intercept = 0, slope = rho, size = 2)</span><br><span class="line">g = g + geom_abline(intercept = 0, slope = 1 / rho, size = 2)</span><br><span class="line"># add appropriate labels</span><br><span class="line">g = g + xlab(&quot;Father&apos;s height, normalized&quot;)</span><br><span class="line">g = g + ylab(&quot;Son&apos;s height, normalized&quot;)</span><br><span class="line">g = g + geom_text(x = 3.8, y = 1.6, label=&quot;x~y&quot;, angle = 25) +</span><br><span class="line">    geom_text(x = 3.2, y = 3.6, label=&quot;cor(y,x)=1&quot;, angle = 35) +</span><br><span class="line">    geom_text(x = 1.6, y = 3.8, label=&quot;y~x&quot;, angle = 60)</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="statistical-linear-regression-models">Statistical Linear Regression Models</h2>
<ul>
<li>goal is use statistics to draw inferences <span class="math inline">\(\rightarrow\)</span> generalize from data to population through models</li>
<li><strong>probabilistic model for linear regression</strong> <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_{i}\]</span> where <span class="math inline">\(\epsilon_{i}\)</span> represents the sampling errors and is assumed to be iid <span class="math inline">\(N(0, \sigma^2)\)</span></li>
<li>this model has the following properties
<ul>
<li><span class="math inline">\(E[Y_i ~|~ X_i = x_i] = E[\beta_0] + E[\beta_1 x_i] + E[\epsilon_{i}] = \mu_i = \beta_0 + \beta_1 x_i\)</span></li>
<li><span class="math inline">\(Var(Y_i ~|~ X_i = x_i) = Var(\beta_0 + \beta_1 x_i) + Var(\epsilon_{i}) = Var(\epsilon_{i}) = \sigma^2\)</span>
<ul>
<li><span class="math inline">\(\beta_0 + \beta_1 x_i\)</span> = line = constant/no variance</li>
</ul></li>
</ul></li>
<li>it can then be said to have <span class="math inline">\(Y_i\)</span> as independent <span class="math inline">\(N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\mu = \beta_0 + \beta_1 x_i\)</span> &lt;– likelihood equivalent model
<ul>
<li><strong>likelihood</strong> = given the outcome, what is the probability?
<ul>
<li>in this case, the likelihood is as follows <span class="math display">\[\mathcal{L}(\beta_0, \beta_1, \sigma) = \prod_{i=1}^{n} \left\{(2\pi \sigma^2)^{-1/2}\exp \left(- \frac{1}{2\sigma^2}(y_i - \mu_i)^2\right)\right\}\]</span> where <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_i\)</span></li>
<li>above is the <strong>probability density function </strong> of <span class="math inline">\(n\)</span> samples from the normal distribution <span class="math inline">\(\rightarrow\)</span> this is because the regression line is normally distributed due to <span class="math inline">\(\epsilon_{i}\)</span></li>
</ul></li>
<li><strong>maximum likelihood estimator (MLE)</strong> = most likely estimate of the population parameter/probability
<ul>
<li>in this case, the maximum likelihood = -2 minimum natural log (<span class="math inline">\(ln\)</span>, base <span class="math inline">\(e\)</span>) likelihood <span class="math display">\[-2 \log{\mathcal{L}(\beta_0, \beta_1, \sigma)} = n\log(2\pi \sigma^2) + \frac{1}{\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2\]</span>
<ul>
<li>since everything else is constant, minimizing this function would only depend on <span class="math inline">\(\sum_{i=1}^n (y_i - \mu_i)^2\)</span>, which from our previous derivations yields <span class="math inline">\(\hat \mu_i = \beta_0 + \beta_1 \hat x_i\)</span></li>
</ul></li>
<li><strong><em>maximum likelihood estimate</em></strong> = <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_i\)</span></li>
</ul></li>
</ul></li>
</ul>
<h3 id="interpreting-regression-coefficients">Interpreting Regression Coefficients</h3>
<ul>
<li>for the linear regression line <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_{i}\]</span> MLE for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are as follows <span class="math display">\[\hat \beta_1 = Cor(Y, X)\frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X\]</span></li>
<li><strong><span class="math inline">\(\beta_0\)</span></strong> = expected value of the outcome/response when the predictor is 0 <span class="math display">\[
E[Y | X = 0] =  \beta_0 + \beta_1 \times 0 = \beta_0
\]</span>
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(X=0\)</span> may not always be of interest as it may be impossible/outside of data range (i.e blood pressure, height etc.) </em></li>
<li>it may be useful to move the intercept at times <span class="math display">\[
\begin{aligned}
Y_i &amp;= \beta_0 + \beta_1 X_i + \epsilon_i\\
&amp;= \beta_0 + a \beta_1 + \beta_1 (X_i - a) + \epsilon_i \\
&amp;= \tilde \beta_0 + \beta_1 (X_i - a) + \epsilon_i ~~~where \tilde \beta_0 = \beta_0 + a \beta_1\\
\end{aligned}
\]</span></li>
<li><em><strong>Note</strong>: shifting <span class="math inline">\(X\)</span> values by value <span class="math inline">\(a\)</span> changes the intercept, but not the slope </em></li>
<li>often, <span class="math inline">\(a\)</span> is set to <span class="math inline">\(\bar X\)</span> so that the intercept is interpreted as the expected response at the average <span class="math inline">\(X\)</span> value</li>
</ul></li>
<li><strong><span class="math inline">\(\beta_1\)</span></strong> = expected change in outcome/response for a 1 unit change in the predictor<span class="math display">\[
E[Y ~|~ X = x+1] - E[Y ~|~ X = x] =
\beta_0 + \beta_1 (x + 1) - (\beta_0 + \beta_1 x ) = \beta_1
\]</span>
<ul>
<li>sometimes it is useful to change the units of <span class="math inline">\(X\)</span> <span class="math display">\[
\begin{aligned}
Y_i &amp; = \beta_0 + \beta_1 X_i + \epsilon_i \\
&amp; = \beta_0 + \frac{\beta_1}{a} (X_i a) + \epsilon_i \\
&amp; = \beta_0 + \tilde \beta_1 (X_i a) + \epsilon_i \\
\end{aligned}
\]</span></li>
<li>multiplication of <span class="math inline">\(X\)</span> by a factor <span class="math inline">\(a\)</span> results in dividing the coefficient by a factor of <span class="math inline">\(a\)</span></li>
<li><strong><em>example</em></strong>:
<ul>
<li><span class="math inline">\(X\)</span> = height in <span class="math inline">\(m\)</span></li>
<li><span class="math inline">\(Y\)</span> = weight in <span class="math inline">\(kg\)</span></li>
<li><span class="math inline">\(\beta_1\)</span> has units of <span class="math inline">\(kg/m\)</span></li>
<li>converting <span class="math inline">\(X\)</span> to <span class="math inline">\(cm\)</span> <span class="math inline">\(\Longrightarrow\)</span> multiplying <span class="math inline">\(X\)</span> by <span class="math inline">\(100 \frac{cm}{m}\)</span></li>
<li>this mean <span class="math inline">\(\beta_1\)</span> has to be divided by <span class="math inline">\(100 \frac{cm}{m}\)</span> for the correct units. <span class="math display">\[
X~m \times 100\frac{cm}{m} = (100~X) cm
~~\mbox{and}~~
\beta_1 \frac{kg}{m} \times \frac{1}{100}\frac{m}{cm} = \left(\frac{\beta_1}{100}\right)\frac{kg}{cm}
\]</span></li>
</ul></li>
</ul></li>
<li>95% confidence intervals for the coefficients can be constructed from the coefficients themselves and their standard errors (from <code>summary(lm)</code>)
<ul>
<li>use the resulting intervals to evaluate the significance of the results</li>
</ul></li>
</ul>
<h3 id="use-regression-coefficients-for-prediction">Use Regression Coefficients for Prediction</h3>
<ul>
<li>for observed values of the predictor, <span class="math inline">\(X_1, X_2, \ldots , X_n\)</span>, the prediction of the outcome/response is as follows <span class="math display">\[\hat \mu_i = \hat Y_i = \hat \beta_0 + \hat \beta_1 X\]</span> where <span class="math inline">\(\mu_i\)</span> describes a point on the regression line</li>
</ul>
<h3 id="example-and-r-commands">Example and R Commands</h3>
<ul>
<li><code>diamond</code> dataset from <code>UsingR</code> package
<ul>
<li>diamond prices in Singapore Dollars, diamond weight in carats (standard measure of diamond mass, 0.2g)</li>
</ul></li>
<li><code>lm(price ~ I(carat - mean(carat)), data=diamond)</code> = mean centered linear regression
<ul>
<li><em><strong>Note</strong>: arithmetic operations must be enclosed in <code>I()</code> to work </em></li>
</ul></li>
<li><code>predict(fitModel, newdata=data.frame(carat=c(0, 1, 2)))</code> = returns predicted outcome from the given model (linear in our case) at the provided points within the <code>newdata</code> data frame
<ul>
<li>if <code>newdata</code> is unspecified (argument omitted), then <code>predict</code> function will return predicted values for all values of the predictor (x variable, carat in this case)
<ul>
<li><em><strong>Note</strong>: <code>newdata</code> has to be a dataframe, and the values you would like to predict (x variable, <code>carat</code> in this case) has to be specified, or the system won’t know what to do with the provided values </em></li>
</ul></li>
</ul></li>
<li><code>summary(fitModel)</code> = prints detailed summary of linear model</li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># standard linear regression for price vs carat</span><br><span class="line">fit &lt;- lm(price ~ carat, data = diamond)</span><br><span class="line"># intercept and slope</span><br><span class="line">coef(fit)</span><br><span class="line"># mean-centered regression</span><br><span class="line">fit2 &lt;- lm(price ~ I(carat - mean(carat)), data = diamond)</span><br><span class="line"># intercept and slope</span><br><span class="line">coef(fit2)</span><br><span class="line"># regression with more granular scale (1/10th carat)</span><br><span class="line">fit3 &lt;- lm(price ~ I(carat * 10), data = diamond)</span><br><span class="line"># intercept and slope</span><br><span class="line">coef(fit3)</span><br><span class="line"># predictions for 3 values</span><br><span class="line">newx &lt;- c(0.16, 0.27, 0.34)</span><br><span class="line"># manual calculations</span><br><span class="line">coef(fit)[1] + coef(fit)[2] * newx</span><br><span class="line"># prediction using the predict function</span><br><span class="line">predict(fit, newdata = data.frame(carat = newx))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>interpretation</strong>
<ul>
<li>we expect <code>r round(coef(fit)[2], 2)</code> (SIN) dollar increase in price for <strong><em>every carat increase</em></strong> in mass of diamond</li>
<li>or <code>r round(coef(fit3)[2], 2)</code> (SIN) dollar increase in price for <strong><em>every 1/10 carat</em></strong> increase in mass of diamond</li>
</ul></li>
<li><strong>prediction</strong>
<ul>
<li>for 0.16, 0.27, and 0.34 carats, we predict the prices to be <code>r round(as.numeric(predict(fit, newdata = data.frame(carat = newx))),2)</code> (SIN) dollars</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># plot the data points</span><br><span class="line">plot(diamond$carat, diamond$price, xlab = &quot;Mass (carats)&quot;, ylab = &quot;Price (SIN $)&quot;,</span><br><span class="line">     bg = &quot;lightblue&quot;, col = &quot;black&quot;, cex = 1.1, pch = 21,frame = FALSE)</span><br><span class="line"># plot linear regression line</span><br><span class="line">abline(fit, lwd = 2)</span><br><span class="line"># plot predictions for every value of carat (in red)</span><br><span class="line">points(diamond$carat, predict(fit), pch = 19, col = &quot;red&quot;)</span><br><span class="line"># add guidelines for predictions for 0.16, 0.27, and 0.34</span><br><span class="line">lines(c(0.16, 0.16, 0.12), c(200, coef(fit)[1] + coef(fit)[2] * 0.16,</span><br><span class="line">      coef(fit)[1] + coef(fit)[2] * 0.16))</span><br><span class="line">lines(c(0.27, 0.27, 0.12), c(200, coef(fit)[1] + coef(fit)[2] * 0.27,</span><br><span class="line">      coef(fit)[1] + coef(fit)[2] * 0.27))</span><br><span class="line">lines(c(0.34, 0.34, 0.12), c(200, coef(fit)[1] + coef(fit)[2] * 0.34,</span><br><span class="line">      coef(fit)[1] + coef(fit)[2] * 0.34))</span><br><span class="line"># add text labels</span><br><span class="line">text(newx+c(0.03, 0, 0), rep(250, 3), labels = newx, pos = 2)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h3 id="derivation-for-maximum-likelihood-estimator">Derivation for Maximum Likelihood Estimator</h3>
<ul>
<li><em><strong>Note</strong>: this derivation is for the maximum likelihood estimator of the mean, <span class="math inline">\(\mu\)</span>, of a normal distribution as it is the basis of the linear regression model </em></li>
<li><strong>linear regression model</strong> <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_{i}\]</span> follows a normal distribution because <span class="math inline">\(\epsilon_{i} \sim N(0, \sigma^2)\)</span></li>
<li>for the above model, <span class="math inline">\(E[Y_i] = \mu_i = \beta_0 + \beta_1 X_i\)</span> and <span class="math inline">\(Var(Y_i) = \sigma^2\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/4.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>the <strong>probability density function (pdf)</strong> for an outcome <span class="math inline">\(x\)</span> from the normal distribution is defined as <span class="math display">\[f(x~|~\mu, \sigma^2) = (2\pi \sigma^2)^{-1/2}\exp \left(- \frac{1}{2\sigma^2}(y_i - \mu_i)^2\right)\]</span></li>
<li>the corresponding pdf for <span class="math inline">\(n\)</span> iid normal random outcomes <span class="math inline">\(x_1, \ldots, x_n\)</span> is defined as <span class="math display">\[f(x_1, \ldots, x_n~|~\mu, \sigma^2) = \prod_{i=1}^n (2\pi \sigma^2)^{-1/2}\exp \left(- \frac{1}{2\sigma^2}(y_i - \mu_i)^2\right)\]</span> which is also known as the <strong>likelihood function</strong>, denoted in this case as <span class="math inline">\(\mathcal{L}(\mu, \sigma)\)</span></li>
<li>to find the <strong>maximum likelihood estimator (MLE)</strong> of the mean, <span class="math inline">\(\mu\)</span>, we take the derivative of the likelihood <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mu}\)</span></li>
<li>since derivatives of products are quite complex to compute, taking the <span class="math inline">\(\log\)</span> (base <span class="math inline">\(e\)</span>) makes the calculation much simpler
<ul>
<li><span class="math inline">\(\log\)</span> properties:
<ul>
<li><span class="math inline">\(\log(AB) = \log(A) + \log(B)\)</span></li>
<li><span class="math inline">\(\log(A^B) = B\log(A)\)</span></li>
</ul></li>
<li>because <span class="math inline">\(\log\)</span> is always increasing and <strong>monotonic</strong>, or preserves order, finding the maximum MLE <strong>=</strong> finding th maximum of <span class="math inline">\(\log\)</span> transformation of MLE</li>
</ul></li>
<li>-2 <span class="math inline">\(\log\)</span> of <strong>likelihood function</strong> <span class="math display">\[
\begin{aligned}
\log(\mathcal{L}(\mu, \sigma)) &amp; = \sum_{i=1}^n -\frac{1}{2}\log(2\pi \sigma^2) - \frac{1}{2\sigma^2}(y_i - \mu_i)^2 \Leftarrow \mbox{multiply -2 on both sides} \\
-2\log(\mathcal{L}(\mu, \sigma)) &amp; = \sum_{i=1}^n \log(2\pi \sigma^2) + \frac{1}{\sigma^2}(y_i - \mu_i)^2 \Leftarrow \sigma^2 \mbox{ does not depend on }i \\
-2\log(\mathcal{L}(\mu, \sigma)) &amp; = n\log(2\pi \sigma^2) + \frac{1}{\sigma^2}\sum_{i=1}^n (y_i - \mu_i)^2\\
\end{aligned}
\]</span></li>
<li>minimizing -2 <span class="math inline">\(\log\)</span> likelihood is <strong><em>computationally equivalent</em></strong> as maximizing <span class="math inline">\(\log\)</span> likelihood <span class="math display">\[
\begin{aligned}
\frac{\partial \log(\mathcal{L}(\mu, \sigma))}{\partial \mu} &amp; = \frac{1}{\sigma^2} \frac{\partial \sum_{i=1}^n (y_i - \mu_i)^2}{\partial \mu} = 0 \Leftarrow \mbox{setting this equal to 0 to find minimum}\\
\Rightarrow -\frac{2}{\sigma^2}\sum_{i=1}^n (y_i - \mu_i) &amp; = 0 \Leftarrow \mbox{divide by }-2/\sigma^2 \mbox{ on both sides}\\
\sum_{i=1}^n (y_i - \mu_i) &amp; = 0 \Leftarrow \mbox{move } \mu \mbox{ term over to the right}\\
\sum_{i=1}^n y_i &amp; = \sum_{i=1}^n \mu_i \Leftarrow \mbox{for the two sums to be equal, all the terms must be equal}\\
y_i &amp; = \mu_i \\
\end{aligned}
\]</span></li>
<li>in the case of our linear regression, <span class="math inline">\(\mu_i = \beta_0 + \beta_1 X_i\)</span> so <span class="math display">\[
\begin{aligned}
\frac{\partial \mathcal{L}(\mu, \sigma))}{\partial \mu} = \frac{\partial \mathcal{L}(\beta_0, \beta_1, \sigma))}{\partial \mu} \\
\mbox{MLE} \Rightarrow Y_i &amp; = \mu_i \\
\mu_i &amp;= \beta_0 + \beta_1 X_i\\
\end{aligned}
\]</span></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="residuals">Residuals</h2>
<ul>
<li>Residual, <span class="math inline">\(e_i\)</span> = difference between the observed and predicted outcome <span class="math display">\[e_i = Y_i - \hat Y_i\]</span>
<ul>
<li>Or, vertical distance between observed data point and regression line</li>
<li>Least squares minimizes <span class="math inline">\(\sum_{i=1}^n e_i^2\)</span></li>
</ul></li>
<li><span class="math inline">\(e_i\)</span> can be interpreted as estimates of the regression error, <span class="math inline">\(\epsilon_i\)</span></li>
<li><span class="math inline">\(e_i\)</span> can also be interpreted as the outcome (<span class="math inline">\(Y\)</span>) with the linear association of the predictor (<span class="math inline">\(X\)</span>) removed
<ul>
<li>or, “Y adjusted for X”</li>
</ul></li>
<li><span class="math inline">\(E[e_i] = 0\)</span> <span class="math inline">\(\rightarrow\)</span> this is because the mean of the residuals is expected to be 0 (assumed Gaussian distribution)
<ul>
<li>the Gaussian distribution assumption also implies that the error is <strong>NOT</strong> correlated with any predictors</li>
<li><code>mean(fitModel$residuals)</code> = returns mean of residuals <span class="math inline">\(\rightarrow\)</span> should equal to 0</li>
<li><code>cov(fit$residuals, predictors)</code> = returns the covariance (measures correlation) of residuals and predictors <span class="math inline">\(\rightarrow\)</span> should also equal to 0</li>
</ul></li>
<li><span class="math inline">\(\sum_{i=1}^n e_i = 0\)</span> (if an intercept is included) and <span class="math inline">\(\sum_{i=1}^n e_i X_i = 0\)</span> (if a regressor variable <span class="math inline">\(X_i\)</span> is included)</li>
<li>for standard linear regression model
<ul>
<li>positive residuals = above the line</li>
<li>negative residuals = below</li>
</ul></li>
<li>residuals/residual plots can highlight poor model fit</li>
</ul>
<h3 id="estimating-residual-variation">Estimating Residual Variation</h3>
<ul>
<li>residual variation measures how well the regression line fit the data points</li>
<li><strong>MLE of variance</strong>, <span class="math inline">\(\sigma^2\)</span>, of the linear model = <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n e_i^2\)</span> or the <strong><em>average squared residual/mean squared error</em></strong>
<ul>
<li>the square root of the estimate, <span class="math inline">\(\sigma\)</span>, = <strong>root mean squared error (RMSE)</strong></li>
</ul></li>
<li>however, a more common approach is to use <span class="math display">\[\hat \sigma^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2\]</span>
<ul>
<li><span class="math inline">\(n-2\)</span> is used instead of <span class="math inline">\(n\)</span> to make the estimator <strong><em>unbiased</em></strong> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(E[\hat \sigma^2] = \sigma^2\)</span></li>
<li><em><strong>Note</strong>: the -2 is accounting for the degrees of freedom for intercept and slope, which had to be estimated </em></li>
</ul></li>
<li><code>deviance(fitModel)</code> = calculates sum of the squared error/residual for the linear model/residual variation</li>
<li><code>summary(fitModel)$sigma</code> = returns the residual variation of a fit model or the <strong>unbiased RMSE</strong>
<ul>
<li><code>summary(fitModel)</code> = creates a list of different parameters of the fit model</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># get data</span><br><span class="line">y &lt;- diamond$price; x &lt;- diamond$carat; n &lt;- length(y)</span><br><span class="line"># linear fit</span><br><span class="line">fit &lt;- lm(y ~ x)</span><br><span class="line"># calculate residual variation through summary and manual</span><br><span class="line">rbind(&quot;from summary&quot; = summary(fit)$sigma, &quot;manual&quot; =sqrt(sum(resid(fit)^2) / (n - 2)))</span><br></pre></td></tr></table></figure>
<h3 id="total-variation-r2-and-derivations">Total Variation, <span class="math inline">\(R^2\)</span>, and Derivations</h3>
<ul>
<li><strong>total variation</strong> = <strong><em>residual variation</em></strong> (variation after removing predictor) + <strong><em>systematic/regression variation</em></strong> (variation explained by regression model) <span class="math display">\[\sum_{i=1}^n (Y_i - \bar Y)^2 = \sum_{i=1}^n (Y_i - \hat Y_i)^2 + \sum_{i=1}^n  (\hat Y_i - \bar Y)^2
\]</span></li>
<li><span class="math inline">\(R^2\)</span> = percent of total variability that is explained by the regression model <span class="math display">\[
\begin{aligned}
R^2 &amp; = \frac{\mbox{regression variation}}{\mbox{total variation}} = \frac{\sum_{i=1}^n  (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2} \\
&amp; = 1- \frac{\mbox{residual variation}}{\mbox{total variation}} = 1- \frac{\sum_{i=1}^n (Y_i - \hat Y_i)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}\\
\end{aligned}
\]</span></li>
<li><span class="math inline">\(0 \leq R^2 \leq 1\)</span></li>
<li><span class="math inline">\(R^2\)</span> = sample correlation squared
<ul>
<li><code>cor(outcome, predictor</code> = calculates the correlation between predictor and outcome <span class="math inline">\(\rightarrow\)</span> the same as calculating <span class="math inline">\(R^2\)</span></li>
</ul></li>
<li><span class="math inline">\(R^2\)</span> can be a <strong><em>misleading</em></strong> summary of model fit
<ul>
<li>deleting data <span class="math inline">\(\rightarrow\)</span> inflate <span class="math inline">\(R^2\)</span></li>
<li>adding terms to a regression model <span class="math inline">\(\rightarrow\)</span> always increases <span class="math inline">\(R^2\)</span></li>
<li><code>example(anscombe)</code> demonstrates the fallacy of <span class="math inline">\(R^2\)</span> through the following
<ul>
<li>basically same mean and variance of X and Y</li>
<li>identical correlations (hence same <span class="math inline">\(R^2\)</span>)</li>
<li>same linear regression relationship</li>
</ul></li>
</ul></li>
<li><p><strong>relationship between <span class="math inline">\(R^2\)</span> and <span class="math inline">\(r\)</span></strong> <span class="math display">\[
\begin{aligned}
\mbox{Correlation between X and Y} \Rightarrow r &amp; = Cor(Y, X)\\
R^2 &amp; = \frac{\sum_{i=1}^n  (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2} \\
&amp;recall \Rightarrow (\hat Y_i - \bar Y) = \hat \beta_1  (X_i - \bar X)\\
(substituting (\hat Y_i - \bar Y)) &amp; = \hat \beta_1^2  \frac{\sum_{i=1}^n(X_i - \bar X)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}\\
&amp;recall \Rightarrow \hat \beta_1 = Cor(Y, X)\frac{Sd(Y)}{Sd(X)}\\
(substituting \hat \beta_1) &amp; = Cor(Y, X)^2\frac{Var(Y)}{Var(X)} \times \frac{\sum_{i=1}^n(X_i - \bar X)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}\\
&amp; recall Var(Y) = \sum_{i=1}^n (Y_i - \bar Y)^2 and Var(X) = \sum_{i=1}^n (X_i - \bar X)^2\\
(simplifying) \Rightarrow R^2 &amp;= Cor(Y,X)^2\\
\mbox{Or } R^2 &amp; = r^2\\
\end{aligned}
\]</span></p></li>
<li><p><strong><em>total variation derivation</em></strong> <span class="math display">\[
\begin{aligned}
\mbox{First, we know that } \bar Y &amp; = \hat \beta_0 + \hat \beta_1 \bar X \\
(transforming) \Rightarrow \hat \beta_0 &amp; = \bar Y - \hat \beta_1 \bar X \\
\\
\mbox{We also know that } \hat Y_i &amp; = \hat \beta_0 + \hat \beta_1 X_i \\
\\
\mbox{Next, the residual } = (Y_i - \hat Y_i) &amp; = Y_i - \hat \beta_0 - \hat \beta_1 X_i \\
(substituting~\hat \beta_0) &amp; = Y_i - (\bar Y - \hat \beta_1 \bar X) - \hat \beta_1 X_i \\
(transforming) \Rightarrow (Y_i - \hat Y_i) &amp; = (Y_i - \bar Y) - \hat \beta_1 (X_i - \bar X) \\
\\
\mbox{Next, the regression difference } = (\hat Y_i - \bar Y) &amp; = \hat \beta_0 - \hat \beta_1 X_i -\bar Y \\
(substituting~\hat \beta_0) &amp; = \bar Y - \hat \beta_1 \bar X - \hat \beta_1 X_i - \bar Y \\
(transforming) \Rightarrow (\hat Y_i - \bar Y) &amp; = \hat \beta_1  (X_i - \bar X) \\
\\
\mbox{Total Variation} = \sum_{i=1}^n (Y_i - \bar Y)^2 &amp; = \sum_{i=1}^n (Y_i - \hat Y_i + \hat Y_i - \bar Y)^2 \Leftarrow (adding~\pm \hat Y_i) \\
(expanding) &amp; = \sum_{i=1}^n (Y_i - \hat Y_i)^2 + 2 \sum_{i=1}^n (Y_i - \hat Y_i)(\hat Y_i - \bar Y) + \sum_{i=1}^n (\hat Y_i - \bar Y)^2 \\
\\
\mbox{Looking at } \sum_{i=1}^n (Y_i - \hat Y_i)(\hat Y_i - \bar Y) &amp;  \\
(substituting~(Y_i - \hat Y_i),(\hat Y_i - \bar Y)) &amp;= \sum_{i=1}^n  \left[(Y_i - \bar Y) - \hat \beta_1 (X_i - \bar X))\right]\left[\hat \beta_1  (X_i - \bar X)\right]\\
(expanding) &amp; = \hat \beta_1 \sum_{i=1}^n (Y_i - \bar Y)(X_i - \bar X) -\hat\beta_1^2\sum_{i=1}^n (X_i - \bar X)^2\\
&amp; (substituting~Y_i, \bar Y) \Rightarrow (Y_i - \bar Y) = (\hat \beta_0 + \hat \beta_1 X_i) - (\hat \beta_0 + \hat \beta_1 \bar X) \\
&amp; (simplifying) \Rightarrow (Y_i - \bar Y) = \hat \beta_1 (X_i - \bar X) \\
(substituting~(Y_i - \bar Y)) &amp; = \hat \beta_1^2 \sum_{i=1}^n (X_i - \bar X)^2-\hat\beta_1^2\sum_{i=1}^n (X_i - \bar X)^2\\
\Rightarrow \sum_{i=1}^n (Y_i - \hat Y_i)(\hat Y_i - \bar Y) &amp;= 0 \\
\\
\mbox{Going back to} \sum_{i=1}^n (Y_i - \bar Y)^2 &amp;= \sum_{i=1}^n (Y_i - \hat Y_i)^2 + 2 \sum_{i=1}^n (Y_i - \hat Y_i)(\hat Y_i - \bar Y) + \sum_{i=1}^n (\hat Y_i - \bar Y)^2 \\
(since~second~term= 0) \Rightarrow \sum_{i=1}^n (Y_i - \bar Y)^2 &amp;= \sum_{i=1}^n (Y_i - \hat Y_i)^2 + \sum_{i=1}^n (\hat Y_i - \bar Y)^2\\
\end{aligned}
\]</span></p></li>
</ul>
<h3 id="example-and-r-commands-1">Example and R Commands</h3>
<ul>
<li><code>resid(fitModel)</code> or <code>fitModel$residuals</code> = extracts model residuals from the fit model (lm in our case) <span class="math inline">\(\rightarrow\)</span> list of residual values for every value of X</li>
<li><code>summary(fitModel)$r.squared</code> = return <span class="math inline">\(R^2\)</span> value of the regression model</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># load multiplot function</span><br><span class="line">source(&quot;multiplot.R&quot;)</span><br><span class="line"># get data</span><br><span class="line">y &lt;- diamond$price; x &lt;- diamond$carat; n &lt;- length(y)</span><br><span class="line"># linear regression</span><br><span class="line">fit &lt;- lm(y ~ x)</span><br><span class="line"># calculate residual</span><br><span class="line">e &lt;- resid(fit)</span><br><span class="line"># calculate predicted values</span><br><span class="line">yhat &lt;- predict(fit)</span><br><span class="line"># create 1 x 2 panel plot</span><br><span class="line">par(mfrow=c(1, 2))</span><br><span class="line"># plot residuals on regression line</span><br><span class="line">plot(x, y, xlab = &quot;Mass (carats)&quot;, ylab = &quot;Price (SIN $)&quot;, bg = &quot;lightblue&quot;,</span><br><span class="line">    col = &quot;black&quot;, cex = 2, pch = 21,frame = FALSE,main = &quot;Residual on Regression Line&quot;)</span><br><span class="line"># draw linear regression line</span><br><span class="line">abline(fit, lwd = 2)</span><br><span class="line"># draw red lines from data points to regression line</span><br><span class="line">for (i in 1 : n)&#123;lines(c(x[i], x[i]), c(y[i], yhat[i]), col = &quot;red&quot; , lwd = 2)&#125;</span><br><span class="line"># plot residual vs x</span><br><span class="line">plot(x, e, xlab = &quot;Mass (carats)&quot;, ylab = &quot;Residuals (SIN $)&quot;, bg = &quot;lightblue&quot;,</span><br><span class="line">    col = &quot;black&quot;, cex = 2, pch = 21,frame = FALSE,main = &quot;Residual vs X&quot;)</span><br><span class="line"># draw horizontal line</span><br><span class="line">abline(h = 0, lwd = 2)</span><br><span class="line"># draw red lines from residual to x axis</span><br><span class="line">for (i in 1 : n)&#123;lines(c(x[i], x[i]), c(e[i], 0), col = &quot;red&quot; , lwd = 2)&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>non-linear data/patterns can be more easily revealed through residual plots</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># create sin wave pattern</span><br><span class="line">x &lt;- runif(100, -3, 3); y &lt;- x + sin(x) + rnorm(100, sd = .2);</span><br><span class="line"># plot data + regression</span><br><span class="line">g &lt;- ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +</span><br><span class="line">	geom_smooth(method = &quot;lm&quot;, colour = &quot;black&quot;) +</span><br><span class="line">	geom_point(size = 3, colour = &quot;black&quot;, alpha = 0.4) +</span><br><span class="line">	geom_point(size = 2, colour = &quot;red&quot;, alpha = 0.4)+</span><br><span class="line"> ggtitle(&quot;Residual on Regression Line&quot;)</span><br><span class="line"># plot residuals</span><br><span class="line">f &lt;- ggplot(data.frame(x = x, y = resid(lm(y ~ x))), aes(x = x, y = y))+</span><br><span class="line">	geom_hline(yintercept = 0, size = 2)+</span><br><span class="line">	geom_point(size = 3, colour = &quot;black&quot;, alpha = 0.4)+</span><br><span class="line">	geom_point(size = 2, colour = &quot;red&quot;, alpha = 0.4)+</span><br><span class="line">	xlab(&quot;X&quot;) + ylab(&quot;Residual&quot;)+ ggtitle(&quot;Residual vs X&quot;)</span><br><span class="line">multiplot(g, f, cols = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>heteroskedasticity</strong> = heteroskedastic model’s variance is not constant and is a function of x</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># create heteroskedastic data</span><br><span class="line">x &lt;- runif(100, 0, 6); y &lt;- x + rnorm(100,  mean = 0, sd = .001 * x)</span><br><span class="line"># plot data + regression</span><br><span class="line">g &lt;- ggplot(data.frame(x = x, y = y), aes(x = x, y = y))+</span><br><span class="line"> geom_smooth(method = &quot;lm&quot;, colour = &quot;black&quot;)+</span><br><span class="line"> geom_point(size = 3, colour = &quot;black&quot;, alpha = 0.4)+</span><br><span class="line"> geom_point(size = 2, colour = &quot;red&quot;, alpha = 0.4) +</span><br><span class="line"> ggtitle(&quot;Residual on Regression Line&quot;)</span><br><span class="line"># plot residuals</span><br><span class="line">f &lt;- ggplot(data.frame(x = x, y = resid(lm(y ~ x))), aes(x = x, y = y))+</span><br><span class="line">           geom_hline(yintercept = 0, size = 2) +</span><br><span class="line">           geom_point(size = 3, colour = &quot;black&quot;, alpha = 0.4)+</span><br><span class="line">           geom_point(size = 2, colour = &quot;red&quot;, alpha = 0.4)+</span><br><span class="line">           xlab(&quot;X&quot;) + ylab(&quot;Residual&quot;) + ggtitle(&quot;Residual vs X&quot;)</span><br><span class="line"># combine two plots</span><br><span class="line">multiplot(g, f, cols = 2)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="inference-in-regression">Inference in Regression</h2>
<ul>
<li>statistics used for hypothesis tests and confidence intervals have the following attributes <span class="math display">\[\frac{\hat \theta - \theta}{\hat \sigma_{\hat \theta}} \sim N(0,1)\]</span>
<ul>
<li>it follows a finite sample Student’s <strong><em>T distribution</em></strong> and is <strong><em>normally distributed</em></strong> if the sample has IID components built in (i.e. <span class="math inline">\(\epsilon_i\)</span>)</li>
<li>used to test <span class="math inline">\(H_0 : \theta = \theta_0\)</span> vs. <span class="math inline">\(H_a : \theta &gt;, &lt;, \neq \theta_0\)</span>.</li>
<li>confidence interval for <span class="math inline">\(\theta\)</span> = <span class="math inline">\(\hat \theta \pm Q_{1-\alpha/2} \hat \sigma_{\hat \theta}\)</span>, where <span class="math inline">\(Q_{1-\alpha/2}\)</span> = relevant quantile from normal(for large samples)/T distribution(small samples, n-1 degrees of freedom)</li>
</ul></li>
</ul>
<h3 id="intervalstests-for-coefficients">Intervals/Tests for Coefficients</h3>
<ul>
<li><p>standard errors for coefficients <span class="math display">\[\begin{aligned}
Var(\hat \beta_1) &amp; = Var\left(\frac{\sum_{i=1}^n (Y_i - \bar Y)(X_i - \bar X)}{((X_i - \bar X)^2)}\right) \\
(expanding) &amp; = Var\left(\frac{\sum_{i=1}^n Y_i (X_i - \bar X) - \bar Y \sum_{i=1}^n (X_i - \bar X)}{((X_i - \bar X)^2)}\right) \\
&amp; Since~ \sum_{i=1}^n X_i - \bar X = 0 \\
(simplifying) &amp; = \frac{\sum_{i=1}^n Y_i (X_i - \bar X)}{(\sum_{i=1}^n (X_i - \bar X)^2)^2} \Leftarrow \mbox{denominator taken out of } Var\\
(Var(Y_i) = \sigma^2) &amp; = \frac{\sigma^2 \sum_{i=1}^n (X_i - \bar X)^2}{(\sum_{i=1}^n (X_i - \bar X)^2)^2} \\
\sigma_{\hat \beta_1}^2 = Var(\hat \beta_1) &amp;= \frac{\sigma^2 }{ \sum_{i=1}^n (X_i - \bar X)^2 }\\
\Rightarrow \sigma_{\hat \beta_1} &amp;= \frac{\sigma}{ \sum_{i=1}^n X_i - \bar X}  \\
\\
\mbox{by the same derivation} \Rightarrow &amp; \\
\sigma_{\hat \beta_0}^2 = Var(\hat \beta_0) &amp; = \left(\frac{1}{n} + \frac{\bar X^2}{\sum_{i=1}^n (X_i - \bar X)^2 }\right)\sigma^2 \\
\Rightarrow \sigma_{\hat \beta_0} &amp;= \sigma \sqrt{\frac{1}{n} + \frac{\bar X^2}{\sum_{i=1}^n (X_i - \bar X)^2 }}\\
\end{aligned}\]</span></p></li>
<li><span class="math inline">\(\sigma\)</span> is unknown but it’s estimate is as follows <span class="math display">\[\hat \sigma^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2\]</span></li>
<li>under IID Gaussian errors (assumed in linear regression with term <span class="math inline">\(\epsilon_0\)</span>), statistics for <span class="math inline">\(\hat \beta_0\)</span> and <span class="math inline">\(\hat \beta_1\)</span> are as follows <span class="math display">\[\frac{\hat \beta_j - \beta_j}{\hat \sigma_{\hat \beta_j}} ~~~where~j = 0, 1\]</span>
<ul>
<li>these statistics follow <strong><span class="math inline">\(t\)</span> distribution</strong> with <strong><span class="math inline">\(n-2\)</span></strong> degrees of freedom for small <span class="math inline">\(n\)</span> and normal distribution for large <span class="math inline">\(n\)</span></li>
</ul></li>
<li><code>summary(fitModel)$coefficients</code> = returns table summarizing the estimate, standar error, t value and p value for the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>
<ul>
<li>the below example reproduces the table <code>summary(fitModel)$coefficients</code> produces</li>
</ul></li>
<li><em><strong>Note</strong>: the variability in the slope, or <span class="math inline">\(Var(\hat \beta_1)\)</span>, is the largest when the predictor values are spread into two cluster that are far apart from each other </em>
<ul>
<li>when modeling linear relationships, it is generally good practice to have many data points that evenly cover the entire range of data, increasing the denominator <span class="math inline">\(\sum_{i=1}^n (X_i - \bar X)^2\)</span></li>
<li>this is so that variance of slope is minimized and we can be more confident about the relationship</li>
</ul></li>
<li><p><strong><em>example</em></strong></p></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># getting data</span><br><span class="line">y &lt;- diamond$price; x &lt;- diamond$carat; n &lt;- length(y)</span><br><span class="line"># calculate beta1</span><br><span class="line">beta1 &lt;- cor(y, x) * sd(y) / sd(x)</span><br><span class="line"># calculate beta0</span><br><span class="line">beta0 &lt;- mean(y) - beta1 * mean(x)</span><br><span class="line"># Gaussian regression error</span><br><span class="line">e &lt;- y - beta0 - beta1 * x</span><br><span class="line"># unbiased estimate for variance</span><br><span class="line">sigma &lt;- sqrt(sum(e^2) / (n-2))</span><br><span class="line"># (X_i - X Bar)</span><br><span class="line">ssx &lt;- sum((x - mean(x))^2)</span><br><span class="line"># calculate standard errors</span><br><span class="line">seBeta0 &lt;- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma</span><br><span class="line">seBeta1 &lt;- sigma / sqrt(ssx)</span><br><span class="line"># testing for H0: beta0 = 0 and beta0 = 0</span><br><span class="line">tBeta0 &lt;- beta0 / seBeta0; tBeta1 &lt;- beta1 / seBeta1</span><br><span class="line"># calculating p-values for Ha: beta0 != 0 and beta0 != 0 (two sided)</span><br><span class="line">pBeta0 &lt;- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)</span><br><span class="line">pBeta1 &lt;- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)</span><br><span class="line"># store results into table</span><br><span class="line">coefTable &lt;- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))</span><br><span class="line">colnames(coefTable) &lt;- c(&quot;Estimate&quot;, &quot;Std. Error&quot;, &quot;t value&quot;, &quot;P(&gt;|t|)&quot;)</span><br><span class="line">rownames(coefTable) &lt;- c(&quot;(Intercept)&quot;, &quot;x&quot;)</span><br><span class="line"># print table</span><br><span class="line">coefTable</span><br><span class="line"># regression model and the generated table from lm (identical to above)</span><br><span class="line">fit &lt;- lm(y ~ x); summary(fit)$coefficients</span><br><span class="line"># store results in matrix</span><br><span class="line">sumCoef &lt;- summary(fit)$coefficients</span><br><span class="line"># print out confidence interval for beta0</span><br><span class="line">sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]</span><br><span class="line"># print out confidence interval for beta1 in 1/10 units</span><br><span class="line">(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>interpretation</strong>: With 95% confidence, we estimate that a 0.1 carat increase in diamond size results in a <code>r round((sumCoef[2,1] - qt(.975, df = fit$df) * sumCoef[2, 2]) / 10, 1)</code> to <code>r round((sumCoef[2,1] + qt(.975, df = fit$df) * sumCoef[2, 2]) / 10, 1)</code> increase in price in (Singapore) dollars.</li>
</ul>
<h3 id="prediction-interval">Prediction Interval</h3>
<ul>
<li>estimated prediction, <span class="math inline">\(\hat y_0\)</span>, at point <span class="math inline">\(x_0\)</span> is <span class="math display">\[\hat y_0 = \hat \beta_0 + \hat \beta_1 x_0\]</span></li>
<li>we can construct two prediction intervals
<ol type="1">
<li>interval for <strong>the line</strong> at <span class="math inline">\(x_0\)</span> <span class="math display">\[
\begin{aligned}
\mbox{Interval}: &amp; \hat y_0 \pm t_{n-2, 1-\alpha/2} \times SE_{line}  \\
\mbox{where } &amp; \hat y_0 = \hat \beta_0 + \hat \beta_1 x_0 \\
\mbox{and } &amp; SE_{line} = \hat \sigma\sqrt{\frac{1}{n} + \frac{(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}\\
\end{aligned}
\]</span>
<ul>
<li>interval has <strong><em>varying width</em></strong></li>
<li>interval is narrow as we are quite confident in the regression line</li>
<li>as <span class="math inline">\(n\)</span> increases, the interval becomes <strong><em>narrower</em></strong>, which makes sense because as more data is collected, we are able to get a better regression line
<ul>
<li><em><strong>Note</strong>: if we knew <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, this interval would have zero width </em></li>
</ul></li>
</ul></li>
<li>interval for the <strong>predicted value</strong>, <span class="math inline">\(\hat y_0\)</span>, at <span class="math inline">\(x_0\)</span> <span class="math display">\[
\begin{aligned}
\mbox{Interval}: &amp; \hat y_0 \pm t_{n-2, 1-\alpha/2} \times SE_{\hat y_0}  \\
\mbox{where } &amp; \hat y_0 = \hat \beta_0 + \hat \beta_1 x_0 \\
\mbox{and } &amp; SE_{\hat y_0} = \hat \sigma\sqrt{1 + \frac{1}{n} + \frac{(x_0 - \bar X)^2}{\sum_{i=1}^n (X_i - \bar X)^2}}\\
\end{aligned}
\]</span>
<ul>
<li>interval has <strong><em>varying width</em></strong></li>
<li>the <span class="math inline">\(1\)</span> part in the <span class="math inline">\(SE_{\hat y_0}\)</span> formula represents the inherent variability in the data
<ul>
<li>no matter how good of a regression line we get, we still can not get rid of the variability in the data</li>
<li><em><strong>Note</strong>: even if we know <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the interval would still have width due to data variance </em></li>
</ul></li>
</ul></li>
</ol></li>
<li><code>predict(fitModel, data, interval = (&quot;confidence&quot;))</code> = returns a 3-column matrix with data for <code>fit</code> (regression line), <code>lwr</code> (lower bound of interval), and <code>upr</code> (upper bound of interval)
<ul>
<li><code>interval = (&quot;confidence&quot;)</code> = returns interval for the line</li>
<li><code>interval = (&quot;prediction&quot;)</code> = returns interval for the prediction</li>
<li><code>data</code> = must be a new data frame with the values you would like to predict</li>
</ul></li>
<li><strong><em>example (<code>ggplot2</code>)</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># create a sequence of values that we want to predict at</span><br><span class="line">newx = data.frame(x = seq(min(x), max(x), length = 100))</span><br><span class="line"># calculate values for both intervals</span><br><span class="line">p1 = data.frame(predict(fit, newdata= newx,interval = (&quot;confidence&quot;)))</span><br><span class="line">p2 = data.frame(predict(fit, newdata = newx,interval = (&quot;prediction&quot;)))</span><br><span class="line"># add column for interval labels</span><br><span class="line">p1$interval = &quot;confidence&quot;; p2$interval = &quot;prediction&quot;</span><br><span class="line"># add column for the x values we want to predict</span><br><span class="line">p1$x = newx$x; p2$x = newx$x</span><br><span class="line"># combine the two dataframes</span><br><span class="line">dat = rbind(p1, p2)</span><br><span class="line"># change the name of the first column to y</span><br><span class="line">names(dat)[1] = &quot;y&quot;</span><br><span class="line"># plot the data</span><br><span class="line">g &lt;- ggplot(dat, aes(x = x, y = y))</span><br><span class="line">g &lt;- g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)</span><br><span class="line">g &lt;- g + geom_line()</span><br><span class="line">g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em>example (<code>base</code>)</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># plot the x and y values</span><br><span class="line">plot(x,y,frame=FALSE,xlab=&quot;Carat&quot;,ylab=&quot;Dollars&quot;,pch=21,col=&quot;black&quot;,bg=&quot;lightblue&quot;,cex=2)</span><br><span class="line"># add the fit line</span><br><span class="line">abline(fit,lwd=2)</span><br><span class="line"># create sequence of x values that we want to predict at</span><br><span class="line">xVals&lt;-seq(min(x),max(x),by=.01)</span><br><span class="line"># calculate the predicted y values</span><br><span class="line">yVals&lt;-beta0+beta1*xVals</span><br><span class="line"># calculate the standard errors for the interval for the line</span><br><span class="line">se1&lt;-sigma*sqrt(1/n+(xVals-mean(x))^2/ssx)</span><br><span class="line"># calculate the standard errors for the interval for the predicted values</span><br><span class="line">se2&lt;-sigma*sqrt(1+1/n+(xVals-mean(x))^2/ssx)</span><br><span class="line"># plot the upper and lower bounds of both intervals</span><br><span class="line">lines(xVals,yVals+2*se1); lines(xVals,yVals-2*se1)</span><br><span class="line">lines(xVals,yVals+2*se2); lines(xVals,yVals-2*se2)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="multivariate-regression">Multivariate Regression</h2>
<ul>
<li>linear models = most important applied statistical/machine learning technique</li>
<li><strong>generalized linear model</strong> extends simple linear regression (SLR) model <span class="math display">\[Y_i =  \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots +\beta_{p} X_{pi} + \epsilon_{i} = \sum_{k=1}^p X_{ik} \beta_j + \epsilon_{i}\]</span> where <span class="math inline">\(X_{1i}=1\)</span> typically, so that an intercept is included</li>
<li>least squares/MLE for the model (under IID Gaussian errors) minimizes <span class="math display">\[\sum_{i=1}^n \left(Y_i - \sum_{k=1}^p X_{ki} \beta_j\right)^2\]</span></li>
<li><strong>linearity of coefficients</strong> is what defines a linear model as transformations of the variables (i.e. squaring) still yields a linear model<span class="math display">\[Y_i =  \beta_1 X_{1i}^2 + \beta_2 X_{2i}^2 + \ldots +\beta_{p} X_{pi}^2 + \epsilon_{i} \]</span></li>
<li>performing multivariate regression = pick any regressor and replace the outcome and all other regressors by their residuals against the chosen one</li>
</ul>
<h3 id="derivation-of-coefficients">Derivation of Coefficients</h3>
<ul>
<li>we know for simple univariate regression through the origin <span class="math display">\[E[Y_i]=X_{1i}\beta_1 \\ \hat \beta_1 =  \frac{\sum_{i=1}^n X_i Y_i}{\sum_{i=1}^n X_i^2}\]</span></li>
<li>we want to minimize <span class="math display">\[\sum_{i=1}^n (Y_i - X_{1i}\beta_1 - X_{2i}\beta_2 - \ldots - X_{pi}\beta_p)^2\]</span></li>
<li>we begin by looking at the two variable model where <span class="math display">\[E[Y_i] = \mu_i  = X_{1i}\beta_1 + X_{2i}\beta_2 \\ \hat \mu_i = X_{1i}\hat \beta_1 + X_{2i}\hat \beta_2\]</span></li>
<li>from our previous derivations, to minimize the sum of squares, the following has to true <span class="math display">\[\sum_{i=1}^n (Y_i - \hat \mu_i) (\hat \mu_i - \mu_i) = 0\]</span></li>
<li>plugging in <span class="math inline">\(\hat \mu_i\)</span> and <span class="math inline">\(\mu_i\)</span> <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - \hat \mu_i) (\hat \mu_i - \mu_i) &amp; = \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2)(X_{1i}\hat \beta_1 + X_{2i}\hat \beta_2 - X_{1i}\beta_1 - X_{2i}\beta_2)\\
(simplifying) &amp; = \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2)\left[ X_{1i}(\hat \beta_1 - \beta_1) + X_{2i} (\hat \beta_2 - \beta_2)\right]\\
(expanding) &amp; = \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{1i}(\hat \beta_1 - \beta_1) + \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{2i} (\hat \beta_2 - \beta_2)\\
\end{aligned}\]</span></li>
<li>for the entire expression to equal to zero, <span class="math display">\[\begin{aligned}
\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{1i}(\hat \beta_1 - \beta_1) &amp; = 0 \\
(since~\hat \beta_1,~\beta_1~don&#39;t~depend~on~i) \Rightarrow \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{1i}&amp; = 0 ~~~~~~\mbox{(1)}\\
\\
\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{2i}(\hat \beta_2 - \beta_2) &amp; = 0 \\
(since~\hat \beta_2,~\beta_2~don&#39;t~depend~on~i) \Rightarrow \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - X_{2i}\hat \beta_2) X_{2i} &amp; = 0 ~~~~~~\mbox{(2)}\\
\end{aligned} \]</span></li>
<li><p>we can hold <span class="math inline">\(\hat \beta_1\)</span> fixed and solve <strong>(2)</strong> for <span class="math inline">\(\hat \beta_2\)</span> <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1) X_{2i} - \sum_{i=1}^n X_{2i}^2 \hat \beta_2&amp; = 0\\
\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1) X_{2i} &amp;= \sum_{i=1}^n X_{2i}^2 \hat \beta_2   \\
\hat \beta_2 &amp; = \frac{\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1) X_{2i}}{\sum_{i=1}^n X_{2i}^2} \\
\end{aligned}
\]</span></p></li>
<li>plugging <span class="math inline">\(\hat \beta_2\)</span> back into <strong>(1)</strong>, we get <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n \left[Y_i - X_{1i}\hat \beta_1 - \frac{\sum_{j=1}^n (Y_j - X_{1j}\hat \beta_1) X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right] X_{1i} &amp; = 0 \\
\sum_{i=1}^n \left[Y_i - X_{1i}\hat \beta_1 - \frac{\sum_{j=1}^n Y_jX_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i} + \frac{\sum_{j=1}^n X_{1j} X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\hat \beta_1\right] X_{1i} &amp; = 0  \\
\sum_{i=1}^n \left[\left(Y_i - \frac{\sum_{j=1}^n Y_jX_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right) - \hat \beta_1 \left(X_{1i} - \frac{\sum_{j=1}^n X_{1j} X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right)\right] X_{1i} &amp; = 0 \\
\Rightarrow \left(Y_i - \frac{\sum_{j=1}^n Y_jX_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right) &amp; = \mbox{residual for } Y_i = X_{i2} \hat \beta_2 + \epsilon_i\\
\Rightarrow \left(X_{1i} - \frac{\sum_{j=1}^n X_{1j} X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right) &amp; = \mbox{residual for } X_{i1} = X_{i2} \hat \gamma + \epsilon_i\\
\end{aligned}
\]</span></li>
<li><p>we can rewrite <span class="math display">\[\sum_{i=1}^n \left[\left(Y_i - \frac{\sum_{j=1}^n Y_jX_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right) - \hat \beta_1 \left(X_{1i} - \frac{\sum_{j=1}^n X_{1j} X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i}\right)\right] X_{1i}  = 0 ~~~~~~(3)\]</span> as <span class="math display">\[ \sum_{i=1}^n \left[ e_{i, Y|X_1} -\hat \beta_1 e_{i, X_1|X_2}  \right] X_{1i}= 0\]</span> where <span class="math display">\[e_{i, a|b} = a_i - \frac{ \sum_{j=1}^n a_j b_j}{ \sum_{j=1}^n b_j^2}b_i\]</span> which is interpreted as the residual when regressing b from a without an intercept</p></li>
<li><p>solving <strong>(3)</strong>, we get <span class="math display">\[\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2}X_{1i}} ~~~~~~(4)\]</span></p></li>
<li><p>to simplify the denominator, we will look at <span class="math display">\[
\begin{aligned}
\sum_{i=1}^n e_{i, X_1 | X_2}^2 &amp; = \sum_{i=1}^n e_{i, X_1 | X_2}\left(X_{1i} - \frac{\sum_{j=1}^n X_{1j}X_{2j} }{ \sum_{j=1}^n X_{2j}^2 } X_{2i}\right) \\
&amp; = \sum_{i=1}^n e_{i, X_1 | X_2}X_{1i} - \frac{\sum_{j=1}^n X_{1j}X_{2j} }{ \sum_{j=1}^n X_{2j}^2 } \sum_{i=1}^n e_{i, X_1 | X_2} X_{2i}\\
&amp; (recall~that~ \sum_{i=1}^n e_{i}X_i = 0,~so~the~2^{nd}~term~is~0) \\
\Rightarrow \sum_{i=1}^n e_{i, X_1 | X_2}^2 &amp; = \sum_{i=1}^n e_{i, X_1 | X_2}X_{1i}\\
\end{aligned}
\]</span></p></li>
<li><p>plugging the above equation back in to <strong>(4)</strong>, we get <span class="math display">\[\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2}^2}\]</span></p></li>
<li><strong>general case</strong>
<ul>
<li>pick one regressor and to replace all other variables by the residuals of their regressions against that one <span class="math display">\[\sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - \ldots - X_{pi}\hat \beta_p)X_k = 0\]</span> for <span class="math inline">\(k = 1, \ldots, p\)</span> yields <span class="math inline">\(p\)</span> equations with <span class="math inline">\(p\)</span> unknowns</li>
<li>holding <span class="math inline">\(\hat \beta_1, \ldots, \hat \beta_{p-1}\)</span> constant, we get <span class="math display">\[\hat \beta_p = \frac{ \sum_{i=1}^n (Y_i - X_{1i}\hat \beta_1 - \ldots - X_{p-1, i}\hat \beta_{p-1})X_{pi}}{\sum_{i=1}^n X_{pi}^2}\]</span></li>
<li>plugging <span class="math inline">\(\hat \beta_p\)</span> back into the equation <span class="math display">\[\sum_{i=1}^n (e_{i,Y|X_p} - e_{i,X_1|X_p}\hat \beta_1 - \ldots - e_{i,X_{p-1}|X_p}\hat \beta_{p-1} )X_k = 0\]</span></li>
<li>since we know that <span class="math display">\[X_k = e_{i,X_i|X_p} + \frac{\sum_{i=1}^n X_{ki}X_{pi}}{\sum_{i=1}^n X_{pi}^2}X_p\]</span> and that <span class="math display">\[\sum_{i=1}^n e_{i,X_i|X_p}X_{pi} = 0\]</span> the equation becomes <span class="math display">\[\sum_{i=1}^n (e_{i,Y|X_p} - e_{i,X_1|X_p}\hat \beta_1 - \ldots - e_{i,X_{p-1}|X_p}\hat \beta_{p-1} )e_{i, X_k|X_p} = 0\]</span></li>
<li>this procedure reduces <span class="math inline">\(p\)</span> LS equations and p unknowns to <span class="math inline">\(p-1\)</span> LS equations and <span class="math inline">\(p-1\)</span> unknowns
<ul>
<li>every variable is replaced by its residual with <span class="math inline">\(X_p\)</span></li>
<li>process iterates until <strong>only <span class="math inline">\(Y\)</span></strong> and <strong>one variable remains</strong></li>
<li>or more intuitively, we take residuals over the confounding variables and do regression through the origin</li>
</ul></li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>for simple linear regression, <span class="math inline">\(Y_{i} = \beta_1 X_{1i} + \beta_2 X_{2i}\)</span> where <span class="math inline">\(X_{2i} = 1\)</span> is an intercept term</li>
<li>the residuals <span class="math display">\[e_{i, Y | X_2} = Y_i - \frac{ \sum_{j=1}^n Y_j X_{2j}}{ \sum_{j=1}^n X_{2j}^2}X_{2i} = Y_i - \bar Y\]</span>
<ul>
<li><em><strong>Note</strong>: this is according to previous derivation of the slope of a regression line through the origin </em></li>
</ul></li>
<li>the residuals <span class="math inline">\(e_{i, X_1 | X_2}= X_{1i} - \frac{\sum_{j=1}^n X_{1j}X_{2j}}{\sum_{j=1}^n X_{2j}^2}X_{2i} = X_{1i} - \bar X_1\)</span>
<ul>
<li><em><strong>Note</strong>: this is according to previous derivation of the slope of a regression line through the origin </em></li>
</ul></li>
<li>Thus <span class="math display">\[\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2}^2} = \frac{\sum_{i=1}^n (X_i - \bar X)(Y_i - \bar Y)}{\sum_{i=1}^n (X_i - \bar X)^2} = Cor(X, Y) \frac{Sd(Y)}{Sd(X)}\]</span></li>
</ul></li>
</ul>
<h3 id="interpretation-of-coefficients">Interpretation of Coefficients</h3>
<ul>
<li>from the derivation in the previous section, we have <span class="math display">\[\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2}^2}\]</span></li>
<li><p>this is interpreted as the effect of variable <span class="math inline">\(X_1\)</span> when the effects of all other variables have been removed from <span class="math inline">\(X_1\)</span> and the predicted result <span class="math inline">\(Y\)</span> (holding everything else constant/adjusting for all other variables)</p></li>
<li>the expected response is as follows <span class="math display">\[E[Y | X_1 = x_1, \ldots, X_p = x_p] = \sum_{k=1}^p x_{k} \beta_k\]</span> so the expected change in the response through change in one variable is <span class="math display">\[
\begin{aligned}
&amp; E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p]  - E[Y | X_1 = x_1, \ldots, X_p = x_p] \\
= &amp; (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k - \sum_{k=1}^p x_{k} \beta_k \\
= &amp; (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k - (x_1 \beta_1 + \sum_{k=2}^p x_{k} \beta_k) \\
= &amp; \beta_1 \\
\end{aligned}
\]</span></li>
<li>therefore, interpretation of a multivariate regression coefficient <span class="math inline">\(\rightarrow\)</span> expected change in the response per unit change in the regressor, holding all of the other regressors fixed</li>
<li>all of the SLR properties/calculations extend to generalized linear model
<ul>
<li><strong><em>model</em></strong> = <span class="math inline">\(Y_i = \sum_{k=1}^p X_{ik} \beta_{k} + \epsilon_{i}\)</span> where <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span></li>
<li><strong><em>fitted response</em></strong> = <span class="math inline">\(\hat Y_i = \sum_{k=1}^p X_{ik} \hat \beta_{k}\)</span></li>
<li><strong><em>residual</em></strong> = <span class="math inline">\(e_i = Y_i - \hat Y_i\)</span></li>
<li><strong><em>variance estimate</em></strong> = <span class="math inline">\(\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i ^2\)</span></li>
<li><strong><em>predicted responses at new values, <span class="math inline">\(x_1, \ldots, x_p\)</span></em></strong> = plug <span class="math inline">\(x\)</span> values into <span class="math inline">\(\sum_{k=1}^p x_{k} \hat \beta_{k}\)</span></li>
<li><strong><em>standard errors of coefficients</em></strong> = <span class="math inline">\(\hat \sigma_{\hat \beta_k}\)</span></li>
<li><strong><em>test/CI statistic</em></strong> = <span class="math inline">\(\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}\)</span> follows a <span class="math inline">\(T\)</span> distribution with <span class="math inline">\(n-p\)</span> degrees of freedom</li>
<li><strong><em>predicted/expected response intervals</em></strong> = calculated using standard errors of predicted responses of <span class="math inline">\(\hat Y_i\)</span></li>
</ul></li>
</ul>
<h3 id="example-linear-model-with-2-variables-and-intercept">Example: Linear Model with 2 Variables and Intercept</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># simulate the data</span><br><span class="line">n = 100; x = rnorm(n); x2 = rnorm(n); x3 = rnorm(n)</span><br><span class="line"># equation = intercept + var1 + var2 + var3 + error</span><br><span class="line">y = 1 + x + x2 + x3 + rnorm(n, sd = .1)</span><br><span class="line"># residual of y regressed on var2 and var3</span><br><span class="line">ey = resid(lm(y ~ x2 + x3))</span><br><span class="line"># residual of y regressed on var2 and var3</span><br><span class="line">ex = resid(lm(x ~ x2 + x3))</span><br><span class="line"># estimate beta1 for var1</span><br><span class="line">sum(ey * ex) / sum(ex ^ 2)</span><br><span class="line"># regression through the origin with xva1 with var2 var3 effect removed</span><br><span class="line">coef(lm(ey ~ ex - 1))</span><br><span class="line"># regression for all three variables</span><br><span class="line">coef(lm(y ~ x + x2 + x3))</span><br></pre></td></tr></table></figure>
<h3 id="example-coefficients-that-reverse-signs">Example: Coefficients that Reverse Signs</h3>
<ul>
<li><em><strong>Note</strong>: more information can be found at <code>?swiss</code> </em></li>
<li>data set is composed of standardized fertility measure and socio-economic indicators for each of 47 French-speaking provinces of Switzerland at about 1888</li>
<li>data frame has 47 observations on 6 variables, each of which is in percent [0, 100]
<ul>
<li><strong><em>Fertility</em></strong> = common standardized fertility measure <span class="math inline">\(\rightarrow\)</span> outcome</li>
<li><strong><em>Agriculture</em></strong> = % of males involved in agriculture as occupation</li>
<li><strong><em>Examination</em></strong> = % draftees receiving highest mark on army examination</li>
<li><strong><em>Education</em></strong> = % education beyond primary school for draftees</li>
<li><strong><em>Catholic</em></strong> = % catholic vs protestant</li>
<li><strong><em>Infant.Mortality</em></strong> = live births who live less than 1 year</li>
</ul></li>
<li>[<code>GGally</code> package] <code>ggpairs(data)</code> = produces pair wise plot for the predictors similar to <code>pairs</code> in base package</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># load dataset</span><br><span class="line">require(datasets); data(swiss); require(GGally)</span><br><span class="line"># produce pairwise plot using ggplot2</span><br><span class="line">ggpairs(swiss, lower = list(continuous = &quot;smooth&quot;),params = c(method = &quot;loess&quot;))</span><br><span class="line"># print coefficients of regression of fertility on all predictors</span><br><span class="line">summary(lm(Fertility ~ . , data = swiss))$coefficients</span><br></pre></td></tr></table></figure>
<ul>
<li>interpretation for Agriculture coefficient
<ul>
<li>we expect an <code>r round(summary(lm(Fertility~.,data=swiss))$coefficients[2,1],2)</code> <strong>decrease</strong> in standardized <strong><em>fertility</em></strong> for every 1% increase in percentage of <strong><em>males involved in agriculture</em></strong> in holding the remaining variables constant</li>
<li>since the p-value is <code>r summary(lm(Fertility~.,data=swiss))$coefficients[2,4]</code>, the t-test for <span class="math inline">\(H_0: \beta_{Agri} = 0\)</span> versus <span class="math inline">\(H_a: \beta_{Agri} \neq 0\)</span> is <strong><em>significant</em></strong></li>
</ul></li>
<li>however, if we look at the unadjusted estimate (marginal regression) for the coefficient for Agriculture</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># run marginal regression on Agriculture</span><br><span class="line">summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients</span><br></pre></td></tr></table></figure>
<ul>
<li>interpretation for Agriculture coefficient
<ul>
<li>we expect an <code>r round(summary(lm(Fertility~Agriculture,data=swiss))$coefficients[2,1],2)</code> <strong>increase</strong> in standardized <strong><em>fertility</em></strong> for every 1% increase in percentage of <strong><em>males involved in agriculture</em></strong> in holding the remaining variables constant
<ul>
<li><em><strong>Note</strong>: the coefficient <strong>flipped signs</strong> </em></li>
</ul></li>
<li>since the p-value is <code>r summary(lm(Fertility~Agriculture,data=swiss))$coefficients[2,4]</code>, the t-test for <span class="math inline">\(H_0: \beta_{Agri} = 0\)</span> versus <span class="math inline">\(H_a: \beta_{Agri} \neq 0\)</span> is <strong><em>significant</em></strong></li>
</ul></li>
<li>to see intuitively how a <strong>sign change</strong> is possible, we can look at the following simulated example</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; x2 &lt;- 1 : n; x1 &lt;- .01 * x2 + runif(n, -.1, .1); y = -x1 + x2 + rnorm(n, sd = .01)</span><br><span class="line"># print coefficients</span><br><span class="line">c(&quot;with x1&quot; = summary(lm(y ~ x1))$coef[2,1],</span><br><span class="line">	&quot;with x1 and x2&quot; = summary(lm(y ~ x1 + x2))$coef[2,1])</span><br><span class="line"># print p-values</span><br><span class="line">c(&quot;with x1&quot; = summary(lm(y ~ x1))$coef[2,4],</span><br><span class="line">	&quot;with x1 and x2&quot; = summary(lm(y ~ x1 + x2))$coef[2,4])</span><br><span class="line"># store all data in one data frame (ey and ex1 are residuals with respect to x2)</span><br><span class="line">dat &lt;- data.frame(y = y, x1 = x1, x2 = x2, ey = resid(lm(y ~ x2)), ex1 = resid(lm(x1 ~ x2)))</span><br><span class="line"># plot y vs x1</span><br><span class="line">g &lt;- ggplot(dat, aes(y = y, x = x1, colour = x2)) +</span><br><span class="line">	geom_point(colour=&quot;grey50&quot;, size = 2) +</span><br><span class="line">	geom_smooth(method = lm, se = FALSE, colour = &quot;black&quot;) + geom_point(size = 1.5) +</span><br><span class="line">	ggtitle(&quot;unadjusted = y vs x1&quot;)</span><br><span class="line"># plot residual of y adjusted for x2 vs residual of x1 adjusted for x2</span><br><span class="line">g2 &lt;- ggplot(dat, aes(y = ey, x = ex1, colour = x2)) +</span><br><span class="line">	geom_point(colour=&quot;grey50&quot;, size = 2) +</span><br><span class="line">	geom_smooth(method = lm, se = FALSE, colour = &quot;black&quot;) + geom_point(size = 1.5) +</span><br><span class="line">	ggtitle(&quot;adjusted = y, x1 residuals with x2 removed&quot;) + labs(x = &quot;resid(x1~x2)&quot;,</span><br><span class="line">		y = &quot;resid(y~x2)&quot;)</span><br><span class="line"># combine plots</span><br><span class="line">multiplot(g, g2, cols = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the correlation between <code>y</code> and <code>x1</code> flips signs when adjusting for <code>x2</code></li>
<li><p>this effectively means that within each consecutive group/subset of points (each color gradient) on the left hand plot (unadjusted), there exists a negative relationship between the points while the overall trend is going up</p></li>
<li><strong><em>going back to the swiss data set</em></strong>, the sign of the coefficient for Agriculture <em>reverses</em> itself with the inclusion of Examination and Education (both are <strong><em>negatively correlated</em></strong> with Agriculture)
<ul>
<li>correlation between Agriculture and Education: <code>r round(cor(swiss$Education,swiss$Agriculture),2)</code></li>
<li>correlation between Agriculture and Examination: <code>r round(cor(swiss$Examination,swiss$Agriculture),2)</code></li>
<li>correlation between Education and Examination: <code>r round(cor(swiss$Education, swiss$Examination),2)</code>
<ul>
<li>this means that the two variables are likely to be measuring the same things</li>
</ul></li>
</ul></li>
<li><p><em><strong>Note</strong>: it is <strong>difficult to interpret</strong> and determine which one is the correct model <span class="math inline">\(\rightarrow\)</span> one <strong>should not</strong> claim positive correlation between Agriculture and Fertility simply based on marginal regression <code>lm(Fertility ~ Agriculture, data=swiss)</code> </em></p></li>
</ul>
<h3 id="example-unnecessary-variables">Example: Unnecessary Variables</h3>
<ul>
<li><strong>unnecessary predictors</strong> = variables that don’t provide any new linear information, meaning that the variable are simply <strong><em>linear combinations</em></strong> (multiples, sums) of other predictors/variables</li>
<li>when running a linear regression with unnecessary variables, R automatically drops the linear combinations and returns <code>NA</code> as their coefficients</li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># add a linear combination of agriculture and education variables</span><br><span class="line">z &lt;- swiss$Agriculture + swiss$Education</span><br><span class="line"># run linear regression with unnecessary variables</span><br><span class="line">lm(Fertility ~ . + z, data = swiss)$coef</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see above, the R dropped the unnecessary variable <code>z</code> by excluding it from the linear regression <span class="math inline">\(\rightarrow\)</span> <code>z</code>’s coefficient is <code>NA</code></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="dummy-variables">Dummy Variables</h2>
<ul>
<li><strong>dummy variables</strong> = binary variables that take on value of <strong>1</strong> when the measurement is in a particular group, and <strong>0</strong> when the measurement is not (i.e. in clinical trials, treated = 1, untreated = 1)</li>
<li>in linear model form, <span class="math display">\[Y_i = \beta_0 + X_{i1} \beta_1 + \epsilon_{i}\]</span> where <span class="math inline">\(X_{i1}\)</span> is a binary/dummy variable so that it is a 1 if measurement <span class="math inline">\(i\)</span> is in a group and 0 otherwise
<ul>
<li>for people in the group, the mean or <span class="math inline">\(\mu_{X_{i1} = 1} = E[Y_i] = \beta_0 + \beta_1\)</span></li>
<li>for people <strong><em>not</em></strong> in the group, the mean or <span class="math inline">\(\mu_{X_{i1} = 0} = E[Y_i] = \beta_0\)</span></li>
<li>predicted mean for group = <span class="math inline">\(\hat \beta_0 + \hat \beta_1\)</span></li>
<li>predicted mean for not in group = <span class="math inline">\(\hat \beta_0\)</span></li>
<li>coefficient <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(X_{i1}\)</span> is interpreted as the <strong><em>increase or decrease</em></strong> in the <strong>mean</strong> when comparing two groups (in vs not)</li>
<li><em><strong>Note</strong>: including a dummy variable that is 1 for not in the group would be <strong>redundant</strong> as it would simply be a linear combination <span class="math inline">\(1 - X_{i1}\)</span> </em></li>
</ul></li>
</ul>
<h3 id="more-than-2-levels">More Than 2 Levels</h3>
<ul>
<li>for 3 factor levels, we would need 2 dummy variables and the model would be <span class="math display">\[Y_i = \beta_0 + X_{i1} \beta_1 + X_{i2} \beta_2 + \epsilon_i\]</span></li>
<li>for this example, we will use the above model to analyze US political party affiliations (Democrats vs Republicans vs independents) and denote the variables as follows:
<ul>
<li><span class="math inline">\(X_{i1} = 1\)</span> for Republicans and <span class="math inline">\(0\)</span> otherwise</li>
<li><span class="math inline">\(X_{i2} = 1\)</span> for Democrats and <span class="math inline">\(0\)</span> otherwise</li>
<li>If <span class="math inline">\(i\)</span> is Republican, <span class="math inline">\(X_{i1} = 1\)</span>, <span class="math inline">\(X_{i2} = 0\)</span>, <span class="math inline">\(E[Y_i] = \beta_0 +\beta_1\)</span></li>
<li>If <span class="math inline">\(i\)</span> is Democrat, <span class="math inline">\(X_{i1} = 0\)</span>, <span class="math inline">\(X_{i2} = 1\)</span>, <span class="math inline">\(E[Y_i] = \beta_0 + \beta_2\)</span></li>
<li>If <span class="math inline">\(i\)</span> is independent, <span class="math inline">\(X_{i1} = 0\)</span>, <span class="math inline">\(X_{i2} = 0\)</span>, <span class="math inline">\(E[Y_i] = \beta_0\)</span></li>
<li><span class="math inline">\(\beta_1\)</span> compares Republicans to independents</li>
<li><span class="math inline">\(\beta_2\)</span> compares Democrats to independents</li>
<li><span class="math inline">\(\beta_1 - \beta_2\)</span> compares Republicans to Democrats</li>
</ul></li>
<li><em><strong>Note</strong>: choice of reference category (independent in this case) changes the interpretation </em>
<ul>
<li><strong>reference category</strong> = the group whose binary variable has been eliminated</li>
</ul></li>
<li>the same principles explained above can be expanded to <span class="math inline">\(p\)</span> level model <span class="math display">\[Y_i = \beta_0 + X_{i1} \beta_1 + X_{i2} \beta_2 + \ldots + X_{ip} \beta_p + \epsilon_i\]</span></li>
</ul>
<h3 id="example-6-factor-level-insect-spray-data">Example: 6 Factor Level Insect Spray Data</h3>
<ul>
<li>below is a violin plot of the 6 different types (A, B, C, D, E, and F) of insect sprays and their potency (kill count) from <code>InsectSprays</code> data set
<ul>
<li><em><strong>Note</strong>: the varying width of each bar indicates the density of measurement at each value </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># load insect spray data</span><br><span class="line">data(InsectSprays)</span><br><span class="line">ggplot(data = InsectSprays, aes(y = count, x = spray, fill  = spray)) +</span><br><span class="line">	geom_violin(colour = &quot;black&quot;, size = 2) + xlab(&quot;Type of spray&quot;) +</span><br><span class="line">	ylab(&quot;Insect count&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>Linear model fit with group A as reference category</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># linear fit with 5 dummy variables</span><br><span class="line">summary(lm(count ~ spray, data = InsectSprays))$coefficients</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: R automatically converts factor variables into <span class="math inline">\(n-1\)</span> dummy variables and uses the first category as reference </em>
<ul>
<li>mean of group A is therefore the default intercept</li>
</ul></li>
<li>the above coefficients can be interpreted as the difference in means between each group (B, C, D, E, and F) and group A (the intercept)
<ul>
<li>example: the mean of group B is <code>r round(lm(count ~ spray, data = InsectSprays)$coefficients[2],2)</code> higher than the mean of group A, which is <code>r round(lm(count ~ spray, data = InsectSprays)$coefficients[1],2)</code></li>
<li>means for group B/C/D/E/F = the intercept + their respective coefficient</li>
</ul></li>
<li>all t-tests are for comparisons of Sprays versus Spray A</li>
</ul>
<p><strong>Hard-coding the dummy variables</strong></p>
<ul>
<li>this produces the exact same result as the command <code>lm(count ~ spray, data = InsectSprays)</code></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># hard coding dummy variables</span><br><span class="line">lm(count ~ I(1 * (spray == &apos;B&apos;)) + I(1 * (spray == &apos;C&apos;)) +</span><br><span class="line">           I(1 * (spray == &apos;D&apos;)) + I(1 * (spray == &apos;E&apos;)) +</span><br><span class="line">           I(1 * (spray == &apos;F&apos;)), data = InsectSprays)$coefficients</span><br></pre></td></tr></table></figure>
<p><strong>Linear model fit with all 6 categories</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># linear fit with 6 dummy variables</span><br><span class="line">lm(count ~ I(1 * (spray == &apos;B&apos;)) + I(1 * (spray == &apos;C&apos;)) +</span><br><span class="line">   		   I(1 * (spray == &apos;D&apos;)) + I(1 * (spray == &apos;E&apos;)) +</span><br><span class="line">   		   I(1 * (spray == &apos;F&apos;)) + I(1 * (spray == &apos;A&apos;)),</span><br><span class="line">   		   data = InsectSprays)$coefficients</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the coefficient for group <code>A</code> is <code>NA</code></li>
<li>this is because <span class="math inline">\(X_{iA} = 1 - X_{iB} - X_{iC}- X_{iD}- X_{iE}- X_{iF}\)</span>, or the dummy variable for A is a linear combination of the rest of the dummy variables</li>
</ul>
<p><strong>Linear model fit with omitted intercept</strong></p>
<ul>
<li>eliminating the intercept would mean that each group is compared to the value 0, which would yield 6 variables since A is no longer the reference category</li>
<li>this means that the coefficients for the 6 variables are simply the <strong><em>mean</em></strong> of each group
<ul>
<li>when <span class="math inline">\(X_{iA} = 1\)</span>, all the other dummy variables become 0, which means the linear model becomes <span class="math display">\[Y_i = \beta_A + \epsilon_i\]</span></li>
<li>then <span class="math inline">\(E[Y_i] = \beta_A = \mu_A\)</span></li>
<li>this makes sense because the <strong><em>best prediction</em></strong> for the kill count of spray of type A is the <strong><em>mean</em></strong> of recorded kill counts of A spray</li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># linear model with omitted intercept</span><br><span class="line">summary(lm(count ~ spray - 1, data = InsectSprays))$coefficients</span><br><span class="line"># actual means of count by each variable</span><br><span class="line">round(tapply(InsectSprays$count, InsectSprays$spray, mean), 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>all t-tests are for whether the groups are different than zero (i.e. are the expected counts 0 for that spray?)</li>
<li><p>to compare between different categories, say B vs C, we can simply subtract the coefficients</p></li>
<li>to reorient the model with other groups as reference categories, we can simply <strong>reorder the levels</strong> for the factor variable
<ul>
<li><code>relevel(var, &quot;l&quot;)</code> = reorders the factor levels within the factor variable <code>var</code> such that the specified level “l” is the reference/base/lowest level
<ul>
<li><em><strong>Note</strong>: R automatically uses the first/base level as the reference category during a linear regression </em></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># reorder the levels of spray variable such that C is the lowest level</span><br><span class="line">spray2 &lt;- relevel(InsectSprays$spray, &quot;C&quot;)</span><br><span class="line"># rerun linear regression with releveled factor</span><br><span class="line">summary(lm(count ~ spray2, data = InsectSprays))$coef</span><br></pre></td></tr></table></figure>
<ul>
<li>it is important to note in this example that
<ul>
<li>counts are bounded from below by 0 <span class="math inline">\(\rightarrow\)</span> <strong><em>violates</em></strong> the assumption of normality of the errors</li>
<li>there are counts near zero <span class="math inline">\(\rightarrow\)</span> <strong><em>violates</em></strong> intent of the assumption <span class="math inline">\(\rightarrow\)</span> not acceptable in assuming normal distribution</li>
<li>variance does not appear to be constant across different type of groups <span class="math inline">\(\rightarrow\)</span> violates assumption
<ul>
<li>taking log(counts) + 1 may help (+1 since there are the zero values)</li>
</ul></li>
<li><strong><em>Poisson GLMs</em></strong> are better (don’t have to worry about the assumptions) for fitting count data</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="interactions">Interactions</h2>
<ul>
<li><strong>interactions</strong> between variables can be added to a regression model to test how the outcomes change under different conditions</li>
<li>we will use the data set from the Millennium Development Goal from the UN which can be found <a href="https://github.com/bcaffo/courses/blob/master/07_RegressionModels/02_02_multivariateExamples/hunger.csv" target="_blank" rel="noopener">here</a>
<ul>
<li><code>Numeric</code> = values for children aged &lt;5 years underweight (%)</li>
<li><code>Sex</code> = records whether</li>
<li><code>Year</code> = year when data was recorded</li>
<li><code>Income</code> = income for the child’s parents</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># load in hunger data</span><br><span class="line">hunger &lt;- read.csv(&quot;hunger.csv&quot;)</span><br><span class="line"># exclude the data with &quot;Both Sexes&quot; as values (only want Male vs Female)</span><br><span class="line">hunger &lt;- hunger[hunger$Sex!=&quot;Both sexes&quot;, ]</span><br><span class="line"># structure of data</span><br><span class="line">str(hunger)</span><br></pre></td></tr></table></figure>
<h3 id="model-hungry-year-by-sex">Model: % Hungry ~ Year by Sex</h3>
<ul>
<li>this will include 2 models with 2 separate lines</li>
<li>model for % hungry (<span class="math inline">\(H_F\)</span>) vs year (<span class="math inline">\(Y_F\)</span>) for females is <span class="math display">\[H_{Fi} = \beta_{F0} + \beta_{F1} Y_{Fi} + \epsilon_{Fi}\]</span>
<ul>
<li><span class="math inline">\(\beta_{F0}\)</span> = % of females hungry at year 0</li>
<li><span class="math inline">\(\beta_{F1}\)</span> = decrease in % females hungry per year</li>
<li><span class="math inline">\(\epsilon_{Fi}\)</span> = standard error (or everything we didn’t measure)</li>
</ul></li>
<li>model for % hungry (<span class="math inline">\(H_M\)</span>) vs year (<span class="math inline">\(Y_M\)</span>) for males is <span class="math display">\[H_{Mi} = \beta_{M0} + \beta_{M1} Y_{Mi} + \epsilon_{Mi}\]</span>
<ul>
<li><span class="math inline">\(\beta_{M0}\)</span> = % of males hungry at year 0</li>
<li><span class="math inline">\(\beta_{M1}\)</span> = decrease in % males hungry per year</li>
<li><span class="math inline">\(\epsilon_{Mi}\)</span> = standard error (or everything we didn’t measure)</li>
</ul></li>
<li>each line has <strong>different</strong> residuals, standard errors, and variances</li>
<li><em><strong>Note</strong>: <span class="math inline">\(\beta_{F0}\)</span> and <span class="math inline">\(\beta_{M0}\)</span> are the interpolated intercept at year 0, which does hold any interpretable value for the model </em>
<ul>
<li>it’s possible to subtract the model by a meaningful value (% hungry at 1970, or average), which moves the intercept of the lines to something interpretable</li>
</ul></li>
<li><em><strong>Note</strong>: we are also assuming the error terms <span class="math inline">\(\epsilon_{Fi}\)</span> and <span class="math inline">\(\epsilon_{Mi}\)</span> are Gaussian distributions <span class="math inline">\(\rightarrow\)</span> mean = 0 </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># run linear model with Numeric vs Year for male and females</span><br><span class="line">male.fit &lt;- lm(Numeric ~ Year, data = hunger[hunger$Sex == &quot;Male&quot;, ])</span><br><span class="line">female.fit &lt;- lm(Numeric ~ Year, data = hunger[hunger$Sex == &quot;Female&quot;, ])</span><br><span class="line"># plot % hungry vs the year</span><br><span class="line">plot(Numeric ~ Year, data = hunger, pch = 19, col=(Sex==&quot;Male&quot;)*1+1)</span><br><span class="line"># plot regression lines for both</span><br><span class="line">abline(male.fit, lwd = 3, col = &quot;black&quot;)</span><br><span class="line">abline(female.fit, lwd = 3, col = &quot;red&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="model-hungry-year-sex-binary-variable">Model: % Hungry ~ Year + Sex (Binary Variable)</h3>
<ul>
<li>this will include 1 model with 2 separate lines with the <strong><em>same</em></strong> slope</li>
<li><p>model for % hungry (<span class="math inline">\(H\)</span>) vs year (<span class="math inline">\(Y\)</span>) and dummy variable for sex (<span class="math inline">\(X\)</span>) is <span class="math display">\[H_{i} = \beta_{0} + \beta_{1}X_i + \beta_{2} Y_{i} + \epsilon_{i}^*\]</span></p>
<ul>
<li><span class="math inline">\(\beta_{0}\)</span> = % of females hungry at year 0</li>
<li><span class="math inline">\(\beta_{0} + \beta_{1}\)</span> = % of males hungry at year 0
<ul>
<li><em><strong>Note</strong>: the term <span class="math inline">\(\beta_{1}X_i\)</span> is effectively an <strong>adjustment</strong> for the intercept for males and DOES NOT alter the slope in anyway </em></li>
<li><span class="math inline">\(\beta_{1}\)</span> = difference in means of males vs females</li>
</ul></li>
<li><span class="math inline">\(\beta_{2}\)</span> = decrease in % hungry (males and females) per year
<ul>
<li>this means that the slope is <strong><em>constant</em></strong> for both females and males</li>
</ul></li>
<li><span class="math inline">\(\epsilon_{i}^*\)</span> = standard error (or everything we didn’t measure)
<ul>
<li>we are still assuming Gaussian error term</li>
</ul></li>
</ul></li>
<li><code>abline(intercept, slope)</code> = adds a line to the existing plot based on the intercept and slope provided
<ul>
<li><code>abline(lm)</code> = plots the linear regression line on the plot</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># run linear model with Numeric vs Year and Sex</span><br><span class="line">both.fit &lt;- lm(Numeric ~ Year+Sex, data = hunger)</span><br><span class="line"># print fit</span><br><span class="line">both.fit$coef</span><br><span class="line"># plot % hungry vs the year</span><br><span class="line">plot(Numeric ~ Year, data = hunger, pch = 19, col=(Sex==&quot;Male&quot;)*1+1)</span><br><span class="line"># plot regression lines for both (same slope)</span><br><span class="line">abline(both.fit$coef[1], both.fit$coef[2], lwd = 3, col = &quot;black&quot;)</span><br><span class="line">abline(both.fit$coef[1]+both.fit$coef[3], both.fit$coef[2], lwd = 3, col = &quot;red&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="model-hungry-year-sex-year-sex-binary-interaction">Model: % Hungry ~ Year + Sex + Year * Sex (Binary Interaction)</h3>
<ul>
<li>this will include 1 model with an interaction term with binary variable, which produces 2 lines with <strong><em>different</em></strong> slopes</li>
<li>we can introduce an interaction term to the previous model to capture the different slopes between males and females</li>
<li>model for % hungry (<span class="math inline">\(H\)</span>) vs year (<span class="math inline">\(Y\)</span>), sex (<span class="math inline">\(X\)</span>), and interaction between year and sex (<span class="math inline">\(Y \times X\)</span>) is <span class="math display">\[H_{i} = \beta_{0} + \beta_{1}X_i + \beta_{2} Y_{i} + \beta_{3}X_i Y_{i} + \epsilon_{i}^+\]</span>
<ul>
<li><span class="math inline">\(\beta_{0}\)</span> = % of females hungry at year 0</li>
<li><span class="math inline">\(\beta_{0} + \beta_{1}\)</span> = % of males hungry at year 0
<ul>
<li><span class="math inline">\(\beta_{1}\)</span> = change in <strong><em>intercept</em></strong> for males</li>
</ul></li>
<li><span class="math inline">\(\beta_{2}\)</span> = decrease in % hungry (females) per year</li>
<li><span class="math inline">\(\beta_{2} + \beta_{3}\)</span> = decrease in % hungry (males) per year
<ul>
<li><span class="math inline">\(\beta_{3}\)</span> = change in <strong><em>slope</em></strong> for males</li>
</ul></li>
<li><span class="math inline">\(\epsilon_{i}^+\)</span> = standard error (or everything we didn’t measure)</li>
</ul></li>
<li>expected value for males is <span class="math inline">\(E[H_i]_M = (\beta_0 + \beta_1) + (\beta_2 + \beta_3) Y_i\)</span></li>
<li>expected value for females is <span class="math inline">\(E[H_i]_F = \beta_0 + \beta_2 Y_i\)</span>
<ul>
<li><span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_3\)</span> are effectively adjusting the intercept and slope for males</li>
</ul></li>
<li><code>lm(outcome ~ var1*var2)</code> = whenever an interaction is specified in <code>lm</code> function using the <code>*</code> operator, the individual terms are added automatically
<ul>
<li><code>lm(outcome ~ var1+var2+var1*var2)</code> = builds the exact same model</li>
<li><code>lm(outcome ~ var1:var2)</code> = builds linear model with <strong><em>only</em></strong> the interaction term (specified by <code>:</code> operator)</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># run linear model with Numeric vs Year and Sex and interaction term</span><br><span class="line">interaction.fit &lt;- lm(Numeric ~ Year*Sex, data = hunger)</span><br><span class="line"># print fit</span><br><span class="line">interaction.fit$coef</span><br><span class="line"># plot % hungry vs the year</span><br><span class="line">plot(Numeric ~ Year, data = hunger, pch = 19, col=(Sex==&quot;Male&quot;)*1+1)</span><br><span class="line"># plot regression lines for both (different slope)</span><br><span class="line">abline(interaction.fit$coef[1], interaction.fit$coef[2], lwd = 3, col = &quot;black&quot;)</span><br><span class="line">abline(interaction.fit$coef[1]+interaction.fit$coef[3],</span><br><span class="line">	interaction.fit$coef[2]+interaction.fit$coef[4], lwd = 3, col = &quot;red&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="example-hungry-year-income-year-income-continuous-interaction">Example: % Hungry ~ Year + Income + Year * Income (Continuous Interaction)</h3>
<ul>
<li>this will include 1 model with an interaction term with continuous variable, which produces a curve through the plot</li>
<li>for <strong>continuous interactions</strong> (two continuous variables) with model <span class="math display">\[Y{i} = \beta_{0} + \beta_{1} X_{1i} + \beta_{2} X_{2i} + \beta_{3} X_{1i} X_{2i} + \epsilon_{i}\]</span> the expected value for a given set of values <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> is defined as <span class="math display">\[E[Y_i|X_{1i}=x_1, X_{2i}=x_2] = \beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \beta_{3} x_{1} x_{2}\]</span></li>
<li>holding <span class="math inline">\(X_2\)</span> constant and varying <span class="math inline">\(X_1\)</span> by <span class="math inline">\(1\)</span>, we have <span class="math display">\[\begin{aligned}
\frac{\partial Y_i}{\partial X_{1i}} &amp; = E[Y_i|X_{1i}=x_1 + 1, X_{2i}=x_2] - E[Y_i|X_{1i}=x_1, X_{2i}=x_2] \\
&amp; = \beta_{0} + \beta_{1} (x_{1}+1) + \beta_{2} x_{2} + \beta_{3} (x_{1}+1) x_{2} - [\beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \beta_{3} x_{1} x_{2}]\\
&amp; = \beta_{1} + \beta_{3} x_{2}\\
\end{aligned}\]</span>
<ul>
<li><em><strong>Note</strong>: this means that slope for <span class="math inline">\(X_{1i}\)</span> <strong>not</strong> a constant and is <strong>dependent</strong> on <span class="math inline">\(X_{2i}\)</span> </em></li>
<li><span class="math inline">\(\beta_1\)</span> is the slope for <span class="math inline">\(X_{1i}\)</span> when <span class="math inline">\(X_{2i}\)</span> = 0</li>
</ul></li>
<li>by the same logic, if we vary <span class="math inline">\(X_1\)</span> by <span class="math inline">\(1\)</span> and find the change, and vary <span class="math inline">\(X_2\)</span> by <span class="math inline">\(1\)</span> and find the change, we get <span class="math display">\[\begin{aligned}
\frac{\partial}{\partial X_{2i}} \left(\frac{\partial Y_i}{\partial X_{1i}}\right) &amp; = E[Y_i|X_{1i}=x_1 + 1, X_{2i}=x_2+1] - E[Y_i|X_{1i}=x_1, X_{2i}=x_2+1]\\
&amp; \qquad - \Big(E[Y_i|X_{1i}=x_1+1, X_{2i}=x_2] - E[Y_i|X_{1i}=x_1, X_{2i}=x_2]\Big)\\
&amp; = \beta_{0} + \beta_{1} (x_{1}+1) + \beta_{2} (x_{2}+1) + \beta_{3} (x_{1}+1) (x_{2}+1) - [\beta_{0} + \beta_{1} x_{1} + \beta_{2} (x_{2}+1) + \beta_{3} x_{1} (x_{2}+1)]\\
&amp; \qquad - \Big(\beta_{0} + \beta_{1} (x_{1}+1) + \beta_{2} x_{2} + \beta_{3} (x_{1}+1) x_{2} - [\beta_{0} + \beta_{1} x_{1} + \beta_{2} x_{2} + \beta_{3} x_{1} x_{2}]\Big)\\
&amp; = \beta_{3}\\
\end{aligned}\]</span>
<ul>
<li>this can be interpreted as <span class="math inline">\(\beta_3\)</span> = the <strong>expected change in <span class="math inline">\(Y\)</span></strong> per <strong>unit change in <span class="math inline">\(X_1\)</span></strong> per <strong>unit change</strong> of <span class="math inline">\(X_2\)</span></li>
<li>in other words, <span class="math inline">\(\beta_3\)</span> = the change in slope of <span class="math inline">\(X_1\)</span> per unit change of <span class="math inline">\(X_2\)</span></li>
</ul></li>
<li>coming back to the hunger data, model for % hungry (<span class="math inline">\(H\)</span>) vs year (<span class="math inline">\(Y\)</span>), income (<span class="math inline">\(I\)</span>), and interaction between year and income (<span class="math inline">\(Y \times I\)</span>) is <span class="math display">\[H_{i} = \beta_{0} + \beta_{1}I_i + \beta_{2} Y_{i} + \beta_{3}I_i Y_{i} + \epsilon_{i}^+\]</span>
<ul>
<li><span class="math inline">\(\beta_{0}\)</span> = % hungry children (whose parents have no income) at year 0</li>
<li><span class="math inline">\(\beta_{1}\)</span> = change in % hungry children for <strong>each dollar in income</strong> in year 0</li>
<li><span class="math inline">\(\beta_{2}\)</span> = change in % hungry children (whose parents have no income) <strong>per year</strong></li>
<li><span class="math inline">\(\beta_{3}\)</span> = change in % hungry children <strong>per year</strong> and for <strong>each dollar in income</strong>
<ul>
<li>if income is <span class="math inline">\(\$10,000\)</span>, then the change in % hungry children <strong>per year</strong> will be <span class="math inline">\(\beta_1 - 10000 \times \beta_3\)</span></li>
</ul></li>
<li><span class="math inline">\(\epsilon_{i}^+\)</span> = standard error (or everything we didn’t measure)</li>
</ul></li>
<li><em><strong>Note</strong>: much care needs to be taken when interpreting these coefficients </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># generate some income data</span><br><span class="line">hunger$Income &lt;- 1:nrow(hunger)*10 + 500*runif(nrow(hunger), 0, 10) +</span><br><span class="line">    runif(nrow(hunger), 0, 500)^1.5</span><br><span class="line"># run linear model with Numeric vs Year and Income and interaction term</span><br><span class="line">lm(Numeric ~ Year*Income, data = hunger)$coef</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="multivariable-simulation">Multivariable Simulation</h2>
<ul>
<li>we will generate a series of simulated data so that we know the true relationships, and then run linear regressions to interpret and compare the results to truth</li>
<li><strong>treatment effect</strong> = effect of adding the treatment variable <span class="math inline">\(t\)</span> to the regression model (i.e. how adding <span class="math inline">\(t\)</span> changes the regression lines)
<ul>
<li>effectively measures how much the regression lines for the two groups separate with regression <code>lm(y ~ x + t)</code></li>
</ul></li>
<li><strong>adjustment effect</strong> = adjusting the regression for effects of <span class="math inline">\(x\)</span> such that we just look at how <span class="math inline">\(t\)</span> is <strong><em>marginally related</em></strong> to <span class="math inline">\(Y\)</span>
<ul>
<li>ignore all variation of <span class="math inline">\(x\)</span> and simply look at the group means of <span class="math inline">\(t = 1\)</span> vs <span class="math inline">\(t = 0\)</span></li>
</ul></li>
</ul>
<h3 id="simulation-1---treatment-adjustment-effect">Simulation 1 - Treatment = Adjustment Effect</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \beta_2 t_i + \epsilon_i\]</span> where <span class="math inline">\(t = \{0, 1\} \rightarrow\)</span> binary variable</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2), runif(n/2));</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 2; beta2 &lt;- 1; sigma &lt;- .2</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x * beta1 + t * beta2 + rnorm(n, sd = sigma)</span><br><span class="line"># set up axes</span><br><span class="line">plot(x, y, type = &quot;n&quot;, frame = FALSE)</span><br><span class="line"># plot linear fit of y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2, col = &quot;blue&quot;)</span><br><span class="line"># plot means of the two groups (t = 0 vs t = 1)</span><br><span class="line">abline(h = mean(y[1 : (n/2)]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line">abline(h = mean(y[(n/2 + 1) : n]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line"># plot linear fit of y vs x and t</span><br><span class="line">fit &lt;- lm(y ~ x + t)</span><br><span class="line"># plot the two lines corresponding to (t = 0 vs t = 1)</span><br><span class="line">abline(coef(fit)[1], coef(fit)[2], lwd = 3)</span><br><span class="line">abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)</span><br><span class="line"># add in the actual data points</span><br><span class="line">points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 1)</span><br><span class="line">points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 1)</span><br><span class="line"># print treatment and adjustment effects</span><br><span class="line">rbind(&quot;Treatment Effect&quot; = lm(y~t+x)$coef[2], &quot;Adjustment Effect&quot; = lm(y~t)$coef[2])</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, the elements are as follows:
<ul>
<li><span class="math inline">\(red\)</span> lines = means for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t)</code></li>
<li><span class="math inline">\(black\)</span> lines = regression lines for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t + x)</code></li>
<li><span class="math inline">\(blue\)</span> line = overall regression of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> line representing <code>lm(y ~ x)</code></li>
</ul></li>
<li>from the graph, we can see that
<ul>
<li><span class="math inline">\(x\)</span> variable is <strong><em>unrelated</em></strong> to group status <span class="math inline">\(t\)</span>
<ul>
<li>distribution of each group (salmon vs light blue) of <span class="math inline">\(Y\)</span> vs <span class="math inline">\(X\)</span> is effectively the same</li>
</ul></li>
<li><span class="math inline">\(x\)</span> variable is <strong><em>related</em></strong> to <span class="math inline">\(Y\)</span>, but the intercept <strong>depends</strong> on group status <span class="math inline">\(t\)</span></li>
<li>group variable <span class="math inline">\(t\)</span> is <strong><em>related</em></strong> to <span class="math inline">\(Y\)</span>
<ul>
<li>relationship between <span class="math inline">\(t\)</span> and <span class="math inline">\(Y\)</span> disregarding <span class="math inline">\(x \approx\)</span> the same as holding <span class="math inline">\(x\)</span> constant</li>
<li>difference in group means <span class="math inline">\(\approx\)</span> difference in regression lines</li>
<li><strong><em>treatment effect</em></strong> (difference in regression lines) <span class="math inline">\(\approx\)</span> <strong><em>adjustment effect</em></strong> (difference in group means)</li>
</ul></li>
</ul></li>
</ul>
<h3 id="simulation-2---no-treatment-effect">Simulation 2 - No Treatment Effect</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \beta_2 t_i + \epsilon_i\]</span> where <span class="math inline">\(t = \{0, 1\} \rightarrow\)</span> binary variable</li>
<li>in this case, <span class="math inline">\(\beta_2\)</span> is set to <span class="math inline">\(0\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2), 1.5 + runif(n/2));</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 2; beta2 &lt;- 0; sigma &lt;- .2</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x * beta1 + t * beta2 + rnorm(n, sd = sigma)</span><br><span class="line"># set up axes</span><br><span class="line">plot(x, y, type = &quot;n&quot;, frame = FALSE)</span><br><span class="line"># plot linear fit of y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2, col = &quot;blue&quot;)</span><br><span class="line"># plot means of the two groups (t = 0 vs t = 1)</span><br><span class="line">abline(h = mean(y[1 : (n/2)]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line">abline(h = mean(y[(n/2 + 1) : n]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line"># plot linear fit of y vs x and t</span><br><span class="line">fit &lt;- lm(y ~ x + t)</span><br><span class="line"># plot the two lines corresponding to (t = 0 vs t = 1)</span><br><span class="line">abline(coef(fit)[1], coef(fit)[2], lwd = 3)</span><br><span class="line">abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)</span><br><span class="line"># add in the actual data points</span><br><span class="line">points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 1)</span><br><span class="line">points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 1)</span><br><span class="line"># print treatment and adjustment effects</span><br><span class="line">rbind(&quot;Treatment Effect&quot; = lm(y~t+x)$coef[2], &quot;Adjustment Effect&quot; = lm(y~t)$coef[2])</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, the elements are as follows:
<ul>
<li><span class="math inline">\(red\)</span> lines = means for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t)</code></li>
<li><span class="math inline">\(black\)</span> lines = regression lines for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t + x)</code>
<ul>
<li>in this case, both lines correspond to <code>lm(y ~ x)</code> since coefficient of <span class="math inline">\(t\)</span> or <span class="math inline">\(\beta_2 = 0\)</span></li>
</ul></li>
<li><span class="math inline">\(blue\)</span> line = overall regression of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> line representing <code>lm(y ~ x)</code>
<ul>
<li>this is overwritten by the <span class="math inline">\(black\)</span> lines</li>
</ul></li>
</ul></li>
<li>from the graph, we can see that
<ul>
<li><span class="math inline">\(x\)</span> variable is <strong><em>highly related</em></strong> to group status <span class="math inline">\(t\)</span>
<ul>
<li>clear shift in <span class="math inline">\(x\)</span> with salmon vs light blue groups</li>
</ul></li>
<li><span class="math inline">\(x\)</span> variable is <strong><em>related</em></strong> to <span class="math inline">\(Y\)</span>, but the intercept <strong>does not depend</strong> on group status <span class="math inline">\(t\)</span>
<ul>
<li>intercepts for both lines are the same</li>
</ul></li>
<li><span class="math inline">\(x\)</span> variable shows <strong><em>similar relationships</em></strong> to <span class="math inline">\(Y\)</span> for both groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>, or salmon vs lightblue)
<ul>
<li>the <span class="math inline">\(x\)</span> values of the two groups of points both seem to be linearly correlated with <span class="math inline">\(Y\)</span></li>
</ul></li>
<li>group variable <span class="math inline">\(t\)</span> is <strong><em>marginally related</em></strong> to <span class="math inline">\(Y\)</span> when disregarding X
<ul>
<li><span class="math inline">\(x\)</span> values capture most of the variation</li>
<li><strong><em>adjustment effect</em></strong> (difference in group means) is very large</li>
</ul></li>
<li>group variable <span class="math inline">\(t\)</span> is <strong><em>unrelated or has very little</em></strong> effect on <span class="math inline">\(Y\)</span>
<ul>
<li><strong><em>treatment effect</em></strong> is very small or non-existent</li>
<li><em><strong>Note</strong>: the groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) are <strong>incomparable</strong> since there is no data to inform the relationship between <span class="math inline">\(t\)</span> and <span class="math inline">\(Y\)</span> </em></li>
<li>the groups (salmon vs lightblue) don’t have any overlaps so we have no idea how they behave</li>
<li>this conclusion is based on the constructed alone</li>
</ul></li>
</ul></li>
</ul>
<h3 id="simulation-3---treatment-reverses-adjustment-effect">Simulation 3 - Treatment Reverses Adjustment Effect</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \beta_2 t_i + \epsilon_i\]</span> where <span class="math inline">\(t = \{0, 1\} \rightarrow\)</span> binary variable</li>
<li>in this case, <span class="math inline">\(\beta_0\)</span> is set to <span class="math inline">\(0\)</span> <span class="math inline">\(\rightarrow\)</span> no intercept</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2), .9 + runif(n/2));</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 2; beta2 &lt;- -1; sigma &lt;- .2</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x * beta1 + t * beta2 + rnorm(n, sd = sigma)</span><br><span class="line"># set up axes</span><br><span class="line">plot(x, y, type = &quot;n&quot;, frame = FALSE)</span><br><span class="line"># plot linear fit of y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2, col = &quot;blue&quot;)</span><br><span class="line"># plot means of the two groups (t = 0 vs t = 1)</span><br><span class="line">abline(h = mean(y[1 : (n/2)]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line">abline(h = mean(y[(n/2 + 1) : n]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line"># plot linear fit of y vs x and t</span><br><span class="line">fit &lt;- lm(y ~ x + t)</span><br><span class="line"># plot the two lines corresponding to (t = 0 vs t = 1)</span><br><span class="line">abline(coef(fit)[1], coef(fit)[2], lwd = 3)</span><br><span class="line">abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)</span><br><span class="line"># add in the actual data points</span><br><span class="line">points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 1)</span><br><span class="line">points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 1)</span><br><span class="line"># print treatment and adjustment effects</span><br><span class="line">rbind(&quot;Treatment Effect&quot; = lm(y~t+x)$coef[2], &quot;Adjustment Effect&quot; = lm(y~t)$coef[2])</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, the elements are as follows:
<ul>
<li><span class="math inline">\(red\)</span> lines = means for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t)</code></li>
<li><span class="math inline">\(black\)</span> lines = regression lines for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t + x)</code></li>
<li><span class="math inline">\(blue\)</span> line = overall regression of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> line representing <code>lm(y ~ x)</code></li>
</ul></li>
<li>from the graph, we can see that
<ul>
<li>disregarding/adjusting for <span class="math inline">\(x\)</span>, the mean for salmon group is <strong><em>higher</em></strong> than the mean of the blue group (<strong><em>adjustment effect</em></strong> is positive)</li>
<li>when adding <span class="math inline">\(t\)</span> into the linear model, the treatment actually reverses the orders of the group <span class="math inline">\(\rightarrow\)</span> the mean for salmon group is <strong><em>lower</em></strong> than the mean of the blue group (<strong><em>treatment effect</em></strong> is negative)</li>
<li>group variable <span class="math inline">\(t\)</span> is <strong><em>related</em></strong> to <span class="math inline">\(x\)</span></li>
<li>some points overlap so it is possible to compare the subsets two groups holding <span class="math inline">\(x\)</span> fixed</li>
</ul></li>
</ul>
<h3 id="simulation-4---no-adjustment-effect">Simulation 4 - No Adjustment Effect</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \beta_2 t_i + \epsilon_i\]</span> where <span class="math inline">\(t = \{0, 1\} \rightarrow\)</span> binary variable</li>
<li>in this case, <span class="math inline">\(\beta_0\)</span> is set to <span class="math inline">\(0\)</span> <span class="math inline">\(\rightarrow\)</span> no intercept</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(.5 + runif(n/2), runif(n/2));</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 2; beta2 &lt;- 1; sigma &lt;- .2</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x * beta1 + t * beta2 + rnorm(n, sd = sigma)</span><br><span class="line"># set up axes</span><br><span class="line">plot(x, y, type = &quot;n&quot;, frame = FALSE)</span><br><span class="line"># plot linear fit of y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2, col = &quot;blue&quot;)</span><br><span class="line"># plot means of the two groups (t = 0 vs t = 1)</span><br><span class="line">abline(h = mean(y[1 : (n/2)]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line">abline(h = mean(y[(n/2 + 1) : n]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line"># plot linear fit of y vs x and t</span><br><span class="line">fit &lt;- lm(y ~ x + t)</span><br><span class="line"># plot the two lines corresponding to (t = 0 vs t = 1)</span><br><span class="line">abline(coef(fit)[1], coef(fit)[2], lwd = 3)</span><br><span class="line">abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)</span><br><span class="line"># add in the actual data points</span><br><span class="line">points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 1)</span><br><span class="line">points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 1)</span><br><span class="line"># print treatment and adjustment effects</span><br><span class="line">rbind(&quot;Treatment Effect&quot; = lm(y~t+x)$coef[2], &quot;Adjustment Effect&quot; = lm(y~t)$coef[2])</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, the elements are as follows:
<ul>
<li><span class="math inline">\(red\)</span> lines = means for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t)</code></li>
<li><span class="math inline">\(black\)</span> lines = regression lines for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t + x)</code></li>
<li><span class="math inline">\(blue\)</span> line = overall regression of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> line representing <code>lm(y ~ x)</code></li>
</ul></li>
<li>from the graph, we can see that
<ul>
<li>no <strong><em>clear relationship</em></strong> between group variable <span class="math inline">\(t\)</span> and <span class="math inline">\(Y\)</span>
<ul>
<li>two groups have similar distributions with respect to <span class="math inline">\(Y\)</span></li>
</ul></li>
<li><strong><em>treatment effect</em></strong> is substantial
<ul>
<li>separation of regression lines is large</li>
</ul></li>
<li><strong><em>adjustment effect</em></strong> is effectively 0 as there are no large differences between the means of the groups</li>
<li>group variable <span class="math inline">\(t\)</span> is <strong><em>not related</em></strong> to <span class="math inline">\(x\)</span>
<ul>
<li>distribution of each group (salmon vs light blue) of <span class="math inline">\(Y\)</span> vs <span class="math inline">\(X\)</span> is effectively the same</li>
</ul></li>
<li>lots of direct evidence for comparing two groups holding <span class="math inline">\(X\)</span> fixed</li>
</ul></li>
</ul>
<h3 id="simulation-5---binary-interaction">Simulation 5 - Binary Interaction</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \beta_2 t_i + \beta_3 x_i t_i + \epsilon_i\]</span> where <span class="math inline">\(t = \{0, 1\} \rightarrow\)</span> binary variable</li>
<li>in this case, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_2\)</span> are set to <span class="math inline">\(0\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; t &lt;- rep(c(0, 1), c(n/2, n/2)); x &lt;- c(runif(n/2, -1, 1), runif(n/2, -1, 1));</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 2; beta2 &lt;- 0; beta3 &lt;- -4; sigma &lt;- .2</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x * beta1 + t * beta2 + t * x * beta3 + rnorm(n, sd = sigma)</span><br><span class="line"># set up axes</span><br><span class="line">plot(x, y, type = &quot;n&quot;, frame = FALSE)</span><br><span class="line"># plot linear fit of y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2, col = &quot;blue&quot;)</span><br><span class="line"># plot means of the two groups (t = 0 vs t = 1)</span><br><span class="line">abline(h = mean(y[1 : (n/2)]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line">abline(h = mean(y[(n/2 + 1) : n]), lwd = 3, col = &quot;red&quot;)</span><br><span class="line"># plot linear fit of y vs x and t and interaction term</span><br><span class="line">fit &lt;- lm(y ~ x + t + I(x * t))</span><br><span class="line"># plot the two lines corresponding to (t = 0 vs t = 1)</span><br><span class="line">abline(coef(fit)[1], coef(fit)[2], lwd = 3)</span><br><span class="line">abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2] + coef(fit)[4], lwd = 3)</span><br><span class="line"># add in the actual data points</span><br><span class="line">points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = &quot;black&quot;, bg = &quot;lightblue&quot;, cex = 1)</span><br><span class="line">points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = &quot;black&quot;, bg = &quot;salmon&quot;, cex = 1)</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, the elements are as follows:
<ul>
<li><span class="math inline">\(red\)</span> lines = means for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t)</code></li>
<li><span class="math inline">\(black\)</span> lines = regression lines for two groups (<span class="math inline">\(t = 0\)</span> vs <span class="math inline">\(t = 1\)</span>) <span class="math inline">\(\rightarrow\)</span> two lines representing <code>lm(y ~ t + x + t*x)</code></li>
<li><span class="math inline">\(blue\)</span> line = overall regression of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> line representing <code>lm(y ~ x)</code>
<ul>
<li>this is completely meaningless in this case</li>
</ul></li>
</ul></li>
<li>from the graph, we can see that
<ul>
<li><strong><em>treatment effect</em></strong> does not apply since it varies with <span class="math inline">\(x\)</span>
<ul>
<li>impact of treatment/group variable <strong>reverses itself</strong> depending on <span class="math inline">\(x\)</span></li>
</ul></li>
<li><strong><em>adjustment effect</em></strong> is effectively zero as the means of the two groups are very similar</li>
<li>both intercept and slope of the two lines depend on the group variable <span class="math inline">\(t\)</span></li>
<li>group variable and <span class="math inline">\(x\)</span> are <strong><em>unrelated</em></strong></li>
<li>lots of information for comparing group effects holding <span class="math inline">\(x\)</span> fixed</li>
</ul></li>
</ul>
<h3 id="simulation-6---continuous-adjustment">Simulation 6 - Continuous Adjustment</h3>
<ul>
<li>the following code simulates the linear model, <span class="math display">\[Y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">p &lt;- 1; n &lt;- 100; x2 &lt;- runif(n); x1 &lt;- p * runif(n) - (1 - p) * x2</span><br><span class="line"># define parameters/coefficients</span><br><span class="line">beta0 &lt;- 0; beta1 &lt;- 1; beta2 &lt;- 4 ; sigma &lt;- .01</span><br><span class="line"># generate outcome using linear model</span><br><span class="line">y &lt;- beta0 + x1 * beta1 + beta2 * x2 + rnorm(n, sd = sigma)</span><br><span class="line"># plot y vs x1 and x2</span><br><span class="line">qplot(x1, y) + geom_point(aes(colour=x2)) + geom_smooth(method = lm)</span><br></pre></td></tr></table></figure>
<ul>
<li>in the above graph, we plotted <span class="math inline">\(y\)</span> vs <span class="math inline">\(x_{1}\)</span> with <span class="math inline">\(x_{2}\)</span> denoted as the gradient of color from blue to white</li>
<li>to investigate the bivariate relationship more clearly, we can use the following command from the <code>rgl</code> package to generate <strong><em>3D plots</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgl::plot3d(x1, x2, y)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_{1}\)</span></em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/5.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_{2}\)</span></em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/6.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong><em><span class="math inline">\(x_{1}\)</span> vs <span class="math inline">\(x_{2}\)</span></em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/7.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>residual plot with effect of <span class="math inline">\(x_2\)</span> removed from both <span class="math inline">\(y\)</span> and <span class="math inline">\(x_1\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># plot the residuals for y and x1 with x2 removed</span><br><span class="line">plot(resid(lm(x1 ~ x2)), resid(lm(y ~ x2)), frame = FALSE,</span><br><span class="line">	col = &quot;black&quot;, bg = &quot;lightblue&quot;, pch = 21, cex = 1)</span><br><span class="line"># add linear fit line</span><br><span class="line">abline(lm(I(resid(lm(y ~ x2))) ~ I(resid(lm(x1 ~ x2)))), lwd = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>from the generated plots above, we can see that
<ul>
<li><span class="math inline">\(x_{1}\)</span> is <strong><em>unrelated</em></strong> to <span class="math inline">\(x_{2}\)</span></li>
<li><span class="math inline">\(x_{2}\)</span> <strong><em>strongly correlated</em></strong> to <span class="math inline">\(y\)</span></li>
<li>relationship between <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(y\)</span> (loosely correlated <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(R^2\)</span> = <code>r round(summary(lm(y ~ x1))$r.squared,2)</code>) <strong><em>largely unchanged</em></strong> by when <span class="math inline">\(x_{2}\)</span> is considered
<ul>
<li><span class="math inline">\(x_{2}\)</span> captures the vast majority of variation in data</li>
<li>there exists almost no residual variability after removing <span class="math inline">\(x_{2}\)</span></li>
</ul></li>
</ul></li>
</ul>
<h3 id="summary-and-considerations">Summary and Considerations</h3>
<ul>
<li>modeling multivariate relationships is <strong><em>difficult</em></strong>
<ul>
<li>modeling for prediction is fairly straight forward</li>
<li>interpreting the regression lines is much harder, as adjusting for variables can have profound effect on variables of interest</li>
</ul></li>
<li>it is often recommended to explore with simulations to see how inclusion or exclusion of another variable affects the relationship of variable of interest and the outcome</li>
<li>variable selection simply affects associations between outcome and predictors, using the model to formulate causal relationship are even more difficult (entire field dedicated to this <span class="math inline">\(\rightarrow\)</span> <strong><em>causal inference</em></strong>)</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="residuals-and-diagnostics">Residuals and Diagnostics</h2>
<ul>
<li>recall that the <strong>generalized linear model</strong> is defined as <span class="math display">\[Y_i =  \sum_{k=1}^p X_{ik} \beta_j + \epsilon_{i}\]</span> where <span class="math inline">\(\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\)</span></li>
<li>the <strong>predicted outcome</strong>, <span class="math inline">\(\hat Y_i\)</span>, is defined as <span class="math display">\[\hat Y_i =  \sum_{k=1}^p X_{ik} \hat \beta_j\]</span></li>
<li>the <strong>residuals</strong>, <span class="math inline">\(e_i\)</span>, is defined as <span class="math display">\[e_i = Y_i -  \hat Y_i =  Y_i - \sum_{k=1}^p X_{ik} \hat \beta_j\]</span></li>
<li>the unbiased estimate for <strong>residual variation</strong> is defined as <span class="math display">\[\hat \sigma^2_{resid} = \frac{\sum_{i=1}^n e_i^2}{n-p}\]</span> where the denominator is <span class="math inline">\(n-p\)</span> so that <span class="math inline">\(E[\hat \sigma^2] = \sigma^2\)</span></li>
<li>to evaluate the fit and residuals of a linear model generated by R (i.e. <code>fit &lt;- lm(y~x)</code>, we can use the <code>plot(fit)</code> to produce a series of <strong><em>4 diagnostic plots</em></strong>
<ul>
<li><strong><em>Residuals vs Fitted</em></strong> = plots ordinary residuals vs fitted values <span class="math inline">\(\rightarrow\)</span> used to detect patterns for missing variables, heteroskedasticity, etc</li>
<li><strong><em>Scale-Location</em></strong> = plots standardized residuals vs fitted values <span class="math inline">\(\rightarrow\)</span> similar residual plot, used to detect patterns in residuals</li>
<li><strong><em>Normal Q-Q</em></strong> = plots theoretical quantiles for standard normal vs actual quantiles of standardized residuals <span class="math inline">\(\rightarrow\)</span> used to evaluate normality of the errors</li>
<li><strong><em>Residuals vs Leverage</em></strong> = plots cooks distances comparison of fit at that point vs potential for influence of that point <span class="math inline">\(\rightarrow\)</span> used to detect any points that have substantial influence on the regression model</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># load swiss data and</span><br><span class="line">data(swiss)</span><br><span class="line"># run linear regression on Fertility vs all other predictors</span><br><span class="line">fit &lt;- lm(Fertility ~ . , data = swiss)</span><br><span class="line"># generate diagnostic plots in 2 x 2 panels</span><br><span class="line">par(mfrow = c(2, 2)); plot(fit)</span><br></pre></td></tr></table></figure>
<h3 id="outliers-and-influential-points">Outliers and Influential Points</h3>
<ul>
<li><strong>outlier</strong> = an observation that is distant from the other observations of the data set
<ul>
<li>can be results of <a href="http://en.wikipedia.org/wiki/Spurious_relationship" target="_blank" rel="noopener">spurious</a> or real processes</li>
<li>can conform to the regression relationship (i.e being marginally outlying in X or Y, but not outlying given the regression relationship)</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># generate data</span><br><span class="line">n &lt;- 100; x &lt;- rnorm(n); y &lt;- x + rnorm(n, sd = .3)</span><br><span class="line"># set up axes</span><br><span class="line">plot(c(-3, 6), c(-3, 6), type = &quot;n&quot;, frame = FALSE, xlab = &quot;X&quot;, ylab = &quot;Y&quot;)</span><br><span class="line"># plot regression line for y vs x</span><br><span class="line">abline(lm(y ~ x), lwd = 2)</span><br><span class="line"># plot actual (x, y) pairs</span><br><span class="line">points(x, y, cex = 1, bg = &quot;lightblue&quot;, col = &quot;black&quot;, pch = 21)</span><br><span class="line"># plot 4 points of interest</span><br><span class="line">points(0, 0, cex = 1.5, bg = &quot;darkorange&quot;, col = &quot;black&quot;, pch = 21)</span><br><span class="line">points(0, 5, cex = 1.5, bg = &quot;darkorange&quot;, col = &quot;black&quot;, pch = 21)</span><br><span class="line">points(5, 5, cex = 1.5, bg = &quot;darkorange&quot;, col = &quot;black&quot;, pch = 21)</span><br><span class="line">points(5, 0, cex = 1.5, bg = &quot;darkorange&quot;, col = &quot;black&quot;, pch = 21)</span><br></pre></td></tr></table></figure>
<ul>
<li>different outliers can have <strong><em>varying</em></strong> degrees of <strong><em>influence</em></strong>
<ul>
<li>influence = actual effect on model fit</li>
<li>leverage = potential for influence</li>
</ul></li>
<li>in the plot above, we examine 4 different points of interest (in orange)
<ul>
<li><strong>lower left</strong>: low leverage, low influence, <strong><em>not</em></strong> an outlier in any sense</li>
<li><strong>upper left</strong>: low leverage, low influence, classified as outlier because it does not conform to the regression relationship
<ul>
<li><em><strong>Note</strong>: this point, though far away from the rest, <strong>does not</strong> impact the regression line since it lies in the middle of the data range because the regression line must always pass through the mean/center of observations </em></li>
</ul></li>
<li><strong>upper right</strong>: high leverage, low influence, classified as outlier because it lies far away from the rest of the data
<ul>
<li><em><strong>Note</strong>: this point has low influence on regression line because it conforms to the overall regression relationship </em></li>
</ul></li>
<li><strong>lower right</strong>: high leverage, high influence, classified as outlier because it lies far away from the rest of the data AND it does not conform to the regression relationship</li>
</ul></li>
</ul>
<h3 id="influence-measures">Influence Measures</h3>
<ul>
<li>there exists many pre-written functions to measure influence of observations already in the <code>stats</code> package in R
<ul>
<li><em><strong>Note</strong>: typing in <code>?influence.measures</code> in R will display the detailed documentation on all available functions to measure influence </em></li>
<li><em><strong>Note</strong>: the <code>model</code> argument referenced in the following functions is simply the linear fit model generated by the <code>lm</code> function (i.e. <code>model &lt;- lm(y~x</code>) </em></li>
<li><code>rstandard(model)</code> = standardized residuals <span class="math inline">\(\rightarrow\)</span> residuals divided by their standard deviations</li>
<li><code>rstudent(model)</code> = standardized residuals <span class="math inline">\(\rightarrow\)</span> residuals divided by their standard deviations, where the <span class="math inline">\(i^{th}\)</span> data point was deleted in the calculation of the standard deviation for the residual to follow a t distribution</li>
<li><code>hatvalues(model)</code> = measures of leverage</li>
<li><code>dffits(model)</code> = change in the predicted response when the <span class="math inline">\(i^{th}\)</span> point is deleted in fitting the model
<ul>
<li>effectively measures influence of a point on prediction</li>
</ul></li>
<li><code>dfbetas(model)</code> = change in individual coefficients when the <span class="math inline">\(i^{th}\)</span> point is deleted in fitting the model
<ul>
<li>effectively measures influence of the individual coefficients</li>
</ul></li>
<li><code>cooks(model).distance</code> = overall change in coefficients when the <span class="math inline">\(i^{th}\)</span> point is deleted</li>
<li><code>resid(model)</code> = returns ordinary residuals</li>
<li><code>resid(model)/(1-hatvalues(model))</code> = returns <em>PRESS</em> residuals (i.e. the leave one out cross validation residuals)
<ul>
<li>PRESS residuals measure the differences in the response and the predicted response at data point <span class="math inline">\(i\)</span>, where it was not included in the model fitting</li>
<li>effectively measures the prediction error based on model constructed with every other point but the one of interest</li>
</ul></li>
</ul></li>
</ul>
<h3 id="using-influence-measures">Using Influence Measures</h3>
<ul>
<li>the purpose of these functions are to probe the given data in different ways to <strong><em>diagnose</em></strong> different problems
<ul>
<li>patterns in <strong>residual plots</strong> (most important tool) <span class="math inline">\(\rightarrow\)</span> generally indicate some poor aspect of model fit
<ul>
<li>heteroskedasticity <span class="math inline">\(\rightarrow\)</span> non-constant variance</li>
<li>missing model terms</li>
<li>temporal patterns <span class="math inline">\(\rightarrow\)</span> residuals versus collection order/index exhibit pattern</li>
</ul></li>
<li><strong>residual Q-Q plots</strong> plots theoretical quantile vs actual quantiles of residuals
<ul>
<li>investigates whether the errors follow the standard normal distribution</li>
</ul></li>
<li><strong>leverage measures</strong> (hat values) measures the potential to influence the regression model
<ul>
<li>only depend on <span class="math inline">\(x\)</span> or predictor variables</li>
<li>can be useful for diagnosing data entry errors</li>
</ul></li>
<li><strong>influence measures</strong> (i.e. dfbetas) measures actual influence of points on the regression model
<ul>
<li>evaluates how deleting or including this point impact a particular aspect of the model</li>
</ul></li>
</ul></li>
<li>it is important to to understand these functions/tools and use carefully in the <strong><em>appropriate context</em></strong></li>
<li>not all measures have <strong><em>meaningful absolute scales</em></strong>, so it may be useful to apply these measures to different values in the same data set but <strong><em>almost never</em></strong> to different datasets</li>
</ul>
<h3 id="example---outlier-causing-linear-relationship">Example - Outlier Causing Linear Relationship</h3>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># generate random data and point (10, 10)</span><br><span class="line">x &lt;- c(10, rnorm(n)); y &lt;- c(10, c(rnorm(n)))</span><br><span class="line"># plot y vs x</span><br><span class="line">plot(x, y, frame = FALSE, cex = 1, pch = 21, bg = &quot;lightblue&quot;, col = &quot;black&quot;)</span><br><span class="line"># perform linear regression</span><br><span class="line">fit &lt;- lm(y ~ x)</span><br><span class="line"># add regression line to plot</span><br><span class="line">abline(fit)</span><br></pre></td></tr></table></figure>
<ul>
<li>data generated
<ul>
<li>100 points are randomly generated from the standard normal distribution</li>
<li>point (10, 10) added to the data set</li>
</ul></li>
<li>there is no regression relationship between X and Y as the points are simply random noise</li>
<li>the regression relationship was able to be constructed precisely because of the presence of the point (10, 10)
<ul>
<li><span class="math inline">\(R^2\)</span> = <code>r summary(fit)$r.squared</code></li>
<li>a single point has created a strong regression relationship where there shouldn’t be one
<ul>
<li>point (10, 10) has high leverage and high influence</li>
</ul></li>
<li>we can use diagnostics to detect this kind of behavior</li>
</ul></li>
<li><code>dfbetas(fit)</code> = difference in coefficients for including vs excluding each data point
<ul>
<li>the function will return a <code>n x m</code> dataframe, where n = number of values in the original dataset, and m = number of coefficients</li>
<li>for this example, the coefficients are <span class="math inline">\(\beta_0\)</span> (intercept), and <span class="math inline">\(\beta_1\)</span> (slope), and we are interested in the slope</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># calculate the dfbetas for the slope the first 10 points</span><br><span class="line">round(dfbetas(fit)[1 : 10, 2], 3)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>as we can see from above, the slope coefficient would <strong><em>change dramatically</em></strong> if the first point (10, 10) is left out</p></li>
<li><p><code>hatvalues(fit)</code> = measures the potential for influence for each point</p></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># calculate the hat values for the first 10 points</span><br><span class="line">round(hatvalues(fit)[1 : 10], 3)</span><br></pre></td></tr></table></figure>
<ul>
<li>again, as we can see from above, the <strong><em>potential for influence is very large</em></strong> for the first point (10, 10)</li>
</ul>
<h3 id="example---real-linear-relationship">Example - Real Linear Relationship</h3>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># generate data</span><br><span class="line">x &lt;- rnorm(n); y &lt;- x + rnorm(n, sd = .3)</span><br><span class="line"># add an outlier that fits the relationship</span><br><span class="line">x &lt;- c(5, x); y &lt;- c(5, y)</span><br><span class="line"># plot the (x, y) pairs</span><br><span class="line">plot(x, y, frame = FALSE, cex = 1, pch = 21, bg = &quot;lightblue&quot;, col = &quot;black&quot;)</span><br><span class="line"># perform the linear regression</span><br><span class="line">fit2 &lt;- lm(y ~ x)</span><br><span class="line"># add the regression line to the plot</span><br><span class="line">abline(fit2)</span><br></pre></td></tr></table></figure>
<ul>
<li>data generated
<ul>
<li>100 directly correlated points are generated</li>
<li>point (5, 5) added to the data set</li>
</ul></li>
<li>there is a linear relationship between X and Y
<ul>
<li><span class="math inline">\(R^2\)</span> = <code>r summary(fit2)$r.squared</code></li>
<li>point (5, 5) has high leverage and low influence</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># calculate the dfbetas for the slope the first 10 points</span><br><span class="line">round(dfbetas(fit2)[1 : 10, 2], 3)</span><br><span class="line"># calculate the hat values for the first 10 points</span><br><span class="line">round(hatvalues(fit2)[1 : 10], 3)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the point (5, 5) no longer has a large <code>dfbetas</code> value (indication of low influence) but still has a substantial <code>hatvalue</code> (indication of high leverage)
<ul>
<li>this is in line with out expectations</li>
</ul></li>
</ul>
<h3 id="example---stefanski-tas-2007">Example - Stefanski TAS 2007</h3>
<ul>
<li>taken from Leonard A. Stefanski’s paper <a href="http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/Residual_Surrealism_TAS_2007.pdf" target="_blank" rel="noopener">Residual (Sur)Realism</a></li>
<li>data set can be found <a href="http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/orly_owl_files/orly_owl_Lin_4p_5_flat.txt" target="_blank" rel="noopener">here</a></li>
<li>the data itself exhibit no sign of correlation between the variables</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># read data</span><br><span class="line">data &lt;- read.table(&apos;http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/orly_owl_files/orly_owl_Lin_4p_5_flat.txt&apos;,</span><br><span class="line">	header = FALSE)</span><br><span class="line"># construct pairwise plot</span><br><span class="line">pairs(data)</span><br><span class="line"># perform regression on V1 with all other predictors (omitting the intercept)</span><br><span class="line">fit &lt;- lm(V1 ~ . - 1, data = data)</span><br><span class="line"># print the coefficient for linear model</span><br><span class="line">summary(fit)$coef</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the p-values for the coefficients indicate that they are significant</li>
<li>if we take a look at the residual plot, an interesting pattern appears</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot the residuals vs fitted values</span><br><span class="line">plot(predict(fit), resid(fit), pch = &apos;.&apos;)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="model-selection">Model Selection</h2>
<blockquote>
<p>“A model is a lense through which to look at your data” – <strong>Scott Zeger</strong></p>
</blockquote>
<blockquote>
<p>“There’s no such thing as a correct model” – <strong>Brian Caffo</strong></p>
</blockquote>
<ul>
<li><strong>goal for modeling</strong> = find <strong><em>parsimonious, interpretable representations</em></strong> of data that enhance our understanding</li>
<li>whichever model connects data to a true, parsimonious statement would be <strong><em>best</em></strong> model</li>
<li>like nearly all aspects of statistics, good modeling decisions are context dependent</li>
<li>good model for prediction <span class="math inline">\(\neq\)</span> model to establish causal effects
<ul>
<li>prediction model may tolerate more variables and variability</li>
</ul></li>
</ul>
<h3 id="rumsfeldian-triplet">Rumsfeldian Triplet</h3>
<blockquote>
<p>“There are known knowns. These are things we know that we know. There are known unknowns. That is to say, there are things that we know we don’t know. But there are also unknown unknowns. There are things we don’t know we don’t know.” – <strong>Donald Rumsfeld</strong></p>
</blockquote>
<ul>
<li><strong>known knowns</strong> = regressors that we know and have, which will be evaluated to be included in the model</li>
<li><strong>known unknowns</strong> = regressors that we but don’t have but would like to include in the model
<ul>
<li>didn’t or couldn’t collect the data</li>
</ul></li>
<li><strong>unknown unknowns</strong> = regressors that we don’t know about that we should have included in the model</li>
</ul>
<h3 id="general-rules">General Rules</h3>
<ul>
<li><strong>omitting variables</strong> <span class="math inline">\(\rightarrow\)</span> generally results in <strong><em>increased bias</em></strong> in coefficients of interest
<ul>
<li>exceptions are when the omitted variables are uncorrelated with the regressors (variables of interest/included in model)
<ul>
<li><em><strong>Note</strong>: this is why randomize treatments/trials/experiments are the norm; it’s the best strategy to balance confounders, or maximizing the probability that the treatment variable is uncorrelated with variables not in the model </em></li>
<li>often times, due to experiment conditions or data availability, we cannot randomize</li>
<li>however, if there are too many unobserved confounding variables, even randomization won’t help</li>
</ul></li>
</ul></li>
<li><strong>including irrelevant/unnecessary variables</strong> <span class="math inline">\(\rightarrow\)</span> generally <strong><em>increases standard errors</em></strong> (estimated standard deviation) of the coefficients
<ul>
<li><em><strong>Note</strong>: including <strong>any</strong> new variables increases true standard errors of other regressors, so it is not wise to idly add variables into model </em></li>
</ul></li>
<li>whenever highly correlated variables are included in the same model <span class="math inline">\(\rightarrow\)</span> the standard error and therefore the<strong><em>variance</em></strong> of the model <strong><em>increases</em></strong> <span class="math inline">\(\rightarrow\)</span> this is known as <strong>variance inflation</strong>
<ul>
<li>actual increase in standard error of coefficients for adding a regressor = estimated by the ratio of the estimated standard errors minus 1, or in other words <span class="math display">\[\Delta_{\sigma~|~adding~x_2} = \frac{\hat \sigma_{y \sim x_1+x_2}}{\hat \sigma_{y \sim x_1}} - 1\]</span> for all coefficients for the regression model
<ul>
<li><strong><em>example</em></strong>: if standard error of the <span class="math inline">\(\beta_1\)</span> of <code>y~x1+x2</code> = 1.5 and standard error for the <span class="math inline">\(\beta_1\)</span> of <code>y~x1</code> = 0.5, then the actual increase in standard error of the <span class="math inline">\(\beta_1\)</span> = 1.5/0.5 - 1 = 200%</li>
</ul></li>
<li><em><strong>Note</strong>: when the regressors added are orthogonal (statistically independent) to the regressor of interest, then there is no variance inflation </em>
<ul>
<li><strong>variance inflation factor</strong> (VIF) = the increase in the variance for the <span class="math inline">\(i_{th}\)</span> regressor compared to the ideal setting where it is orthogonal to the other regressors</li>
<li><span class="math inline">\(\sqrt{VIF}\)</span> = increase in standard error</li>
</ul></li>
<li><em><strong>Note</strong>: variance inflation is only part of the picture in that sometimes we will <strong>include variables</strong> even though they dramatically inflate the variation because it is an <strong>empirical part of the relationship</strong> we are attempting to model </em></li>
</ul></li>
<li>as the number of <em>non-redundant</em> variables increases or approaches <span class="math inline">\(n\)</span>, the model <strong><em>approaches a perfect fit</em></strong> for the data
<ul>
<li><span class="math inline">\(R^2\)</span> monotonically increases as more regressors are included</li>
<li>Sum of Squared Errors (SSE) monotonically decreases as more regressors are included</li>
</ul></li>
</ul>
<h3 id="example---r2-v-n">Example - <span class="math inline">\(R^2\)</span> v <span class="math inline">\(n\)</span></h3>
<ul>
<li>for the simulation below, no actual regression relationship exist as the data generated are simply standard normal noise</li>
<li>it is clear, however, that as <span class="math inline">\(p\)</span>, the number of regressors included in the model, approaches <span class="math inline">\(n\)</span>, the <span class="math inline">\(R^2\)</span> value approaches 1.0, which signifies perfect fit</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># set number of measurements</span><br><span class="line">n &lt;- 100</span><br><span class="line"># set up the axes of the plot</span><br><span class="line">plot(c(1, n), 0 : 1, type = &quot;n&quot;, xlab = &quot;p&quot;, ylab = expression(R^2),</span><br><span class="line">	main = expression(paste(R^2, &quot; vs n&quot;)))</span><br><span class="line"># for each value of p from 1 to n</span><br><span class="line">r &lt;- sapply(1 : n, function(p)&#123;</span><br><span class="line">	# create outcome and p regressors</span><br><span class="line">	y &lt;- rnorm(n); x &lt;- matrix(rnorm(n * p), n, p)</span><br><span class="line">	# calculate the R^2</span><br><span class="line">	summary(lm(y ~ x))$r.squared</span><br><span class="line">&#125;)</span><br><span class="line"># plot the R^2 values and connect them with a line</span><br><span class="line">lines(1 : n, r, lwd = 2)</span><br></pre></td></tr></table></figure>
<h3 id="adjusted-r2">Adjusted <span class="math inline">\(R^2\)</span></h3>
<ul>
<li>recall that <span class="math inline">\(R^2\)</span> is defined as the percent of total variability that is explained by the regression model, or <span class="math display">\[R^2 = \frac{\mbox{regression variation}}{\mbox{total variation}} =  1- \frac{\sum_{i=1}^n (Y_i - \hat Y_i)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2} = 1 - \frac{Var(e_i)}{Var(Y_i)}\]</span></li>
<li>Estimating <span class="math inline">\(R^2\)</span> with the above definition is <strong><em>acceptable</em></strong> when there is a <em>single</em> variable, but it becomes less and <strong><em>less helpful</em></strong> as the <em>number of variables increases</em>
<ul>
<li>as we have shown previously, <span class="math inline">\(R^2\)</span> always increases as more variables are introduced and is thus <strong><em>biased</em></strong></li>
</ul></li>
<li><strong>adjusted <span class="math inline">\(R^2\)</span></strong> is a better estimate of variability explained by the model and is defined as <span class="math display">\[R^2_{adj} = 1 - \frac{Var(e_i)}{Var(Y_i)} \times \frac{n-1}{n-k-1}\]</span> where <span class="math inline">\(n\)</span> = number of observations, and <span class="math inline">\(k\)</span> = number of predictors in the model
<ul>
<li>since <span class="math inline">\(k\)</span> is always greater than zero, the adjusted <span class="math inline">\(R^2\)</span> is <strong><em>always smaller</em></strong> than the unadjusted <span class="math inline">\(R^2\)</span></li>
<li>adjusted <span class="math inline">\(R^2\)</span> also penalizes adding large numbers of regressors, which would have inflated the unadjusted <span class="math inline">\(R^2\)</span></li>
</ul></li>
</ul>
<h3 id="example---unrelated-regressors">Example - Unrelated Regressors</h3>
<ul>
<li>in the simulation below, outcome <span class="math inline">\(y\)</span> is only related to <span class="math inline">\(x_1\)</span>
<ul>
<li><span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> are random noise</li>
</ul></li>
<li>we will run 1000 simulations of 3 linear regression models, and calculate the <strong><em>standard error of the slope</em></strong>
<ul>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1\)</span></li>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1 + x_2\)</span></li>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1 + x_2 + x_3\)</span></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 100; nosim &lt;- 1000</span><br><span class="line"># generate 3 random noise, unrelated variables</span><br><span class="line">x1 &lt;- rnorm(n); x2 &lt;- rnorm(n); x3 &lt;- rnorm(n);</span><br><span class="line"># calculate beta1s of three different regression</span><br><span class="line">betas &lt;- sapply(1 : nosim, function(i)&#123;</span><br><span class="line">	# generate outcome as only related to x1</span><br><span class="line">	y &lt;- x1 + rnorm(n, sd = .3)</span><br><span class="line">	# store beta1 of linear regression on y vs x1</span><br><span class="line">	c(coef(lm(y ~ x1))[2],</span><br><span class="line">		# store beta1 of linear regression on y vs x1 and x2</span><br><span class="line">		coef(lm(y ~ x1 + x2))[2],</span><br><span class="line">		# store beta1 of linear regression on y vs x1 x2 and x3</span><br><span class="line">		coef(lm(y ~ x1 + x2 + x3))[2])</span><br><span class="line">&#125;)</span><br><span class="line"># calculate the standard error of the beta1s for the three regressions</span><br><span class="line">beta1.se &lt;- round(apply(betas, 1, sd), 5)</span><br><span class="line"># print results</span><br><span class="line">rbind(&quot;y ~ x1&quot; = c(&quot;beta1SE&quot; = beta1.se[1]),</span><br><span class="line">      &quot;y ~ x1 + x2&quot; = beta1.se[2],</span><br><span class="line">      &quot;y ~ x1 + x2 + x3&quot; = beta1.se[3])</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from the above result, if we include unrelated regressors <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span>, the <strong><em>standard error increases</em></strong></li>
</ul>
<h3 id="example---highly-correlated-regressors-variance-inflation">Example - Highly Correlated Regressors / Variance Inflation</h3>
<ul>
<li>in the simulation below, outcome <span class="math inline">\(y\)</span> is related to <span class="math inline">\(x_1\)</span>
<ul>
<li><span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> are highly correlated with <span class="math inline">\(x_1\)</span></li>
<li><span class="math inline">\(x_3\)</span> is more correlated with <span class="math inline">\(x_1\)</span> than <span class="math inline">\(x_2\)</span></li>
</ul></li>
<li>we will run 1000 simulations of 3 linear regression models, and calculate the <strong><em>standard error of <span class="math inline">\(\beta_1\)</span></em></strong>, the coefficient of <span class="math inline">\(x_1\)</span>
<ul>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1\)</span></li>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1 + x_2\)</span></li>
<li><span class="math inline">\(y\)</span> vs <span class="math inline">\(x_1 + x_2 + x_3\)</span></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># generate number of measurements and trials</span><br><span class="line">n &lt;- 100; nosim &lt;- 1000</span><br><span class="line"># generate random variables that are correlated with each other</span><br><span class="line">x1 &lt;- rnorm(n); x2 &lt;- x1/sqrt(2) + rnorm(n) /sqrt(2)</span><br><span class="line">x3 &lt;- x1 * 0.95 + rnorm(n) * sqrt(1 - 0.95^2);</span><br><span class="line"># calculate the beta1s for 1000 trials</span><br><span class="line">betas &lt;- sapply(1 : nosim, function(i)&#123;</span><br><span class="line">	# generate outcome as only related to x1</span><br><span class="line">	y &lt;- x1 + rnorm(n, sd = .3)</span><br><span class="line">	# store beta1 of linear regression on y vs x1</span><br><span class="line">	c(coef(lm(y ~ x1))[2],</span><br><span class="line">		# store beta1 of linear regression on y vs x1 and x2</span><br><span class="line">		coef(lm(y ~ x1 + x2))[2],</span><br><span class="line">		# store beta1 of linear regression on y vs x1 x2 and x3</span><br><span class="line">		coef(lm(y ~ x1 + x2 + x3))[2])</span><br><span class="line">&#125;)</span><br><span class="line"># calculate the standard error of the beta1 for the three regressions</span><br><span class="line">beta1.se &lt;- round(apply(betas, 1, sd), 5)</span><br><span class="line"># print results</span><br><span class="line">rbind(&quot;y ~ x1&quot; = c(&quot;beta1SE&quot; = beta1.se[1]),</span><br><span class="line">      &quot;y ~ x1 + x2&quot; = beta1.se[2],</span><br><span class="line">      &quot;y ~ x1 + x2 + x3&quot; = beta1.se[3])</span><br></pre></td></tr></table></figure>
<ul>
<li><p>as we can see from above, adding highly correlated regressors <strong><em>drastically increases</em></strong> the standard errors of the coefficients</p></li>
<li>to estimate the actual change in variance, we can use the ratio of estimated variances for the <span class="math inline">\(\beta_1\)</span> coefficient for the different models
<ul>
<li><code>summary(fit)$cov.unscaled</code> = returns p x p covariance matrix for p coefficients, with the diagonal values as the true variances of coefficients
<ul>
<li><code>summary(fit)$cov.unscaled[2,2]</code> = true variance for the <span class="math inline">\(\beta_1\)</span></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># generate outcome that is correlated with x1</span><br><span class="line">y &lt;- x1 + rnorm(n, sd = .3)</span><br><span class="line"># store the variance of beta1 for the 1st model</span><br><span class="line">a &lt;- summary(lm(y ~ x1))$cov.unscaled[2,2]</span><br><span class="line"># calculate the ratio of variances of beta1 for 2nd and 3rd models with respect to 1st model</span><br><span class="line">c(summary(lm(y ~ x1 + x2))$cov.unscaled[2,2],</span><br><span class="line">	summary(lm(y~ x1 + x2 + x3))$cov.unscaled[2,2]) / a - 1</span><br><span class="line"># alternatively, the change in variance can be estimated by calculating ratio of trials variance</span><br><span class="line">temp &lt;- apply(betas, 1, var); temp[2 : 3] / temp[1] - 1</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from the above results
<ul>
<li>adding <span class="math inline">\(x_2\)</span> increases the variance by approximately <code>r round(temp[2]/ temp[1] - 1)</code> fold</li>
<li>adding <span class="math inline">\(x_2\)</span> and <span class="math inline">\(x_3\)</span> increases the variance by approximately <code>r round(temp[3]/ temp[1] - 1)</code> folds</li>
</ul></li>
<li>the estimated values from the 1000 trials are <strong><em>different but close</em></strong> to the true increases, and they will approach the true values as the number of trials increases</li>
</ul>
<h3 id="example-variance-inflation-factors">Example: Variance Inflation Factors</h3>
<ul>
<li>we will use the <code>swiss</code> data set for this example, and compare the following models
<ul>
<li>Fertility vs Agriculture</li>
<li>Fertility vs Agriculture + Examination</li>
<li>Fertility vs Agriculture + Examination + Education</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>warning </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># load swiss data</span><br><span class="line">data(swiss)</span><br><span class="line"># run linear regression for Fertility vs Agriculture</span><br><span class="line">fit &lt;- lm(Fertility ~ Agriculture, data = swiss)</span><br><span class="line"># variance for coefficient of Agriculture</span><br><span class="line">a &lt;- summary(fit)$cov.unscaled[2,2]</span><br><span class="line"># run linear regression for Fertility vs Agriculture + Examination</span><br><span class="line">fit2 &lt;- update(fit, Fertility ~ Agriculture + Examination)</span><br><span class="line"># run linear regression for Fertility vs Agriculture + Examination + Education</span><br><span class="line">fit3 &lt;- update(fit, Fertility ~ Agriculture + Examination + Education)</span><br><span class="line"># calculate ratios of variances for Agriculture coef for fit2 and fit3 w.r.t fit1</span><br><span class="line">c(summary(fit2)$cov.unscaled[2,2], summary(fit3)$cov.unscaled[2,2]) / a - 1</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above
<ul>
<li>adding Examination variable to the model increases the variance by <code>r round(summary(fit2)$cov.unscaled[2,2]/a-1, 2)*100</code>%</li>
<li>adding Examination and Education variables to the model increases the variance by <code>r round(summary(fit3)$cov.unscaled[2,2]/a-1, 2)*100</code>%</li>
</ul></li>
<li>we can also calculate the <strong>variance inflation factors</strong> for all the predictors and see how variance will change by adding each predictor (assuming all predictor are orthogonal/independent of each other)
<ul>
<li>[<code>car</code> library] <code>vit(fit)</code> = returns the variance inflation factors for the predictors of the given linear model</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># load car library</span><br><span class="line">library(car)</span><br><span class="line"># run linear regression on Fertility vs all other predictors</span><br><span class="line">fit &lt;- lm(Fertility ~ . , data = swiss)</span><br><span class="line"># calculate the variance inflation factors</span><br><span class="line">vif(fit)</span><br><span class="line"># calculate the standard error inflation factors</span><br><span class="line">sqrt(vif(fit))</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from the above results, Education and Examination both have relatively higher inflation factors, which makes sense as the two variables are likely to be correlated with each other</li>
</ul>
<h3 id="residual-variance-estimates">Residual Variance Estimates</h3>
<ul>
<li>assuming that the model is linear with additive iid errors (with finite variance), we can mathematically describe the impact of omitting necessary variables or including unnecessary ones
<ul>
<li><strong>underfitting</strong> the model <span class="math inline">\(\rightarrow\)</span> variance estimate is <strong><em>biased</em></strong> <span class="math inline">\(\rightarrow ~ E[\hat \sigma^2] \neq \sigma^2\)</span></li>
<li><strong>correctly fitting</strong> or <strong>overfitting</strong> the model <span class="math inline">\(\rightarrow\)</span> variance estimate is <strong><em>unbiased</em></strong> <span class="math inline">\(\rightarrow ~ E[\hat \sigma^2] = \sigma^2\)</span>
<ul>
<li>however, if unnecessary variables are included, the variance estimate is <strong><em>larger</em></strong> than that of the correctly fitted variables <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(Var(\hat \sigma_{overfitted}) \geq Var(\hat \sigma_{correct})\)</span></li>
<li>in other words, adding unnecessary variables increases the variability of estimate for the true model</li>
</ul></li>
</ul></li>
</ul>
<h3 id="covariate-model-selection">Covariate Model Selection</h3>
<ul>
<li>automated covariate/predictor selection is difficult
<ul>
<li>the space of models explodes quickly with interactions and polynomial terms</li>
<li><em><strong>Note</strong>: in the <strong>Practical Machine Learning</strong> class, many modern methods for traversing large model spaces for the purposes of prediction will be covered </em></li>
</ul></li>
<li>principal components analysis (PCA) or factor analytic models on covariates are often useful for reducing complex covariate spaces
<ul>
<li>find linear combinations of variables that captures the most variation</li>
</ul></li>
<li>good experiment design can often eliminate the need for complex model searches during analyses
<ul>
<li>randomization, stratification can help simply the end models</li>
<li>unfortunately, control over the design is <strong><em>often limited</em></strong></li>
</ul></li>
<li>it is also viable to manually explore the covariate space based on understanding of the data
<ul>
<li>use covariate adjustment and multiple models to probe that effect of adding a particular predictor on the model</li>
<li><em><strong>Note</strong>: this isn’t a terribly systematic or efficient approach, but it tends to teach you a lot about the the data through the process </em></li>
</ul></li>
<li>if the models of interest are nested (i.e. one model is a special case of another with one or more coefficients set to zero) and without lots of parameters differentiating them, it’s fairly possible to use nested likelihood ratio tests (ANOVA) to help find the best model
<ul>
<li><strong>Analysis of Variance</strong> (ANOVA) works well when adding one or two regressors at a time
<ul>
<li><code>anova(fit1, fit2, fit3)</code> = performs ANOVA or analysis of variance (or deviance) tables for a series of nested linear regressions models</li>
</ul></li>
<li><em><strong>Note</strong>: it is extremely important to get the order of the models correct to ensure the results are sensible </em></li>
<li>an example can be found <a href="#example-anova">here</a></li>
</ul></li>
<li>another alternative to search through different models is the <strong>step-wise search</strong> algorithm that repeatedly adds/removes regressors one at a time to find the best model with the least <a href="http://en.wikipedia.org/wiki/Akaike_information_criterion" target="_blank" rel="noopener">Akaike Information Criterion (AIC)</a>
<ul>
<li><code>step(lm, k=df)</code> = performs step wise regression on a given linear model to find and return best linear model
<ul>
<li><code>k=log(n)</code> = specifying the value of <code>k</code> as the log of the number of observation will force the step-wise regression model to use Bayesian Information Criterion (BIC) instead of the AIC</li>
<li><em><strong>Note</strong>: both BIC and AIC penalizes adding parameters to the regression model with an additional penalty term; the penalty is <strong>larger</strong> for BIC than AIC </em></li>
</ul></li>
<li><code>MASS::stepAIC(lm, k = df)</code> = more versatile, rigorous implementation of the step wise regression</li>
<li>an example can be found <a href="#example-step-wise-model-search">here</a></li>
</ul></li>
</ul>
<h3 id="example-anova">Example: ANOVA</h3>
<ul>
<li>we will use the <code>swiss</code> data set for this example, and compare the following nested models
<ul>
<li>Fertility vs Agriculture</li>
<li>Fertility vs Agriculture + Examination + Education</li>
<li>Fertility vs Agriculture + Examination + Education + Catholic + Infant.Mortality</li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># three different regressions that are nested</span><br><span class="line">fit1 &lt;- lm(Fertility ~ Agriculture, data = swiss)</span><br><span class="line">fit3 &lt;- update(fit, Fertility ~ Agriculture + Examination + Education)</span><br><span class="line">fit5 &lt;- update(fit, Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality)</span><br><span class="line"># perform ANOVA</span><br><span class="line">anova(fit1, fit3, fit5)</span><br></pre></td></tr></table></figure>
<ul>
<li>the ANOVA function returns a formatted table with the follow information
<ul>
<li><code>Res.Df</code> = residual degrees of freedom for the models</li>
<li><code>RSS</code> = residual sum of squares for the models, measure of fit</li>
<li><code>Df</code> = change in degrees of freedom from one model to the next</li>
<li><code>Sum of Sq</code> = difference/change in residual sum of squares from one model to the next</li>
<li><code>F</code> = F statistic, measures the ratio of two scaled sums of squares reflecting different sources of variability <span class="math display">\[F = \frac{\frac{RSS_1 - RSS_2}{p_2 - p_1}}{\frac{RSS_2}{n-p_2}}\]</span> where <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> = number of parameters in the two models for comparison, and <span class="math inline">\(n\)</span> = number of observations</li>
<li><code>Pr(&gt;F)</code> = p-value for the F statistic to indicate whether the change in model is significant or not</li>
</ul></li>
<li>from the above result, we can see that both going from first to second, and second to third models result in significant reductions in RSS and <strong><em>better model fits</em></strong></li>
</ul>
<h3 id="example-step-wise-model-search">Example: Step-wise Model Search</h3>
<ul>
<li>we will use the <code>mtcars</code> data set for this example, and perform step-wise regression/model selection algorithm on the following initial model
<ul>
<li>Miles Per Gallon vs Number of Cylinder + Displacement + Gross Horse Power + Rear Axle Ratio + Weight</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># load the mtcars data starting regression model</span><br><span class="line">data(mtcars); fit &lt;- lm(mpg ~ cyl + disp + hp + drat + wt, data = mtcars)</span><br><span class="line"># step-wise search using BIC</span><br><span class="line">step(fit, k = log(nrow(mtcars)))</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the best model that captures most of the variability in the data is simply <code>mpg ~ cyl + wt</code> <span class="math inline">\(\pagebreak\)</span></li>
</ul>
<h2 id="general-linear-models-overview">General Linear Models Overview</h2>
<ul>
<li>limitations of linear models:
<ul>
<li>response can be discrete (i.e. 0, 1, etc.) or strictly positive <span class="math inline">\(\rightarrow\)</span> linear response models don’t make much sense</li>
<li>if outcome must be positive, Gaussian errors (<span class="math inline">\(\pm\)</span> errors) don’t make sense as negative outcomes are possible</li>
<li>transformations on predictors (log + 1) are often hard to interpret
<ul>
<li>modeling the data on the scale that it was collected is most ideal</li>
<li>even for interpretable transformations, <em>natural logarithms</em> specifically, aren’t applicable for negative/zero values</li>
</ul></li>
</ul></li>
<li><strong>general linear models</strong> = introduced in 1972 RSSB paper by Nelder and Wedderburn and has <strong><em>3</em></strong> parts
<ol type="1">
<li>exponential family model for response/outcome (i.e. Gaussian, Bernoulli distribution)</li>
<li>systematic component for linear predictor <span class="math inline">\(\rightarrow\)</span> incorporates the information about the independent variables into the model
<ul>
<li>denoted by <span class="math inline">\(\eta = X \beta\)</span> where <span class="math inline">\(X\)</span> is a matrix of independent variables/predictors and <span class="math inline">\(\beta\)</span> is the coefficients</li>
</ul></li>
<li>link function that connects means of the outcome/distribution to linear predictor
<ul>
<li>the relationship is defined as <span class="math inline">\(\eta = g(\mu)\)</span>, or the linear predictor <span class="math inline">\(\eta\)</span> is a function of the mean of the distribution <span class="math inline">\(\mu\)</span></li>
</ul></li>
</ol></li>
</ul>
<h3 id="simple-linear-model">Simple Linear Model</h3>
<ul>
<li><em>exponential family distribution</em>: Gaussian distribution, assumed <span class="math inline">\(Y_i \sim N(\mu_i, \sigma^2)\)</span></li>
<li><em>linear predictor</em>: <span class="math inline">\(\eta_i = \sum_{k=1}^p X_{ik} \beta_k\)</span></li>
<li><em>link function</em> : <span class="math inline">\(g(\mu) = \eta = \mu\)</span>
<ul>
<li>for linear models, <span class="math inline">\(g(\mu) = \mu\)</span>, so <span class="math inline">\(\eta_i = \mu_i\)</span></li>
</ul></li>
<li><strong>result</strong>: the same likelihood model (see <a href="#derivation-for-maximum-likelihood-estimator">derivation</a>)as the additive Gaussian error linear model <span class="math display">\[Y_i = \sum_{k=1}^p X_{ik} \beta_k + \epsilon_{i}\]</span> where <span class="math inline">\(\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\)</span></li>
</ul>
<h3 id="logistic-regression">Logistic Regression</h3>
<ul>
<li><em>exponential family distribution</em>: binomial/Bernoulli distribution, assumed <span class="math inline">\(Y_i \sim Bernoulli(\mu_i)\)</span> where the probability of success is <span class="math inline">\(\mu_i\)</span>
<ul>
<li>due to the properties of the binomial/Bernoulli distribution, <span class="math inline">\(E[Y_i] = \mu_i\)</span> where <span class="math inline">\(0 \leq \mu_i \leq 1\)</span></li>
</ul></li>
<li><em>linear predictor</em>: <span class="math inline">\(\eta_i = \sum_{k=1}^p X_{ik} \beta_k\)</span></li>
<li><em>link function</em> : <span class="math inline">\(g(\mu) = \eta = \log\left(\frac{\mu}{1 - \mu}\right)\)</span>
<ul>
<li><strong>odds</strong> for success for a binomial/Bernoulli distribution is defined as <span class="math display">\[\mbox{odds} = \frac{p}{1-p}\]</span></li>
<li><strong>logit</strong> is defined as <span class="math display">\[\log(\mbox{odds}) = \log \frac{\mu}{1-\mu}\]</span>
<ul>
<li><em><strong>Note</strong>: the <span class="math inline">\(\log\)</span> here is the <strong>natural</strong> log </em></li>
</ul></li>
<li><strong>inverse logit</strong> is defined as <span class="math display">\[\mu_i = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\]</span></li>
<li>complement of inverse logit is <span class="math display">\[1 - \mu_i = \frac{1}{1 + \exp(\eta_i)}\]</span></li>
</ul></li>
<li><strong>result</strong>: the likelihood model <span class="math display">\[\begin{aligned}
L(\beta) &amp; = \prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1-y_i}  \\
(plug~in~\mu_i~and~1 - \mu_i~from~above)&amp; = \prod_{i=1}^n \left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right)^{y_i} \left(\frac{1}{1 + \exp(\eta_i)}\right)^{1-y_i}\\
(multiply~2^{nd}~term~by~\frac{exp(\eta_i)}{exp(\eta_i)})&amp; = \prod_{i=1}^n \left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right)^{y_i} \left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right)^{1-y_i} \left(\frac{1}{\exp(\eta_i)}\right)^{1-y_i}\\
(simplify) &amp; = \prod_{i=1}^n \left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right) \left(\frac{1}{\exp(\eta_i)}\right)^{1-y_i}\\
(simplify) &amp; = \prod_{i=1}^n \left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right) \exp(\eta_i)^{y_i-1}\\
(simplify) &amp; = \prod_{i=1}^n \frac{\exp(\eta_i)^{y_i}}{1 + \exp(\eta_i)} \\
(change~form~of~numerator) &amp; = \exp\left(\sum_{i=1}^n y_i \eta_i \right)\prod_{i=1}^n \frac{1}{1 + \exp(\eta_i)}\\
(substitute~\eta_i) \Rightarrow L(\beta) &amp; = \exp\left(\sum_{i=1}^n y_i \big(\sum_{k=1}^p X_{ik} \beta_k \big) \right)\prod_{i=1}^n \frac{1}{1 + \exp\big(\sum_{k=1}^p X_{ik} \beta_k \big)}\\
\end{aligned}\]</span>
<ul>
<li>maximizing the likelihood <span class="math inline">\(L(\beta)\)</span> (solving for <span class="math inline">\(\frac{\partial L}{\partial \beta} = 0)\)</span> would return a set of optimized coefficients <span class="math inline">\(\beta\)</span> that will fit the data</li>
</ul></li>
</ul>
<h3 id="poisson-regression">Poisson Regression</h3>
<ul>
<li><em>exponential family distribution</em>: Poisson distribution, assumed <span class="math inline">\(Y_i \sim Poisson(\mu_i)\)</span> where <span class="math inline">\(E[Y_i] = \mu_i\)</span></li>
<li><em>linear predictor</em>: <span class="math inline">\(\eta_i = \sum_{k=1}^p X_{ik} \beta_k\)</span></li>
<li><em>link function</em> : <span class="math inline">\(g(\mu) = \eta = \log(\mu)\)</span>
<ul>
<li><em><strong>Note</strong>: the <span class="math inline">\(\log\)</span> here is the <strong>natural</strong> log </em></li>
<li>since <span class="math inline">\(e^x\)</span> is the inverse of <span class="math inline">\(\log(x)\)</span>, then <span class="math inline">\(\eta_i = \log(\mu_i)\)</span> can be transformed into <span class="math inline">\(\mu_i = e^{\eta_i}\)</span></li>
</ul></li>
<li><strong>result</strong>: the likelihood model <span class="math display">\[\begin{aligned}
L(\beta) &amp; = \prod_{i=1}^n (y_i !)^{-1} \mu_i^{y_i}e^{-\mu_i}\\
(substitute~\mu_i = e^{\eta_i}) &amp; = \prod_{i=1}^n \frac{(e^{\eta_i})^{y_i}}{y_i! e^{e^{\eta_i}}}\\
(transform) &amp;= \prod_{i=1}^n \frac{\exp(\eta_i y_i)}{y_i! \exp(e^{\eta_i})}\\
(taking~\log~of~both~sides)~\mathcal{L}(\beta) &amp; = \sum_{i=1}^n \eta_i y_i - \sum_{i=1}^n e^{\eta_i} - \sum_{i=1}^n log(y_i!) \\
(since~y_i~is~given,~we~can~ignore~\log y_i!)~\mathcal{L}(\beta) &amp; \propto \sum_{i=1}^n \eta_i y_i - \sum_{i=1}^n e^{\eta_i}\\
(substitute~\eta_i= \sum_{k=1}^p X_{ik} \beta_k) \Rightarrow  \mathcal{L}(\beta) &amp; \propto \sum_{i=1}^n y_i \left(\sum_{k=1}^p X_{ik}\beta_k\right) - \sum_{i=1}^n \exp \left(\sum_{k=1}^p X_{ik} \beta_k \right) \\
\end{aligned}\]</span>
<ul>
<li>maximizing the log likelihood <span class="math inline">\(\mathcal{L}(beta)\)</span> (solving for <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \beta} = 0)\)</span> would return a set of optimized coefficients <span class="math inline">\(\beta\)</span> that will fit the data</li>
</ul></li>
</ul>
<h3 id="variances-and-quasi-likelihoods">Variances and Quasi-Likelihoods</h3>
<ul>
<li>in each of the linear/Bernoulli/Poisson cases, the <strong><em>only</em></strong> term in the likelihood functions that depend on the <strong><em>data</em></strong> is <span class="math display">\[\sum_{i=1}^n y_i \eta_i =\sum_{i=1}^n y_i\sum_{k=1}^p X_{ik} \beta_k = \sum_{k=1}^p \beta_k\sum_{i=1}^n X_{ik} y_i\]</span></li>
<li>this means that we don’t need need all of the data collected to maximize the likelihoods/find the coefficients <span class="math inline">\(\beta\)</span>, but <strong><em>only</em></strong> need <span class="math inline">\(\sum_{i=1}^n X_{ik} y_i\)</span>
<ul>
<li><em><strong>Note</strong>: this simplification is a consequence of choosing “<strong>canonical</strong>” link functions, <span class="math inline">\(g(\mu)\)</span>, to be in specific forms </em></li>
</ul></li>
<li>[Derivation needed] all models achieve their <strong><em>maximum</em></strong> at the root of the <strong>normal equations</strong> <span class="math display">\[\sum_{i=1}^n \frac{(Y_i - \mu_i)}{Var(Y_i)}W_i = 0\]</span> where <span class="math inline">\(W_i = \frac{\partial g^{-1}(\mu_i)}{\mu_i}\)</span> or the derivative of the inverse of the link function
<ul>
<li><em><strong>Note</strong>: this is similar to deriving the least square equation where the middle term must be set to 0 to find the solution (see <a href="#derivation-for-">Derivation for <span class="math inline">\(\beta\)</span></a>) </em></li>
<li><em><strong>Note</strong>: <span class="math inline">\(\mu_i = g^{-1}(\eta_i) =g^{-1}\left(\sum_{k=1}^p X_{ik} \beta_k\right)\)</span>, the normal functions are really functions of <span class="math inline">\(\beta\)</span> </em></li>
</ul></li>
<li>the variance, <span class="math inline">\(Var(Y_i)\)</span>, is defined as
<ul>
<li><strong><em>linear model</em></strong>: <span class="math inline">\(Var(Y_i) = \sigma^2\)</span>, where <span class="math inline">\(\sigma\)</span> is constant</li>
<li><strong><em>binomial model</em></strong>: <span class="math inline">\(Var(Y_i) = \mu_i (1 - \mu_i)\)</span></li>
<li><strong><em>Poisson model</em></strong>: <span class="math inline">\(Var(Y_i) = \mu_i\)</span></li>
</ul></li>
<li>for binomial and Poisson models, there are <strong><em>strict relationships</em></strong> between the mean and variance that can be easily tested from the data:
<ul>
<li>binomial: mean = <span class="math inline">\(\mu_i\)</span>, variance = <span class="math inline">\(\mu_i (1 - \mu_i)\)</span></li>
<li>Poisson: mean = <span class="math inline">\(\mu_i\)</span>, variance = <span class="math inline">\(\mu_i\)</span></li>
</ul></li>
<li>it is often relevant to have a <strong><em>more flexible</em></strong> variance model (i.e. data doesn’t follow binomial/Poisson distributions exactly but are approximated), even if it doesn’t correspond to an actual likelihood, so we can add an extra parameter, <span class="math inline">\(\phi\)</span>, to the normal equations to form <strong>quasi-likelihood normal equations</strong> <span class="math display">\[
binomial:~\sum_{i=1}^n \frac{(Y_i - \mu_i)}{\phi \mu_i (1 - \mu_i ) } W_i=0 \\
Poisson:~\sum_{i=1}^n \frac{(Y_i - \mu_i)}{\phi \mu_i} W_i=0
\]</span> where <span class="math inline">\(W_i = \frac{\partial g^{-1}(\mu_i)}{\mu_i}\)</span> or the derivative of the inverse of the link function
<ul>
<li>for R function <code>glm()</code>, its possible to specify for the model to solve using quasi-likelihood normal equations instead of normal equations through the parameter <code>family = quasi-binomial</code> and <code>family = quasi-poisson</code> respectively</li>
<li><em><strong>Note</strong>: the quasi-likelihoods models generally same properties as normal GLM </em></li>
</ul></li>
</ul>
<h3 id="solving-for-normal-and-quasi-likelihood-normal-equations">Solving for Normal and Quasi-Likelihood Normal Equations</h3>
<ul>
<li>normal equations have to be solved <strong><em>iteratively</em></strong>
<ul>
<li>the results are <span class="math inline">\(\hat \beta_k\)</span>, estimated coefficients for the predictors</li>
<li>for quasi-likelihood normal equations, <span class="math inline">\(\hat \phi\)</span> will be part of the results as well</li>
<li>in R, <a href="http://en.wikipedia.org/wiki/Newton%27s_method" target="_blank" rel="noopener">Newton/Raphson’s algorithm</a> is used to solve the equations</li>
<li><strong><em>asymptotics</em></strong> are used for inference of results to broader population (see <strong><em>Statistical Inference</em></strong> course)</li>
<li><em><strong>Note</strong>: many of the ideas, interpretation, and conclusions derived from simple linear models are applicable to GLMs </em></li>
</ul></li>
<li><strong>predicted linear predictor responses</strong> are defined as <span class="math display">\[\hat \eta = \sum_{k=1}^p X_k \hat \beta_k\]</span></li>
<li><strong>predicted mean responses</strong> can be solved from <span class="math display">\[\hat \mu = g^{*1}(\hat \eta)\]</span></li>
<li><strong>coefficients</strong> are interpreted as the <strong><em>expected change in the link function</em></strong> of the expected response <strong><em>per unit change</em></strong> in <span class="math inline">\(X_k\)</span> holding other regressors constant, or <span class="math display">\[\beta_k = g(E[Y | X_k = x_k + 1, X_{\sim k} = x_{\sim k}]) - g(E[Y | X_k = x_k, X_{\sim k}=x_{\sim k}])\]</span></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="general-linear-models---binary-models">General Linear Models - Binary Models</h2>
<ul>
<li><strong>Bernoulli/binary</strong> models are frequently used to model outcomes that have two values
<ul>
<li>alive vs dead</li>
<li>win vs loss</li>
<li>success vs failure</li>
<li>disease vs healthy</li>
</ul></li>
<li><strong>binomial outcomes</strong> = collection of exchangeable binary outcomes (i.e. flipping coins repeatedly) for the same covariate data
<ul>
<li>in other words, we are interested in the count of predicted <span class="math inline">\(1\)</span>s vs <span class="math inline">\(0\)</span>s rather individual outcomes of <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span></li>
</ul></li>
</ul>
<h3 id="odds">Odds</h3>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Odds_ratio" target="_blank" rel="noopener"><strong>odds</strong></a> are useful in constructing logistic regression models and fairly easy to interpret
<ul>
<li>imagine flipping a coin with success probability <span class="math inline">\(p\)</span>
<ul>
<li>if heads, you win <span class="math inline">\(X\)</span></li>
<li>if tails, you lose <span class="math inline">\(Y\)</span></li>
</ul></li>
<li>how should <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be set so that the game is <strong><em>fair</em></strong>? <span class="math display">\[E[earnings]= X p - Y (1 - p) = 0 \Rightarrow \frac{Y}{X} = \frac{p}{1 - p}\]</span></li>
<li>odds can be interpreted as “How much should you be willing to pay for a <span class="math inline">\(p\)</span> probability of winning a dollar?”
<ul>
<li>if <span class="math inline">\(p &gt; 0.5\)</span>, you have to pay more if you lose than you get if you win</li>
<li>if <span class="math inline">\(p &lt; 0.5\)</span>, you have to pay less if you lose than you get if you win</li>
</ul></li>
</ul></li>
<li>odds are <strong>NOT</strong> probabilities</li>
<li>odds ratio of 1 = no difference in odds or 50% - 50%
<ul>
<li><span class="math inline">\(p = 0.5 \Rightarrow odds = \frac{0.5}{1-0.5} = 1\)</span></li>
<li>log odds ratio of 0 = no difference in odds
<ul>
<li><span class="math inline">\(p = 0.5 \Rightarrow odds = \log\left(\frac{0.5}{1-0.5}\right) = \log(1) = 0\)</span></li>
</ul></li>
</ul></li>
<li>odds ratio &lt; 0.5 or &gt; 2 commonly a “moderate effect”</li>
<li><strong>relative risk</strong> = ratios of probabilities instead of odds, and are often easier to interpret but harder to estimate <span class="math display">\[\frac{Pr(W_i | S_i = 10)}{Pr(W_i | S_i = 0)}\]</span>
<ul>
<li><em><strong>Note</strong>: relative risks often have <strong>boundary problems</strong> as the range of <span class="math inline">\(\log(p)\)</span> is <span class="math inline">\((-\infty,~0]\)</span> where as the range of logit <span class="math inline">\(\frac{p}{1-p}\)</span> is <span class="math inline">\((-\infty,\infty)\)</span> </em></li>
<li>for small probabilities Relative Risk <span class="math inline">\(\approx\)</span> Odds Ratio but <strong>they are not the same</strong>!</li>
</ul></li>
</ul>
<h3 id="example---baltimore-ravens-win-vs-loss">Example - Baltimore Ravens Win vs Loss</h3>
<ul>
<li>the data for this example can be found <a href="https://dl.dropboxusercontent.com/u/7710864/data/ravensData.rda" target="_blank" rel="noopener">here</a>
<ul>
<li>the data contains the records 20 games for Baltimore Ravens, a professional American Football team</li>
<li>there are 4 columns
<ul>
<li><code>ravenWinNum</code> = 1 for Raven win, 0 for Raven loss</li>
<li><code>ravenWin</code> = W for Raven win, L for Raven loss</li>
<li><code>ravenScore</code> = score of the Raven team during the match</li>
<li><code>opponentScore</code> = score of the Raven team during the match</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># load the data</span><br><span class="line">load(&quot;ravensData.rda&quot;)</span><br><span class="line">head(ravensData)</span><br></pre></td></tr></table></figure>
<h3 id="example---simple-linear-regression">Example - Simple Linear Regression</h3>
<ul>
<li><strong>simple linear regression</strong> can be used model win vs loss for the Ravens <span class="math display">\[ W_i = \beta_0 + \beta_1 S_i + \epsilon_i \]</span>
<ul>
<li><span class="math inline">\(W_i\)</span> = binary outcome, 1 if a Ravens win, 0 if not</li>
<li><span class="math inline">\(S_i\)</span> = number of points Ravens scored</li>
<li><span class="math inline">\(\beta_0\)</span> = probability of a Ravens win if they score 0 points</li>
<li><span class="math inline">\(\beta_1\)</span> = increase in probability of a Ravens win for each additional point</li>
<li><span class="math inline">\(\epsilon_i\)</span> = residual variation, error</li>
</ul></li>
<li>the expected value for the model is defined as <span class="math display">\[E[W_i | S_i, \beta_0, \beta_1] = \beta_0 + \beta_1 S_i\]</span></li>
<li>however, the model wouldn’t work well as the predicted results <strong><em>won’t</em></strong> be 0 vs 1
<ul>
<li>the error term, <span class="math inline">\(\epsilon_i\)</span>, is assumed to be continuous and normally distributed, meaning that the prediction will likely be a decimal</li>
<li>therefore, this is <strong><em>not</em></strong> a good assumption for the model</li>
</ul></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># perform linear regression</span><br><span class="line">summary(lm(ravenWinNum ~ ravenScore, data = ravensData))</span><br></pre></td></tr></table></figure>
<ul>
<li>as expected, the model produces a poor fit for the data (<span class="math inline">\(R^2_{adj} =\)</span> <code>r round(summary(lm(ravenWinNum ~ ravenScore, data = ravensData))$adj.r.squared, 4)</code>)</li>
<li>adding a threshold to the predicted outcome (i.e. if <span class="math inline">\(\hat W_i &lt; 0.5, \hat W_i = 0\)</span>) and using the model to predict the results would be <strong><em>viable</em></strong>
<ul>
<li>however, the coefficients for the model are <strong><em>not very interpretable</em></strong></li>
</ul></li>
</ul>
<h3 id="example---logistic-regression">Example - Logistic Regression</h3>
<ul>
<li><strong>probability</strong> of Ravens win is defined as <span class="math display">\[Pr(W_i | S_i, \beta_0, \beta_1)\]</span></li>
<li><strong>odds</strong> is defined as <span class="math display">\[\frac{Pr(W_i | S_i, \beta_0, \beta_1 )}{1-Pr(W_i | S_i, \beta_0, \beta_1)}\]</span> which ranges from 0 to <span class="math inline">\(\infty\)</span></li>
<li>log odds or <strong>logit</strong> is defined as <span class="math display">\[\log\left(\frac{Pr(W_i | S_i, \beta_0, \beta_1 )}{1-Pr(W_i | S_i, \beta_0, \beta_1)}\right)\]</span> which ranges from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span></li>
<li>we can use the link function and linear predictors to construct the <strong>logistic regression</strong> model <span class="math display">\[\begin{aligned}
g(\mu_i) &amp; = \log \left(\frac{\mu_i}{1 - \mu_i} \right) = \eta_i\\
(substitute~\mu_i = Pr(W_i | S_i, \beta_0, \beta_1))~g(\mu_i) &amp; = \log\left(\frac{Pr(W_i | S_i, \beta_0, \beta_1)}{1-Pr(W_i | S_i, \beta_0, \beta_1)}\right) = \eta_i \\
(substitute~\eta_i=\beta_0 + \beta_1 S_i) \Rightarrow ~g(\mu_i) &amp; = \log\left(\frac{Pr(W_i | S_i, \beta_0, \beta_1)}{1-Pr(W_i | S_i, \beta_0, \beta_1)}\right) = \beta_0 + \beta_1 S_i\\
\end{aligned}
 \]</span> which can also be written as <span class="math display">\[Pr(W_i | S_i, \beta_0, \beta_1 ) = \frac{\exp(\beta_0 + \beta_1 S_i)}{1 + \exp(\beta_0 + \beta_1 S_i)}\]</span></li>
<li>for the model <span class="math display">\[\log\left(\frac{Pr(W_i | S_i, \beta_0, \beta_1)}{1-Pr(W_i | S_i, \beta_0, \beta_1)}\right) = \beta_0 + \beta_1 S_i\]</span>
<ul>
<li><span class="math inline">\(\beta_0\)</span> = log odds of a Ravens win if they score zero points</li>
<li><span class="math inline">\(\beta_1\)</span> = log odds ratio of win probability for each point scored (compared to zero points) <span class="math display">\[\beta_1 = \log\left(odds(S_i = S_i+1)\right) - \log\left(odds(S_i = S_i)\right) = \log\left(\frac{odds(S_i = S_i+1)}{odds(S_i = S_i)} \right)\]</span></li>
<li><span class="math inline">\(\exp(\beta_1)\)</span> = odds ratio of win probability for each point scored (compared to zero points) <span class="math display">\[\exp(\beta_1) =  \frac{odds(S_i = S_i+1)}{odds(S_i = S_i)}\]</span></li>
</ul></li>
<li>we can leverage the <code>manupulate</code> function vary <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> to fit logistic regression curves for simulated data</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set x values for the points to be plotted</span></span><br><span class="line">x &lt;- seq(-<span class="number">10</span>, <span class="number">10</span>, length = <span class="number">1000</span>)</span><br><span class="line"><span class="comment"># "library(manipulate)" is needed to use the manipulate function</span></span><br><span class="line">manipulate(</span><br><span class="line">	<span class="comment"># plot the logistic regression curve</span></span><br><span class="line">    plot(x, exp(beta0 + beta1 * x) / (<span class="number">1</span> + exp(beta0 + beta1 * x)),</span><br><span class="line">         type = <span class="string">"l"</span>, lwd = <span class="number">3</span>, frame = <span class="literal">FALSE</span>),</span><br><span class="line">    <span class="comment"># slider for beta1</span></span><br><span class="line">    beta1 = slider(-<span class="number">2</span>, <span class="number">2</span>, step = <span class="number">.1</span>, initial = <span class="number">2</span>),</span><br><span class="line">    <span class="comment"># slider for beta0</span></span><br><span class="line">    beta0 = slider(-<span class="number">2</span>, <span class="number">2</span>, step = <span class="number">.1</span>, initial = <span class="number">0</span>)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/8.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>we can use the <code>glm(outcome ~ predictor, family = &quot;binomial&quot;)</code> to fit a logistic regression to the data</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># run logistic regression on data</span><br><span class="line">logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data = ravensData,family=&quot;binomial&quot;)</span><br><span class="line"># print summary</span><br><span class="line">summary(logRegRavens)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see above, the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are <code>r round(logRegRavens$coef,3)</code>, which are interpreted to be the log odds ratios</li>
<li>we can convert the log ratios as well as the log confidence intervals to ratios and confidence intervals (in the same units as the data)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># take e^coefs to find the log ratios</span><br><span class="line">exp(logRegRavens$coeff)</span><br><span class="line"># take e^log confidence interval to find the confidence intervals</span><br><span class="line">exp(confint(logRegRavens))</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(\exp(x) \approx 1 + x\)</span> for small values (close to 0) of x, this can be a quick way to estimate the coefficients </em></li>
<li>we can interpret the slope, <span class="math inline">\(\beta_1\)</span> as <code>r round((exp(logRegRavens$coeff)[2]-1)*100, 3)</code> % increase in probability of winning for every point scored</li>
<li>we can interpret the intercept, <span class="math inline">\(\beta_0\)</span> as <code>r round(exp(logRegRavens$coeff)[1], 3)</code> is the odds for Ravens winning if they scored 0 points
<ul>
<li><em><strong>Note</strong>: similar to the intercept of a simple linear regression model, the intercept should be interpreted carefully as it is an extrapolated value from the model and may not hold practical meaning </em></li>
</ul></li>
<li>to calculate specific probability of winning for a given number of points <span class="math display">\[Pr(W_i | S_i, \hat \beta_0, \hat \beta_1) = \frac{\exp(\hat \beta_0 + \hat \beta_1 S_i)}{1 + \exp(\hat \beta_0 + \hat \beta_1 S_i)}\]</span></li>
<li>the resulting logistic regression curve can be seen below</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot the logistic regression</span><br><span class="line">plot(ravensData$ravenScore,logRegRavens$fitted,pch=19,col=&quot;blue&quot;,xlab=&quot;Score&quot;,ylab=&quot;Prob Ravens Win&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="example---anova-for-logistic-regression">Example - ANOVA for Logistic Regression</h3>
<ul>
<li>ANOVA can be performed on a single logistic regression, in which it will analyze the change in variances with addition of parameters in the model, or multiple nested logistic regression (similar to linear models)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># perform analysis of variance</span><br><span class="line">anova(logRegRavens,test=&quot;Chisq&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>ANOVA returns information about the model, link function, response, as well as analysis of variance for adding terms
<ul>
<li><code>Df</code> = change in degrees of freedom
<ul>
<li>the value 1 refers to adding the <code>ravenScore</code> parameter (slope)</li>
</ul></li>
<li><code>Deviance</code> = measure of goodness of model fit compare to the previous model</li>
<li><code>Resid. Dev</code> = residual deviance for current model</li>
<li><code>Pr(&gt;Chi)</code> = used to evaluate the significance of the added parameter
<ul>
<li>in this case, the <code>Deviance</code> value of 3.54 is used to find the corresponding p-value from the Chi Squared distribution, which is 0.06
<ul>
<li><em><strong>Note</strong>: Chi Squared distribution with 1 degree of freedom is simply the squared of normal distribution, so z statistic of 2 corresponds to 95% for normal distribution indicates that deviance of 4 corresponds to approximately 5% in the Chi Squared distribution (which is what our result shows) </em></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="further-resources">Further resources</h3>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Logistic_regression" target="_blank" rel="noopener">Wikipedia on Logistic Regression</a></li>
<li><a href="http://data.princeton.edu/R/glms.html" target="_blank" rel="noopener">Logistic regression and GLMs in R</a></li>
<li>Brian Caffo’s lecture notes on: <a href="http://ocw.jhsph.edu/courses/MethodsInBiostatisticsII/PDFs/lecture23.pdf" target="_blank" rel="noopener">Simpson’s Paradox</a>, <a href="http://ocw.jhsph.edu/courses/MethodsInBiostatisticsII/PDFs/lecture24.pdf" target="_blank" rel="noopener">Retrospective Case-control Studies</a></li>
<li><a href="http://www.openintro.org/stat/down/oiStat2_08.pdf" target="_blank" rel="noopener">Open Intro Chapter on Logistic Regression</a></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="general-linear-models---poisson-models">General Linear Models - Poisson Models</h2>
<ul>
<li><strong>Poisson distribution</strong> is a useful model for counts and rates
<ul>
<li><strong>rate</strong> = count per unit of time</li>
<li>linear regression with transformation is an alternative</li>
</ul></li>
<li>count data examples
<ul>
<li>calls to a call center</li>
<li>number of flu cases in an area</li>
<li>number of cars that cross a bridge</li>
</ul></li>
<li>rate data examples
<ul>
<li>percent of children passing a test</li>
<li>percent of hits to a website from a country</li>
<li>radioactive decay</li>
</ul></li>
<li>Poisson model examples
<ul>
<li>modeling web traffic hits incidence rates</li>
<li>approximating binomial probabilities with small <span class="math inline">\(p\)</span> and large <span class="math inline">\(n\)</span></li>
<li>analyzing contingency table data (tabulated counts for categorical variables)</li>
</ul></li>
</ul>
<h3 id="properties-of-poisson-distribution">Properties of Poisson Distribution</h3>
<ul>
<li>a set of data <span class="math inline">\(X\)</span> is said to follow the Poisson distribution, or <span class="math inline">\(X \sim Poisson(t\lambda)\)</span>, if <span class="math display">\[P(X = x) = \frac{(t\lambda)^x e^{-t\lambda}}{x!}\]</span> where <span class="math inline">\(x = 0, 1, \ldots\)</span>
<ul>
<li><span class="math inline">\(\lambda\)</span> = rate or expected count per unit time</li>
<li><span class="math inline">\(t\)</span> = monitoring time</li>
</ul></li>
<li><strong>mean</strong> of Poisson distribution is <span class="math inline">\(E[X] = t\lambda\)</span>, thus <span class="math inline">\(E[X / t] = \lambda\)</span></li>
<li><strong>variance</strong> of the Poisson is <span class="math inline">\(Var(X) = t\lambda = \mu (mean)\)</span>
<ul>
<li><em><strong>Note</strong>: Poisson approaches a Gaussian/normal distribution as <span class="math inline">\(t\lambda\)</span> gets large </em></li>
</ul></li>
<li>below are the Poisson distributions for various values of <span class="math inline">\(\lambda\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># set up 1x3 panel plot</span><br><span class="line">par(mfrow = c(1, 3))</span><br><span class="line"># Poisson distribution for t = 1, and lambda = 2</span><br><span class="line">plot(0 : 10, dpois(0 : 10, lambda = 2), type = &quot;h&quot;, frame = FALSE)</span><br><span class="line"># Poisson distribution for t = 1, and lambda = 10</span><br><span class="line">plot(0 : 20, dpois(0 : 20, lambda = 10), type = &quot;h&quot;, frame = FALSE)</span><br><span class="line"># Poisson distribution for t = 1, and lambda = 100</span><br><span class="line">plot(0 : 200, dpois(0 : 200, lambda = 100), type = &quot;h&quot;, frame = FALSE)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, for large values of <span class="math inline">\(\lambda\)</span>, the distribution looks like the Gaussian</li>
</ul>
<h3 id="example---leek-group-website-traffic">Example - Leek Group Website Traffic</h3>
<ul>
<li>for this example, we will be modeling the daily traffic to Jeff Leek’s web site: <a href="http://biostat.jhsph.edu/~jleek/" class="uri" target="_blank" rel="noopener">http://biostat.jhsph.edu/~jleek/</a>
<ul>
<li>the data comes from Google Analytics and was extracted by the <code>rga</code> package that can be found at <a href="http://skardhamar.github.com/rga/" class="uri" target="_blank" rel="noopener">http://skardhamar.github.com/rga/</a></li>
</ul></li>
<li>for the purpose of the example, the time is <strong><em>always</em></strong> one day, so <span class="math inline">\(t = 1\)</span>, Poisson mean is interpreted as web hits per day
<ul>
<li>if <span class="math inline">\(t = 24\)</span>, we would be modeling web hits per hour</li>
</ul></li>
<li>the data can be found <a href="https://dl.dropboxusercontent.com/u/7710864/data/gaData.rda" target="_blank" rel="noopener">here</a></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># laod data</span><br><span class="line">load(&quot;gaData.rda&quot;)</span><br><span class="line"># convert the dates to proper formats</span><br><span class="line">gaData$julian &lt;- julian(gaData$date)</span><br><span class="line"># plot visits vs dates</span><br><span class="line">plot(gaData$julian,gaData$visits,pch=19,col=&quot;darkgrey&quot;,xlab=&quot;Julian&quot;,ylab=&quot;Visits&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="example---linear-regression">Example - Linear Regression</h3>
<ul>
<li>the traffic can be modeled using linear model as follows <span class="math display">\[ NH_i = \beta_0 + \beta_1 JD_i + \epsilon_i \]</span>
<ul>
<li><span class="math inline">\(NH_i\)</span> = number of hits to the website</li>
<li><span class="math inline">\(JD_i\)</span> = day of the year (Julian day)</li>
<li><span class="math inline">\(\beta_0\)</span> = number of hits on Julian day 0 (1970-01-01)</li>
<li><span class="math inline">\(\beta_1\)</span> = increase in number of hits per unit day</li>
<li><span class="math inline">\(\epsilon_i\)</span> = variation due to everything we didn’t measure</li>
</ul></li>
<li>the expected outcome is defined as <span class="math display">\[ E[NH_i | JD_i, \beta_0, \beta_1] = \beta_0 + \beta_1 JD_i\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># plot the visits vs dates</span><br><span class="line">plot(gaData$julian,gaData$visits,pch=19,col=&quot;darkgrey&quot;,xlab=&quot;Julian&quot;,ylab=&quot;Visits&quot;)</span><br><span class="line"># perform linear regression</span><br><span class="line">lm1 &lt;- lm(gaData$visits ~ gaData$julian)</span><br><span class="line"># plot regression line</span><br><span class="line">abline(lm1,col=&quot;red&quot;,lwd=3)</span><br></pre></td></tr></table></figure>
<h3 id="example---log-outcome">Example - log Outcome</h3>
<ul>
<li>if we are interested in relative increases in web traffic, we can the natural log of the outcome, so the linear model becomes <span class="math display">\[ \log(NH_i) = \beta_0 + \beta_1 JD_i + \epsilon_i\]</span>
<ul>
<li><span class="math inline">\(\log(NH_i)\)</span> = number of hits to the website</li>
<li><span class="math inline">\(JD_i\)</span> = day of the year (Julian day)</li>
<li><span class="math inline">\(\beta_0\)</span> = log number of hits on Julian day 0 (1970-01-01)</li>
<li><span class="math inline">\(\beta_1\)</span> = increase in log number of hits per unit day</li>
<li><span class="math inline">\(\epsilon_i\)</span> = variation due to everything we didn’t measure</li>
</ul></li>
<li>when we take the natural log of outcomes and fit a regression model, the exponentiated coefficients estimate quantities based on the geometric means rather than the measured values
<ul>
<li><span class="math inline">\(e^{E[\log(Y)]}\)</span> = geometric mean of <span class="math inline">\(Y\)</span>
<ul>
<li>geometric means are defined as <span class="math display">\[e^{\frac{1}{n}\sum_{i=1}^n \log(y_i)} = (\prod_{i=1}^n y_i)^{1/n}\]</span> which is the estimate for the <strong>population geometric mean</strong></li>
<li>as we collect infinite amount of data, <span class="math inline">\(\prod_{i=1}^n y_i)^{1/n} \to E[\log(Y)]\)</span></li>
</ul></li>
<li><span class="math inline">\(e^{\beta_0}\)</span> = estimated geometric mean hits on day 0</li>
<li><span class="math inline">\(e^{\beta_1}\)</span> = estimated relative increase or decrease in geometric mean hits per day</li>
<li><em><strong>Note</strong>: not we can not take the natural log of zero counts, so often we need to adding a constant (i.e. 1) to construct the log model </em>
<ul>
<li>adding the constant changes the interpretation of coefficient slightly</li>
<li><span class="math inline">\(e^{\beta_1}\)</span> is now the relative increase or decrease in geometric mean hits <strong>+ 1</strong> per day</li>
</ul></li>
</ul></li>
<li>the expected outcome is <span class="math display">\[E[\log(NH_i | JD_i, \beta_0, \beta_1)] = \beta_0 + \beta_1 JD_i \]</span></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">round(exp(coef(lm(I(log(gaData$visits + 1)) ~ gaData$julian))), 5)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the daily increase in hits is 0.2%</li>
</ul>
<h3 id="example---poisson-regression">Example - Poisson Regression</h3>
<ul>
<li>the Poisson model can be constructed as log of the mean <span class="math display">\[\log\left(E[NH_i | JD_i, \beta_0, \beta_1]\right) = \beta_0 + \beta_1 JD_i\]</span> or in other form <span class="math display">\[E[NH_i | JD_i, \beta_0, \beta_1] = \exp\left(\beta_0 + \beta_1 JD_i\right)\]</span>
<ul>
<li><span class="math inline">\(NH_i\)</span> = number of hits to the website</li>
<li><span class="math inline">\(JD_i\)</span> = day of the year (Julian day)</li>
<li><span class="math inline">\(\beta_0\)</span> = expected number of hits on Julian day 0 (1970-01-01)</li>
<li><span class="math inline">\(\beta_1\)</span> = expected increase in number of hits per unit day</li>
<li><em><strong>Note</strong>: Poisson model differs from the log outcome model in that the coefficients are interpreted naturally as expected value of outcome where as the log model is interpreted on the log scale of outcome </em></li>
</ul></li>
<li>we can transform the Poisson model to <span class="math display">\[E[NH_i | JD_i, \beta_0, \beta_1] = \exp\left(\beta_0 + \beta_1 JD_i\right) = \exp\left(\beta_0 \right)\exp\left(\beta_1 JD_i\right)\]</span>
<ul>
<li><span class="math inline">\(\beta_1 = E[NH_i | JD_i+1, \beta_0, \beta_1] - E[NH_i | JD_i, \beta_0, \beta_1]\)</span></li>
<li><span class="math inline">\(\beta_1\)</span> can therefore be interpreted as the <strong>relative</strong> increase/decrease in web traffic hits per one day increase</li>
</ul></li>
<li><code>glm(outcome~predictor, family = &quot;poisson&quot;)</code> = performs Poisson regression</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># plot visits vs dates</span><br><span class="line">plot(gaData$julian,gaData$visits,pch=19,col=&quot;darkgrey&quot;,xlab=&quot;Julian&quot;,ylab=&quot;Visits&quot;)</span><br><span class="line"># construct Poisson regression model</span><br><span class="line">glm1 &lt;- glm(gaData$visits ~ gaData$julian,family=&quot;poisson&quot;)</span><br><span class="line"># plot linear regression line in red</span><br><span class="line">abline(lm1,col=&quot;red&quot;,lwd=3)</span><br><span class="line"># plot Poisson regression line in</span><br><span class="line">lines(gaData$julian,glm1$fitted,col=&quot;blue&quot;,lwd=3)</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: the Poisson fit is non-linear since it is linear only on the log of the mean scale </em></li>
</ul>
<h3 id="example---robust-standard-errors-with-poisson-regression">Example - Robust Standard Errors with Poisson Regression</h3>
<ul>
<li>variance of the Poisson distribution is defined to be the mean of the distribution, so we would expect the variance to increase with higher values of <span class="math inline">\(X\)</span></li>
<li>below is the residuals vs fitted value plot for the Poisson regression model</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot residuals vs fitted values</span><br><span class="line">plot(glm1$fitted,glm1$residuals,pch=19,col=&quot;grey&quot;,ylab=&quot;Residuals&quot;,xlab=&quot;Fitted&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the residuals don’t appear to be increasing with higher fitted values</li>
<li>even if the mean model is correct in principle, there could always be a certain degree of <strong><em>model mis-specification</em></strong></li>
<li>to account for mis-specifications for the model, we can use
<ol type="1">
<li><code>glm(outcome~predictor, family = &quot;quasi-poisson&quot;)</code> = introduces an additional multiplicative factor <span class="math inline">\(\phi\)</span> to denominator of model so that the variance is <span class="math inline">\(\phi \mu\)</span> rather than just <span class="math inline">\(\mu\)</span> (see <a href="#variances-and-quasi-likelihoods">Variances and Quasi-Likelihoods</a>)</li>
<li>more generally, <em>robust standard errors</em> (effectively constructing wider confidence intervals) can be used</li>
</ol></li>
<li><strong>model agnostic standard errors</strong>, implemented through the <code>sandwich</code> package, is one way to calculate the robust standard errors
<ul>
<li>algorithm assumes the mean relationship is specified correctly and attempts to get a general estimates the variance that isn’t highly dependent on the model</li>
<li>it uses assumption of large sample sizes and asymptotics to estimate the confidence intervals that is robust to model mis-specification</li>
<li><em><strong>Note</strong>: more information can be found at <a href="http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval" class="uri" target="_blank" rel="noopener">http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval</a> </em></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># load sandwich package</span><br><span class="line">library(sandwich)</span><br><span class="line"># compute</span><br><span class="line">confint.agnostic &lt;- function (object, parm, level = 0.95, ...)</span><br><span class="line">&#123;</span><br><span class="line">    cf &lt;- coef(object); pnames &lt;- names(cf)</span><br><span class="line">    if (missing(parm))</span><br><span class="line">        parm &lt;- pnames</span><br><span class="line">    else if (is.numeric(parm))</span><br><span class="line">        parm &lt;- pnames[parm]</span><br><span class="line">    a &lt;- (1 - level)/2; a &lt;- c(a, 1 - a)</span><br><span class="line">    pct &lt;- stats:::format.perc(a, 3)</span><br><span class="line">    fac &lt;- qnorm(a)</span><br><span class="line">    ci &lt;- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,</span><br><span class="line">                                                               pct))</span><br><span class="line">    ses &lt;- sqrt(diag(sandwich::vcovHC(object)))[parm]</span><br><span class="line">    ci[] &lt;- cf[parm] + ses %o% fac</span><br><span class="line">    ci</span><br><span class="line">&#125;</span><br><span class="line"># regular confidence interval from Poisson Model</span><br><span class="line">confint(glm1)</span><br><span class="line"># model agnostic standard errors</span><br><span class="line">confint.agnostic(glm1)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the robust standard error produced slightly wider confidence intervals</li>
</ul>
<h3 id="example---rates">Example - Rates</h3>
<ul>
<li>if we were to model the percentage of total web hits that are coming from the <em>Simply Statistics</em> blog, we could construct the following model <span class="math display">\[\begin{aligned}
E[NHSS_i | JD_i, \beta_0, \beta_1]/NH_i &amp; = \exp\left(\beta_0 + \beta_1 JD_i\right) \\
(take~\log~of~both~sides)~\log\left(E[NHSS_i | JD_i, \beta_0, \beta_1]\right) - \log(NH_i) &amp; = \beta_0 + \beta_1 JD_i \\
(move~\log(NH_i)~to~right~side)~\log\left(E[NHSS_i | JD_i, \beta_0, \beta_1]\right) &amp; = \log(NH_i) + \beta_0 + \beta_1 JD_i \\
\end{aligned}\]</span>
<ul>
<li>when <strong>offset</strong> term, <span class="math inline">\(\log(NH_i)\)</span>, is present in the Poisson model, the interpretation of the coefficients will be relative to the offset quantity</li>
<li>it’s important to recognize that the fitted response doesn’t change</li>
<li><strong><em>example</em></strong>: to convert the outcome from daily data to hourly, we can add a factor 24 so that the model becomes <span class="math display">\[\begin{aligned}
E[NHSS_i | JD_i, \beta_0, \beta_1]/24 &amp; = \exp\left(\beta_0 + \beta_1 JD_i\right) \\
(take~\log~of~both~sides)~\log\left(E[NHSS_i | JD_i, \beta_0, \beta_1]\right) - \log(24) &amp; = \beta_0 + \beta_1 JD_i \\
(move~\log(24)~to~right~side)~\log\left(E[NHSS_i | JD_i, \beta_0, \beta_1]\right) &amp; = \log(24) + \log(NH_i) + \beta_0 + \beta_1 JD_i \\
\end{aligned}\]</span></li>
</ul></li>
<li>back to the rates model, we fit the Poisson model now with an offset so that the model is interpreted with respect to the number of visits
<ul>
<li><code>glm(outcome ~ predictor, offset = log(offset), family = &quot;poisson&quot;)</code> = perform Poisson regression with offset</li>
<li><code>glm(outcome ~ predictor + log(offset))</code> = produces the same result</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># perform Poisson regression with offset for number of visits</span><br><span class="line">glm2 &lt;- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),</span><br><span class="line">            family=&quot;poisson&quot;,data=gaData)</span><br><span class="line"># plot the fitted means (from simply statistics)</span><br><span class="line">plot(julian(gaData$date),glm2$fitted,col=&quot;blue&quot;,pch=19,xlab=&quot;Date&quot;,ylab=&quot;Fitted Counts&quot;)</span><br><span class="line"># plot the fitted means (total visit)</span><br><span class="line">points(julian(gaData$date),glm1$fitted,col=&quot;red&quot;,pch=19)</span><br><span class="line"># plot the rates for simply stats</span><br><span class="line">plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col=&quot;grey&quot;,xlab=&quot;Date&quot;,</span><br><span class="line">     ylab=&quot;Fitted Rates&quot;,pch=19)</span><br><span class="line"># plot the fitted rates for simply stats (visit/day)</span><br><span class="line">lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col=&quot;blue&quot;,lwd=3)</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: we added 1 to the <code>log(visits)</code> to address 0 values </em></li>
</ul>
<h3 id="further-resources-1">Further Resources</h3>
<ul>
<li><a href="http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html" target="_blank" rel="noopener">Log-linear mMdels and Multi-way Tables</a></li>
<li><a href="http://en.wikipedia.org/wiki/Poisson_regression" target="_blank" rel="noopener">Wikipedia on Poisson Regression</a></li>
<li><a href="http://en.wikipedia.org/wiki/Overdispersion" target="_blank" rel="noopener">Wikipedia on Overdispersion</a></li>
<li><a href="http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf" target="_blank" rel="noopener">Regression Models for count data in R</a></li>
<li><a href="http://cran.r-project.org/web/packages/pscl/index.html" target="_blank" rel="noopener"><code>pscl</code> package</a> -
<ul>
<li>often time in modeling counts, they maybe more zero counts in the data than anticipated, which the regular Poisson model doesn’t account for</li>
<li>the function <code>zeroinfl</code> fits zero inflated Poisson (ziP) models to such data</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="fitting-functions">Fitting Functions</h2>
<ul>
<li><strong>scatterplot smoothing</strong> = fitting functions (multiple linear models, piece-wise zig-zag lines) to data in the form <span class="math inline">\(Y_i = f(X_i) + \epsilon_i\)</span></li>
<li>consider the model <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \sum_{k=1}^d (x_i - \xi_k)_+ \gamma_k + \epsilon_{i}\]</span> where <span class="math inline">\((a)_+ = a\)</span> if <span class="math inline">\(a &gt; 0\)</span> and <span class="math inline">\(0\)</span> otherwise and <span class="math inline">\(\xi_1 \leq ... \leq \xi_d\)</span> are known <strong>knot points</strong></li>
<li>the mean function <span class="math display">\[E[Y_i] = \beta_0 + \beta_1 X_i + \sum_{k=1}^d (x_i - \xi_k)_+ \gamma_k\]</span> is continuous at the knot points
<ul>
<li>for <span class="math inline">\(\xi_k = 5\)</span>, the expected value for <span class="math inline">\(Y_i\)</span> as <span class="math inline">\(x_i\)</span> approaches 5 from the left is <span class="math display">\[\begin{aligned}
E[Y_i]_{\xi = 5 | left} &amp; = \beta_0 + \beta_1 x_i + (x_i - 5)_+ \gamma_k \\
(since~x_i&lt;5)~ E[Y_i]_{\xi = 5 | left} &amp; = \beta_0 + \beta_1 x_i \\
\end{aligned}\]</span></li>
<li>the expected value for <span class="math inline">\(Y_i\)</span> as <span class="math inline">\(x_i\)</span> approaches 5 from the right is <span class="math display">\[\begin{aligned}
E[Y_i]_{\xi = 5 | right} &amp; = \beta_0 + \beta_1 x_i + (x_i - 5)_+ \gamma_k \\
(since~x_i&gt;5)~ E[Y_i]_{\xi = 5 | right} &amp; = \beta_0 + \beta_1 x_i + (x_i - 5) \gamma_k\\
(simplify)~ E[Y_i]_{\xi = 5 | right} &amp; = \beta_0 - 5 \gamma_k + (\beta_1 + \gamma_k) x_i \\
\end{aligned}\]</span></li>
<li>as we can see from above, the right side is just another line with different intercept (<span class="math inline">\(\beta_0 - 5 \gamma_k\)</span>) and slope (<span class="math inline">\(\beta_1 + \gamma_k\)</span>)</li>
<li>so as <span class="math inline">\(x\)</span> approaches 5, both sides converge</li>
</ul></li>
</ul>
<h3 id="considerations">Considerations</h3>
<ul>
<li><strong>basis</strong> = the collection of regressors</li>
<li>single knot point terms can fit <em>hockey-stick-like</em> processes</li>
<li>these bases can be used in GLMs (as an additional term/predictor) as well</li>
<li>issue with these approaches is the <strong>large</strong> number of parameters introduced
<ul>
<li>requires some method of <strong><em>regularization</em></strong>, or penalize for large number of parameters (see <strong>Practical Machine Learning</strong> course)</li>
<li>introducing large number of knots have significant consequences</li>
</ul></li>
</ul>
<h3 id="example---fitting-piecewise-linear-function">Example - Fitting Piecewise Linear Function</h3>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># simulate data</span><br><span class="line">n &lt;- 500; x &lt;- seq(0, 4 * pi, length = n); y &lt;- sin(x) + rnorm(n, sd = .3)</span><br><span class="line"># define 20 knot points</span><br><span class="line">knots &lt;- seq(0, 8 * pi, length = 20);</span><br><span class="line"># define the ()+ function to only take the values that are positive after the knot pt</span><br><span class="line">splineTerms &lt;- sapply(knots, function(knot) (x &gt; knot) * (x - knot))</span><br><span class="line"># define the predictors as X and spline term</span><br><span class="line">xMat &lt;- cbind(x, splineTerms)</span><br><span class="line"># fit linear models for y vs predictors</span><br><span class="line">yhat &lt;- predict(lm(y ~ xMat))</span><br><span class="line"># plot data points (x, y)</span><br><span class="line">plot(x, y, frame = FALSE, pch = 21, bg = &quot;lightblue&quot;)</span><br><span class="line"># plot fitted values</span><br><span class="line">lines(x, yhat, col = &quot;red&quot;, lwd = 2)</span><br></pre></td></tr></table></figure>
<h3 id="example---fitting-piecewise-quadratic-function">Example - Fitting Piecewise Quadratic Function</h3>
<ul>
<li>adding squared terms makes it <strong><em>continuous</em></strong> AND <strong><em>differentiable</em></strong> at the knot points, and the model becomes <span class="math display">\[Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \sum_{k=1}^d (x_i - \xi_k)_+^2 \gamma_k + \epsilon_{i}\]</span> where <span class="math inline">\((a)^2_+ = a^2\)</span> if <span class="math inline">\(a &gt; 0\)</span> and <span class="math inline">\(0\)</span> otherwise</li>
<li>adding cubic terms makes it twice continuously differentiable at the knot points, etcetera</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># define the knot terms in the model</span><br><span class="line">splineTerms &lt;- sapply(knots, function(knot) (x &gt; knot) * (x - knot)^2)</span><br><span class="line"># define the predictors as x, x^2 and knot terms</span><br><span class="line">xMat &lt;- cbind(x, x^2, splineTerms)</span><br><span class="line"># fit linear models for y vs predictors</span><br><span class="line">yhat &lt;- predict(lm(y ~ xMat))</span><br><span class="line"># plot data points (x, y)</span><br><span class="line">plot(x, y, frame = FALSE, pch = 21, bg = &quot;lightblue&quot;)</span><br><span class="line"># plot fitted values</span><br><span class="line">lines(x, yhat, col = &quot;red&quot;, lwd = 2)</span><br></pre></td></tr></table></figure>
<h3 id="example---harmonics-using-linear-models">Example - Harmonics using Linear Models</h3>
<ul>
<li><strong>discrete Fourier transforms</strong> = instance of linear regression model, use sin and cosine functions as basis to fit data</li>
<li>to demonstrate this, we will generate 2 seconds of sound data using sin waves, simulate a chord, and apply linear regression to find out which notes are playing</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># frequencies for white keys from c4 to c5</span><br><span class="line">notes4 &lt;- c(261.63, 293.66, 329.63, 349.23, 392.00, 440.00, 493.88, 523.25)</span><br><span class="line"># generate sequence for 2 seconds</span><br><span class="line">t &lt;- seq(0, 2, by = .001); n &lt;- length(t)</span><br><span class="line"># define data for c4 e4 g4 using sine waves with their frequencies</span><br><span class="line">c4 &lt;- sin(2 * pi * notes4[1] * t); e4 &lt;- sin(2 * pi * notes4[3] * t);</span><br><span class="line">g4 &lt;- sin(2 * pi * notes4[5] * t)</span><br><span class="line"># define data for a chord and add a bit of noise</span><br><span class="line">chord &lt;- c4 + e4 + g4 + rnorm(n, 0, 0.3)</span><br><span class="line"># generate profile data for all notes</span><br><span class="line">x &lt;- sapply(notes4, function(freq) sin(2 * pi * freq * t))</span><br><span class="line"># fit the chord using the profiles for all notes</span><br><span class="line">fit &lt;- lm(chord ~ x - 1)</span><br></pre></td></tr></table></figure>
<ul>
<li>after generating the data and running the linear regression, we can plot the results to see if the notes are correctly identified</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># set up plot</span><br><span class="line">plot(c(0, 9), c(0, 1.5), xlab = &quot;Note&quot;, ylab = &quot;Coef^2&quot;, axes = FALSE, frame = TRUE, type = &quot;n&quot;)</span><br><span class="line"># set up axes</span><br><span class="line">axis(2)</span><br><span class="line">axis(1, at = 1 : 8, labels = c(&quot;c4&quot;, &quot;d4&quot;, &quot;e4&quot;, &quot;f4&quot;, &quot;g4&quot;, &quot;a4&quot;, &quot;b4&quot;, &quot;c5&quot;))</span><br><span class="line"># add vertical lines for each note</span><br><span class="line">for (i in 1 : 8) abline(v = i, lwd = 3, col = grey(.8))</span><br><span class="line"># plot the linear regression fits</span><br><span class="line">lines(c(0, 1 : 8, 9), c(0, coef(fit)^2, 0), type = &quot;l&quot;, lwd = 3, col = &quot;red&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the correct notes were identified</li>
<li>we can also use the <a href="http://en.wikipedia.org/wiki/Fast_Fourier_transform" target="_blank" rel="noopener"><strong>Fast Fourier Transforms</strong></a> to identify the notes
<ul>
<li><code>fft(data)</code> = performs fast Fourier transforms on provided data</li>
<li><code>Re(data)</code> = subset to only the real components of the complex data</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># perform fast fourier transforms on the chord matrix</span><br><span class="line">a &lt;- fft(chord)</span><br><span class="line"># plot only the real components of the fft</span><br><span class="line">plot(Re(a)^2, type = &quot;l&quot;)</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: the algorithm checks for all possible notes at all frequencies it can detect, which is why the peaks are very high in magnitude </em></li>
<li><em><strong>Note</strong>: the symmetric display of the notes are due to periodic symmetries of the sine functions </em></li>
</ul>

      
    </div>

    
      


    

    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Xing Su</li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="http://yoursite.com/2018/08/07/Regression-Models/" title="Regression Models">http://yoursite.com/2018/08/07/Regression-Models/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/R/" rel="tag"># R</a>
          
            <a href="/tags/DataScience/" rel="tag"># DataScience</a>
          
            <a href="/tags/R-Markdown/" rel="tag"># R Markdown</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/07/Statistical-Inference/" rel="next" title="Statistical Inference">
                <i class="fa fa-chevron-left"></i> Statistical Inference
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/07/Practical-Machine-Learning/" rel="prev" title="Practical Machine Learning">
                Practical Machine Learning <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zODY0Mi8xNTE3MA=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avator.png"
                alt="Autoz" />
            
              <p class="site-author-name" itemprop="name">Autoz</p>
              <p class="site-description motion-element" itemprop="description">Fidelty.</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/autolordz" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:autolordz@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction-to-regression"><span class="nav-number">1.</span> <span class="nav-text">Introduction to Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#notation"><span class="nav-number">2.</span> <span class="nav-text">Notation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#empiricalsample-mean"><span class="nav-number">2.1.</span> <span class="nav-text">Empirical/Sample Mean</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#empiricalsample-standard-deviation-variance"><span class="nav-number">2.2.</span> <span class="nav-text">Empirical/Sample Standard Deviation &amp; Variance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#normalization"><span class="nav-number">2.3.</span> <span class="nav-text">Normalization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#empirical-covariance-correlation"><span class="nav-number">2.4.</span> <span class="nav-text">Empirical Covariance &amp; Correlation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#daltons-data-and-least-squares"><span class="nav-number">3.</span> <span class="nav-text">Dalton’s Data and Least Squares</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#derivation-for-least-squares-empirical-mean-finding-the-minimum"><span class="nav-number">3.1.</span> <span class="nav-text">Derivation for Least Squares = Empirical Mean (Finding the Minimum)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regression-through-the-origin"><span class="nav-number">4.</span> <span class="nav-text">Regression through the Origin</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#derivation-for-beta"><span class="nav-number">4.1.</span> <span class="nav-text">Derivation for \(\beta\)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#finding-the-best-fit-line-ordinary-least-squares"><span class="nav-number">5.</span> <span class="nav-text">Finding the Best Fit Line (Ordinary Least Squares)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#least-squares-model-fit"><span class="nav-number">5.1.</span> <span class="nav-text">Least Squares Model Fit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#derivation-for-beta_0-and-beta_1"><span class="nav-number">5.2.</span> <span class="nav-text">Derivation for \(\beta_0\) and \(\beta_1\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#examples-and-r-commands"><span class="nav-number">5.3.</span> <span class="nav-text">Examples and R Commands</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#regression-to-the-mean"><span class="nav-number">6.</span> <span class="nav-text">Regression to the Mean</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#daltons-investigation-on-regression-to-the-mean"><span class="nav-number">6.1.</span> <span class="nav-text">Dalton’s Investigation on Regression to the Mean</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#statistical-linear-regression-models"><span class="nav-number">7.</span> <span class="nav-text">Statistical Linear Regression Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#interpreting-regression-coefficients"><span class="nav-number">7.1.</span> <span class="nav-text">Interpreting Regression Coefficients</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#use-regression-coefficients-for-prediction"><span class="nav-number">7.2.</span> <span class="nav-text">Use Regression Coefficients for Prediction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-and-r-commands"><span class="nav-number">7.3.</span> <span class="nav-text">Example and R Commands</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#derivation-for-maximum-likelihood-estimator"><span class="nav-number">7.4.</span> <span class="nav-text">Derivation for Maximum Likelihood Estimator</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#residuals"><span class="nav-number">8.</span> <span class="nav-text">Residuals</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#estimating-residual-variation"><span class="nav-number">8.1.</span> <span class="nav-text">Estimating Residual Variation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#total-variation-r2-and-derivations"><span class="nav-number">8.2.</span> <span class="nav-text">Total Variation, \(R^2\), and Derivations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-and-r-commands-1"><span class="nav-number">8.3.</span> <span class="nav-text">Example and R Commands</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#inference-in-regression"><span class="nav-number">9.</span> <span class="nav-text">Inference in Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#intervalstests-for-coefficients"><span class="nav-number">9.1.</span> <span class="nav-text">Intervals/Tests for Coefficients</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prediction-interval"><span class="nav-number">9.2.</span> <span class="nav-text">Prediction Interval</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multivariate-regression"><span class="nav-number">10.</span> <span class="nav-text">Multivariate Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#derivation-of-coefficients"><span class="nav-number">10.1.</span> <span class="nav-text">Derivation of Coefficients</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#interpretation-of-coefficients"><span class="nav-number">10.2.</span> <span class="nav-text">Interpretation of Coefficients</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-linear-model-with-2-variables-and-intercept"><span class="nav-number">10.3.</span> <span class="nav-text">Example: Linear Model with 2 Variables and Intercept</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-coefficients-that-reverse-signs"><span class="nav-number">10.4.</span> <span class="nav-text">Example: Coefficients that Reverse Signs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-unnecessary-variables"><span class="nav-number">10.5.</span> <span class="nav-text">Example: Unnecessary Variables</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dummy-variables"><span class="nav-number">11.</span> <span class="nav-text">Dummy Variables</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#more-than-2-levels"><span class="nav-number">11.1.</span> <span class="nav-text">More Than 2 Levels</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-6-factor-level-insect-spray-data"><span class="nav-number">11.2.</span> <span class="nav-text">Example: 6 Factor Level Insect Spray Data</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#interactions"><span class="nav-number">12.</span> <span class="nav-text">Interactions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model-hungry-year-by-sex"><span class="nav-number">12.1.</span> <span class="nav-text">Model: % Hungry ~ Year by Sex</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-hungry-year-sex-binary-variable"><span class="nav-number">12.2.</span> <span class="nav-text">Model: % Hungry ~ Year + Sex (Binary Variable)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-hungry-year-sex-year-sex-binary-interaction"><span class="nav-number">12.3.</span> <span class="nav-text">Model: % Hungry ~ Year + Sex + Year * Sex (Binary Interaction)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-hungry-year-income-year-income-continuous-interaction"><span class="nav-number">12.4.</span> <span class="nav-text">Example: % Hungry ~ Year + Income + Year * Income (Continuous Interaction)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multivariable-simulation"><span class="nav-number">13.</span> <span class="nav-text">Multivariable Simulation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#simulation-1---treatment-adjustment-effect"><span class="nav-number">13.1.</span> <span class="nav-text">Simulation 1 - Treatment = Adjustment Effect</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#simulation-2---no-treatment-effect"><span class="nav-number">13.2.</span> <span class="nav-text">Simulation 2 - No Treatment Effect</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#simulation-3---treatment-reverses-adjustment-effect"><span class="nav-number">13.3.</span> <span class="nav-text">Simulation 3 - Treatment Reverses Adjustment Effect</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#simulation-4---no-adjustment-effect"><span class="nav-number">13.4.</span> <span class="nav-text">Simulation 4 - No Adjustment Effect</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#simulation-5---binary-interaction"><span class="nav-number">13.5.</span> <span class="nav-text">Simulation 5 - Binary Interaction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#simulation-6---continuous-adjustment"><span class="nav-number">13.6.</span> <span class="nav-text">Simulation 6 - Continuous Adjustment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-and-considerations"><span class="nav-number">13.7.</span> <span class="nav-text">Summary and Considerations</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#residuals-and-diagnostics"><span class="nav-number">14.</span> <span class="nav-text">Residuals and Diagnostics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#outliers-and-influential-points"><span class="nav-number">14.1.</span> <span class="nav-text">Outliers and Influential Points</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#influence-measures"><span class="nav-number">14.2.</span> <span class="nav-text">Influence Measures</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#using-influence-measures"><span class="nav-number">14.3.</span> <span class="nav-text">Using Influence Measures</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---outlier-causing-linear-relationship"><span class="nav-number">14.4.</span> <span class="nav-text">Example - Outlier Causing Linear Relationship</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---real-linear-relationship"><span class="nav-number">14.5.</span> <span class="nav-text">Example - Real Linear Relationship</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---stefanski-tas-2007"><span class="nav-number">14.6.</span> <span class="nav-text">Example - Stefanski TAS 2007</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model-selection"><span class="nav-number">15.</span> <span class="nav-text">Model Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rumsfeldian-triplet"><span class="nav-number">15.1.</span> <span class="nav-text">Rumsfeldian Triplet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#general-rules"><span class="nav-number">15.2.</span> <span class="nav-text">General Rules</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---r2-v-n"><span class="nav-number">15.3.</span> <span class="nav-text">Example - \(R^2\) v \(n\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adjusted-r2"><span class="nav-number">15.4.</span> <span class="nav-text">Adjusted \(R^2\)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---unrelated-regressors"><span class="nav-number">15.5.</span> <span class="nav-text">Example - Unrelated Regressors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---highly-correlated-regressors-variance-inflation"><span class="nav-number">15.6.</span> <span class="nav-text">Example - Highly Correlated Regressors / Variance Inflation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-variance-inflation-factors"><span class="nav-number">15.7.</span> <span class="nav-text">Example: Variance Inflation Factors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#residual-variance-estimates"><span class="nav-number">15.8.</span> <span class="nav-text">Residual Variance Estimates</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#covariate-model-selection"><span class="nav-number">15.9.</span> <span class="nav-text">Covariate Model Selection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-anova"><span class="nav-number">15.10.</span> <span class="nav-text">Example: ANOVA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-step-wise-model-search"><span class="nav-number">15.11.</span> <span class="nav-text">Example: Step-wise Model Search</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#general-linear-models-overview"><span class="nav-number">16.</span> <span class="nav-text">General Linear Models Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#simple-linear-model"><span class="nav-number">16.1.</span> <span class="nav-text">Simple Linear Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#logistic-regression"><span class="nav-number">16.2.</span> <span class="nav-text">Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#poisson-regression"><span class="nav-number">16.3.</span> <span class="nav-text">Poisson Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#variances-and-quasi-likelihoods"><span class="nav-number">16.4.</span> <span class="nav-text">Variances and Quasi-Likelihoods</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#solving-for-normal-and-quasi-likelihood-normal-equations"><span class="nav-number">16.5.</span> <span class="nav-text">Solving for Normal and Quasi-Likelihood Normal Equations</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#general-linear-models---binary-models"><span class="nav-number">17.</span> <span class="nav-text">General Linear Models - Binary Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#odds"><span class="nav-number">17.1.</span> <span class="nav-text">Odds</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---baltimore-ravens-win-vs-loss"><span class="nav-number">17.2.</span> <span class="nav-text">Example - Baltimore Ravens Win vs Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---simple-linear-regression"><span class="nav-number">17.3.</span> <span class="nav-text">Example - Simple Linear Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---logistic-regression"><span class="nav-number">17.4.</span> <span class="nav-text">Example - Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---anova-for-logistic-regression"><span class="nav-number">17.5.</span> <span class="nav-text">Example - ANOVA for Logistic Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#further-resources"><span class="nav-number">17.6.</span> <span class="nav-text">Further resources</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#general-linear-models---poisson-models"><span class="nav-number">18.</span> <span class="nav-text">General Linear Models - Poisson Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#properties-of-poisson-distribution"><span class="nav-number">18.1.</span> <span class="nav-text">Properties of Poisson Distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---leek-group-website-traffic"><span class="nav-number">18.2.</span> <span class="nav-text">Example - Leek Group Website Traffic</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---linear-regression"><span class="nav-number">18.3.</span> <span class="nav-text">Example - Linear Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---log-outcome"><span class="nav-number">18.4.</span> <span class="nav-text">Example - log Outcome</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---poisson-regression"><span class="nav-number">18.5.</span> <span class="nav-text">Example - Poisson Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---robust-standard-errors-with-poisson-regression"><span class="nav-number">18.6.</span> <span class="nav-text">Example - Robust Standard Errors with Poisson Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---rates"><span class="nav-number">18.7.</span> <span class="nav-text">Example - Rates</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#further-resources-1"><span class="nav-number">18.8.</span> <span class="nav-text">Further Resources</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fitting-functions"><span class="nav-number">19.</span> <span class="nav-text">Fitting Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#considerations"><span class="nav-number">19.1.</span> <span class="nav-text">Considerations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---fitting-piecewise-linear-function"><span class="nav-number">19.2.</span> <span class="nav-text">Example - Fitting Piecewise Linear Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---fitting-piecewise-quadratic-function"><span class="nav-number">19.3.</span> <span class="nav-text">Example - Fitting Piecewise Quadratic Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---harmonics-using-linear-models"><span class="nav-number">19.4.</span> <span class="nav-text">Example - Harmonics using Linear Models</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Autoz</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Symbols count total: </span>
    
    <span title="Symbols count total">514k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    
    <span title="Reading time total">7:47</span>
  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.5.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Gemini</a> v6.3.0</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="Total Visitors">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="Total Views">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.3.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=6.3.0"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=6.3.0"></script>


  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('Copy').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('Copied')
          else $(this).text('Copy failed')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('Copy')
        }, 300)
      }).append(e)
    })
  </script>


</body>
</html>
