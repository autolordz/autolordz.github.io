<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.3.0',
    sidebar: {"position":"right","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="\(\pagebreak\) Overview  Statistical Inference = generating conclusions about a population from a noisy sample Goal = extend beyond data to population Statistical Inference = only formal system o">
<meta name="keywords" content="R,DataScience,R Markdown">
<meta property="og:type" content="article">
<meta property="og:title" content="Statistical Inference">
<meta property="og:url" content="http://yoursite.com/2018/08/07/Statistical-Inference/index.html">
<meta property="og:site_name" content="AutozLand">
<meta property="og:description" content="\(\pagebreak\) Overview  Statistical Inference = generating conclusions about a population from a noisy sample Goal = extend beyond data to population Statistical Inference = only formal system o">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Statistical-Inference/figures/9.png">
<meta property="og:updated_time" content="2018-08-07T06:21:25.421Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Statistical Inference">
<meta name="twitter:description" content="\(\pagebreak\) Overview  Statistical Inference = generating conclusions about a population from a noisy sample Goal = extend beyond data to population Statistical Inference = only formal system o">
<meta name="twitter:image" content="http://yoursite.com/2018/08/07/Statistical-Inference/figures/9.png">






  <link rel="canonical" href="http://yoursite.com/2018/08/07/Statistical-Inference/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Statistical Inference | AutozLand</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AutozLand</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Autoz's Learning Blogs</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives<span class="badge">12</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags<span class="badge">3</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories<span class="badge">2</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />About</a>
  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Statistical-Inference/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Statistical Inference<a href="https://github.com/theme-next/theme-next.org/_posts/tree/master/Statistical-Inference.md" class="post-edit-link" title="Edit this post" target="_blank">
                    <i class="fa fa-pencil"></i>
                  </a>
                
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 14:07:49 / Modified: 14:21:25" itemprop="dateCreated datePublished" datetime="2018-08-07T14:07:49+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon"
            >
            <i class="fa fa-eye"></i>
             Views:  
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">70k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">1:04</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="overview">Overview</h2>
<ul>
<li><strong>Statistical Inference</strong> = generating conclusions about a population from a noisy sample</li>
<li>Goal = extend beyond data to population</li>
<li>Statistical Inference = only formal system of inference we have</li>
<li>many different modes, but <strong>two</strong> broad flavors of inference (inferential paradigms): <strong><em>Bayesian</em></strong> vs <strong><em>Frequencist</em></strong>
<ul>
<li><strong>Frequencist</strong> = uses long run proportion of times an event occurs independent identically distributed repetitions
<ul>
<li>frequentist is what this class is focused on</li>
<li>believes if an experiment is repeated many many times, the resultant percentage of success/something happening defines that population parameter</li>
</ul></li>
<li><strong>Bayesian</strong> = probability estimate for a hypothesis is updated as additional evidence is acquired</li>
</ul></li>
<li><strong>statistic</strong> = number computed from a sample of data
<ul>
<li>statistics are used to infer information about a population</li>
</ul></li>
<li><strong>random variable</strong> = outcome from an experiment
<ul>
<li>deterministic processes (variance/means) produce additional random variables when applied to random variables, and they have their own distributions</li>
</ul></li>
</ul>
<h2 id="probability">Probability</h2>
<ul>
<li><strong>Probability</strong> = the study of quantifying the likelihood of particular events occurring
<ul>
<li>given a random experiment, <strong><em>probability</em></strong> = population quantity that summarizes the randomness
<ul>
<li>not in the data at hand, but a conceptual quantity that exist in the population that we want to estimate</li>
</ul></li>
</ul></li>
</ul>
<h3 id="general-probability-rules">General Probability Rules</h3>
<ul>
<li>discovered by Russian mathematician Kolmogorov, also known as “Probability Calculus”</li>
<li>probability = function of any set of outcomes and assigns it a number between 0 and 1
<ul>
<li><span class="math inline">\(0 \le P(E) \le 1\)</span>, where <span class="math inline">\(E\)</span> = event</li>
</ul></li>
<li>probability that nothing occurs = 0 (impossible, have to roll dice to create outcome), that something occurs is 1 (certain)</li>
<li>probability of outcome or event <span class="math inline">\(E\)</span>, <span class="math inline">\(P(E)\)</span> = ratio of ways that <span class="math inline">\(E\)</span> could occur to number of all possible outcomes or events</li>
<li>probability of something = 1 - probability of the opposite occurring</li>
<li>probability of the <strong>union</strong> of any two sets of outcomes that have nothing in common (mutually exclusive) = sum of respective probabilities</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library(grid);library(png)</span><br><span class="line">grid.raster(readPNG(&quot;figures/1.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>if <span class="math inline">\(A\)</span> implies occurrence of <span class="math inline">\(B\)</span>, then <span class="math inline">\(P(A)\)</span> occurring <span class="math inline">\(&lt; P(B)\)</span> occurring</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/2.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>for any two events, probability of at least one occurs = the sum of their probabilities - their intersection (in other words, probabilities can not be added simply if they have non-trivial intersection)</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/3.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>for independent events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, <span class="math inline">\(P(A \:\cup\: B) = P(A) \times P(B)\)</span></li>
<li>for outcomes that can occur with different combination of events and these combinations are mutually exclusive, the <span class="math inline">\(P(E_{total}) = \sum P(E_{part})\)</span></li>
</ul>
<h3 id="conditional-probability">Conditional Probability</h3>
<ul>
<li>let B = an event so that <span class="math inline">\(P(B) &gt; 0\)</span></li>
<li><strong>conditional probability</strong> of an event A, given B is defined as the probability that BOTH A and B occurring divided by the probability of B occurring <span class="math display">\[P(A\:|\:B) = \frac{P(A \:\cap\: B)}{P(B)}\]</span></li>
<li>if A and B are <strong><em>independent</em></strong>, then <span class="math display">\[P(A\:|\:B) = \frac{P(A)P(B)}{P(B)} = P(A)\]</span></li>
<li><strong><em>example</em></strong>
<ul>
<li>for die roll, <span class="math inline">\(A = \{1\}\)</span>, <span class="math inline">\(B = \{1, 3, 5\}\)</span>, then <span class="math display">\[P(1~|~Odd) = P(A\:|\:B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A)}{P(B)} = \frac{1/6}{3/6} = \frac{1}{3}\]</span></li>
</ul></li>
</ul>
<h3 id="bayes-rule">Baye’s Rule</h3>
<ul>
<li>definition <span class="math display">\[P(B\:|\:A) = \frac{P(A\:|\:B)P(B)}{P(A\:|\:B)P(B)+P(A\:|\:B^c)P(B^c)}\]</span> where <span class="math inline">\(B^c\)</span> = corresponding probability of event <span class="math inline">\(B\)</span>, <span class="math inline">\(P(B^c) = 1 - P(B)\)</span></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="random-variables">Random Variables</h2>
<ul>
<li><strong>random variable</strong> = numeric outcome of experiment</li>
<li><strong>discrete</strong> (what you can count/categories) = assign probabilities to every number/value the variable can take
<ul>
<li>coin flip, rolling a die, web traffic in a day</li>
</ul></li>
<li><strong>continuous</strong> (any number within a continuum) = assign probabilities to the range the variable can take
<ul>
<li>BMI index, intelligence quotients</li>
<li><em><strong>Note</strong>: limitations of precision in taking the measurements may imply that the values are discrete, but we in fact consider them continuous </em></li>
</ul></li>
<li><code>rbinom()</code>, <code>rnorm()</code>, <code>rgamma()</code>, <code>rpois()</code>, <code>runif()</code> = functions to generate random variables from the binomial, normal, Gamma, Poisson, and uniform distributions</li>
<li>density and mass functions (population quantities, not what occurs in data) for random variables = best starting point to model/think about probabilities for numeric outcome of experiments (variables)
<ul>
<li>use data to estimate properties of population <span class="math inline">\(\rightarrow\)</span> linking sample to population</li>
</ul></li>
</ul>
<h3 id="probability-mass-function-pmf">Probability Mass Function (PMF)</h3>
<ul>
<li>evaluates the probability that the <strong>discrete random variable</strong> takes on a specific value
<ul>
<li>measures the chance of a particular outcome happening</li>
<li>always <span class="math inline">\(\ge\)</span> 0 for every possible outcome</li>
<li><span class="math inline">\(\sum\)</span> possible values that the variable can take = 1</li>
</ul></li>
<li><strong><em>Bernoulli distribution example</em></strong>
<ul>
<li>X = 0 <span class="math inline">\(\rightarrow\)</span> tails, X = 1 <span class="math inline">\(\rightarrow\)</span> heads
<ul>
<li>X here represents potential outcome</li>
</ul></li>
<li><span class="math inline">\(P(X = x) = (\frac{1}{2})^x(\frac{1}{2})^{1-x}\)</span> for <span class="math inline">\(X = 0, 1\)</span>
<ul>
<li><span class="math inline">\(x\)</span> here represents a value we can plug into the PMF</li>
<li>general form <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(p(x) = (\theta)^x(1-\theta)^{1-x}\)</span></li>
</ul></li>
</ul></li>
<li><code>dbinom(k, n, p)</code> = return the probability of getting <code>k</code> successes out of <code>n</code> trials, given probability of success is <code>p</code></li>
</ul>
<h3 id="probability-density-function-pdf">Probability Density Function (PDF)</h3>
<ul>
<li>evaluates the probability that the <strong>continuous random variable</strong> takes on a specific value
<ul>
<li>always <span class="math inline">\(\ge\)</span> 0 everywhere</li>
<li>total area under curve must = 1</li>
</ul></li>
<li><strong>areas under PDFs</strong> correspond to the probabilities for that random variable taking on that range of values (PMF)</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/4-1.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>but the probability of the variable taking a specific value = 0 (area of a line is 0)</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/4-2.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><p><em><strong>Note</strong>: the above is true because it is modeling random variables as if they have infinite precision, when in reality they do not </em></p></li>
<li><p><code>dnorm()</code>, <code>dgamma()</code>, <code>dpois()</code>, <code>dunif()</code> = return probability of a certain value from the normal, Gamma, Poisson, and uniform distributions</p></li>
</ul>
<h3 id="cumulative-distribution-function-cdf">Cumulative Distribution Function (CDF)</h3>
<ul>
<li>CDF of a random variable <span class="math inline">\(X\)</span> = probability that the random variable is <span class="math inline">\(\le\)</span> value <span class="math inline">\(x\)</span>
<ul>
<li><span class="math inline">\(F(x) = P(X \le x)\)</span> = applies when <span class="math inline">\(X\)</span> is discrete/continuous</li>
</ul></li>
<li>PDF = derivative of CDF
<ul>
<li>integrate PDF <span class="math inline">\(\rightarrow\)</span> CDF
<ul>
<li><code>integrate(function, lower=0, upper=1)</code> <span class="math inline">\(\rightarrow\)</span> can be used to evaluate integrals for a specified range</li>
</ul></li>
</ul></li>
<li><code>pbinom()</code>, <code>pnorm()</code>, <code>pgamma()</code>, <code>ppois()</code>, <code>punif()</code> = returns the cumulative probabilities from 0 up to a specified value from the binomial, normal, Gamma, Poisson, and uniform distributions</li>
</ul>
<h3 id="survival-function">Survival Function</h3>
<ul>
<li>survival function of a random variable <span class="math inline">\(X\)</span> = probability the random variable <span class="math inline">\(&gt; x\)</span>, complement of CDF
<ul>
<li><span class="math inline">\(S(x) = P(X &gt; x) = 1 - F(x)\)</span>, where <span class="math inline">\(F(x) =\)</span> CDF</li>
</ul></li>
</ul>
<h3 id="quantile">Quantile</h3>
<ul>
<li>the <span class="math inline">\(\alpha^{th}\)</span> quantile of a distribution with distribution function F = point <span class="math inline">\(x_{\alpha}\)</span>
<ul>
<li><span class="math inline">\(F(x_{\alpha}) = \alpha\)</span></li>
<li>percentile = quantile with <span class="math inline">\(\alpha\)</span> expressed as a percent</li>
<li>median = 50<sup>th</sup> percentile</li>
<li><span class="math inline">\(\alpha\)</span>% of the possible outcomes lie below it</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/5.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>qbeta(quantileInDecimals, 2, 1)</code> = returns quantiles for beta distribution
<ul>
<li>works for <code>qnorm()</code>, <code>qbinom()</code>, <code>qgamma()</code>, <code>qpois()</code>, etc.</li>
</ul></li>
<li>median estimated in this fashion = a population median</li>
<li>probability model connects data to population using assumptions
<ul>
<li>population median = <strong><em>estimand</em></strong>, sample median = <strong><em>estimator</em></strong></li>
</ul></li>
</ul>
<h3 id="independence">Independence</h3>
<ul>
<li>two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong><em>independent</em></strong> if the following is true
<ul>
<li><span class="math inline">\(P(A\:\cap\:B) = P(A)P(B)\)</span></li>
<li><span class="math inline">\(P(A\:|\:B) = P(A)\)</span></li>
</ul></li>
<li>two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong><em>independent</em></strong>, if for any two sets, <strong>A</strong> and <strong>B</strong>, the following is true
<ul>
<li><span class="math inline">\(P([X \in A]\cap[Y \in B]) = P(X \in A)P(Y \in B)\)</span></li>
</ul></li>
<li><strong>independence</strong> = statistically unrelated from one another</li>
<li>if <span class="math inline">\(A\)</span> is <strong><em>independent</em></strong> of <span class="math inline">\(B\)</span>, then the following are true
<ul>
<li><span class="math inline">\(A^c\)</span> is independent of <span class="math inline">\(B\)</span></li>
<li><span class="math inline">\(A\)</span> is independent of <span class="math inline">\(B^c\)</span></li>
<li><span class="math inline">\(A^c\)</span> is independent of <span class="math inline">\(B^c\)</span></li>
</ul></li>
</ul>
<h3 id="iid-random-variables">IID Random Variables</h3>
<ul>
<li>random variables are said to be <strong>IID</strong> if they are <strong><em>independent and identically distributed</em></strong>
<ul>
<li><strong>independent</strong> = statistically unrelated from each other</li>
<li><strong>identically distributed</strong> = all having been drawn from the same population distribution</li>
</ul></li>
<li>IID random variables = default model for random samples = default starting point of inference</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="diagnostic-test">Diagnostic Test</h2>
<ul>
<li>Let <span class="math inline">\(+\)</span> and <span class="math inline">\(-\)</span> be the results, positive and negative respectively, of a diagnostic test</li>
<li>Let <span class="math inline">\(D\)</span> = subject of the test has the disease, <span class="math inline">\(D^c\)</span> = subject does not</li>
<li><strong>sensitivity</strong> = <span class="math inline">\(P(+\:|\:D)\)</span> = probability that the test is positive given that the subject has the disease (the higher the better)</li>
<li><strong>specificity</strong> = <span class="math inline">\(P(-\:|\:D^c)\)</span> = probability that the test is negative given that the subject does not have the disease (the higher the better)</li>
<li><strong>positive predictive value</strong> = <span class="math inline">\(P(D\:|\:+)\)</span> = probability that that subject has the disease given that the test is positive</li>
<li><strong>negative predictive value</strong> = <span class="math inline">\(P(D^c\:|\:-)\)</span> = probability that the subject does not have the disease given the test is negative</li>
<li><strong>prevalence of disease</strong> = <span class="math inline">\(P(D)\)</span> = marginal probability of disease</li>
</ul>
<h3 id="example">Example</h3>
<ul>
<li>specificity of 98.5%, sensitivity = 99.7%, prevalence of disease = .1% <span class="math display">\[\begin{aligned}
P(D ~|~ +) &amp; = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}\\
&amp; = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + \{1-P(-~|~D^c)\}\{1 - P(D)\}} \\
&amp; = \frac{.997\times .001}{.997 \times .001 + .015 \times .999}\\
&amp; =  .062
\end{aligned}\]</span></li>
<li>low positive predictive value <span class="math inline">\(\rightarrow\)</span> due to low prevalence of disease and somewhat modest specificity
<ul>
<li>suppose it was know that the subject uses drugs and has regular intercourse with an HIV infect partner (his probability of being + is higher than suspected)</li>
<li>evidence implied by a positive test result</li>
</ul></li>
</ul>
<h3 id="likelihood-ratios">Likelihood Ratios</h3>
<ul>
<li><strong>diagnostic likelihood ratio</strong> of a <strong>positive</strong> test result is defined as <span class="math display">\[DLR_+ = \frac{sensitivity}{1-specificity} =  \frac{P(+\:|\:D)}{P(+\:|\:D^c)}\]</span></li>
<li><strong>diagnostic likelihood ratio</strong> of a <strong>negative</strong> test result is defined as <span class="math display">\[DLR_- = \frac{1 - sensitivity}{specificity} =  \frac{P(-\:|\:D)}{P(-\:|\:D^c)}\]</span></li>
<li>from Baye’s Rules, we can derive the <em>positive predictive value</em> and <em>false positive value</em> <span class="math display">\[P(D\:|\:+) = \frac{P(+\:|\:D)P(D)}{P(+\:|\:D)P(D)+P(+\:|\:D^c)P(D^c)}~~~~~~\mbox{(1)}\]</span> <span class="math display">\[P(D^c\:|\:+) = \frac{P(+\:|\:D^c)P(D^c)}{P(+\:|\:D)P(D)+P(+\:|\:D^c)P(D^c)}~~~~~~\mbox{(2)}\]</span></li>
<li>if we divide equation <span class="math inline">\((1)\)</span> over <span class="math inline">\((2)\)</span>, the quantities over have the same denominator so we get the following <span class="math display">\[\frac{P(D\:|\:+)}{P(D^c\:|\:+)} = \frac{P(+\:|\:D)}{P(+\:|\:D^c)} \times \frac{P(D)}{P(D^c)}\]</span> which can also be written as <span class="math display">\[\mbox{post-test odds of D} = DLR_+ \times \mbox{pre-test odds of D}\]</span>
<ul>
<li><strong>odds</strong> = <span class="math inline">\(p/(1-p)\)</span></li>
<li><span class="math inline">\(\frac{P(D)}{P(D^c)}\)</span> = <strong>pre-test odds</strong>, or odds of disease in absence of test</li>
<li><span class="math inline">\(\frac{P(D\:|\:+)}{P(+\:|\:D^c)}\)</span> = <strong>post-test odds</strong>, or odds of disease given a positive test result</li>
<li><span class="math inline">\(DLR_+\)</span> = factor by which the odds in the presence of a positive test can be multiplied to obtain the post-test odds</li>
<li><span class="math inline">\(DLR_-\)</span> = relates the decrease in odds of disease after a negative result</li>
</ul></li>
<li>following the previous example, for sensitivity of 0.997 and specificity of 0.985, so the diagnostic likelihood ratios are as follows <span class="math display">\[DLR_+ = .997/(1-.985) = 66 ~~~~~~ DLR_- =(1-.997)/.985 = 0.003\]</span>
<ul>
<li>this indicates that the result of the positive test is the odds of disease is 66 times the pretest odds</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="expected-valuesmean">Expected Values/Mean</h2>
<ul>
<li>useful for characterizing a distribution (properties of distributions)</li>
<li><strong>mean</strong> = characterization of the center of the distribution = <em>expected value</em></li>
<li>expected value operation = <strong><em>linear</em></strong> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(E(aX +bY) = aE(X) + bE(Y)\)</span></li>
<li><strong>variance/standard deviation</strong> = characterization of how spread out the distribution is</li>
<li><p><em>sample</em> expected values for sample mean and variance will estimate the <em>population</em> counterparts</p></li>
<li><strong>population mean</strong>
<ul>
<li>expected value/mean of a random variable = center of its distribution (center of mass)</li>
<li><strong><em>discrete variables</em></strong>
<ul>
<li>for <span class="math inline">\(X\)</span> with PMF <span class="math inline">\(p(x)\)</span>, the population mean is defined as <span class="math display">\[E[X] = \sum_{x} xp(x)\]</span> where the sum is taken over <strong><em>all</em></strong> possible values of <span class="math inline">\(x\)</span></li>
<li><span class="math inline">\(E[X]\)</span> = center of mass of a collection of location and weights <span class="math inline">\({x,~p(x)}\)</span></li>
<li><em>coin flip example</em>: <span class="math inline">\(E[X] = 0 \times (1-p) + 1 \times p = p\)</span></li>
</ul></li>
<li><strong><em>continuous variable</em></strong>
<ul>
<li>for <span class="math inline">\(X\)</span> with PDF <span class="math inline">\(f(x)\)</span>, the expected value = the center of mass of the density</li>
<li>instead of summing over discrete values, the expectation <strong><em>integrates</em></strong> over a continuous function
<ul>
<li>PDF = <span class="math inline">\(f(x)\)</span></li>
<li><span class="math inline">\(\int xf(x)\)</span> = area under the PDF curve = mean/expected value of <span class="math inline">\(X\)</span></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>sample mean</strong>
<ul>
<li>sample mean estimates the population mean
<ul>
<li>sample mean = center of mass of observed data = empirical mean <span class="math display">\[\bar X = \sum_{x}^n x_i p(x_i)\]</span> where <span class="math inline">\(p(x_i) = 1/n\)</span></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># load relevant packages</span><br><span class="line">library(UsingR); data(galton); library(ggplot2)</span><br><span class="line"># plot galton data</span><br><span class="line">g &lt;- ggplot(galton, aes(x = child))</span><br><span class="line"># add histogram for children data</span><br><span class="line">g &lt;- g + geom_histogram(fill = &quot;salmon&quot;, binwidth=1, aes(y=..density..), colour=&quot;black&quot;)</span><br><span class="line"># add density smooth</span><br><span class="line">g &lt;- g + geom_density(size = 2)</span><br><span class="line"># add vertical line</span><br><span class="line">g &lt;- g + geom_vline(xintercept = mean(galton$child), size = 2)</span><br><span class="line"># print graph</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>average of random variables</strong> = a new random variable where its distribution has an expected value that is the <strong>same</strong> as the original distribution (centers are the same)
<ul>
<li>the mean of the averages = average of the original data <span class="math inline">\(\rightarrow\)</span> estimates average of the population</li>
<li>if <span class="math inline">\(E[sample~mean]\)</span> = population mean, then estimator for the sample mean is <strong>unbiased</strong>
<ul>
<li>[<strong>derivation</strong>] let <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span>, … <span class="math inline">\(X_n\)</span> be a collection of <span class="math inline">\(n\)</span> samples from the population with mean <span class="math inline">\(\mu\)</span></li>
<li>mean of this sample <span class="math display">\[\bar X = \frac{X_1 + X_2 + X_3 +  .  + X_n}{n}\]</span></li>
<li>since <span class="math inline">\(E(aX) = aE(X)\)</span>, the expected value of the mean is can be written as <span class="math display">\[E\left[\frac{X_1 + X_2 + X_3 + ... + X_n}{n}\right] = \frac{1}{n} \times [E(X_1) + E(X_2) + E(X_3) + ... + E(X_n)]\]</span></li>
<li>since each of the <span class="math inline">\(E(X_i)\)</span> is drawn from the population with mean <span class="math inline">\(\mu\)</span>, the expected value of each sample should be <span class="math display">\[E(X_i) = \mu\]</span></li>
<li>therefore <span class="math display">\[\begin{aligned}
E\left[\frac{X_1 + X_2 + X_3 + ... + X_n}{n}\right] &amp; = \frac{1}{n} \times [E(X_1) + E(X_2) + E(X_3) + ... + E(X_n)]\\
&amp; = \frac{1}{n} \times [\mu + \mu + \mu + ... + \mu]\\
&amp; = \frac{1}{n} \times n \times \mu\\
&amp; = \mu\\
\end{aligned}\]</span></li>
</ul></li>
</ul></li>
<li><em><strong>Note</strong>: the more data that goes into the sample mean, the more concentrated its density/mass functions are around the population mean </em></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">nosim &lt;- 1000</span><br><span class="line"># simulate data for sample size 1 to 4</span><br><span class="line">dat &lt;- data.frame(</span><br><span class="line">	x = c(sample(1 : 6, nosim, replace = TRUE),</span><br><span class="line">        apply(matrix(sample(1 : 6, nosim * 2, replace = TRUE), nosim), 1, mean),</span><br><span class="line">        apply(matrix(sample(1 : 6, nosim * 3, replace = TRUE), nosim), 1, mean),</span><br><span class="line">        apply(matrix(sample(1 : 6, nosim * 4, replace = TRUE), nosim), 1, mean)),</span><br><span class="line">	size = factor(rep(1 : 4, rep(nosim, 4))))</span><br><span class="line"># plot histograms of means by sample size</span><br><span class="line">g &lt;- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.25, colour = &quot;black&quot;)</span><br><span class="line">g + facet_grid(. ~ size)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="variance">Variance</h2>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># generate x value ranges</span><br><span class="line">xvals &lt;- seq(-10, 10, by = .01)</span><br><span class="line"># generate data from normal distribution for sd of 1 to 4</span><br><span class="line">dat &lt;- data.frame(</span><br><span class="line">    y = c(dnorm(xvals, mean = 0, sd = 1),</span><br><span class="line">        dnorm(xvals, mean = 0, sd = 2),</span><br><span class="line">        dnorm(xvals, mean = 0, sd = 3),</span><br><span class="line">        dnorm(xvals, mean = 0, sd = 4)),</span><br><span class="line">    x = rep(xvals, 4),</span><br><span class="line">    factor = factor(rep(1 : 4, rep(length(xvals), 4)))</span><br><span class="line">)</span><br><span class="line"># plot 4 lines for the different standard deviations</span><br><span class="line">ggplot(dat, aes(x = x, y = y, color = factor)) + geom_line(size = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>variance</strong> = measure of spread or dispersion, the expected squared distance of the variable from its mean (expressed in <span class="math inline">\(X\)</span>’s units<span class="math inline">\(^2\)</span>)
<ul>
<li>as we can see from above, higher variances <span class="math inline">\(\rightarrow\)</span> more spread, lower <span class="math inline">\(\rightarrow\)</span> smaller spread</li>
<li><span class="math inline">\(Var(X) = E[(X-\mu)^2] = E[X^2] - E[X]^2\)</span></li>
<li><strong>standard deviation</strong> <span class="math inline">\(= \sqrt{Var(X)}\)</span> <span class="math inline">\(\rightarrow\)</span> has same units as X</li>
<li><strong><em>example</em></strong>
<ul>
<li>for die roll, <span class="math inline">\(E[X] = 3.5\)</span></li>
<li><span class="math inline">\(E[X^2] = 1^2 \times 1/6 + 2^2 \times 1/6 + 3^2 \times 1/6 + 4^2 \times 1/6 + 5^2 \times 1/6 + 6^2 \times 1/6 = 15.17\)</span></li>
<li><span class="math inline">\(Var(X) = E[X^2] - E[X]^2 \approx 2.92\)</span></li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>for coin flip, <span class="math inline">\(E[X] = p\)</span></li>
<li><span class="math inline">\(E[X^2] = 0^2 \times (1 - p) + 1^2 \times p= p\)</span></li>
<li><span class="math inline">\(Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1-p)\)</span></li>
</ul></li>
</ul></li>
</ul>
<h3 id="sample-variance">Sample Variance</h3>
<ul>
<li>the <strong>sample variance</strong> is defined as <span class="math display">\[S^2 = \frac{\sum_{i=1} (X_i - \bar X)^2}{n-1}\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/6.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li>on the above line representing the population (in magenta), any subset of data (3 of 14 selected, marked in blue) will most likely have a variance that is <strong><em>lower than</em></strong> the population variance</li>
<li>dividing by <span class="math inline">\(n - 1\)</span> will make the variance estimator <strong><em>larger</em></strong> to adjust for this fact <span class="math inline">\(\rightarrow\)</span> leads to more accurate estimation <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(S^2\)</span> = so called <strong><em>unbiased estimate of population variance</em></strong>
<ul>
<li><span class="math inline">\(S^2\)</span> is a random variable, and therefore has an associated population distribution
<ul>
<li><span class="math inline">\(E[S^2]\)</span> = population variance, where <span class="math inline">\(S\)</span> = sample standard deviation</li>
<li>as we see from the simulation results below, with more data, the distribution for <span class="math inline">\(S^2\)</span> gets more concentrated around population variance</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># specify number of simulations</span><br><span class="line">nosim &lt;- 10000;</span><br><span class="line"># simulate data for various sample sizes</span><br><span class="line">dat &lt;- data.frame(</span><br><span class="line">    x = c(apply(matrix(rnorm(nosim * 10), nosim), 1, var),</span><br><span class="line">          apply(matrix(rnorm(nosim * 20), nosim), 1, var),</span><br><span class="line">          apply(matrix(rnorm(nosim * 30), nosim), 1, var)),</span><br><span class="line">    n = factor(rep(c(&quot;10&quot;, &quot;20&quot;, &quot;30&quot;), c(nosim, nosim, nosim))) )</span><br><span class="line"># plot density function for different sample size data</span><br><span class="line">ggplot(dat, aes(x = x, fill = n)) + geom_density(size = 1, alpha = .2) +</span><br><span class="line">	geom_vline(xintercept = 1, size = 1)</span><br></pre></td></tr></table></figure>
<ul>
<li><em><strong>Note</strong>: for any variable, properties of the population = <strong>parameter</strong>, estimates of properties for samples = <strong>statistic</strong> </em>
<ul>
<li>below is a summary for the mean and variance for population and sample</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid.raster(readPNG(&quot;figures/8.png&quot;))</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>distribution for mean of random samples</strong>
<ul>
<li>expected value of the <strong>mean</strong> of distribution of means = expected value of the sample mean = population mean
<ul>
<li><span class="math inline">\(E[\bar X]=\mu\)</span></li>
</ul></li>
<li>expected value of the variance of distribution of means
<ul>
<li><span class="math inline">\(Var(\bar X) = \sigma^2/n\)</span></li>
<li>as <strong>n</strong> becomes larger, the mean of random sample <span class="math inline">\(\rightarrow\)</span> more concentrated around the population mean <span class="math inline">\(\rightarrow\)</span> variance approaches 0
<ul>
<li>this again confirms that sample mean estimates population mean</li>
</ul></li>
</ul></li>
<li><em><strong>Note</strong>: normally we only have 1 sample mean (from collected sample) and can estimate the variance <span class="math inline">\(\sigma^2\)</span> <span class="math inline">\(\rightarrow\)</span> so we know a lot about the <strong>distribution of the means</strong> from the data observed </em></li>
</ul></li>
<li><strong>standard error (SE)</strong>
<ul>
<li>the standard error of the mean is defined as <span class="math display">\[SE_{mean} = \sigma/\sqrt{n}\]</span></li>
<li>this quantity is effectively the standard deviation of the distribution of a statistic (i.e. mean)</li>
<li>represents variability of means</li>
</ul></li>
</ul>
<h3 id="entire-estimator-estimation-relationship">Entire Estimator-Estimation Relationship</h3>
<ul>
<li>Start with a sample</li>
<li><span class="math inline">\(S^2\)</span> = sample variance
<ul>
<li>estimates how variable the population is</li>
<li>estimates population variance <span class="math inline">\(\sigma^2\)</span></li>
<li><span class="math inline">\(S^2\)</span> = a random variable and has its own distribution centered around <span class="math inline">\(\sigma^2\)</span>
<ul>
<li>more concentrated around <span class="math inline">\(\sigma^2\)</span> as <span class="math inline">\(n\)</span> increases</li>
</ul></li>
</ul></li>
<li><span class="math inline">\(\bar X\)</span> = sample mean
<ul>
<li>estimates population mean <span class="math inline">\(\mu\)</span></li>
<li><span class="math inline">\(\bar X\)</span> = a random variable and has its own distribution centered around <span class="math inline">\(\mu\)</span>
<ul>
<li>more concentrated around <span class="math inline">\(\mu\)</span> as <span class="math inline">\(n\)</span> increases</li>
<li>variance of distribution of <span class="math inline">\(\bar X = \sigma^2/n\)</span></li>
<li>estimate of variance = <span class="math inline">\(S^2/n\)</span></li>
<li>estimate of standard error = <span class="math inline">\(S/\sqrt{n}\)</span> <span class="math inline">\(\rightarrow\)</span> “sample standard error of the mean”
<ul>
<li>estimates how variable sample means (<span class="math inline">\(n\)</span> size) from the population are</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h3 id="example---standard-normal">Example - Standard Normal</h3>
<ul>
<li>variance = 1</li>
<li>means of <strong>n</strong> standard normals (sample) have standard deviation = <span class="math inline">\(1/\sqrt{n}\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># specify number of simulations with 10 as number of observations per sample</span><br><span class="line">nosim &lt;- 1000; n &lt;-10</span><br><span class="line"># estimated standard deviation of mean</span><br><span class="line">sd(apply(matrix(rnorm(nosim * n), nosim), 1, mean))</span><br><span class="line"># actual standard deviation of mean of standard normals</span><br><span class="line">1 / sqrt(n)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>rnorm()</code> = generate samples from the standard normal</li>
<li><code>matrix()</code> = puts all samples into a nosim by <span class="math inline">\(n\)</span> matrix, so that each row represents a simulation with <code>nosim</code> observations</li>
<li><code>apply()</code> = calculates the mean of the <span class="math inline">\(n\)</span> samples</li>
<li><code>sd()</code> = returns standard deviation</li>
</ul>
<h3 id="example---standard-uniform">Example - Standard Uniform</h3>
<ul>
<li>standard uniform <span class="math inline">\(\rightarrow\)</span> triangle straight line distribution <span class="math inline">\(\rightarrow\)</span> mean = 1/2 and variance = 1/12</li>
<li>means of random samples of <span class="math inline">\(n\)</span> uniforms have have standard deviation of <span class="math inline">\(1/\sqrt{12 \times n}\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># estimated standard deviation of the sample means</span><br><span class="line">sd(apply(matrix(runif(nosim * n), nosim), 1, mean))</span><br><span class="line"># actual standard deviation of the means</span><br><span class="line">1/sqrt(12*n)</span><br></pre></td></tr></table></figure>
<h3 id="example---poisson">Example - Poisson</h3>
<ul>
<li><span class="math inline">\(Poisson(x^2)\)</span> have variance of <span class="math inline">\(x^2\)</span></li>
<li>means of random samples of <span class="math inline">\(n~ Poisson(4)\)</span> have standard deviation of <span class="math inline">\(2/\sqrt{n}\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># estimated standard deviation of the sample means</span><br><span class="line">sd(apply(matrix(rpois(nosim * n, lambda=4), nosim), 1, mean))</span><br><span class="line"># actual standard deviation of the means</span><br><span class="line">2/sqrt(n)</span><br></pre></td></tr></table></figure>
<h3 id="example---bernoulli">Example - Bernoulli</h3>
<ul>
<li>for <span class="math inline">\(p = 0.5\)</span>, the Bernoulli distribution has variance of 0.25</li>
<li>means of random samples of <span class="math inline">\(n\)</span> coin flips have standard deviations of <span class="math inline">\(1 / (2 \sqrt{n})\)</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># estimated standard deviation of the sample means</span><br><span class="line">sd(apply(matrix(sample(0 : 1, nosim * n, replace = TRUE), nosim), 1, mean))</span><br><span class="line"># actual standard deviation of the means</span><br><span class="line">1/(2*sqrt(n))</span><br></pre></td></tr></table></figure>
<h3 id="example---fatherson">Example - Father/Son</h3>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">library(UsingR); data(father.son);</span><br><span class="line"># define son height as the x variable</span><br><span class="line">x &lt;- father.son$sheight</span><br><span class="line"># n is the length</span><br><span class="line">n&lt;-length(x)</span><br><span class="line"># plot histogram for son&apos;s heights</span><br><span class="line">g &lt;- ggplot(data = father.son, aes(x = sheight))</span><br><span class="line">g &lt;- g + geom_histogram(aes(y = ..density..), fill = &quot;lightblue&quot;, binwidth=1, colour = &quot;black&quot;)</span><br><span class="line">g &lt;- g + geom_density(size = 2, colour = &quot;black&quot;)</span><br><span class="line">g</span><br><span class="line"># we calculate the parameters for variance of distribution and sample mean,</span><br><span class="line">round(c(sampleVar = var(x),</span><br><span class="line">	sampleMeanVar = var(x) / n,</span><br><span class="line">	# as well as standard deviation of distribution and sample mean</span><br><span class="line">	sampleSd = sd(x),</span><br><span class="line">	sampleMeanSd = sd(x) / sqrt(n)),2)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="binomial-distribution">Binomial Distribution</h2>
<ul>
<li><strong>binomial random variable</strong> = sum of <strong>n</strong> Bernoulli variables <span class="math display">\[X = \sum_{i=1}^n X_i\]</span> where <span class="math inline">\(X_1,\ldots,X_n = Bernoulli(p)\)</span>
<ul>
<li>PMF is defined as <span class="math display">\[P(X=x) = {n \choose x}p^x(1-p)^{n-x}\]</span> where <span class="math inline">\({n \choose x}\)</span> = number of ways selecting <span class="math inline">\(x\)</span> items out of <span class="math inline">\(n\)</span> options without replacement or regard to order and for <span class="math inline">\(x=0,\ldots,n\)</span></li>
<li><strong>combination</strong> or “<span class="math inline">\(n\)</span> choose <span class="math inline">\(x\)</span>” is defined as <span class="math display">\[{n \choose x} = \frac{n!}{x!(n-x)!}\]</span></li>
<li>the base cases are <span class="math display">\[{n \choose n} = {n \choose 0} = 1\]</span></li>
</ul></li>
<li><strong>Bernoulli distribution</strong> = binary outcome
<ul>
<li>only possible outcomes
<ul>
<li>1 = “success” with probability of <span class="math inline">\(p\)</span></li>
<li>0 = “failure” with probability of <span class="math inline">\(1 - p\)</span></li>
</ul></li>
<li>PMF is defined as <span class="math display">\[P(X=x) = p^x(1 - p)^{1-x}\]</span></li>
<li>mean = <span class="math inline">\(p\)</span></li>
<li>variance = <span class="math inline">\(p(1 - p)\)</span></li>
</ul></li>
</ul>
<h3 id="example-1">Example</h3>
<ul>
<li>of 8 children, whats the probability of 7 or more girls (50/50 chance)? <span class="math display">\[{8 \choose 7}.5^7(1-.5)^{1} + {8 \choose 8}.5^8(1-.5)^{0} \approx 0.04\]</span></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># calculate probability using PMF</span><br><span class="line">choose(8, 7) * .5 ^ 8 + choose(8, 8) * .5 ^ 8</span><br><span class="line"># calculate probability using CMF from distribution</span><br><span class="line">pbinom(6, size = 8, prob = .5, lower.tail = FALSE)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>choose(8, 7)</code> = R function to calculate n choose x</li>
<li><code>pbinom(6, size=8, prob =0.5, lower.tail=TRUE)</code> = probability of 6 or less successes out of 8 samples with probability of 0.5 (CMF)
<ul>
<li><code>lower.tail=FALSE</code> = returns the complement, in this case it’s the probability of greater than 6 successes out of 8 samples with probability of 0.5</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="normal-distribution">Normal Distribution</h2>
<ul>
<li>normal/Gaussian distribution for random variable X
<ul>
<li>notation = <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span></li>
<li>mean = <span class="math inline">\(E[X] = \mu\)</span></li>
<li>variance = <span class="math inline">\(Var(X) = \sigma^2\)</span></li>
<li>PMF is defined as <span class="math display">\[f(x)=(2\pi\sigma^2)^{-1/2}e^{-(x-\mu)^2/2\sigma^2}\]</span></li>
</ul></li>
<li><span class="math inline">\(X \sim N(0, 1)\)</span> = <strong>standard normal distribution</strong> (standard normal random variables often denoted using <span class="math inline">\(Z_1, Z_2, \ldots\)</span>)
<ul>
<li><em><strong>Note</strong>: see below graph for reference for the following observations </em></li>
<li>~68% of data/normal density <span class="math inline">\(\rightarrow\)</span> between <span class="math inline">\(\pm\)</span> 1 standard deviation from <span class="math inline">\(\mu\)</span></li>
<li>~95% of data/normal density <span class="math inline">\(\rightarrow\)</span> between <span class="math inline">\(\pm\)</span> 2 standard deviation from <span class="math inline">\(\mu\)</span></li>
<li>~99% of data/normal density <span class="math inline">\(\rightarrow\)</span> between <span class="math inline">\(\pm\)</span> 3 standard deviation from <span class="math inline">\(\mu\)</span></li>
<li><span class="math inline">\(\pm\)</span> 1.28 standard deviations from <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> 10<span class="math inline">\(^{th}\)</span> (-) and 90<span class="math inline">\(^{th}\)</span> (+) percentiles</li>
<li><span class="math inline">\(\pm\)</span> 1.645 standard deviations from <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> 5<span class="math inline">\(^{th}\)</span> (-) and 95<span class="math inline">\(^{th}\)</span> (+) percentiles</li>
<li><span class="math inline">\(\pm\)</span> 1.96 standard deviations from <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> 2.5<span class="math inline">\(^{th}\)</span> (-) and 97.5<span class="math inline">\(^{th}\)</span> (+) percentiles</li>
<li><span class="math inline">\(\pm\)</span> 2.33 standard deviations from <span class="math inline">\(\mu\)</span> <span class="math inline">\(\rightarrow\)</span> 1<span class="math inline">\(^{st}\)</span> (-) and 99<span class="math inline">\(^{th}\)</span> (+) percentiles</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># plot standard normal</span><br><span class="line">x &lt;- seq(-3, 3, length = 1000)</span><br><span class="line">g &lt;- ggplot(data.frame(x = x, y = dnorm(x)),</span><br><span class="line">            aes(x = x, y = y)) + geom_line(size = 2)</span><br><span class="line">g &lt;- g + geom_vline(xintercept = -3 : 3, size = 2)</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<ul>
<li>for any <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, calculating the number of standard deviations each observation is from the mean <strong><em>converts</em></strong> the random variable to a <strong><em>standard normal</em></strong> (denoted as <span class="math inline">\(Z\)</span> below) <span class="math display">\[Z=\frac{X-\mu}{\sigma} \sim N(0,1)\]</span></li>
<li>conversely, a standard normal can then be converted to <strong><em>any normal distribution</em></strong> by multiplying by standard deviation and adding the mean <span class="math display">\[X = \mu + \sigma Z \sim N(\mu, \sigma^2)\]</span></li>
<li><code>qnorm(n, mean=mu, sd=sd)</code> = returns the <span class="math inline">\(n^{th}\)</span> percentiles for the given normal distribution</li>
<li><code>pnorm(x, mean=mu, sd=sd, lower.tail=F)</code> = returns the probability of an observation drawn from the given distribution is larger in value than the specified threshold <span class="math inline">\(x\)</span></li>
</ul>
<h3 id="example-2">Example</h3>
<ul>
<li>the number of daily ad clicks for a company is (approximately) normally distributed with a mean of 1020 and a standard deviation of 50</li>
<li>What’s the probability of getting more than 1,160 clicks in a day?</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># calculate number of standard deviations from the mean</span><br><span class="line">(1160 - 1020) / 50</span><br><span class="line"># calculate probability using given distribution</span><br><span class="line">pnorm(1160, mean = 1020, sd = 50, lower.tail = FALSE)</span><br><span class="line"># calculate probability using standard normal</span><br><span class="line">pnorm(2.8, lower.tail = FALSE)</span><br></pre></td></tr></table></figure>
<ul>
<li>therefore, it is not very likely (<code>r pnorm(2.8, lower.tail = FALSE)*100</code>% chance), since 1,160 is <code>r (1160 - 1020) / 50</code> standard deviations from the mean</li>
<li>What number of daily ad clicks would represent the one where 75% of days have fewer clicks (assuming days are independent and identically distributed)?</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qnorm(0.75, mean = 1020, sd = 50)</span><br></pre></td></tr></table></figure>
<ul>
<li>therefore, <code>r qnorm(0.75, mean = 1020, sd = 50)</code> would represent the threshold that has more clicks than 75% of days</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="poisson-distribution">Poisson Distribution</h2>
<ul>
<li>used to model counts
<ul>
<li>mean = <span class="math inline">\(\lambda\)</span></li>
<li>variance = <span class="math inline">\(\lambda\)</span></li>
<li>PMF is defined as <span class="math display">\[P(X = x; \lambda)=\frac{\lambda^xe^{-\lambda}}{x!}\]</span> where <span class="math inline">\(X = 0, 1, 2, ... \infty\)</span></li>
</ul></li>
<li>modeling uses for Poisson distribution
<ul>
<li>count data</li>
<li>event-time/survival <span class="math inline">\(\rightarrow\)</span> cancer trials, some patients never develop and some do, dealing with the data for both (“censoring”)</li>
<li>contingency tables <span class="math inline">\(\rightarrow\)</span> record results for different characteristic measurements</li>
<li>approximating binomials <span class="math inline">\(\rightarrow\)</span> instances where <strong>n</strong> is large and <strong>p</strong> is small (i.e. pollution on lung disease)
<ul>
<li><span class="math inline">\(X \sim Binomial(n, p)\)</span></li>
<li><span class="math inline">\(\lambda = np\)</span></li>
</ul></li>
<li>rates <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(X \sim Poisson(\lambda t)\)</span>
<ul>
<li><span class="math inline">\(\lambda = E[X/t]\)</span> <span class="math inline">\(\rightarrow\)</span> expected count per unit of time</li>
<li><span class="math inline">\(t\)</span> = total monitoring time</li>
</ul></li>
</ul></li>
<li><code>ppois(n, lambda = lambda*t)</code> = returns probability of <span class="math inline">\(n\)</span> or fewer events happening given the rate <span class="math inline">\(\lambda\)</span> and time <span class="math inline">\(t\)</span></li>
</ul>
<h3 id="example-3">Example</h3>
<ul>
<li>number of people that show up at a bus stop can be modeled with Poisson distribution with a mean of 2.5 per hour</li>
<li>after watching the bus stop for 4 hours, what is the probability that 3 or fewer people show up for the whole time?</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># calculate using distribution</span><br><span class="line">ppois(3, lambda = 2.5 * 4)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, there is a <code>r ppois(3, lambda = 2.5 * 4)*100</code>% chance for 3 or fewer people show up total at the bus stop during 4 hours of monitoring</li>
</ul>
<h3 id="example---approximating-binomial-distribution">Example - Approximating Binomial Distribution</h3>
<ul>
<li>flip a coin with success probability of 0.01 a total 500 times (low <span class="math inline">\(p\)</span>, large <span class="math inline">\(n\)</span>)</li>
<li>what’s the probability of 2 or fewer successes?</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># calculate correct probability from Binomial distribution</span><br><span class="line">pbinom(2, size = 500, prob = .01)</span><br><span class="line"># estimate probability using Poisson distribution</span><br><span class="line">ppois(2, lambda=500 * .01)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the two probabilities (<code>r pbinom(2, size = 500, prob = .01)*100</code>% vs <code>r pbinom(2, size = 500, prob = .01)*100</code>%) are extremely close</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="asymptotics">Asymptotics</h2>
<ul>
<li><strong>asymptotics</strong> = behavior of statistics as sample size <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\infty\)</span></li>
<li>useful for simple statistical inference/approximations</li>
<li>form basis for frequentist interpretation of probabilities (“Law of Large Numbers”)</li>
</ul>
<h3 id="law-of-large-numbers-lln">Law of Large Numbers (LLN)</h3>
<ul>
<li>IID sample statistic that estimates property of the sample (i.e. mean, variance) <strong><em>becomes</em></strong> the population statistic (i.e. population mean, population variance) as <span class="math inline">\(n\)</span> increases</li>
<li><em><strong>Note</strong>: an estimator is <strong>consistent</strong> if it converges to what it is estimating </em></li>
<li>sample mean/variance/standard deviation are all <strong><em>consistent estimators</em></strong> for their population counterparts
<ul>
<li><span class="math inline">\(\bar X_n\)</span> is average of the result of <span class="math inline">\(n\)</span> coin flips (i.e. the sample proportion of heads)</li>
<li>as we flip a fair coin over and over, it <strong><em>eventually converges</em></strong> to the true probability of a head</li>
</ul></li>
</ul>
<h3 id="example---lln-for-normal-and-bernoulli-distribution">Example - LLN for Normal and Bernoulli Distribution</h3>
<ul>
<li>for this example, we will simulate 10000 samples from the normal and Bernoulli distributions respectively</li>
<li>we will plot the distribution of sample means as <span class="math inline">\(n\)</span> increases and compare it to the population means</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># load library</span><br><span class="line">library(gridExtra)</span><br><span class="line"># specify number of trials</span><br><span class="line">n &lt;- 10000</span><br><span class="line"># calculate sample (from normal distribution) means for different size of n</span><br><span class="line">means &lt;- cumsum(rnorm(n)) / (1  : n)</span><br><span class="line"># plot sample size vs sample mean</span><br><span class="line">g &lt;- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y))</span><br><span class="line">g &lt;- g + geom_hline(yintercept = 0) + geom_line(size = 2)</span><br><span class="line">g &lt;- g + labs(x = &quot;Number of obs&quot;, y = &quot;Cumulative mean&quot;)</span><br><span class="line">g &lt;- g + ggtitle(&quot;Normal Distribution&quot;)</span><br><span class="line"># calculate sample (coin flips) means for different size of n</span><br><span class="line">means &lt;- cumsum(sample(0 : 1, n , replace = TRUE)) / (1  : n)</span><br><span class="line"># plot sample size vs sample mean</span><br><span class="line">p &lt;- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y))</span><br><span class="line">p &lt;- p + geom_hline(yintercept = 0.5) + geom_line(size = 2)</span><br><span class="line">p &lt;- p + labs(x = &quot;Number of obs&quot;, y = &quot;Cumulative mean&quot;)</span><br><span class="line">p &lt;- p + ggtitle(&quot;Bernoulli Distribution (Coin Flip)&quot;)</span><br><span class="line"># combine plots</span><br><span class="line">grid.arrange(g, p, ncol = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, for both distributions the sample means undeniably approach the respective population means as <span class="math inline">\(n\)</span> increases</li>
</ul>
<h3 id="central-limit-theorem">Central Limit Theorem</h3>
<ul>
<li>one of the most important theorems in statistics</li>
<li>distribution of means of IID variables approaches the standard normal as sample size <span class="math inline">\(n\)</span> increases</li>
<li>in other words, for large values of <span class="math inline">\(n\)</span>, <span class="math display">\[\frac{\mbox{Estimate} - \mbox{Mean of Estimate}}{\mbox{Std. Err. of Estimate}} = \frac{\bar X_n - \mu}{\sigma / \sqrt{n}}=\frac{\sqrt n (\bar X_n - \mu)}{\sigma} \longrightarrow N(0, 1)\]</span></li>
<li>this translates to the distribution of the sample mean <span class="math inline">\(\bar X_n\)</span> is approximately <span class="math inline">\(N(\mu, \sigma^2/n)\)</span>
<ul>
<li>distribution is centered at the population mean</li>
<li>with standard deviation = standard error of the mean</li>
</ul></li>
<li>typically the Central Limit Theorem can be applied when <span class="math inline">\(n \geq 30\)</span></li>
</ul>
<h3 id="example---clt-with-bernoulli-trials-coin-flips">Example - CLT with Bernoulli Trials (Coin Flips)</h3>
<ul>
<li>for this example, we will simulate <span class="math inline">\(n\)</span> flips of a possibly unfair coin
<ul>
<li>let <span class="math inline">\(X_i\)</span> be the 0 or 1 result of the <span class="math inline">\(i^{th}\)</span> flip of a possibly unfair coin</li>
<li>sample proportion , <span class="math inline">\(\hat p\)</span>, is the average of the coin flips</li>
<li><span class="math inline">\(E[X_i] = p\)</span> and <span class="math inline">\(Var(X_i) = p(1-p)\)</span></li>
<li>standard error of the mean is <span class="math inline">\(SE = \sqrt{p(1-p)/n}\)</span></li>
</ul></li>
<li>in principle, normalizing the random variable <span class="math inline">\(X_i\)</span>, we should get an approximately standard normal distribution <span class="math display">\[\frac{\hat p - p}{\sqrt{p(1-p)/n}} \sim N(0,~1)\]</span></li>
<li>therefore, we will flip a coin <span class="math inline">\(n\)</span> times, take the sample proportion of heads (successes with probability <span class="math inline">\(p\)</span>), subtract off 0.5 (ideal sample proportion) and multiply the result by <span class="math inline">\(\frac{1}{2 \sqrt{n}}\)</span> and compare it to the standard normal</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># specify number of simulations</span><br><span class="line">nosim &lt;- 1000</span><br><span class="line"># convert to standard normal</span><br><span class="line">cfunc &lt;- function(x, n) 2 * sqrt(n) * (mean(x) - 0.5)</span><br><span class="line"># simulate data for sample sizes 10, 20, and 30</span><br><span class="line">dat &lt;- data.frame(</span><br><span class="line">	x = c(apply(matrix(sample(0:1, nosim*10, replace=TRUE), nosim), 1, cfunc, 10),</span><br><span class="line">        apply(matrix(sample(0:1, nosim*20, replace=TRUE), nosim), 1, cfunc, 20),</span><br><span class="line">        apply(matrix(sample(0:1, nosim*30, replace=TRUE), nosim), 1, cfunc, 30)),</span><br><span class="line">	size = factor(rep(c(10, 20, 30), rep(nosim, 3))))</span><br><span class="line"># plot histograms for the trials</span><br><span class="line">g &lt;- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3,</span><br><span class="line">	colour = &quot;black&quot;, aes(y = ..density..))</span><br><span class="line"># plot standard normal distribution for reference</span><br><span class="line">g &lt;- g + stat_function(fun = dnorm, size = 2)</span><br><span class="line"># plot panel plots by sample size</span><br><span class="line">g + facet_grid(. ~ size)</span><br></pre></td></tr></table></figure>
<ul>
<li>now, we can run the same simulation trials for an extremely unfair coin with <span class="math inline">\(p\)</span> = 0.9</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># specify number of simulations</span><br><span class="line">nosim &lt;- 1000</span><br><span class="line"># convert to standard normal</span><br><span class="line">cfunc &lt;- function(x, n) sqrt(n) * (mean(x) - 0.9) / sqrt(.1 * .9)</span><br><span class="line"># simulate data for sample sizes 10, 20, and 30</span><br><span class="line">dat &lt;- data.frame(</span><br><span class="line">	x = c(apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 10, replace = TRUE),</span><br><span class="line">                     nosim), 1, cfunc, 10),</span><br><span class="line">        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 20, replace = TRUE),</span><br><span class="line">                     nosim), 1, cfunc, 20),</span><br><span class="line">        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 30, replace = TRUE),</span><br><span class="line">                     nosim), 1, cfunc, 30)),</span><br><span class="line">	size = factor(rep(c(10, 20, 30), rep(nosim, 3))))</span><br><span class="line"># plot histograms for the trials</span><br><span class="line">g &lt;- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3,</span><br><span class="line">	colour = &quot;black&quot;, aes(y = ..density..))</span><br><span class="line"># plot standard normal distribution for reference</span><br><span class="line">g &lt;- g + stat_function(fun = dnorm, size = 2)</span><br><span class="line"># plot panel plots by sample size</span><br><span class="line">g + facet_grid(. ~ size)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from both simulations, the converted/standardized distribution of the samples convert to the standard normal distribution</li>
<li><em><strong>Note</strong>: speed at which the normalized coin flips converge to normal distribution depends on how biased the coin is (value of <span class="math inline">\(p\)</span>) </em></li>
<li><em><strong>Note</strong>: does not guarantee that the normal distribution will be a good approximation, but just that eventually it will be a good approximation as n <span class="math inline">\(\rightarrow \infty\)</span> </em></li>
</ul>
<h3 id="confidence-intervals---normal-distributionz-intervals">Confidence Intervals - Normal Distribution/Z Intervals</h3>
<ul>
<li><strong>Z confidence interval</strong> is defined as <span class="math display">\[Estimate \pm ZQ \times SE_{Estimate}\]</span> where <span class="math inline">\(ZQ\)</span> = quantile from the standard normal distribution</li>
<li>according to CLT, the sample mean, <span class="math inline">\(\bar X\)</span>, is approximately normal with mean <span class="math inline">\(\mu\)</span> and sd <span class="math inline">\(\sigma / \sqrt{n}\)</span></li>
<li><strong>95% confidence interval for the population mean <span class="math inline">\(\mu\)</span></strong> is defined as <span class="math display">\[\bar X \pm 2\sigma/\sqrt{n}\]</span> for the sample mean <span class="math inline">\(\bar X \sim N(\mu, \sigma^2/n)\)</span>
<ul>
<li>you can choose to use 1.96 to be more accurate for the confidence interval</li>
<li><span class="math inline">\(P(\bar{X} &gt; \mu + 2\sigma/\sqrt{n}~or~\bar{X} &lt; \mu - 2\sigma/\sqrt{n}) = 5\%\)</span></li>
<li><strong>interpretation</strong>: if we were to repeatedly draw samples of size <span class="math inline">\(n\)</span> from the population and construct this confidence interval for each case, approximately 95% of the intervals will contain <span class="math inline">\(\mu\)</span></li>
</ul></li>
<li>confidence intervals get <strong>narrower</strong> with less variability or larger sample sizes</li>
<li><em><strong>Note</strong>: Poisson and binomial distributions have exact intervals that don’t require CLT </em></li>
<li><strong><em>example</em></strong>
<ul>
<li>for this example, we will compute the 95% confidence interval for sons height data in inches</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># load son height data</span><br><span class="line">data(father.son); x &lt;- father.son$sheight</span><br><span class="line"># calculate confidence interval for sons height in inches</span><br><span class="line">mean(x) + c(-1, 1) * qnorm(0.975) * sd(x)/sqrt(length(x))</span><br></pre></td></tr></table></figure>
<h3 id="confidence-interval---bernoulli-distributionwald-interval">Confidence Interval - Bernoulli Distribution/Wald Interval</h3>
<ul>
<li>for Bernoulli distributions, <span class="math inline">\(X_i\)</span> is 0 or 1 with success probability <span class="math inline">\(p\)</span> and the variance is <span class="math inline">\(\sigma^2 = p(1 - p)\)</span></li>
<li>the confidence interval takes the form of <span class="math display">\[\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}\]</span></li>
<li>since the population proportion <span class="math inline">\(p\)</span> is unknown, we can use the sampled proportion of success <span class="math inline">\(\hat{p} = X/n\)</span> as estimate</li>
<li><span class="math inline">\(p(1-p)\)</span> is largest when <span class="math inline">\(p = 1/2\)</span>, so 95% confidence interval can be calculated by <span class="math display">\[\begin{aligned}
\hat{p} \pm Z_{0.95} \sqrt{\frac{0.5(1-0.5)}{n}} &amp; = \hat{p} \pm qnorm(.975) \sqrt{\frac{1}{4n}}\\
&amp; = \hat{p} \pm 1.96 \sqrt{\frac{1}{4n}}\\
&amp; = \hat{p} \pm \frac{1.96}{2} \sqrt{\frac{1}{n}}\\
&amp; \approx \hat{p} \pm \frac{1}{\sqrt{n}}\\
\end{aligned}\]</span>
<ul>
<li>this is known as the <strong>Wald Confidence Interval</strong> and is useful in <strong><em>roughly estimating</em></strong> confidence intervals</li>
<li>generally need <span class="math inline">\(n\)</span> = 100 for 1 decimal place, 10,000 for 2, and 1,000,000 for 3</li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>suppose a random sample of 100 likely voters, 56 intent to vote for you, can you secure a victory?</li>
<li>we can use the Wald interval to quickly estimate the 95% confidence interval</li>
<li>as we can see below, because the interval [<code>r 0.56 + c(-1, 1) * 1/sqrt(100)</code>] contains values below 50%, victory is not guaranteed</li>
<li><code>binom.test(k, n)$conf</code> = returns confidence interval binomial distribution (collection of Bernoulli trial) with <code>k</code> successes in <code>n</code> draws</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>message </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># define sample probability and size</span><br><span class="line">p = 0.56; n = 100</span><br><span class="line"># Wald interval</span><br><span class="line">c(&quot;WaldInterval&quot; = p + c(-1, 1) * 1/sqrt(n))</span><br><span class="line"># 95% confidence interval</span><br><span class="line">c(&quot;95CI&quot; = p + c(-1, 1) * qnorm(.975) * sqrt(p * (1-p)/n))</span><br><span class="line"># perform binomial test</span><br><span class="line">binom.test(p*100, n*100)$conf.int</span><br></pre></td></tr></table></figure>
<h3 id="confidence-interval---binomial-distributionagresti-coull-interval">Confidence Interval - Binomial Distribution/Agresti-Coull Interval</h3>
<ul>
<li>for a binomial distribution with smaller values of <span class="math inline">\(n\)</span> (when <span class="math inline">\(n\)</span> &lt; 30, thus not large enough for CLT), often time the normal confidence intervals, as defined by <span class="math display">\[\hat{p} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}\]</span> <strong>do not</strong> provide accurate estimates</li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># simulate 1000 samples of size 20 each</span><br><span class="line">n &lt;- 20; nosim &lt;- 1000</span><br><span class="line"># simulate for p values from 0.1 to 0.9</span><br><span class="line">pvals &lt;- seq(.1, .9, by = .05)</span><br><span class="line"># calculate the confidence intervals</span><br><span class="line">coverage &lt;- sapply(pvals, function(p)&#123;</span><br><span class="line">	# simulate binomial data</span><br><span class="line">	phats &lt;- rbinom(nosim, prob = p, size = n) / n</span><br><span class="line">	# calculate lower 95% CI bound</span><br><span class="line">	ll &lt;- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)</span><br><span class="line">	# calculate upper 95% CI bound</span><br><span class="line">	ul &lt;- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)</span><br><span class="line">	# calculate percent of intervals that contain p</span><br><span class="line">	mean(ll &lt; p &amp; ul &gt; p)</span><br><span class="line">&#125;)</span><br><span class="line"># plot CI results vs 95%</span><br><span class="line">ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the interval do not provide adequate coverage as 95% confidence intervals (frequently only provide 80 to 90% coverage)</li>
<li>we can construct the <strong>Agresti-Coull Interval</strong>, which is defined uses the adjustment <span class="math display">\[\hat{p} = \frac{X+2}{n+4}\]</span> where we effectively <strong><em>add 2</em></strong> to number of successes, <span class="math inline">\(X\)</span>, and <strong><em>add 2</em></strong> to number of failure</li>
<li>therefore the interval becomes <span class="math display">\[\frac{X+2}{n+4} \pm z_{1-\alpha/2}\sqrt{\frac{p(1-p)}{n}}\]</span></li>
<li><em><strong>Note</strong>: interval tend to be <strong>conservative</strong> </em></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># simulate 1000 samples of size 20 each</span><br><span class="line">n &lt;- 20; nosim &lt;- 1000</span><br><span class="line"># simulate for p values from 0.1 to 0.9</span><br><span class="line">pvals &lt;- seq(.1, .9, by = .05)</span><br><span class="line"># calculate the confidence intervals</span><br><span class="line">coverage &lt;- sapply(pvals, function(p)&#123;</span><br><span class="line">	# simulate binomial data with Agresti/Coull Interval adjustment</span><br><span class="line">	phats &lt;- (rbinom(nosim, prob = p, size = n) + 2) / (n + 4)</span><br><span class="line">		# calculate lower 95% CI bound</span><br><span class="line">	ll &lt;- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)</span><br><span class="line">	# calculate upper 95% CI bound</span><br><span class="line">	ul &lt;- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)</span><br><span class="line">	# calculate percent of intervals that contain p</span><br><span class="line">	mean(ll &lt; p &amp; ul &gt; p)</span><br><span class="line">&#125;)</span><br><span class="line"># plot CI results vs 95%</span><br><span class="line">ggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)</span><br><span class="line">`</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, the coverage is much better for the 95% interval</li>
<li>in fact, all of the estimates are more conservative as we previously discussed, indicating the Agresti-Coull intervals are <strong><em>wider</em></strong> than the regular confidence intervals</li>
</ul>
<h3 id="confidence-interval---poisson-interval">Confidence Interval - Poisson Interval</h3>
<ul>
<li>for <span class="math inline">\(X \sim Poisson(\lambda t)\)</span>
<ul>
<li>estimate rate <span class="math inline">\(\hat{\lambda} = X/t\)</span></li>
<li><span class="math inline">\(var(\hat{\lambda}) = \lambda/t\)</span></li>
<li>variance estimate <span class="math inline">\(= \hat{\lambda}/t\)</span></li>
</ul></li>
<li>so the confidence interval is defined as <span class="math display">\[\hat \lambda \pm z_{1-\alpha/2}\sqrt{\frac{\lambda}{t}}\]</span>
<ul>
<li>however, for small values of <span class="math inline">\(\lambda\)</span> (few events larger time interval), we <strong>should not</strong> use the asymptotic interval estimated</li>
<li><strong><em>example</em></strong>
<ul>
<li>for this example, we will go through a specific scenario as well as a simulation exercise to demonstrate the ineffectiveness of asymptotic intervals for small values of <span class="math inline">\(\lambda\)</span></li>
<li>nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate per day?</li>
<li><code>poisson.test(x, T)$conf</code> = returns Poisson 95% confidence interval for given <code>x</code> occurrence over <code>T</code> time period</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># define parameters</span><br><span class="line">x &lt;- 5; t &lt;- 94.32; lambda &lt;- x / t</span><br><span class="line"># calculate confidence interval</span><br><span class="line">round(lambda + c(-1, 1) * qnorm(.975) * sqrt(lambda / t), 3)</span><br><span class="line"># return accurate confidence interval from poisson.test</span><br><span class="line">poisson.test(x, T = 94.32)$conf</span><br><span class="line"># small lambda simulations</span><br><span class="line">lambdavals &lt;- seq(0.005, 0.10, by = .01); nosim &lt;- 1000; t &lt;- 100</span><br><span class="line"># calculate coverage using Poisson intervals</span><br><span class="line">coverage &lt;- sapply(lambdavals, function(lambda)&#123;</span><br><span class="line">	# calculate Poisson rates</span><br><span class="line">	lhats &lt;- rpois(nosim, lambda = lambda * t) / t</span><br><span class="line">	# lower bound of 95% CI</span><br><span class="line">	ll &lt;- lhats - qnorm(.975) * sqrt(lhats / t)</span><br><span class="line">	# upper bound of 95% CI</span><br><span class="line">	ul &lt;- lhats + qnorm(.975) * sqrt(lhats / t)</span><br><span class="line">	# calculate percent of intervals that contain lambda</span><br><span class="line">	mean(ll &lt; lambda &amp; ul &gt; lambda)</span><br><span class="line">&#125;)</span><br><span class="line"># plot CI results vs 95%</span><br><span class="line">ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ylim(0, 1.0)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see above, for small values of <span class="math inline">\(\lambda = X/t\)</span>, the confidence interval produced by the asymptotic interval is <strong><em>not</em></strong> an accurate estimate of the actual 95% interval (not enough coverage)</li>
<li>however, as <span class="math inline">\(t \to \infty\)</span>, the interval becomes the <strong><em>true 95% interval</em></strong></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># small lambda simulations</span><br><span class="line">lambdavals &lt;- seq(0.005, 0.10, by = .01); nosim &lt;- 1000; t &lt;- 1000</span><br><span class="line"># calculate coverage using Poisson intervals</span><br><span class="line">coverage &lt;- sapply(lambdavals, function(lambda)&#123;</span><br><span class="line">	# calculate Poisson rates</span><br><span class="line">	lhats &lt;- rpois(nosim, lambda = lambda * t) / t</span><br><span class="line">	# lower bound of 95% CI</span><br><span class="line">	ll &lt;- lhats - qnorm(.975) * sqrt(lhats / t)</span><br><span class="line">	# upper bound of 95% CI</span><br><span class="line">	ul &lt;- lhats + qnorm(.975) * sqrt(lhats / t)</span><br><span class="line">	# calculate percent of intervals that contain lambda</span><br><span class="line">	mean(ll &lt; lambda &amp; ul &gt; lambda)</span><br><span class="line">&#125;)</span><br><span class="line"># plot CI results vs 95%</span><br><span class="line">ggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(0, 1.0)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from above, as <span class="math inline">\(t\)</span> increases, the Poisson intervals become closer to the actual 95% confidence intervals</li>
</ul>
<h3 id="confidence-intervals---t-distributionsmall-samples">Confidence Intervals - T Distribution(Small Samples)</h3>
<ul>
<li><strong>t</strong> confidence interval is defined as <span class="math display">\[Estimate \pm TQ \times SE_{Estimate} = \bar X \pm \frac{t_{n-1} S}{\sqrt{n}}\]</span>
<ul>
<li><span class="math inline">\(TQ\)</span> = quantile from T distribution</li>
<li><span class="math inline">\(t_{n-1}\)</span> = relevant quantile</li>
<li><span class="math inline">\(t\)</span> interval assumes data is IID normal so that <span class="math display">\[\frac{\bar X - \mu}{S/\sqrt{n}}\]</span> follows Gosset’s <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom</li>
<li>works well with data distributions that are roughly symmetric/mound shaped, and <strong><em>does not</em></strong> work with skewed distributions
<ul>
<li>skewed distribution <span class="math inline">\(\rightarrow\)</span> meaningless to center interval around the mean <span class="math inline">\(\bar X\)</span></li>
<li>logs/median can be used instead</li>
</ul></li>
<li>paired observations (multiple measurements from same subjects) can be analyzed by t interval of differences</li>
<li>as more data collected (large degrees of freedom), t interval <span class="math inline">\(\rightarrow\)</span> z interval</li>
<li><code>qt(0.975, df=n-1)</code> = calculate the relevant quantile using t distribution</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Plot normal vs t distributions</span><br><span class="line">k &lt;- 1000; xvals &lt;- seq(-5, 5, length = k); df &lt;- 10</span><br><span class="line">d &lt;- data.frame(y = c(dnorm(xvals), dt(xvals, df)),x = xvals,</span><br><span class="line">              dist = factor(rep(c(&quot;Normal&quot;, &quot;T&quot;), c(k,k))))</span><br><span class="line">g &lt;- ggplot(d, aes(x = x, y = y))</span><br><span class="line">g &lt;- g + geom_line(size = 2, aes(colour = dist)) + ggtitle(&quot;Normal vs T Distribution&quot;)</span><br><span class="line"># plot normal vs t quantiles</span><br><span class="line">d &lt;- data.frame(n= qnorm(pvals),t=qt(pvals, df),p = pvals)</span><br><span class="line">h &lt;- ggplot(d, aes(x= n, y = t))</span><br><span class="line">h &lt;- h + geom_abline(size = 2, col = &quot;lightblue&quot;)</span><br><span class="line">h &lt;- h + geom_line(size = 2, col = &quot;black&quot;)</span><br><span class="line">h &lt;- h + geom_vline(xintercept = qnorm(0.975))</span><br><span class="line">h &lt;- h + geom_hline(yintercept = qt(0.975, df)) + ggtitle(&quot;Normal vs T Quantiles&quot;)</span><br><span class="line"># plot 2 graphs together</span><br><span class="line">grid.arrange(g, h, ncol = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>William Gosset’s <strong>t</strong> Distribution (“Student’s T distribution”)
<ul>
<li>test = Gosset’s pseudoname which he published under</li>
<li>indexed/defined by <strong><em>degrees of freedom</em></strong>, and becomes more like standard normal as degrees of freedom gets larger</li>
<li>thicker tails centered around 0, thus confidence interval = <strong><em>wider</em></strong> than Z interval (more mass concentrated away from the center)</li>
<li>for <strong><em>small</em></strong> sample size (value of n), normalizing the distribution by <span class="math inline">\(\frac{\bar X - \mu}{S/\sqrt{n}}\)</span> <span class="math inline">\(\rightarrow\)</span> t distribution, <strong><em>not</em></strong> the standard normal distribution
<ul>
<li><span class="math inline">\(S\)</span> = standard deviation may be inaccurate, as the std of the data sample may not be truly representative of the population std</li>
<li>using the Z interval here thus may produce an interval that is too <strong><em>narrow</em></strong></li>
</ul></li>
</ul></li>
</ul>
<h3 id="confidence-interval---paired-t-tests">Confidence Interval - Paired T Tests</h3>
<ul>
<li>compare observations for the same subjects over two different sets of data (i.e. different times, different treatments)</li>
<li>the confidence interval is defined by <span class="math display">\[ \bar X_1 - \bar X_2 \pm \frac{t_{n-1} S}{\sqrt{n}}\]</span> where <span class="math inline">\(\bar X_1\)</span> represents the first observations and <span class="math inline">\(\bar X_2\)</span> the second set of observations</li>
<li><code>t.test(difference)</code> = performs group mean t test and returns metrics as results, which includes the confidence intervals
<ul>
<li><code>t.test(g2, g1, paired = TRUE)</code> = performs the same paired t test with data directly</li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>the data used here is for a study of the effects of two soporific drugs (increase in hours of sleep compared to control) on 10 patients</li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.width </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">data(sleep)</span><br><span class="line"># plot the first and second observations</span><br><span class="line">g &lt;- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))</span><br><span class="line">g &lt;- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = &quot;salmon&quot;, alpha = .5)</span><br><span class="line">g</span><br><span class="line"># define groups</span><br><span class="line">g1 &lt;- sleep$extra[1 : 10]; g2 &lt;- sleep$extra[11 : 20]</span><br><span class="line"># define difference</span><br><span class="line">difference &lt;- g2 - g1</span><br><span class="line"># calculate mean and sd of differences</span><br><span class="line">mn &lt;- mean(difference); s &lt;- sd(difference); n &lt;- 10</span><br><span class="line"># calculate intervals manually</span><br><span class="line">mn + c(-1, 1) * qt(.975, n-1) * s / sqrt(n)</span><br><span class="line"># perform the same test to get confidence intervals</span><br><span class="line">t.test(difference)</span><br><span class="line">t.test(g2, g1, paired = TRUE)</span><br></pre></td></tr></table></figure>
<h3 id="independent-group-t-intervals---same-variance">Independent Group <strong>t</strong> Intervals - Same Variance</h3>
<ul>
<li>compare two groups in randomized trial (“A/B Testing”)</li>
<li>cannot use the paired t test because the groups are independent and may have different sample sizes</li>
<li>perform randomization to balance unobserved covariance that may otherwise affect the result</li>
<li><span class="math inline">\(t\)</span> confidence interval for <span class="math inline">\(\mu_y - \mu_x\)</span> is defined as <span class="math display">\[\bar Y - \bar X \pm t_{n_x + n_y - 2, 1 - \alpha/2}S_p\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2}\]</span>
<ul>
<li><span class="math inline">\(t_{n_x + n_y - 2, 1 - \alpha/2}\)</span> = relevant quantile</li>
<li><span class="math inline">\(n_x + n_y - 2\)</span> = degrees of freedom</li>
<li><span class="math inline">\(S_p\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2}\)</span> = standard error</li>
<li><span class="math inline">\(S_p^2 = \{(n_x - 1) S_x^2 + (n_y - 1) S_y^2\}/(n_x + n_y - 2)\)</span> = pooled variance estimator
<ul>
<li>this is effectively a weighted average between the two variances, such that different sample sizes are taken in to account</li>
<li>For equal sample sizes, <span class="math inline">\(n_x = n_y\)</span>, <span class="math inline">\(S_p^2 = \frac{S_x^2 + S_y^2}{2}\)</span> (average of variance of two groups)</li>
</ul></li>
<li><em><strong>Note:</strong> this interval assumes <strong>constant variance</strong> across two groups; if variance is different, use the next interval </em></li>
</ul></li>
</ul>
<h3 id="independent-group-t-intervals---different-variance">Independent Group t Intervals - Different Variance</h3>
<ul>
<li>confidence interval for <span class="math inline">\(\mu_y - \mu_x\)</span> is defined as <span class="math display">\[\bar Y - \bar X \pm t_{df} \times \left(\frac{s_x^2}{n_x} + \frac{s_y^2}{n_y}\right)^{1/2}\]</span>
<ul>
<li><span class="math inline">\(t_{df}\)</span> = relevant quantile with df as defined below</li>
<li><em><strong>Note</strong>: normalized statistic does not follow t distribution but can be approximated through the formula with df defined below </em> <span class="math display">\[df = \frac{\left(S_x^2 / n_x + S_y^2/n_y\right)^2}
  {\left(\frac{S_x^2}{n_x}\right)^2 / (n_x - 1) +
  \left(\frac{S_y^2}{n_y}\right)^2 / (n_y - 1)}\]</span>
<ul>
<li><span class="math inline">\(\left(\frac{s_x^2}{n_x} + \frac{s_y^2}{n_y}\right)^{1/2}\)</span> = standard error</li>
</ul></li>
</ul></li>
<li>Comparing other kinds of data
<ul>
<li>binomial <span class="math inline">\(\rightarrow\)</span> relative risk, risk difference, odds ratio</li>
<li>binomial <span class="math inline">\(\rightarrow\)</span> Chi-squared test, normal approximations, exact tests</li>
<li>count <span class="math inline">\(\rightarrow\)</span> Chi-squared test, exact tests</li>
</ul></li>
<li>R commands
<ul>
<li>t Confidence Intervals
<ul>
<li><code>mean + c(-1, 1) * qt(0.975, n - 1) * std / sqrt(n)</code>
<ul>
<li><strong><em>c(-1, 1)</em></strong> = plus and minus, <span class="math inline">\(\pm\)</span></li>
</ul></li>
</ul></li>
<li>Difference Intervals (all equivalent)
<ul>
<li><code>mean2 - mean1 + c(-1, 1) * qt(0.975, n - 1) * std / sqrt(n)</code>
<ul>
<li><strong><em>n</em></strong> = number of paired observations</li>
<li><strong><em>qt(0.975, n - 1)</em></strong> = relevant quantile for paired</li>
<li><strong><em>qt(0.975, n<span class="math inline">\(_x\)</span> + n<span class="math inline">\(_y\)</span> - 2)</em></strong> = relevant quantile for independent</li>
</ul></li>
<li><code>t.test(mean2 - mean1)</code></li>
<li><code>t.test(data2, data1, paired = TRUE, var.equal = TRUE)</code>
<ul>
<li><strong><em>paired</em></strong> = whether or not the two sets of data are paired (same subjects different observations for treatment) <span class="math inline">\(\rightarrow\)</span> <code>TRUE</code> for paired, <code>FALSE</code> for independent</li>
<li><strong><em>var.equal</em></strong> = whether or not the variance of the datasets should be treated as equal <span class="math inline">\(\rightarrow\)</span> <code>TRUE</code> for same variance, <code>FALSE</code> for unequal variances</li>
</ul></li>
<li><code>t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)</code>
<ul>
<li><strong><em>relevel(factor, ref)</em></strong> = reorders the levels in the factor so that “ref” is changed to the first level <span class="math inline">\(\rightarrow\)</span> doing this here is so that the second set of measurements come first (1, 2 <span class="math inline">\(\rightarrow\)</span> 2, 1) in order to perform mean<span class="math inline">\(_2\)</span> - mean<span class="math inline">\(_1\)</span></li>
<li><strong><em>I(object)</em></strong> = prepend the class “AsIs” to the object</li>
<li><em><strong>Note</strong>: I(relevel(group, 2)) = explanatory variable, must be <strong>factor</strong> and have <strong>two levels</strong> </em></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="hypothesis-testing">Hypothesis Testing</h2>
<ul>
<li>Hypothesis testing = making decisions using data
<ul>
<li><strong>null</strong> hypothesis (<strong>H<span class="math inline">\(_0\)</span></strong>) = status quo</li>
<li>assumed to be <strong><em>true</em></strong> <span class="math inline">\(\rightarrow\)</span> statistical evidence required to reject it for <strong>alternative</strong> or “research” hypothesis (<strong>H<span class="math inline">\(_a\)</span></strong>)
<ul>
<li>alternative hypothesis typically take form of &gt;, &lt; or <span class="math inline">\(\ne\)</span></li>
</ul></li>
<li><strong>Results</strong></li>
</ul></li>
</ul>
<table>
<thead>
<tr class="header">
<th>Truth</th>
<th>Decide</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(H_0\)</span></td>
<td><span class="math inline">\(H_0\)</span></td>
<td>Correctly accept null</td>
</tr>
<tr class="even">
<td><span class="math inline">\(H_0\)</span></td>
<td><span class="math inline">\(H_a\)</span></td>
<td>Type I error</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(H_a\)</span></td>
<td><span class="math inline">\(H_a\)</span></td>
<td>Correctly reject null</td>
</tr>
<tr class="even">
<td><span class="math inline">\(H_a\)</span></td>
<td><span class="math inline">\(H_0\)</span></td>
<td>Type II error</td>
</tr>
</tbody>
</table>
<ul>
<li><strong><span class="math inline">\(\alpha\)</span></strong> = Type I error rate
<ul>
<li>probability of <strong><em>rejecting</em></strong> the null hypothesis when the hypothesis is <strong><em>correct</em></strong></li>
<li><span class="math inline">\(\alpha\)</span> = 0.05 <span class="math inline">\(\rightarrow\)</span> standard for hypothesis testing</li>
<li><em><strong>Note</strong>: as Type I error rate increases, Type II error rate decreases and vice versa </em></li>
</ul></li>
<li>for large samples (large n), use the <strong>Z Test</strong> for <span class="math inline">\(H_0:\mu = \mu_0\)</span>
<ul>
<li><strong><span class="math inline">\(H_a\)</span>:</strong>
<ul>
<li><span class="math inline">\(H_1: \mu &lt; \mu_0\)</span></li>
<li><span class="math inline">\(H_2: \mu \neq \mu_0\)</span></li>
<li><span class="math inline">\(H_3: \mu &gt; \mu_0\)</span></li>
</ul></li>
<li>Test statistic <span class="math inline">\(TS = \frac{\bar{X} - \mu_0}{S / \sqrt{n}}\)</span></li>
<li>Reject the null hypothesis <span class="math inline">\(H_0\)</span> when
<ul>
<li><span class="math inline">\(H_1: TS \leq Z_{\alpha}\)</span> OR <span class="math inline">\(-Z_{1 - \alpha}\)</span></li>
<li><span class="math inline">\(H_2: |TS| \geq Z_{1 - \alpha / 2}\)</span></li>
<li><span class="math inline">\(H_3: TS \geq Z_{1 - \alpha}\)</span></li>
</ul></li>
<li><em><strong>Note</strong>: In case of <span class="math inline">\(\alpha\)</span> = 0.05 (most common), <span class="math inline">\(Z_{1-\alpha}\)</span> = 1.645 (95 percentile) </em></li>
<li><span class="math inline">\(\alpha\)</span> = low, so that when <span class="math inline">\(H_0\)</span> is rejected, original model <span class="math inline">\(\rightarrow\)</span> wrong or made an error (low probability)</li>
</ul></li>
<li>For small samples (small n), use the <strong>T Test</strong> for <span class="math inline">\(H_0:\mu = \mu_0\)</span>
<ul>
<li><strong><span class="math inline">\(H_a\)</span>:</strong>
<ul>
<li><span class="math inline">\(H_1: \mu &lt; \mu_0\)</span></li>
<li><span class="math inline">\(H_2: \mu \neq \mu_0\)</span></li>
<li><span class="math inline">\(H_3: \mu &gt; \mu_0\)</span></li>
</ul></li>
<li>Test statistic <span class="math inline">\(TS = \frac{\bar{X} - \mu_0}{S / \sqrt{n}}\)</span></li>
<li>Reject the null hypothesis <span class="math inline">\(H_0\)</span> when
<ul>
<li><span class="math inline">\(H_1: TS \leq T_{\alpha}\)</span> OR <span class="math inline">\(-T_{1 - \alpha}\)</span></li>
<li><span class="math inline">\(H_2: |TS| \geq T_{1 - \alpha / 2}\)</span></li>
<li><span class="math inline">\(H_3: TS \geq T_{1 - \alpha}\)</span></li>
</ul></li>
<li><em><strong>Note</strong>: In case of <span class="math inline">\(\alpha\)</span> = 0.05 (most common), <span class="math inline">\(T_{1-\alpha}\)</span> = <code>qt(.95, df = n-1)</code> </em></li>
<li>R commands for T test:
<ul>
<li><code>t.test(vector1 - vector2)</code></li>
<li><code>t.test(vector1, vector2, paired = TRUE)</code>
<ul>
<li><code>alternative</code> argument can be used to specify one-sided tests: <code>less</code> or <code>greater</code></li>
<li><code>alternative</code> default = <code>two-sided</code></li>
</ul></li>
<li>prints test statistic (<code>t</code>), degrees of freedom (<code>df</code>), <code>p-value</code>, 95% confidence interval, and mean of sample
<ul>
<li>confidence interval in units of data, and can be used to intepret the practical significance of the results</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>rejection region</strong> = region of TS values for which you reject <span class="math inline">\(H_0\)</span></li>
<li><strong>power</strong> = probability of rejecting <span class="math inline">\(H_0\)</span>
<ul>
<li>power is used to calculate sample size for experiments</li>
</ul></li>
<li><strong>two-sided tests</strong> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(H_a: \mu \neq \mu_0\)</span>
<ul>
<li>reject <span class="math inline">\(H_0\)</span> only if test statistic is too larger/small</li>
<li>for <span class="math inline">\(\alpha\)</span> = 0.05, split equally to 2.5% for upper and 2.5% for lower tails
<ul>
<li>equivalent to <span class="math inline">\(|TS| \geq T_{1 - \alpha / 2}\)</span></li>
<li>example: for T test, <code>qt(.975, df)</code> and <code>qt(.025, df)</code></li>
</ul></li>
<li><em><strong>Note</strong>: failing to reject one-sided test = fail to reject two-sided</em></li>
</ul></li>
<li><strong>tests vs confidence intervals</strong>
<ul>
<li>(<span class="math inline">\(1-\alpha\)</span>)% confidence interval for <span class="math inline">\(\mu\)</span> = set of all possible values that fail to reject <span class="math inline">\(H_0\)</span></li>
<li>if (<span class="math inline">\(1-\alpha\)</span>)% confidence interval contains <span class="math inline">\(\mu_0\)</span>, fail to reject <span class="math inline">\(H_0\)</span></li>
</ul></li>
<li><strong>two-group intervals/test</strong>
<ul>
<li>Rejection rules the same</li>
<li>Test <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_1 = \mu_2\)</span> <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\mu_1 - \mu_2 = 0\)</span></li>
<li>Test statistic: <span class="math display">\[\frac{Estimate - H_0 Value}{SE_{Estimate}} = \frac{\bar X_1 - \bar X_2 - 0}{\sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}}\]</span></li>
<li>R Command
<ul>
<li><code>t.test(values ~ factor, paired = FALSE, var.equal = TRUE, data = data)</code>
<ul>
<li><code>paired = FALSE</code> = independent values</li>
<li><code>factor</code> argument must have only two levels</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>p values</strong>
<ul>
<li>most common measure of statistical significance</li>
<li><strong>p-value</strong> = probability under the null hypothesis of obtaining evidence as extreme or more than that of the obtained
<ul>
<li>Given that <span class="math inline">\(H_0\)</span> is true, how likely is it to obtain the result (test statistic)?</li>
</ul></li>
<li><strong>attained significance level</strong> = smallest value for <span class="math inline">\(\alpha\)</span> for which <span class="math inline">\(H_0\)</span> is rejected <span class="math inline">\(\rightarrow\)</span> equivalent to p-value
<ul>
<li>if p-value &lt; <span class="math inline">\(\alpha\)</span>, reject <span class="math inline">\(H_0\)</span></li>
<li>for two-sided tests, double the p-values</li>
</ul></li>
<li>if p-value is small, either <span class="math inline">\(H_0\)</span> is true AND the obeserved is a rare event <strong>OR</strong> <span class="math inline">\(H_0\)</span> is false</li>
<li>R Command
<ul>
<li>p-value = <code>pt(statistic, df, lower.tail = FALSE)</code>
<ul>
<li><code>lower.tail = FALSE</code> = returns the probability of getting a value from the t distribution that is larger than the test statistic</li>
</ul></li>
<li>Binomial (coin flips)
<ul>
<li>probability of getting x results out of n trials and event probability of p = <code>pbinom(x, size = n, prob = p, lower.tail = FALSE)</code></li>
<li>two-sided interval (testing for <span class="math inline">\(\ne\)</span>): find the smaller of two one-sided intervals (X &lt; value, X &gt; value), and double the result</li>
<li><em><strong>Note</strong>: <code>lower.tail = FALSE</code> = strictly greater </em></li>
</ul></li>
<li>Poisson
<ul>
<li>probability of getting x results given the rate r = <code>ppois(x - 1, r, lower.tail = FALSE)</code></li>
<li><code>x - 1</code> is used here because the upper tail includes the specified number (since we want greater than x, we start at x - 1)</li>
<li><code>r</code> = events that should occur given the rate (multiplied by 100 to yield an integer)</li>
<li><em><strong>Note</strong>: <code>lower.tail = FALSE</code> = strictly greater </em></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="power">Power</h2>
<ul>
<li><strong>Power</strong> = probability of rejecting the null hypothesis when it is false (the more power the better)
<ul>
<li>most often used in designing studies so that there’s a reasonable chance to detect the alternative hypothesis if the alternative hypothesis is true</li>
</ul></li>
<li><span class="math inline">\(\beta\)</span> = probability of type II error = failing to reject the null hypothesis when it’s false</li>
<li>power = <span class="math inline">\(1 - \beta\)</span></li>
<li><strong><em>example</em></strong>
<ul>
<li><span class="math inline">\(H_0: \mu = 30 \to \bar X \sim N(\mu_0, \sigma^2/n)\)</span></li>
<li><span class="math inline">\(H_a: \mu &gt; 30 \to \bar X \sim N(\mu_a, \sigma^2/n)\)</span></li>
<li>Power: <span class="math display">\[Power = P\left(\frac{\bar X - 30}{s /\sqrt{n}} &gt; t_{1-\alpha,n-1} ~;~ \mu = \mu_a \right)\]</span>
<ul>
<li><em><strong>Note</strong>: the above function depends on value of <span class="math inline">\(\mu_a\)</span> </em></li>
<li><em><strong>Note</strong>: as <span class="math inline">\(\mu_a\)</span> approaches 30, power approaches <span class="math inline">\(\alpha\)</span> </em></li>
</ul></li>
<li>assuming the sample mean is normally distributed, <span class="math inline">\(H_0\)</span> is rejected when <span class="math inline">\(\frac{\bar X - 30}{\sigma /\sqrt{n}} &gt; Z_{1-\alpha}\)</span></li>
<li>or, <span class="math inline">\(\bar X &gt; 30 + Z_{1-\alpha} \frac{\sigma}{\sqrt{n}}\)</span></li>
</ul></li>
<li>R commands:
<ul>
<li><code>alpha = 0.05; z = qnorm(1-alpha)</code> <span class="math inline">\(\rightarrow\)</span> calculates <span class="math inline">\(Z_{1-\alpha}\)</span></li>
<li><code>pnorm(mu0 + z * sigma/sqrt(n), mean = mua, sd = sigma/sqrt(n), lower.tail = FALSE)</code> <span class="math inline">\(\rightarrow\)</span> calculates the probability of getting a sample mean that is larger than <span class="math inline">\(Z_{1-\alpha} \frac{\sigma}{\sqrt{n}}\)</span> given that the population mean is <span class="math inline">\(\mu_a\)</span>
<ul>
<li><em><strong>Note</strong>: using <code>mean = mu0</code> in the function would = <span class="math inline">\(\alpha\)</span> </em></li>
</ul></li>
<li>Power curve behavior
<ul>
<li>Power increases as <span class="math inline">\(mu_a\)</span> increases <span class="math inline">\(\rightarrow\)</span> we are more likely to detect the difference in <span class="math inline">\(mu_a\)</span> and <span class="math inline">\(mu_0\)</span></li>
<li>Power increases as <strong>n</strong> increases <span class="math inline">\(\rightarrow\)</span> with more data, more likely to detect any alternative <span class="math inline">\(mu_a\)</span></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.align</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">mu0 = 30; mua = 32; sigma = 4; n = 16</span><br><span class="line">alpha = 0.05</span><br><span class="line">z = qnorm(1 - alpha)</span><br><span class="line">nseq = c(8, 16, 32, 64, 128)</span><br><span class="line">mu_a = seq(30, 35, by = 0.1)</span><br><span class="line">power = sapply(nseq, function(n)</span><br><span class="line">    pnorm(mu0 + z * sigma / sqrt(n), mean = mu_a, sd = sigma / sqrt(n),</span><br><span class="line">          lower.tail = FALSE)</span><br><span class="line">    )</span><br><span class="line">colnames(power) &lt;- paste(&quot;n&quot;, nseq, sep = &quot;&quot;)</span><br><span class="line">d &lt;- data.frame(mu_a, power)</span><br><span class="line">library(reshape2)</span><br><span class="line">d2 &lt;- melt(d, id.vars = &quot;mu_a&quot;)</span><br><span class="line">names(d2) &lt;- c(&quot;mu_a&quot;, &quot;n&quot;, &quot;power&quot;)</span><br><span class="line">g &lt;- ggplot(d2,</span><br><span class="line">            aes(x = mu_a, y = power, col = n)) + geom_line(size = 2)</span><br><span class="line">g</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Solving for Power</strong>
<ul>
<li>When testing <span class="math inline">\(H_a : \mu &gt; \mu_0\)</span> (or <span class="math inline">\(&lt;\)</span> or <span class="math inline">\(\neq\)</span>) <span class="math display">\[Power = 1 - \beta = P\left(\bar X &gt; \mu_0 + Z_{1-\alpha} \frac{\sigma}{\sqrt{n}} ; \mu = \mu_a \right)\]</span> where <span class="math inline">\(\bar X \sim N(\mu_a, \sigma^2 / n)\)</span></li>
<li>Unknowns = <span class="math inline">\(\mu_a\)</span>, <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(n\)</span>, <span class="math inline">\(\beta\)</span></li>
<li>Knowns = <span class="math inline">\(\mu_0\)</span>, <span class="math inline">\(\alpha\)</span></li>
<li>Specify any 3 of the unknowns and you can solve for the remainder; most common are two cases
<ol type="1">
<li>Given power desired, mean to detect, variance that we can tolerate, find the <strong>n</strong> to produce desired power (designing experiment/trial)</li>
<li>Given the size <strong>n</strong> of the sample, find the power that is achievable (finding the utility of experiment)</li>
</ol></li>
<li><em><strong>Note</strong>: for <span class="math inline">\(H_a: \mu \neq mu_0\)</span>, calculated one-sided power using <span class="math inline">\(z_{1-\alpha / 2}\)</span>; however, the power calculation here exclusdes the probability of getting a large TS in the opposite direction of the truth, but this is only applicable when <span class="math inline">\(\mu_a\)</span> and <span class="math inline">\(\mu_0\)</span> are close together</em></li>
</ul></li>
<li><strong>Power Behavior</strong>
<ul>
<li>Power increases as <span class="math inline">\(\alpha\)</span> becomes larger</li>
<li>Power of one-sided test <span class="math inline">\(&gt;\)</span> power of associated two-sided test</li>
<li>Power increases as <span class="math inline">\(\mu_a\)</span> gets further away from <span class="math inline">\(\mu_0\)</span></li>
<li>Power increases as <strong>n</strong> increases (sample mean has less variability)</li>
<li>Power increases as <span class="math inline">\(\sigma\)</span> decreases (again less variability)</li>
<li>Power usually depends only <span class="math inline">\(\frac{\sqrt{n}(\mu_a - \mu_0)}{\sigma}\)</span>, and not <span class="math inline">\(\mu_a\)</span>, <span class="math inline">\(\sigma\)</span>, and <span class="math inline">\(n\)</span>
<ul>
<li><strong>effect size</strong> = <span class="math inline">\(\frac{\mu_a - \mu_0}{\sigma}\)</span> <span class="math inline">\(\rightarrow\)</span> unit free, can be interpretted across settings</li>
</ul></li>
</ul></li>
<li><strong>T-test Power</strong>
<ul>
<li>for Gossett’s T test, <span class="math display">\[Power = P\left(\frac{\bar X - \mu_0}{S/\sqrt{n}} &gt; t_{1-\alpha, n-1} ; \mu = \mu_a \right)\]</span>
<ul>
<li><span class="math inline">\(\frac{\bar X - \mu_0}{S/\sqrt{n}}\)</span> does not follow a t distribution if the true mean is <span class="math inline">\(\mu_a\)</span> and NOT <span class="math inline">\(\mu_0\)</span> <span class="math inline">\(\rightarrow\)</span> follows a non-central t distribution instead</li>
</ul></li>
<li><code>power.t.test</code> = evaluates the non-central t distribution and solves for a parameter given all others are specified
<ul>
<li><code>power.t.test(n = 16, delta = 0.5, sd = 1, type = &quot;one.sample&quot;, alt = &quot;one.sided&quot;)$power</code> = calculates power with inputs of n, difference in means, and standard deviation
<ul>
<li><code>delta</code> = argument for difference in means</li>
<li><em><strong>Note</strong>: since effect size = <code>delta/sd</code>, as <code>n</code>, <code>type</code>, and <code>alt</code> are held constant, any distribution with the same effect size will have the same power </em></li>
</ul></li>
<li><code>power.t.test(power = 0.8, delta = 0.5, sd = 1, type = &quot;one.sample&quot;, alt = &quot;one.sided&quot;)$n</code> = calculates size n with inputs of power, difference in means, and standard deviation
<ul>
<li><em><strong>Note</strong>: n should always be rounded up (ceiling) </em></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="multiple-testing">Multiple Testing</h2>
<ul>
<li>Hypothesis testing/significant analysis commonly overused</li>
<li>correct for multiple testing to avoid false positives/conclusions (two key components)
<ol type="1">
<li>error measure</li>
<li>correction</li>
</ol></li>
<li>multiple testing is needed because of the increase in ubiquitous data collection technology and analysis
<ul>
<li>DNA sequencing machines</li>
<li>imaging patients in clinical studies</li>
<li>electronic medical records</li>
<li>individualized movement data (fitbit)</li>
</ul></li>
</ul>
<h3 id="type-of-errors">Type of Errors</h3>
<table>
<thead>
<tr class="header">
<th>Actual <span class="math inline">\(H_0\)</span> = True</th>
<th>Actual <span class="math inline">\(H_a\)</span> = True</th>
<th>Total</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Conclude <span class="math inline">\(H_0\)</span> = True (non-significant)</td>
<td><span class="math inline">\(U\)</span></td>
<td><span class="math inline">\(T\)</span></td>
<td><span class="math inline">\(m-R\)</span></td>
</tr>
<tr class="even">
<td>Conclude <span class="math inline">\(H_a\)</span> = True (significant)</td>
<td><span class="math inline">\(V\)</span></td>
<td><span class="math inline">\(S\)</span></td>
<td><span class="math inline">\(R\)</span></td>
</tr>
<tr class="odd">
<td><strong>Total</strong></td>
<td><span class="math inline">\(m_0\)</span></td>
<td><span class="math inline">\(m-m_0\)</span></td>
<td><span class="math inline">\(m\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li><strong><span class="math inline">\(m_0\)</span></strong> = number of true null hypotheses, or cases where <span class="math inline">\(H_0\)</span> = actually true (unknown)</li>
<li><strong><span class="math inline">\(m - m_0\)</span></strong> = number of true alternative hypotheses, or cases where <span class="math inline">\(H_a\)</span> = actually true (unknown)</li>
<li><strong><span class="math inline">\(R\)</span></strong> = number of null hypotheses rejected, or cases where <span class="math inline">\(H_a\)</span> = concluded to be true (measurable)</li>
<li><strong><span class="math inline">\(m - R\)</span></strong> = number of null hypotheses that failed to be rejected, or cases where <span class="math inline">\(H_0\)</span> = concluded to be true (measurable)</li>
<li><strong><span class="math inline">\(V\)</span></strong> = Type I Error / false positives, concludes <span class="math inline">\(H_a\)</span> = True when <span class="math inline">\(H_0\)</span> = actually True</li>
<li><strong><span class="math inline">\(T\)</span></strong> = Type II Error / false negatives, concludes <span class="math inline">\(H_0\)</span> = True when <span class="math inline">\(H_a\)</span> = actually True</li>
<li><strong><span class="math inline">\(S\)</span></strong> = true positives, concludes <span class="math inline">\(H_a\)</span> = True when <span class="math inline">\(H_a\)</span> = actually True</li>
<li><strong><span class="math inline">\(U\)</span></strong> = true negatives, concludes <span class="math inline">\(H_0\)</span> = True when <span class="math inline">\(H_0\)</span> = actually True</li>
</ul>
<h3 id="error-rates">Error Rates</h3>
<ul>
<li><strong><em>false positive rate</em></strong> = rate at which false results are called significant <span class="math inline">\(E[\frac{V}{m_0}]\)</span> <span class="math inline">\(\rightarrow\)</span> average fraction of times that <span class="math inline">\(H_a\)</span> is claimed to be true when <span class="math inline">\(H_0\)</span> is actually true
<ul>
<li><em><strong>Note</strong>: mathematically equal to type I error rate <span class="math inline">\(\rightarrow\)</span> false positive rate is associated with a post-prior result, which is the expected number of false positives divided by the total number of hypotheses under the real combination of true and non-true null hypotheses (disregarding the “global null” hypothesis). Since the false positive rate is a parameter that is not controlled by the researcher, it cannot be identified with the significance level, which is what determines the type I error rate. </em></li>
</ul></li>
<li><strong><em>family wise error rate (FWER)</em></strong> = probability of at least one false positive <span class="math inline">\(Pr(V \ge 1)\)</span></li>
<li><p><strong><em>false discovery rate (FDR)</em></strong> = rate at which claims of significance are false <span class="math inline">\(E[\frac{V}{R}]\)</span></p></li>
<li><strong>controlling error rates (adjusting <span class="math inline">\(\alpha\)</span>)</strong>
<ul>
<li>false positive rate
<ul>
<li>if we call all <span class="math inline">\(P&lt;\alpha\)</span> significant (reject <span class="math inline">\(H_0\)</span>), we are expected to get <span class="math inline">\(\alpha \times m\)</span> false positives, where <span class="math inline">\(m\)</span> = total number of hypothesis test performed</li>
<li>with high values of <span class="math inline">\(m\)</span>, false positive rate is very large as well</li>
</ul></li>
<li>family-wise error rate (FWER)
<ul>
<li>controlling FWER = controlling the probability of even one false positive</li>
<li><em>bonferroni</em> correction (oldest multiple testing correction)
<ul>
<li>for <span class="math inline">\(m\)</span> tests, we want <span class="math inline">\(Pr(V \ge 1) &lt; \alpha\)</span></li>
<li>calculate P-values normally, and deem them significant if and only if <span class="math inline">\(P &lt; \alpha_{fewer} = \alpha / m\)</span></li>
</ul></li>
<li>easy to calculate, but tend to be very <strong><em>conservative</em></strong></li>
</ul></li>
<li>false discovery rate (FDR)
<ul>
<li>most popular correction = controlling FDR</li>
<li>for <span class="math inline">\(m\)</span> tests, we want <span class="math inline">\(E[\frac{V}{R}] \le \alpha\)</span></li>
<li>calculate P-values normally and sort some from smallest to largest <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(P_{(1)},P_{(1)}, ... , P_{(m)}\)</span></li>
<li>deem the P-values significant if <span class="math inline">\(P_{(i)} \le \alpha \times \frac{i}{m}\)</span></li>
<li>easy to calculate, less conservative, but allows for more false positives and may behave strangely under dependence (related hypothesis tests/regression with different variables)</li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>10 P-values with <span class="math inline">\(\alpha = 0.20\)</span></li>
</ul>
<img src="figures/9.png" title="fig:" alt="alt text"></li>
</ul></li>
<li><strong>adjusting for p-values</strong>
<ul>
<li><em><strong>Note</strong>: changing P-values will fundamentally change their properties but they can be used directly without adjusting <span class="math inline">\(/alpha\)</span> </em></li>
<li><em>bonferroni</em> (FWER)
<ul>
<li><span class="math inline">\(P_i^{fewer} = max(mP_i, 1)\)</span> <span class="math inline">\(\rightarrow\)</span> since p cannot exceed value of 1</li>
<li>deem P-values significant if <span class="math inline">\(P_i^{fewer} &lt; \alpha\)</span></li>
<li>similar to controlling FWER</li>
</ul></li>
</ul></li>
</ul>
<h3 id="example-4">Example</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">set.seed(1010093)</span><br><span class="line">pValues &lt;- rep(NA,1000)</span><br><span class="line">for(i in 1:1000)&#123;</span><br><span class="line">  x &lt;- rnorm(20)</span><br><span class="line">  # First 500 beta=0, last 500 beta=2</span><br><span class="line">  if(i &lt;= 500)&#123;y &lt;- rnorm(20)&#125;else&#123; y &lt;- rnorm(20,mean=2*x)&#125;</span><br><span class="line">  # calculating p-values by using linear model; the [2, 4] coeff in result = pvalue</span><br><span class="line">  pValues[i] &lt;- summary(lm(y ~ x))$coeff[2,4]</span><br><span class="line">&#125;</span><br><span class="line"># Controls false positive rate</span><br><span class="line">trueStatus &lt;- rep(c(&quot;zero&quot;,&quot;not zero&quot;),each=500)</span><br><span class="line">table(pValues &lt; 0.05, trueStatus)</span><br><span class="line"># Controls FWER</span><br><span class="line">table(p.adjust(pValues,method=&quot;bonferroni&quot;) &lt; 0.05,trueStatus)</span><br><span class="line"># Controls FDR (Benjamin Hochberg)</span><br><span class="line">table(p.adjust(pValues,method=&quot;BH&quot;) &lt; 0.05,trueStatus)</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="resample-inference">Resample Inference</h2>
<ul>
<li><strong>Bootstrap</strong> = useful tool for constructing confidence intervals and caclulating standard errors for difficult statistics
<ul>
<li><strong><em>principle</em></strong> = if a statistic’s (i.e. median) sampling distribution is unknown, then use distribution defined by the data to approximate it</li>
<li><strong><em>procedures</em></strong>
<ol type="1">
<li>simulate <span class="math inline">\(n\)</span> observations <strong>with replacement</strong> from the observed data <span class="math inline">\(\rightarrow\)</span> results in <span class="math inline">\(1\)</span> simulated complete data set</li>
<li>calculate desired statistic (i.e. median) for each simulated data set</li>
<li>repeat the above steps <span class="math inline">\(B\)</span> times, resulting in <span class="math inline">\(B\)</span> simulated statistics</li>
<li>these statistics are approximately drawn from the sampling distribution of the true statistic of <span class="math inline">\(n\)</span> observations</li>
<li>perform one of the following
<ul>
<li>plot a histogram</li>
<li>calculate standard deviation of the statistic to estimate its standard error</li>
<li>take quantiles (2.5<sup>th</sup> and 97.5<sup>th</sup>) as a confidence interval for the statistic (“<em>bootstrap CI</em>”)</li>
</ul></li>
</ol></li>
<li><strong><em>example</em></strong>
<ul>
<li>Bootstrap procedure for calculating confidence interval for the median from a data set of <span class="math inline">\(n\)</span> observations <span class="math inline">\(\rightarrow\)</span> approximate sampling distribution</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.align</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">library(UsingR); data(father.son)</span><br><span class="line"># observed dataset</span><br><span class="line">x &lt;- father.son$sheight</span><br><span class="line"># number of simulated statistic</span><br><span class="line">B &lt;- 1000</span><br><span class="line"># generate samples</span><br><span class="line">resamples &lt;- matrix(</span><br><span class="line">    sample(x,               # sample to draw frome</span><br><span class="line">           n * B,           # draw B datasets with n observations each</span><br><span class="line">           replace = TRUE), # cannot draw n*B elements from x (has n elements) without replacement</span><br><span class="line">    B, n)                   # arrange results into n x B matrix</span><br><span class="line">                            # (every row = bootstrap sample with n observations)</span><br><span class="line"># take median for each row/generated sample</span><br><span class="line">medians &lt;- apply(resamples, 1, median)</span><br><span class="line"># estimated standard error of median</span><br><span class="line">sd(medians)</span><br><span class="line"># confidence interval of median</span><br><span class="line">quantile(medians, c(.025, .975))</span><br><span class="line"># histogram of bootstraped samples</span><br><span class="line">hist(medians)</span><br></pre></td></tr></table></figure>
<ul>
<li><p><em><strong>Note:</strong> better percentile bootstrap confidence interval = “bias corrected and accelerated interval” in <code>bootstrap</code> package</em></p></li>
<li><strong>Permutation Tests</strong>
<ul>
<li><strong><em>procedures</em></strong>
<ul>
<li>compare groups of data and test the null hypothesis that the distribution of the observations from each group = same
<ul>
<li><em><strong>Note</strong>: if this is true, then group labels/divisions are irrelevant </em></li>
</ul></li>
<li>permute the labels for the groups</li>
<li>recalculate the statistic
<ul>
<li>Mean difference in counts</li>
<li>Geometric means</li>
<li>T statistic</li>
</ul></li>
<li>Calculate the percentage of simulations where the simulated statistic was more extreme (toward the alternative) than the observed</li>
</ul></li>
<li><p><strong><em>variations</em></strong></p>
<table>
<thead>
<tr class="header">
<th>Data type</th>
<th>Statistic</th>
<th>Test name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ranks</td>
<td>rank sum</td>
<td>rank sum test</td>
</tr>
<tr class="even">
<td>Binary</td>
<td>hypergeometric prob</td>
<td>Fisher’s exact test</td>
</tr>
<tr class="odd">
<td>Raw data</td>
<td></td>
<td>ordinary permutation test</td>
</tr>
</tbody>
</table>
<ul>
<li><em><strong>Note</strong>: randomization tests are exactly permutation tests, with a different motivation </em></li>
<li>For matched data, one can randomize the signs</li>
<li>For ranks, this results in the <strong>signed rank test</strong></li>
<li>Permutation strategies work for regression by permuting a regressor of interest</li>
<li>Permutation tests work very well in multivariate settings</li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>we will compare groups <strong>B</strong> and <strong>C</strong> in this dataset for null hypothesis <span class="math inline">\(H_0:\)</span> there are no difference between the groups</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight plain"><figcaption><span>fig.height</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># load data</span><br><span class="line">data(InsectSprays)</span><br><span class="line"># plot boxplot of dataset</span><br><span class="line">ggplot(InsectSprays, aes(spray, count, fill = spray)) + geom_boxplot()</span><br></pre></td></tr></table></figure>
<ul>
<li>we will compare groups <strong>B</strong> and <strong>C</strong> in this dataset for null hypothesis <span class="math inline">\(H_0:\)</span> there are no difference between the groups</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># subset to only &quot;B&quot; and &quot;C&quot; groups</span><br><span class="line">subdata &lt;- InsectSprays[InsectSprays$spray %in% c(&quot;B&quot;, &quot;C&quot;),]</span><br><span class="line"># values</span><br><span class="line">y &lt;- subdata$count</span><br><span class="line"># labels</span><br><span class="line">group &lt;- as.character(subdata$spray)</span><br><span class="line"># find mean difference between the groups</span><br><span class="line">testStat &lt;- function(w, g) mean(w[g == &quot;B&quot;]) - mean(w[g == &quot;C&quot;])</span><br><span class="line">observedStat &lt;- testStat(y, group)</span><br><span class="line">observedStat</span><br></pre></td></tr></table></figure>
<ul>
<li>the observed difference between the groups is <code>r observedStat</code></li>
<li>now we changed the resample the lables for groups <strong>B</strong> and <strong>C</strong></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create 10000 permutations of the data with the labels&apos; changed</span><br><span class="line">permutations &lt;- sapply(1 : 10000, function(i) testStat(y, sample(group)))</span><br><span class="line"># find the number of permutations whose difference that is bigger than the observed</span><br><span class="line">mean(permutations &gt; observedStat)</span><br></pre></td></tr></table></figure>
<ul>
<li>we created 1000 permutations from the observed dataset, and found <strong><em>no datasets</em></strong> with mean differences between groups <strong>B</strong> and <strong>C</strong> larger than the original data</li>
<li>therefore, p-value is very small and we can <strong><em>reject the null</em></strong> hypothesis with any reasonable <span class="math inline">\(\alpha\)</span> levels</li>
<li>below is the plot for the null distribution/permutations</li>
</ul>
<figure class="highlight plain"><figcaption><span>echo</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># plot distribution of permutations</span><br><span class="line">ggplot(data.frame(permutations = permutations),</span><br><span class="line">           aes(permutations)) + geom_histogram(fill = &quot;lightblue&quot;, color = &quot;black&quot;, binwidth = 1) + geom_vline(xintercept = observedStat, size = 2)</span><br></pre></td></tr></table></figure>
<ul>
<li>as we can see from the black line, the observed difference/statistic is very far from the mean <span class="math inline">\(\rightarrow\)</span> likely 0 is <strong><em>not</em></strong> the true difference
<ul>
<li>with this information, formal confidence intervals can be constructed and p-values can be calculated</li>
</ul></li>
</ul>

      
    </div>

    
      


    

    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Xing Su</li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="http://yoursite.com/2018/08/07/Statistical-Inference/" title="Statistical Inference">http://yoursite.com/2018/08/07/Statistical-Inference/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/R/" rel="tag"># R</a>
          
            <a href="/tags/DataScience/" rel="tag"># DataScience</a>
          
            <a href="/tags/R-Markdown/" rel="tag"># R Markdown</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/07/Reproducible-Research/" rel="next" title="Reproducible Research">
                <i class="fa fa-chevron-left"></i> Reproducible Research
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/07/Regression-Models/" rel="prev" title="Regression Models">
                Regression Models <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zODY0Mi8xNTE3MA=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avator.png"
                alt="Autoz" />
            
              <p class="site-author-name" itemprop="name">Autoz</p>
              <p class="site-description motion-element" itemprop="description">Fidelty.</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/autolordz" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:autolordz@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#overview"><span class="nav-number">1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#probability"><span class="nav-number">2.</span> <span class="nav-text">Probability</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#general-probability-rules"><span class="nav-number">2.1.</span> <span class="nav-text">General Probability Rules</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conditional-probability"><span class="nav-number">2.2.</span> <span class="nav-text">Conditional Probability</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bayes-rule"><span class="nav-number">2.3.</span> <span class="nav-text">Baye’s Rule</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#random-variables"><span class="nav-number">3.</span> <span class="nav-text">Random Variables</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#probability-mass-function-pmf"><span class="nav-number">3.1.</span> <span class="nav-text">Probability Mass Function (PMF)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#probability-density-function-pdf"><span class="nav-number">3.2.</span> <span class="nav-text">Probability Density Function (PDF)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cumulative-distribution-function-cdf"><span class="nav-number">3.3.</span> <span class="nav-text">Cumulative Distribution Function (CDF)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#survival-function"><span class="nav-number">3.4.</span> <span class="nav-text">Survival Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#quantile"><span class="nav-number">3.5.</span> <span class="nav-text">Quantile</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#independence"><span class="nav-number">3.6.</span> <span class="nav-text">Independence</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#iid-random-variables"><span class="nav-number">3.7.</span> <span class="nav-text">IID Random Variables</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#diagnostic-test"><span class="nav-number">4.</span> <span class="nav-text">Diagnostic Test</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#example"><span class="nav-number">4.1.</span> <span class="nav-text">Example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#likelihood-ratios"><span class="nav-number">4.2.</span> <span class="nav-text">Likelihood Ratios</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#expected-valuesmean"><span class="nav-number">5.</span> <span class="nav-text">Expected Values/Mean</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#variance"><span class="nav-number">6.</span> <span class="nav-text">Variance</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sample-variance"><span class="nav-number">6.1.</span> <span class="nav-text">Sample Variance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#entire-estimator-estimation-relationship"><span class="nav-number">6.2.</span> <span class="nav-text">Entire Estimator-Estimation Relationship</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---standard-normal"><span class="nav-number">6.3.</span> <span class="nav-text">Example - Standard Normal</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---standard-uniform"><span class="nav-number">6.4.</span> <span class="nav-text">Example - Standard Uniform</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---poisson"><span class="nav-number">6.5.</span> <span class="nav-text">Example - Poisson</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---bernoulli"><span class="nav-number">6.6.</span> <span class="nav-text">Example - Bernoulli</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---fatherson"><span class="nav-number">6.7.</span> <span class="nav-text">Example - Father/Son</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#binomial-distribution"><span class="nav-number">7.</span> <span class="nav-text">Binomial Distribution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#example-1"><span class="nav-number">7.1.</span> <span class="nav-text">Example</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#normal-distribution"><span class="nav-number">8.</span> <span class="nav-text">Normal Distribution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#example-2"><span class="nav-number">8.1.</span> <span class="nav-text">Example</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#poisson-distribution"><span class="nav-number">9.</span> <span class="nav-text">Poisson Distribution</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#example-3"><span class="nav-number">9.1.</span> <span class="nav-text">Example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---approximating-binomial-distribution"><span class="nav-number">9.2.</span> <span class="nav-text">Example - Approximating Binomial Distribution</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#asymptotics"><span class="nav-number">10.</span> <span class="nav-text">Asymptotics</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#law-of-large-numbers-lln"><span class="nav-number">10.1.</span> <span class="nav-text">Law of Large Numbers (LLN)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---lln-for-normal-and-bernoulli-distribution"><span class="nav-number">10.2.</span> <span class="nav-text">Example - LLN for Normal and Bernoulli Distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#central-limit-theorem"><span class="nav-number">10.3.</span> <span class="nav-text">Central Limit Theorem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---clt-with-bernoulli-trials-coin-flips"><span class="nav-number">10.4.</span> <span class="nav-text">Example - CLT with Bernoulli Trials (Coin Flips)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#confidence-intervals---normal-distributionz-intervals"><span class="nav-number">10.5.</span> <span class="nav-text">Confidence Intervals - Normal Distribution/Z Intervals</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#confidence-interval---bernoulli-distributionwald-interval"><span class="nav-number">10.6.</span> <span class="nav-text">Confidence Interval - Bernoulli Distribution/Wald Interval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#confidence-interval---binomial-distributionagresti-coull-interval"><span class="nav-number">10.7.</span> <span class="nav-text">Confidence Interval - Binomial Distribution/Agresti-Coull Interval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#confidence-interval---poisson-interval"><span class="nav-number">10.8.</span> <span class="nav-text">Confidence Interval - Poisson Interval</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#confidence-intervals---t-distributionsmall-samples"><span class="nav-number">10.9.</span> <span class="nav-text">Confidence Intervals - T Distribution(Small Samples)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#confidence-interval---paired-t-tests"><span class="nav-number">10.10.</span> <span class="nav-text">Confidence Interval - Paired T Tests</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#independent-group-t-intervals---same-variance"><span class="nav-number">10.11.</span> <span class="nav-text">Independent Group t Intervals - Same Variance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#independent-group-t-intervals---different-variance"><span class="nav-number">10.12.</span> <span class="nav-text">Independent Group t Intervals - Different Variance</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hypothesis-testing"><span class="nav-number">11.</span> <span class="nav-text">Hypothesis Testing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#power"><span class="nav-number">12.</span> <span class="nav-text">Power</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multiple-testing"><span class="nav-number">13.</span> <span class="nav-text">Multiple Testing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#type-of-errors"><span class="nav-number">13.1.</span> <span class="nav-text">Type of Errors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#error-rates"><span class="nav-number">13.2.</span> <span class="nav-text">Error Rates</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example-4"><span class="nav-number">13.3.</span> <span class="nav-text">Example</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#resample-inference"><span class="nav-number">14.</span> <span class="nav-text">Resample Inference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Autoz</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Symbols count total: </span>
    
    <span title="Symbols count total">514k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    
    <span title="Reading time total">7:47</span>
  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.5.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Gemini</a> v6.3.0</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="Total Visitors">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="Total Views">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.3.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=6.3.0"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=6.3.0"></script>


  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('Copy').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('Copied')
          else $(this).text('Copy failed')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('Copy')
        }, 300)
      }).append(e)
    })
  </script>


</body>
</html>
