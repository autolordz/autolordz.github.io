<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.3.0',
    sidebar: {"position":"right","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="\(\pagebreak\) Prediction  process for prediction = population \(\rightarrow\) probability and sampling to pick set of data \(\rightarrow\) split into training and test set \(\rightarrow\) build pr">
<meta name="keywords" content="R,DataScience,R Markdown">
<meta property="og:type" content="article">
<meta property="og:title" content="Practical Machine Learning">
<meta property="og:url" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/index.html">
<meta property="og:site_name" content="AutozLand">
<meta property="og:description" content="\(\pagebreak\) Prediction  process for prediction = population \(\rightarrow\) probability and sampling to pick set of data \(\rightarrow\) split into training and test set \(\rightarrow\) build pr">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-1-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-2-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-3-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-4-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-5-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-6-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-7-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-8-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-9-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-10-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-11-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-12-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-13-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-14-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-21-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-22-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-23-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-25-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-27-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-31-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-32-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-35-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-36-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-37-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-38-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-39-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-40-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-41-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-42-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-43-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-44-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-45-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-45-2.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-46-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-47-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-48-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-49-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-51-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-54-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-55-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-56-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-58-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-58-2.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-59-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-59-2.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-61-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-62-1.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-62-2.png">
<meta property="og:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-63-1.png">
<meta property="og:updated_time" content="2018-08-07T09:27:54.327Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Practical Machine Learning">
<meta name="twitter:description" content="\(\pagebreak\) Prediction  process for prediction = population \(\rightarrow\) probability and sampling to pick set of data \(\rightarrow\) split into training and test set \(\rightarrow\) build pr">
<meta name="twitter:image" content="http://yoursite.com/2018/08/07/Practical-Machine-Learning/Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-1-1.png">






  <link rel="canonical" href="http://yoursite.com/2018/08/07/Practical-Machine-Learning/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Practical Machine Learning | AutozLand</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">AutozLand</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Autoz's Learning Blogs</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives<span class="badge">13</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />Tags<span class="badge">3</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />Categories<span class="badge">2</span></a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />About</a>
  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/08/07/Practical-Machine-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Autoz">
      <meta itemprop="description" content="Fidelty.">
      <meta itemprop="image" content="/images/avator.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AutozLand">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Practical Machine Learning<a href="https://github.com/theme-next/theme-next.org/_posts/tree/master/Practical-Machine-Learning.md" class="post-edit-link" title="Edit this post" target="_blank">
                    <i class="fa fa-pencil"></i>
                  </a>
                
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-08-07 14:16:57 / Modified: 17:27:54" itemprop="dateCreated datePublished" datetime="2018-08-07T14:16:57+08:00">2018-08-07</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/R/" itemprop="url" rel="index"><span itemprop="name">R</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon"
            >
            <i class="fa fa-eye"></i>
             Views:  
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">115k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">1:44</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="prediction">Prediction</h2>
<ul>
<li><strong>process for prediction</strong> = population <span class="math inline">\(\rightarrow\)</span> probability and sampling to pick set of data <span class="math inline">\(\rightarrow\)</span> split into training and test set <span class="math inline">\(\rightarrow\)</span> build prediction function <span class="math inline">\(\rightarrow\)</span> predict for new data <span class="math inline">\(\rightarrow\)</span> evaluate
<ul>
<li><em><strong>Note</strong>: choosing the right dataset and knowing what the specific question is are paramount to the success of the prediction algorithm (GoogleFlu failed to predict accurately when people’s search habits changed) </em></li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><strong>components of predictor</strong> = question <span class="math inline">\(\rightarrow\)</span> input data <span class="math inline">\(\rightarrow\)</span> features <em>(extracting variables/characteristics)</em> <span class="math inline">\(\rightarrow\)</span> algorithm <span class="math inline">\(\rightarrow\)</span> parameters <em>(estimate)</em> <span class="math inline">\(\rightarrow\)</span> evaluation</li>
<li><strong>relative order of importance</strong> = question (concrete/specific) <strong>&gt;</strong> data (relevant) <strong>&gt;</strong> features (properly extract) <strong>&gt;</strong> algorithms</li>
<li><strong>data selection</strong>
<ul>
<li><em><strong>Note</strong>: “garbage in = garbage out” <span class="math inline">\(\rightarrow\)</span> having the correct/relevant data will decide whether the model is successful </em></li>
<li>data for what you are trying to predict is most helpful</li>
<li>more data <span class="math inline">\(\rightarrow\)</span> better models (usually)</li>
</ul></li>
<li><strong>feature selection</strong>
<ul>
<li>good features <span class="math inline">\(\rightarrow\)</span> lead to data compression, retain relevant information, created based on expert domain knowledge</li>
<li>common mistakes <span class="math inline">\(\rightarrow\)</span> automated feature selection (can yield good results but likely to behave inconsistently with slightly different data), not understanding/dealing with skewed data/outliers, throwing away information unnecessarily</li>
</ul></li>
<li><strong>algorithm selection</strong>
<ul>
<li>matter less than one would expect</li>
<li>getting a sensible approach/algorithm will be the basis for a successful prediction</li>
<li>more complex algorithms can yield incremental improvements</li>
<li>ideally <em>interpretable</em> (simple to explain), accurate, scalable/fast (may leverage parallel computation)</li>
</ul></li>
<li>prediction is effectively about <strong><em>trade-offs</em></strong>
<ul>
<li>find the correct balance between interpretability vs accuracy vs speed vs simplicity vs scalability</li>
<li><em>interpretability</em> is especially important in conveying how features are used to predict outcome</li>
<li>scalability is important because for an algorithm to be of practical use, it needs to be implementable on large datasets without incurring large costs (computational complexity/time)</li>
</ul></li>
</ul>
<h3 id="in-sample-vs-out-of-sample-errors">In Sample vs Out of Sample Errors</h3>
<ul>
<li><strong>in sample error</strong> = error resulted from applying your prediction algorithm to the dataset you built it with
<ul>
<li>also known as <em>resubstitution error</em></li>
<li>often optimistic (less than on a new sample) as the model may be tuned to error of the sample</li>
</ul></li>
<li><strong>out of sample error</strong> = error resulted from applying your prediction algorithm to a new data set
<ul>
<li>also known as <em>generalization error</em></li>
<li>out of sample error most important as it better evaluates how the model should perform</li>
</ul></li>
<li>in sample error <strong>&lt;</strong> out of sample error
<ul>
<li>reason is <strong><em>over-fitting</em></strong>: model too adapted/optimized for the initial dataset
<ul>
<li>data have two parts: <em>signal</em> vs <em>noise</em></li>
<li>goal of predictor (should be simple/robust) = find signal</li>
<li>it is possible to design an accurate in-sample predictor, but it captures both signal and noise</li>
<li>predictor won’t perform as well on new sample</li>
</ul></li>
<li>often times it is better to give up a little accuracy for more robustness when predicting on new data</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line"><span class="keyword">library</span>(kernlab); data(spam);</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## Warning: package <span class="string">'kernlab'</span> was built under R version <span class="number">3.4</span><span class="number">.4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># picking a small subset (10 values) from spam data set</span></span><br><span class="line">smallSpam &lt;- spam[sample(dim(spam)[<span class="number">1</span>],size=<span class="number">10</span>),]</span><br><span class="line"><span class="comment"># label spam = 2 and ham = 1</span></span><br><span class="line">spamLabel &lt;- (smallSpam$type==<span class="string">"spam"</span>)*<span class="number">1</span> + <span class="number">1</span></span><br><span class="line"><span class="comment"># plot the capitalAve values for the dataset with colors differentiated by spam/ham (2 vs 1)</span></span><br><span class="line">plot(smallSpam$capitalAve,col=spamLabel)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># first rule (over-fitting to capture all variation)</span></span><br><span class="line">rule1 &lt;- <span class="keyword">function</span>(x)&#123;</span><br><span class="line">	prediction &lt;- rep(<span class="literal">NA</span>,length(x))</span><br><span class="line">	prediction[x &gt; <span class="number">2.7</span>] &lt;- <span class="string">"spam"</span></span><br><span class="line">	prediction[x &lt; <span class="number">2.40</span>] &lt;- <span class="string">"nonspam"</span></span><br><span class="line">	prediction[(x &gt;= <span class="number">2.40</span> &amp; x &lt;= <span class="number">2.45</span>)] &lt;- <span class="string">"spam"</span></span><br><span class="line">	prediction[(x &gt; <span class="number">2.45</span> &amp; x &lt;= <span class="number">2.70</span>)] &lt;- <span class="string">"nonspam"</span></span><br><span class="line">	<span class="keyword">return</span>(prediction)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># tabulate results of prediction algorithm 1 (in sample error -&gt; no error in this case)</span></span><br><span class="line">table(rule1(smallSpam$capitalAve),smallSpam$type)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#           nonspam spam</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   nonspam       5    2</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   spam          0    3</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># second rule (simple, setting a threshold)</span></span><br><span class="line">rule2 &lt;- <span class="keyword">function</span>(x)&#123;</span><br><span class="line">	prediction &lt;- rep(<span class="literal">NA</span>,length(x))</span><br><span class="line">	prediction[x &gt; <span class="number">2.8</span>] &lt;- <span class="string">"spam"</span></span><br><span class="line">	prediction[x &lt;= <span class="number">2.8</span>] &lt;- <span class="string">"nonspam"</span></span><br><span class="line">	<span class="keyword">return</span>(prediction)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># tabulate results of prediction algorithm 2(in sample error -&gt; 10% in this case)</span></span><br><span class="line">table(rule2(smallSpam$capitalAve),smallSpam$type)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#           nonspam spam</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   nonspam       5    2</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   spam          0    3</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tabulate out of sample error for algorithm 1</span></span><br><span class="line">table(rule1(spam$capitalAve),spam$type)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#           nonspam spam</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   nonspam    2141  588</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   spam        647 1225</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tabulate out of sample error for algorithm 2</span></span><br><span class="line">table(rule2(spam$capitalAve),spam$type)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#           nonspam spam</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   nonspam    2224  642</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   spam        564 1171</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># accuracy and total correct for algorithm 1 and 2</span></span><br><span class="line">rbind(<span class="string">"Rule 1"</span> = c(Accuracy = mean(rule1(spam$capitalAve)==spam$type),</span><br><span class="line">	<span class="string">"Total Correct"</span> = sum(rule1(spam$capitalAve)==spam$type)),</span><br><span class="line">	<span class="string">"Rule 2"</span> = c(Accuracy = mean(rule2(spam$capitalAve)==spam$type),</span><br><span class="line">	<span class="string">"Total Correct"</span> = sum(rule2(spam$capitalAve)==spam$type)))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#         Accuracy Total Correct</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Rule 1 0.7315801          3366</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Rule 2 0.7378831          3395</span></span></span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="prediction-study-design">Prediction Study Design</h2>
<ul>
<li><strong>procedures</strong>
<ol type="1">
<li>define error rate (type I/type II)</li>
<li>split data into:
<ul>
<li>training, testing, validation (optional)</li>
</ul></li>
<li>pick features from the training set
<ul>
<li>use cross-validation</li>
</ul></li>
<li>pick prediction function (model) on the training set
<ul>
<li>use cross-validation</li>
</ul></li>
<li>if no validation set
<ul>
<li>apply <strong>1 time</strong> to test set</li>
</ul></li>
<li>if there is a validation set
<ul>
<li>apply to test set and refine</li>
<li>apply <strong>1 time</strong> to validation</li>
</ul></li>
</ol>
<ul>
<li><em><strong>Note</strong>: it’s important to hold out an untouched sample to accurately estimate the out of sample error rate </em></li>
</ul></li>
<li>benchmarks (i.e. set all variables = 0) can help pinpoint/test the model to see what is wrong with the model</li>
<li>avoid small sample sizes
<ul>
<li>consider binary outcomes (i.e. coin flip)</li>
<li>for n = 1, the probability of perfect classification (100% accuracy) is 50%</li>
<li>for n = 10, the probability of perfect classification (100% accuracy) is 0.1%</li>
<li>so it’s important to have bigger samples so that when you do get a high accuracy, it may actually be a significant result and not just by chance</li>
</ul></li>
<li><strong><em>example: Netflix rating prediction competition</em></strong>
<ul>
<li>split data between training and “held-out”
<ul>
<li>held-out included probe, quiz and test sets</li>
<li>probe is used to test the predictor built from the training dataset</li>
<li>quiz is used to realistically evaluate out of sample error rates</li>
<li>test is used to finally evaluate the validity of algorithm</li>
</ul></li>
<li>important to not tune model to quiz set specifically</li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;"></p>
<h3 id="sample-division-guidelines-for-prediction-study-design">Sample Division Guidelines for Prediction Study Design</h3>
<ul>
<li>for large sample sizes
<ul>
<li>60% training</li>
<li>20% test</li>
<li>20% validation</li>
</ul></li>
<li>for medium sample sizes
<ul>
<li>60% training</li>
<li>40% test</li>
<li>no validation set to refine model (to ensure test set is of sufficient size)</li>
</ul></li>
<li>for small sample sizes
<ul>
<li>carefully consider if there are enough sample to build a prediction algorithm</li>
<li>no test/validation sets</li>
<li>perform cross validation</li>
<li>report caveat of small sample size and highlight the fact that the prediction algorithm has never been tested for out of sample error</li>
</ul></li>
<li>there should always be a test/validation set that is held away and should <strong><em>NOT</em></strong> be looked at when building model
<ul>
<li>when complete, apply the model to the held-out set only one time</li>
</ul></li>
<li><strong><em>randomly sample</em></strong> training and test sets
<ul>
<li>for data collected over time, build training set in chunks of times</li>
</ul></li>
<li>datasets must reflect structure of problem
<ul>
<li>if prediction evolves with time, split train/test sets in time chunks (known as <em>backtesting</em> in finance)</li>
</ul></li>
<li>subsets of data should reflect as much diversity as possible</li>
</ul>
<h3 id="picking-the-right-data">Picking the Right Data</h3>
<ul>
<li>use like data to predict like</li>
<li>to predict a variable/process X, use the data that’s as closely related to X as possible</li>
<li>weighting the data/variables by understanding and intuition can help to improve accuracy of prediction</li>
<li>data properties matter <span class="math inline">\(\rightarrow\)</span> knowing how the data connects to what you are trying to measure</li>
<li>predicting on unrelated data is the most common mistake
<ul>
<li>if unrelated data must be used, be careful about interpreting the model as to why it works/doesn’t work</li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="types-of-errors">Types of Errors</h2>
<ul>
<li>when discussing the outcome decided on by the algorithm, <strong>Positive</strong> = identified and <strong>negative</strong> = rejected
<ul>
<li><strong>True positive</strong> = correctly identified (predicted true when true)</li>
<li><strong>False positive</strong> = incorrectly identified (predicted true when false)</li>
<li><strong>True negative</strong> = correctly rejected (predicted false when false)</li>
<li><strong>False negative</strong> = incorrectly rejected (predicted false when true)</li>
</ul></li>
<li><em><strong>example: medical testing</strong></em>
<ul>
<li><em>True positive</em> = Sick people correctly diagnosed as sick</li>
<li><em>False positive</em> = Healthy people incorrectly identified as sick</li>
<li><em>True negative</em> = Healthy people correctly identified as healthy</li>
<li><em>False negative</em> = Sick people incorrectly identified as healthy</li>
</ul></li>
</ul>
<h3 id="notable-measurements-for-error-binary-variables">Notable Measurements for Error – Binary Variables</h3>
<ul>
<li><strong>accuracy</strong> = weights false positives/negatives equally</li>
<li><strong>concordance</strong> = for multi-class cases, <span class="math display">\[\kappa = \frac{accuracy - P(e)}{1 - P(e)}\]</span> where <span class="math display">\[P(e) = \frac{TP+FP}{total} \times \frac{TP+FN}{total} + \frac{TN+FN}{total} \times \frac{FP+TN}{total}\]</span></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;"> <img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><strong><em>example</em></strong>
<ul>
<li>given that a disease has 0.1% prevalence in the population, we want to know what’s probability of a person having the disease given the test result is positive? the test kit for the disease is 99% sensitive (most positives = disease) and 99% specific (most negatives = no disease)</li>
<li>what about 10% prevalence?</li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;"> <img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;"></p>
<h3 id="notable-measurements-for-error-continuous-variables">Notable Measurements for Error – Continuous Variables</h3>
<ul>
<li><strong>Mean squared error (MSE)</strong> = <span class="math display">\[\frac{1}{n} \sum_{i=1}^n (Prediction_i - Truth_i)^2\]</span></li>
<li><strong>Root mean squared error (RMSE)</strong> - <span class="math display">\[\sqrt{\frac{1}{n} \sum_{i=1}^n(Prediction_i - Truth_i)^2}\]</span>
<ul>
<li>in the same units as variable</li>
<li>most commonly used error measure for continuous data</li>
<li>is not an effective measures when there are outliers
<ul>
<li>one large value may significantly raise the RMSE</li>
</ul></li>
</ul></li>
<li><strong>median absolute deviation</strong> = <span class="math display">\[median(|Prediction_i - Truth_i|)\]</span></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="receiver-operating-characteristic-curves">Receiver Operating Characteristic Curves</h2>
<ul>
<li>are commonly used techniques to measure the quality of a prediction algorithm.</li>
<li>predictions for binary classification often are quantitative (i.e. probability, scale of 1 to 10)
<ul>
<li>different cutoffs/threshold of classification (&gt; 0.8 <span class="math inline">\(\rightarrow\)</span> one outcome) yield different results/predictions</li>
<li><strong>Receiver Operating Characteristic</strong> curves are generated to compare the different outcomes</li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>ROC Curves
<ul>
<li><strong><em>x-axis</em></strong> = 1 - specificity (or, probability of false positive)</li>
<li><strong><em>y-axis</em></strong> = sensitivity (or, probability of true positive)</li>
<li><strong><em>points plotted</em></strong> = cutoff/combination</li>
<li><strong><em>areas under curve</em></strong> = quantifies whether the prediction model is viable or not
<ul>
<li>higher area <span class="math inline">\(\rightarrow\)</span> better predictor</li>
<li>area = 0.5 <span class="math inline">\(\rightarrow\)</span> effectively random guessing (diagonal line in the ROC curve)</li>
<li>area = 1 <span class="math inline">\(\rightarrow\)</span> perfect classifier</li>
<li>area = 0.8 <span class="math inline">\(\rightarrow\)</span> considered good for a prediction algorithm</li>
</ul></li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><strong><em>example</em></strong>
<ul>
<li>each point on the graph corresponds with a specificity and sensitivity</li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;"></p>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="cross-validation">Cross Validation</h2>
<ul>
<li><strong>procedures</strong>
<ol type="1">
<li>split training set into sub-training/test sets</li>
<li>build model on sub-training set</li>
<li>evaluate on sub-test set</li>
<li>repeat and average estimated errors</li>
</ol></li>
<li><strong>result</strong>
<ul>
<li>we are able to fit/test various different models with different variables included to the find the best one on the cross-validated test sets</li>
<li>we are able to test out different types of prediction algorithms to use and pick the best performing one</li>
<li>we are able to choose the parameters in prediction function and estimate their values</li>
<li><em><strong>Note</strong>: original test set completely untouched, so when final prediction algorithm is applied, the result will be an unbiased measurement of the <strong>out of sample accuracy</strong> of the model </em></li>
</ul></li>
<li><strong>approaches</strong>
<ul>
<li>random subsampling</li>
<li>K-fold</li>
<li>leave one out</li>
</ul></li>
<li><strong>considerations</strong>
<ul>
<li>for time series data data must be used in “chunks”
<ul>
<li>one time period might depending all time periods previously (should not take random samples)</li>
</ul></li>
<li>if you cross-validate to pick predictors, the out of sample error rate may not be the most accurate and thus the errors should still be measured on independent data</li>
</ul></li>
</ul>
<h3 id="random-subsampling">Random Subsampling</h3>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>a randomly sampled test set is subsetted out from the original training set</li>
<li>the predictor is built on the remaining training data and applied to the test set</li>
<li>the above are <strong><em>three</em></strong> random subsamplings from the same training set</li>
<li><strong><em>considerations</em></strong>
<ul>
<li>must be done <em>without replacement</em></li>
<li>random sampling with replacement = <em>bootstrap</em>
<ul>
<li>underestimates of the error, since the if we get one right and the sample appears more than once we’ll get the other right</li>
<li>can be corrected with the (<a href="http://www.jstor.org/discover/10.2307/2965703?uid=2&amp;uid=4&amp;sid=21103054448997" target="_blank" rel="noopener">0.632 Bootstrap</a>), but it is complicated</li>
</ul></li>
</ul></li>
</ul>
<h3 id="k-fold">K-Fold</h3>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>break training set into <span class="math inline">\(K\)</span> subsets (above is a 3-fold cross validation)</li>
<li>build the model/predictor on the remaining training data in each subset and applied to the test subset</li>
<li>rebuild the data <span class="math inline">\(K\)</span> times with the training and test subsets and average the findings</li>
<li><strong><em>considerations</em></strong>
<ul>
<li>larger k = less bias, more variance</li>
<li>smaller k = more bias, less variance</li>
</ul></li>
</ul>
<h3 id="leave-one-out">Leave One Out</h3>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>leave out exactly one sample and build predictor on the rest of training data</li>
<li>predict value for the left out sample</li>
<li>repeat for each sample</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="caret-package-tutorial"><code>caret</code> Package (<a href="http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf" target="_blank" rel="noopener">tutorial</a>)</h2>
<ul>
<li><strong>core functionality</strong>
<ul>
<li>preprocessing/cleaning data <span class="math inline">\(\rightarrow\)</span> <code>preProcess()</code></li>
<li>cross validation/data splitting <span class="math inline">\(\rightarrow\)</span> <code>createDataPartition()</code>, <code>createResample()</code>, <code>createTimeSlices()</code></li>
<li>train algorithms on training data and apply to test sets <span class="math inline">\(\rightarrow\)</span> <code>train()</code>, <code>predict()</code></li>
<li>model comparison (evaluate the accuracy of model on new data) <span class="math inline">\(\rightarrow\)</span> <code>confusionMatrix()</code></li>
</ul></li>
<li>machine learning algorithms in <code>caret</code> package
<ul>
<li>linear discriminant analysis</li>
<li>regression</li>
<li>naive Bayes</li>
<li>support vector machines</li>
<li>classification and regression trees</li>
<li>random forests</li>
<li>boosting</li>
<li>many others</li>
</ul></li>
<li><code>caret</code> provides uniform framework to build/predict using different models
<ul>
<li>create objects of different classes for different algorithms, and <code>caret</code> package allows algorithms to be run the same way through <code>predict()</code> function</li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;"></p>
<h3 id="data-slicing">Data Slicing</h3>
<ul>
<li><code>createDataPartition(y=data$var, times=1, p=0.75, list=FALSE)</code> <span class="math inline">\(\rightarrow\)</span> creates data partitions using given variable
<ul>
<li><code>y=data$var</code> = specifies what outcome/variable to split the data on</li>
<li><code>times=1</code> = specifies number of partitions to create (number of data splitting performed)</li>
<li><code>p=0.75</code> = percent of data that will be for training the model</li>
<li><code>list=FALSE</code> = returns a matrix of indices corresponding to <strong><code>p</code>%</strong> of the data (training set)
<ul>
<li><em><strong>Note</strong>: matrix is easier to subset the data with, so <code>list = FALSE</code> is generally what is used </em></li>
<li><code>list=TRUE</code> = returns a list of indices corresponding to <strong><code>p</code>%</strong> of the data (training set)</li>
</ul></li>
<li>the function effectively returns a list of indexes of the training set which can then be leveraged to subset the data
<ul>
<li><code>training&lt;-data[inTrain, ]</code> = subsets the data to training set only</li>
<li><code>testing&lt;-data[-inTrain, ]</code> = the rest of the data set can then be stored as the test set</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load packages and data</span></span><br><span class="line"><span class="keyword">library</span>(caret)</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## Warning: package <span class="string">'caret'</span> was built under R version <span class="number">3.4</span><span class="number">.4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## Warning: package <span class="string">'lattice'</span> was built under R version <span class="number">3.4</span><span class="number">.4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## Warning: package <span class="string">'ggplot2'</span> was built under R version <span class="number">3.4</span><span class="number">.4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create training set indexes with 75% of data</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=spam$type,p=<span class="number">0.75</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line"><span class="comment"># subset spam data to training</span></span><br><span class="line">training &lt;- spam[inTrain,]</span><br><span class="line"><span class="comment"># subset spam data (the rest) to test</span></span><br><span class="line">testing &lt;- spam[-inTrain,]</span><br><span class="line"><span class="comment"># dimension of original and training dataset</span></span><br><span class="line">rbind(<span class="string">"original dataset"</span> = dim(spam),<span class="string">"training set"</span> = dim(training))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                  [,1] [,2]</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># original dataset 4601   58</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># training set     3451   58</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>createFolds(y=data$var, k=10, list=TRUE, returnTrain=TRUE)</code> = slices the data in to <span class="math inline">\(k\)</span> folds for cross validation and returns <span class="math inline">\(k\)</span> lists of indices
<ul>
<li><code>y=data$var</code> = specifies what outcome/variable to split the data on</li>
<li><code>k=10</code> = specifies number of folds to create (See <strong><em><a href="#k-folds">K Fold Cross Validation</a></em></strong>)
<ul>
<li>each training set has approximately has <span class="math inline">\(\frac{k-1}{k} \%\)</span> of the data (in this case 90%)</li>
<li>each training set has approximately has <span class="math inline">\(\frac{1}{k} \%\)</span> of the data (in this case 10%)</li>
</ul></li>
<li><code>list=TRUE</code> = returns <code>k</code> list of indices that corresponds to the cross-validated sets
<ul>
<li><em><strong>Note</strong>: the returned list conveniently splits the data into <code>k</code> datasets/vectors of indices, so <code>list=TRUE</code> is generally what is used </em></li>
<li>when the returned object is a list (called <code>folds</code> in the case), you can use <code>folds[[1]][1:10]</code> to access different elements from that list</li>
<li><code>list=FALSE</code> = returns a vector indicating which of the <code>k</code> folds each data point belongs to (i.e. 1 - 10 is assigned for each of the data points in this case)
<ul>
<li><em><strong>Note</strong>: these group values corresponds to test sets for each cross validation, which means everything else besides the marked points should be used for training </em></li>
</ul></li>
</ul></li>
<li>[only works when <code>list=T</code>] <code>returnTrain=TRUE</code> = returns the indices of the training sets
<ul>
<li>[default value when unspecified]<code>returnTrain=FALSE</code> = returns indices of the test sets</li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create 10 folds for cross validation and return the training set indices</span></span><br><span class="line">folds &lt;- createFolds(y=spam$type,k=<span class="number">10</span>,list=<span class="literal">TRUE</span>,returnTrain=<span class="literal">TRUE</span>)</span><br><span class="line"><span class="comment"># structure of the training set indices</span></span><br><span class="line">str(folds)</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">## List <span class="keyword">of</span> <span class="number">10</span></span><br><span class="line">##  $ Fold01: int [<span class="number">1</span>:<span class="number">4141</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> ...</span><br><span class="line">##  $ Fold02: int [<span class="number">1</span>:<span class="number">4141</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> ...</span><br><span class="line">##  $ Fold03: int [<span class="number">1</span>:<span class="number">4141</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> ...</span><br><span class="line">##  $ Fold04: int [<span class="number">1</span>:<span class="number">4141</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> ...</span><br><span class="line">##  $ Fold05: int [<span class="number">1</span>:<span class="number">4141</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> ...</span><br><span class="line">##  $ Fold06: int [<span class="number">1</span>:<span class="number">4141</span>] <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">7</span> <span class="number">8</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> ...</span><br><span class="line">##  $ Fold07: int [<span class="number">1</span>:<span class="number">4140</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> ...</span><br><span class="line">##  $ Fold08: int [<span class="number">1</span>:<span class="number">4141</span>] <span class="number">1</span> <span class="number">3</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> ...</span><br><span class="line">##  $ Fold09: int [<span class="number">1</span>:<span class="number">4142</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> ...</span><br><span class="line">##  $ Fold10: int [<span class="number">1</span>:<span class="number">4140</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">8</span> <span class="number">9</span> <span class="number">11</span> <span class="number">12</span> ...</span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># return the test set indices instead</span></span><br><span class="line"><span class="comment"># note: returnTrain = FALSE is unnecessary as it is the default behavior</span></span><br><span class="line">folds.test &lt;- createFolds(y=spam$type,k=<span class="number">10</span>,list=<span class="literal">TRUE</span>,returnTrain=<span class="literal">FALSE</span>)</span><br><span class="line">str(folds.test)</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">## List <span class="keyword">of</span> <span class="number">10</span></span><br><span class="line">##  $ Fold01: int [<span class="number">1</span>:<span class="number">461</span>] <span class="number">21</span> <span class="number">43</span> <span class="number">46</span> <span class="number">87</span> <span class="number">97</span> <span class="number">99</span> <span class="number">102</span> <span class="number">128</span> <span class="number">133</span> <span class="number">157</span> ...</span><br><span class="line">##  $ Fold02: int [<span class="number">1</span>:<span class="number">459</span>] <span class="number">1</span> <span class="number">4</span> <span class="number">6</span> <span class="number">11</span> <span class="number">14</span> <span class="number">22</span> <span class="number">34</span> <span class="number">35</span> <span class="number">40</span> <span class="number">48</span> ...</span><br><span class="line">##  $ Fold03: int [<span class="number">1</span>:<span class="number">461</span>] <span class="number">15</span> <span class="number">19</span> <span class="number">39</span> <span class="number">44</span> <span class="number">51</span> <span class="number">55</span> <span class="number">59</span> <span class="number">63</span> <span class="number">70</span> <span class="number">71</span> ...</span><br><span class="line">##  $ Fold04: int [<span class="number">1</span>:<span class="number">459</span>] <span class="number">7</span> <span class="number">29</span> <span class="number">42</span> <span class="number">52</span> <span class="number">56</span> <span class="number">62</span> <span class="number">64</span> <span class="number">67</span> <span class="number">76</span> <span class="number">91</span> ...</span><br><span class="line">##  $ Fold05: int [<span class="number">1</span>:<span class="number">460</span>] <span class="number">10</span> <span class="number">26</span> <span class="number">28</span> <span class="number">38</span> <span class="number">47</span> <span class="number">83</span> <span class="number">101</span> <span class="number">115</span> <span class="number">136</span> <span class="number">143</span> ...</span><br><span class="line">##  $ Fold06: int [<span class="number">1</span>:<span class="number">460</span>] <span class="number">9</span> <span class="number">25</span> <span class="number">31</span> <span class="number">36</span> <span class="number">37</span> <span class="number">54</span> <span class="number">61</span> <span class="number">65</span> <span class="number">74</span> <span class="number">75</span> ...</span><br><span class="line">##  $ Fold07: int [<span class="number">1</span>:<span class="number">460</span>] <span class="number">12</span> <span class="number">13</span> <span class="number">30</span> <span class="number">41</span> <span class="number">53</span> <span class="number">89</span> <span class="number">96</span> <span class="number">112</span> <span class="number">122</span> <span class="number">138</span> ...</span><br><span class="line">##  $ Fold08: int [<span class="number">1</span>:<span class="number">460</span>] <span class="number">2</span> <span class="number">5</span> <span class="number">16</span> <span class="number">33</span> <span class="number">45</span> <span class="number">49</span> <span class="number">58</span> <span class="number">66</span> <span class="number">77</span> <span class="number">82</span> ...</span><br><span class="line">##  $ Fold09: int [<span class="number">1</span>:<span class="number">460</span>] <span class="number">8</span> <span class="number">18</span> <span class="number">20</span> <span class="number">23</span> <span class="number">32</span> <span class="number">57</span> <span class="number">60</span> <span class="number">79</span> <span class="number">86</span> <span class="number">110</span> ...</span><br><span class="line">##  $ Fold10: int [<span class="number">1</span>:<span class="number">461</span>] <span class="number">3</span> <span class="number">17</span> <span class="number">24</span> <span class="number">27</span> <span class="number">72</span> <span class="number">80</span> <span class="number">90</span> <span class="number">95</span> <span class="number">104</span> <span class="number">120</span> ...</span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># return first 10 elements of the first training set</span></span><br><span class="line">folds[[<span class="number">1</span>]][<span class="number">1</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">##  [<span class="number">1</span>]  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>createResample(y=data$var, times=10, list=TRUE)</code> = create 10 resamplings from the given data with replacement
<ul>
<li><code>list=TRUE</code> = returns list of n vectors that contain indices of the sample
<ul>
<li><em><strong>Note</strong>: each of the vectors is of length of the data, and contains indices </em></li>
</ul></li>
<li><code>times=10</code> = number of samples to create</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create 10 resamples</span></span><br><span class="line">resamples &lt;- createResample(y=spam$type,times=<span class="number">10</span>,list=<span class="literal">TRUE</span>)</span><br><span class="line"><span class="comment"># structure of the resamples (note some samples are repeated)</span></span><br><span class="line">str(resamples)</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">## List <span class="keyword">of</span> <span class="number">10</span></span><br><span class="line">##  $ Resample01: int [<span class="number">1</span>:<span class="number">4601</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">9</span> <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">13</span> ...</span><br><span class="line">##  $ Resample02: int [<span class="number">1</span>:<span class="number">4601</span>] <span class="number">1</span> <span class="number">3</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">5</span> <span class="number">5</span> <span class="number">6</span> <span class="number">6</span> <span class="number">7</span> ...</span><br><span class="line">##  $ Resample03: int [<span class="number">1</span>:<span class="number">4601</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">5</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">7</span> <span class="number">7</span> ...</span><br><span class="line">##  $ Resample04: int [<span class="number">1</span>:<span class="number">4601</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">5</span> <span class="number">6</span> <span class="number">6</span> <span class="number">7</span> <span class="number">9</span> <span class="number">10</span> ...</span><br><span class="line">##  $ Resample05: int [<span class="number">1</span>:<span class="number">4601</span>] <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">4</span> <span class="number">5</span> <span class="number">5</span> <span class="number">6</span> <span class="number">6</span> <span class="number">7</span> <span class="number">9</span> ...</span><br><span class="line">##  $ Resample06: int [<span class="number">1</span>:<span class="number">4601</span>] <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">3</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span> <span class="number">6</span> <span class="number">7</span> ...</span><br><span class="line">##  $ Resample07: int [<span class="number">1</span>:<span class="number">4601</span>] <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">8</span> <span class="number">8</span> ...</span><br><span class="line">##  $ Resample08: int [<span class="number">1</span>:<span class="number">4601</span>] <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">4</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">10</span> <span class="number">12</span> <span class="number">12</span> ...</span><br><span class="line">##  $ Resample09: int [<span class="number">1</span>:<span class="number">4601</span>] <span class="number">1</span> <span class="number">2</span> <span class="number">2</span> <span class="number">4</span> <span class="number">6</span> <span class="number">6</span> <span class="number">7</span> <span class="number">7</span> <span class="number">8</span> <span class="number">12</span> ...</span><br><span class="line">##  $ Resample10: int [<span class="number">1</span>:<span class="number">4601</span>] <span class="number">2</span> <span class="number">2</span> <span class="number">3</span> <span class="number">5</span> <span class="number">5</span> <span class="number">5</span> <span class="number">5</span> <span class="number">6</span> <span class="number">6</span> <span class="number">7</span> ...</span><br></pre></td></tr></table></figure>
<ul>
<li><code>createTimeSlices(y=data, initialWindow=20, horizon=10)</code> = creates training sets with specified window length and the corresponding test sets
<ul>
<li><code>initialWindow=20</code> = number of consecutive values in each time slice/training set (i.e. values 1 - 20)</li>
<li><code>horizon=10</code> = number of consecutive values in each predict/test set (i.e. values 21 - 30)</li>
<li><code>fixedWindow=FALSE</code> = training sets always start at the first observation
<ul>
<li>this means that the first training set would be 1 - 20, the second will be 1 - 21, third 1 - 22, etc.</li>
<li>but the test sets are still like before (21 - 30, 22 - 31, etc.)</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create time series data</span></span><br><span class="line">tme &lt;- <span class="number">1</span>:<span class="number">1000</span></span><br><span class="line"><span class="comment"># create time slices</span></span><br><span class="line">folds &lt;- createTimeSlices(y=tme,initialWindow=<span class="number">20</span>,horizon=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># name of lists</span></span><br><span class="line">names(folds)</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## [<span class="number">1</span>] <span class="string">"train"</span> <span class="string">"test"</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># first training set</span></span><br><span class="line">folds$train[[<span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">##  [<span class="number">1</span>]  <span class="number">1</span>  <span class="number">2</span>  <span class="number">3</span>  <span class="number">4</span>  <span class="number">5</span>  <span class="number">6</span>  <span class="number">7</span>  <span class="number">8</span>  <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> <span class="number">16</span> <span class="number">17</span> <span class="number">18</span> <span class="number">19</span> <span class="number">20</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># first test set</span></span><br><span class="line">folds$test[[<span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">##  [<span class="number">1</span>] <span class="number">21</span> <span class="number">22</span> <span class="number">23</span> <span class="number">24</span> <span class="number">25</span> <span class="number">26</span> <span class="number">27</span> <span class="number">28</span> <span class="number">29</span> <span class="number">30</span></span><br></pre></td></tr></table></figure>
<h3 id="training-options-tutorial">Training Options (<a href="http://topepo.github.io/caret/training.html" target="_blank" rel="noopener">tutorial</a>)</h3>
<ul>
<li><code>train(y ~ x, data=df, method=&quot;glm&quot;)</code> = function to apply the machine learning algorithm to construct model from training data</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># returns the arguments of the default train function</span></span><br><span class="line"><span class="comment"># args(train.default)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>train</code> function has a large set of parameters, below are the default options
<ul>
<li><code>method=&quot;rf&quot;</code> = default algorithm is random forest for training a given data set; <code>caret</code> contains a large number of algorithms
<ul>
<li><code>names(getModelInfo())</code> = returns all the options for <code>method</code> argument</li>
<li>list of models and their information can be found <a href="http://topepo.github.io/caret/modelList.html" target="_blank" rel="noopener">here</a></li>
</ul></li>
<li><code>preProcess=NULL</code> = set preprocess options (see <strong><em><a href="#preprocessing-tutorial">Preprocessing</a></em></strong>)</li>
<li><code>weights=NULL</code> = can be used to add weights to observations, useful for unbalanced distribution (a lot more of one type than another)</li>
<li><code>metric=ifelse(is.factor(y), &quot;Accuracy&quot;, &quot;RMSE&quot;)</code> = default metric for algorithm is <em>Accuracy</em> for factor variables, and <em>RMSE</em>, or root mean squared error, for continuous variables
<ul>
<li><em>Kappa</em> = measure of concordance (see <strong><em><a href="#notable-measurements-for-error-binary-variables">Notable Measurements for Error – Binary Variables</a></em></strong>)</li>
<li><em>RSquared</em> can also be used here as a metric, which represents R<sup>2</sup> from regression models (only useful for linear models)</li>
</ul></li>
<li><code>maximize=ifelse(metric==&quot;RMSE&quot;, FALSE, TRUE)</code> = the algorithm should maximize <em>accuracy</em> and minimize <em>RMSE</em></li>
<li><code>trControl=trainControl()</code> = training controls for the model, more details below</li>
<li><code>tuneGrid=NULL</code></li>
<li><code>tuneLength=3</code></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># returns the default arguments for the trainControl object</span></span><br><span class="line">args(trainControl)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># function (method = "boot", number = ifelse(grepl("cv", method), </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     10, 25), repeats = ifelse(grepl("[d_]cv$", method), 1, NA), </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     p = 0.75, search = "grid", initialWindow = NULL, horizon = 1, </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     fixedWindow = TRUE, skip = 0, verboseIter = FALSE, returnData = TRUE, </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     returnResamp = "final", savePredictions = FALSE, classProbs = FALSE, </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     summaryFunction = defaultSummary, selectionFunction = "best", </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     preProcOptions = list(thresh = 0.95, ICAcomp = 3, k = 5, </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#         freqCut = 95/5, uniqueCut = 10, cutoff = 0.9), sampling = NULL, </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     index = NULL, indexOut = NULL, indexFinal = NULL, timingSamps = 0, </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     predictionBounds = rep(FALSE, 2), seeds = NA, adaptive = list(min = 5, </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#         alpha = 0.05, method = "gls", complete = TRUE), trim = FALSE, </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     allowParallel = TRUE) </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># NULL</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>trainControl</code> creates an object that sets many options for how the model will be applied to the training data
<ul>
<li><em><strong>Note</strong>: the default values are listed below but you can use them to set the parameters to your discretion </em></li>
<li><code>method=&quot;boot&quot;</code> =
<ul>
<li><code>&quot;boot&quot;</code> = bootstrapping (drawing with replacement)</li>
<li><code>&quot;boot632&quot;</code> = bootstrapping with adjustment</li>
<li><code>&quot;cv&quot;</code> = cross validation</li>
<li><code>&quot;repeatedcv&quot;</code> = repeated cross validation</li>
<li><code>&quot;LOOCV&quot;</code> = leave one out cross validation</li>
</ul></li>
<li><code>number=ifelse(grepl(&quot;cv&quot;, method),10, 25)</code> = number of subsamples to take
<ul>
<li><code>number=10</code> = default for any kind of cross validation</li>
<li><code>number=25</code> = default for bootstrapping</li>
<li><em><strong>Note</strong>: <code>number</code> should be increased when fine-tuning model with large number of parameter </em></li>
</ul></li>
<li><code>repeats=ifelse(grepl(&quot;cv&quot;, method), 1, number)</code> = numbers of times to repeat the subsampling
<ul>
<li><code>repeats=1</code> = default for any cross validation method</li>
<li><code>repeats=25</code> = default for bootstrapping</li>
</ul></li>
<li><code>p=0.75</code> = default percentage of data to create training sets</li>
<li><code>initialWindow=NULL, horizon=1, fixedWindow=TRUE</code> = parameters for time series data</li>
<li><code>verboseIter=FALSE</code> = print the training logs</li>
<li><code>returnData=TRUE</code>, returnResamp = “final”,</li>
<li><code>savePredictions=FALSE</code> = save the predictions for each resample</li>
<li><code>classProbs=FALSE</code> = return classification probabilities along with the predictions</li>
<li><code>summaryFunction=defaultSummary</code> = default summary of the model,</li>
<li><code>preProcOptions=list(thresh = 0.95, ICAcomp = 3, k = 5)</code> = specifies preprocessing options for the model</li>
<li><code>predictionBounds=rep(FALSE, 2)</code> = specify the range of the predicted value
<ul>
<li>for numeric predictions, <code>predictionBounds=c(10, NA)</code> would mean that any value lower than 10 would be treated as 10 and no upper bounds</li>
</ul></li>
<li><code>seeds=NA</code> = set the seed for the operation
<ul>
<li><em><strong>Note</strong>: setting this is important when you want to reproduce the same results when the <code>train</code> function is run </em></li>
</ul></li>
<li><code>allowParallel=TRUE</code> = sets for parallel processing/computations</li>
</ul></li>
</ul>
<h3 id="plotting-predictors-tutorial">Plotting Predictors (<a href="http://caret.r-forge.r-project.org/visualizations.html" target="_blank" rel="noopener">tutorial</a>)</h3>
<ul>
<li>it is important to only plot the data in the training set
<ul>
<li>using the test data may lead to over-fitting (model should not be adjusted to test set)</li>
</ul></li>
<li>goal of producing these exploratory plots = look for potential outliers, skewness, imbalances in outcome/predictors, and explainable groups of points/patterns</li>
<li><code>featurePlot(x=predictors, y=outcome, plot=&quot;pairs&quot;)</code> = short cut to plot the relationships between the predictors and outcomes</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load relevant libraries</span></span><br><span class="line"><span class="keyword">library</span>(ISLR); <span class="keyword">library</span>(ggplot2);</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## Warning: package <span class="string">'ISLR'</span> was built under R version <span class="number">3.4</span><span class="number">.3</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load wage data</span></span><br><span class="line">data(Wage)</span><br><span class="line"><span class="comment"># create training and test sets</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=Wage$wage,p=<span class="number">0.7</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- Wage[inTrain,]</span><br><span class="line">testing &lt;- Wage[-inTrain,]</span><br><span class="line"><span class="comment"># plot relationships between the predictors and outcome</span></span><br><span class="line">featurePlot(x=training[,c(<span class="string">"age"</span>,<span class="string">"education"</span>,<span class="string">"jobclass"</span>)], y = training$wage,plot=<span class="string">"pairs"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><code>qplot(age, wage, color=eduction, data=training)</code> = can be used to separate data by a factor variable (by coloring the points differently)
<ul>
<li><code>geom_smooth(method = &quot;lm&quot;)</code> = adds a regression line to the plots</li>
<li><code>geom=c(&quot;boxplot&quot;, &quot;jitter&quot;)</code> = specifies what kind of plot to produce, in this case both the boxplot and the point cloud</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># qplot plus linear regression lines</span></span><br><span class="line">qplot(age,wage,colour=education,data=training)+geom_smooth(method=<span class="string">'lm'</span>,formula=y~x)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-22-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><code>cut2(variable, g=3)</code> = creates a new factor variable by cutting the specified variable into n groups (3 in this case) based on percentiles
<ul>
<li><em><strong>Note</strong>: <code>cut2</code> function is part of the <code>Hmisc</code> package, so <code>library(Hmisc)</code> must be run first </em></li>
<li>this variable can then be used to tabulate/plot the data</li>
</ul></li>
<li><code>grid.arrange(p1, p2, ncol=2)</code> = <code>ggplot2</code> function the print multiple graphs on the same plot
<ul>
<li><em><strong>Note</strong>: <code>grid.arrange</code> function is part of the <code>gridExtra</code> package, so <code>library(gridExtra)</code> must be run first </em></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load Hmisc and gridExtra packages</span></span><br><span class="line"><span class="keyword">library</span>(Hmisc);<span class="keyword">library</span>(gridExtra);</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## Warning: package <span class="string">'Hmisc'</span> was built under R version <span class="number">3.4</span><span class="number">.3</span></span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## Warning: package <span class="string">'survival'</span> was built under R version <span class="number">3.4</span><span class="number">.4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## Warning: package <span class="string">'Formula'</span> was built under R version <span class="number">3.4</span><span class="number">.4</span></span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## Warning: package <span class="string">'gridExtra'</span> was built under R version <span class="number">3.4</span><span class="number">.2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cute the wage variable</span></span><br><span class="line">cutWage &lt;- cut2(training$wage,g=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># plot the boxplot</span></span><br><span class="line">p1 &lt;- qplot(cutWage,age, data=training,fill=cutWage,</span><br><span class="line">      geom=c(<span class="string">"boxplot"</span>))</span><br><span class="line"><span class="comment"># plot boxplot and point clusters</span></span><br><span class="line">p2 &lt;- qplot(cutWage,age, data=training,fill=cutWage,</span><br><span class="line">      geom=c(<span class="string">"boxplot"</span>,<span class="string">"jitter"</span>))</span><br><span class="line"><span class="comment"># plot the two graphs side by side</span></span><br><span class="line">grid.arrange(p1,p2,ncol=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-23-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><code>table(cutVariable, data$var2)</code> = tabulates the cut factor variable vs another variable in the dataset (ie; builds a contingency table using cross-classifying factors)</li>
<li><code>prop.table(table, margin=1)</code> = converts a table to a proportion table
<ul>
<li><code>margin=1</code> = calculate the proportions based on the rows</li>
<li><code>margin=2</code> = calculate the proportions based on the columns</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tabulate the cutWage and jobclass variables</span></span><br><span class="line">t &lt;- table(cutWage,training$jobclass)</span><br><span class="line"><span class="comment"># print table</span></span><br><span class="line">t</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#              </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># cutWage       1. Industrial 2. Information</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   [ 20.9, 93)           442            271</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   [ 93.0,119)           364            349</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   [118.9,318]           274            402</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># convert to proportion table based on the rows</span></span><br><span class="line">prop.table(t,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#              </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># cutWage       1. Industrial 2. Information</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   [ 20.9, 93)     0.6199158      0.3800842</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   [ 93.0,119)     0.5105189      0.4894811</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   [118.9,318]     0.4053254      0.5946746</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>qplot(var1, color=var2, data=training, geom=&quot;density&quot;)</code> = produces density plot for the given numeric and factor variables
<ul>
<li>effectively smoothed out histograms</li>
<li>provides for easy overlaying of groups of data
<ul>
<li>break different variables up by group and see how outcomes change between groups</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># produce density plot</span></span><br><span class="line">qplot(wage,colour=education,data=training,geom=<span class="string">"density"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-25-1.png" style="display: block; margin: auto;"></p>
<h3 id="preprocessing-tutorial">Preprocessing (<a href="http://caret.r-forge.r-project.org/preprocess.html" target="_blank" rel="noopener">tutorial</a>)</h3>
<ul>
<li>some predictors may have strange distributions (i.e. skewed) and may need to be transformed to be more useful for prediction algorithm
<ul>
<li>particularly true for model based algorithms <span class="math inline">\(\rightarrow\)</span> naive Bayes, linear discriminate analysis, linear regression</li>
</ul></li>
<li><strong>centering</strong> = subtracting the observations of a particular variable by its mean</li>
<li><strong>scaling</strong> = dividing the observations of a particular variable by its standard deviation</li>
<li><strong>normalizing</strong> = centering and scaling the variable <span class="math inline">\(\rightarrow\)</span> effectively converting each observation to the number of standard deviations away from the mean
<ul>
<li>the distribution of the normalized variable will have a mean of 0 and standard deviation of 1</li>
<li><em><strong>Note</strong>: normalizing data can help remove bias and high variability, but may not be applicable in all cases </em></li>
</ul></li>
<li><em><strong>Note</strong>: if a predictor/variable is standardized when training the model, the same transformations must be performed on the <code>test</code> set with the mean and standard deviation of the <code>train</code> variables </em>
<ul>
<li>this means that the mean and standard deviation of the normalized test variable will <strong><em>NOT</em></strong> be 0 and 1, respectively, but will be close</li>
<li>transformations must likely be imperfect but test/train sets must be processed the same way</li>
</ul></li>
<li><code>train(y~x, data=training, preProcess=c(&quot;center&quot;, &quot;scale&quot;))</code> = preprocessing can be directly specified in the <code>train</code> function
<ul>
<li><code>preProcess=c(&quot;center&quot;, &quot;scale&quot;)</code> = normalize all predictors before constructing model</li>
</ul></li>
<li><code>preProcess(trainingData, method=c(&quot;center&quot;, &quot;scale&quot;)</code> = function in the <code>caret</code> to standardize data
<ul>
<li>you can store the result of the <code>preProcess</code> function as an object and apply it to the <code>train</code> and <code>test</code> sets using the <code>predict</code> function</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(kernlab)</span><br><span class="line"><span class="comment"># load spam data</span></span><br><span class="line">data(spam)</span><br><span class="line"><span class="comment"># create train and test sets</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=spam$type,p=<span class="number">0.75</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- spam[inTrain,]</span><br><span class="line">testing &lt;- spam[-inTrain,]</span><br><span class="line"><span class="comment"># create preProcess object for all predictors ("-58" because 58th = outcome)</span></span><br><span class="line">preObj &lt;- preProcess(training[,-<span class="number">58</span>],method=c(<span class="string">"center"</span>,<span class="string">"scale"</span>))</span><br><span class="line"><span class="comment"># normalize training set</span></span><br><span class="line">trainCapAveS &lt;- predict(preObj,training[,-<span class="number">58</span>])$capitalAve</span><br><span class="line"><span class="comment"># normalize test set using training parameters</span></span><br><span class="line">testCapAveS &lt;- predict(preObj,testing[,-<span class="number">58</span>])$capitalAve</span><br><span class="line"><span class="comment"># compare results for capitalAve variable</span></span><br><span class="line">rbind(train = c(mean = mean(trainCapAveS), std = sd(trainCapAveS)),</span><br><span class="line">	test = c(mean(testCapAveS), sd(testCapAveS)))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                mean      std</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># train -1.976753e-18 1.000000</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># test   2.732032e-02 1.495311</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>preprocess(data, method=&quot;BoxCox&quot;)</code> = applies BoxCox transformations to continuous data to help normalize the variables through maximum likelihood
<ul>
<li><em><strong>Note</strong>: note it assumes continuous values and DOES NOT deal with repeated values </em></li>
<li><code>qqnorm(processedVar)</code> = can be used to produce the Q-Q plot which compares the theoretical quantiles with the sample quantiles to see the normality of the data</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set up BoxCox transforms</span></span><br><span class="line">preObj &lt;- preProcess(training[,-<span class="number">58</span>],method=c(<span class="string">"BoxCox"</span>))</span><br><span class="line"><span class="comment"># perform preprocessing on training data</span></span><br><span class="line">trainCapAveS &lt;- predict(preObj,training[,-<span class="number">58</span>])$capitalAve</span><br><span class="line"><span class="comment"># plot histogram and QQ Plot</span></span><br><span class="line"><span class="comment"># Note: the transformation definitely helped to</span></span><br><span class="line"><span class="comment"># normalize the data but it does not produce perfect result</span></span><br><span class="line">par(mfrow=c(<span class="number">1</span>,<span class="number">2</span>)); hist(trainCapAveS); qqnorm(trainCapAveS)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-27-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><code>preProcess(data, method=&quot;knnImpute&quot;)</code> = impute/estimate the missing data using <strong>k nearest neighbors (knn)</strong> imputation
<ul>
<li><code>knnImpute</code> = takes the k nearest neighbors from the missing value and averages the value to impute the missing observations</li>
<li><em><strong>Note</strong>: most prediction algorithms are not build to handle missing data </em></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Make some values NA</span></span><br><span class="line">training$capAve &lt;- training$capitalAve</span><br><span class="line">selectNA &lt;- rbinom(dim(training)[<span class="number">1</span>],size=<span class="number">1</span>,prob=<span class="number">0.05</span>)==<span class="number">1</span></span><br><span class="line">training$capAve[selectNA] &lt;- <span class="literal">NA</span></span><br><span class="line"><span class="comment"># Impute and standardize</span></span><br><span class="line">preObj &lt;- preProcess(training[,-<span class="number">58</span>],method=<span class="string">"knnImpute"</span>)</span><br><span class="line">capAve &lt;- predict(preObj,training[,-<span class="number">58</span>])$capAve</span><br><span class="line"><span class="comment"># Standardize true values</span></span><br><span class="line">capAveTruth &lt;- training$capitalAve</span><br><span class="line">capAveTruth &lt;- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)</span><br><span class="line"><span class="comment"># compute differences between imputed values and true values</span></span><br><span class="line">quantile(capAve - capAveTruth)</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">##            <span class="number">0</span>%           <span class="number">25</span>%           <span class="number">50</span>%           <span class="number">75</span>%          <span class="number">100</span>% </span><br><span class="line">## <span class="number">-1.5787898095</span> <span class="number">-0.0008705889</span>  <span class="number">0.0004638890</span>  <span class="number">0.0010980964</span>  <span class="number">0.2204953814</span></span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="covariate-creationfeature-extraction">Covariate Creation/Feature Extraction</h2>
<ul>
<li><strong>[level 1]</strong>: construct covariate (usable metric, feature) from raw data depends heavily on application
<ul>
<li>ideally we want to summarize data without too much information loss</li>
<li>examples
<ul>
<li><em>text files</em>: frequency of words, frequency of phrases (<a href="https://books.google.com/ngrams" target="_blank" rel="noopener">Google ngrams</a>), frequency of capital letters</li>
<li><em>images</em>: edges, corners, blobs, ridges (<a href="http://en.wikipedia.org/wiki/Feature_detection_(computer_vision)" target="_blank" rel="noopener">computer vision feature detection</a>)</li>
<li><em>webpages</em>: number and type of images, position of elements, colors, videos (<a href="http://en.wikipedia.org/wiki/A/B_testing" target="_blank" rel="noopener">A/B Testing</a>)</li>
<li><em>people</em>: height, weight, hair color, sex, country of origin</li>
</ul></li>
<li>generally, more knowledge and understanding you have of the system/data, the easier it will be to extract the summarizing features
<ul>
<li>when in doubt, more features is always safer <span class="math inline">\(\rightarrow\)</span> lose less information and the features can be filtered during model construction</li>
</ul></li>
<li>this process can be automated (i.e. PCA) but generally have to be very careful, as one very useful feature in the training data set may not have as much effect on the test data set</li>
<li><em><strong>Note</strong>: science is the key here, Google “feature extraction for [data type]” for more guidance </em>
<ul>
<li>the goal is always to find the salient characteristics that are likely to be different from observation to observation</li>
</ul></li>
</ul></li>
<li><strong>[level 2]</strong>: construct new covariates from extracted covariate
<ul>
<li>generally transformations of features you extract from raw data</li>
<li>used more for methods like regression and support vector machines (SVM), whose accuracy depend more on the distribution of input variables</li>
<li>models like classification trees don’t require as many complex covariates</li>
<li>best approach is through exploratory analysis (tables/plots)</li>
<li>should only be performed on the train dataset</li>
<li>new covariates should be added to data frames under recognizable names so they can be used later</li>
<li><code>preProcess()</code> can be leveraged to handle creating new covariates</li>
<li><em><strong>Note</strong>: always be careful about over-fitting </em></li>
</ul></li>
</ul>
<h3 id="creating-dummy-variables">Creating Dummy Variables</h3>
<ul>
<li>convert factor variables to indicator/dummy variable <span class="math inline">\(\rightarrow\)</span> qualitative become quantitative</li>
<li><code>dummyVars(outcome~var, data=training)</code> = creates a dummy variable object that can be used through <code>predict</code> function to create dummy variables
<ul>
<li><code>predict(dummyObj, newdata=training)</code> = creates appropriate columns to represent the factor variable with appropriate 0s and 1s
<ul>
<li>2 factor variable <span class="math inline">\(\rightarrow\)</span> two columns which have 0 or 1 depending on the outcome</li>
<li>3 factor variable <span class="math inline">\(\rightarrow\)</span> three columns which have 0, 0, and 1 representing the outcome</li>
<li><em><strong>Note</strong>: only one of the columns can have values of 1 for each observation </em></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># setting up data</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=Wage$wage,p=<span class="number">0.7</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- Wage[inTrain,]; testing &lt;- Wage[-inTrain,]</span><br><span class="line"><span class="comment"># create a dummy variable object</span></span><br><span class="line">dummies &lt;- dummyVars(wage ~ jobclass,data=training)</span><br><span class="line"><span class="comment"># create the dummy variable columns</span></span><br><span class="line">head(predict(dummies,newdata=training))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#        jobclass.1. Industrial jobclass.2. Information</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 231655                      1                       0</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 86582                       0                       1</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 161300                      1                       0</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 376662                      0                       1</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 377954                      0                       1</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 228963                      0                       1</span></span></span><br></pre></td></tr></table></figure>
<h3 id="removing-zero-covariates">Removing Zero Covariates</h3>
<ul>
<li>some variables have no variability at all (i.e. variable indicating if an email contained letters)</li>
<li>these variables are not useful when we want to construct a prediction model</li>
<li><code>nearZeroVar(training, saveMetrics=TRUE)</code> = returns list of variables in training data set with information on frequency ratios, percent uniques, whether or not it has zero variance
<ul>
<li><code>freqRatio</code> = ratio of frequencies for the most common value over second most common value</li>
<li><code>percentUnique</code> = percentage of unique data points out of total number of data points</li>
<li><code>zeroVar</code> = TRUE/FALSE indicating whether the predictor has only one distinct value</li>
<li><code>nzv</code> = TRUE/FALSE indicating whether the predictor is a near zero variance predictor</li>
<li><em><strong>Note</strong>: when <code>nzv</code> = TRUE, those variables should be thrown out </em></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print nearZeroVar table</span></span><br><span class="line">nearZeroVar(training,saveMetrics=<span class="literal">TRUE</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">##            freqRatio percentUnique zeroVar   nzv</span><br><span class="line">## year        <span class="number">1.115854</span>    <span class="number">0.33301618</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></span><br><span class="line">## age         <span class="number">1.052632</span>    <span class="number">2.80685062</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></span><br><span class="line">## maritl      <span class="number">3.178022</span>    <span class="number">0.23786870</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></span><br><span class="line">## race        <span class="number">8.252381</span>    <span class="number">0.19029496</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></span><br><span class="line">## education   <span class="number">1.445148</span>    <span class="number">0.23786870</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></span><br><span class="line">## region      <span class="number">0.000000</span>    <span class="number">0.04757374</span>    <span class="literal">TRUE</span>  <span class="literal">TRUE</span></span><br><span class="line">## jobclass    <span class="number">1.042760</span>    <span class="number">0.09514748</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></span><br><span class="line">## health      <span class="number">2.451560</span>    <span class="number">0.09514748</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></span><br><span class="line">## health_ins  <span class="number">2.258915</span>    <span class="number">0.09514748</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></span><br><span class="line">## logwage     <span class="number">1.064103</span>   <span class="number">19.41008563</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></span><br><span class="line">## wage        <span class="number">1.064103</span>   <span class="number">19.41008563</span>   <span class="literal">FALSE</span> <span class="literal">FALSE</span></span><br></pre></td></tr></table></figure>
<h3 id="creating-splines-polynomial-functions">Creating Splines (Polynomial Functions)</h3>
<ul>
<li>when you want to fit curves through the data, basis functions can be leveraged</li>
<li>[<code>splines</code> package] <code>bs(data$var, df=3)</code> = creates 3 new columns corresponding to the var, var<sup>2</sup>, and var<sup>3</sup> terms</li>
<li><code>ns()</code> and <code>poly()</code> can also be used to generate polynomials</li>
<li><code>gam()</code> function can also be used and it allows for smoothing of multiple variables with different values for each variable</li>
<li><em><strong>Note</strong>: the same polynomial operations must be performed on the test sets using the <code>predict</code> function </em></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load splines package</span></span><br><span class="line"><span class="keyword">library</span>(splines)</span><br><span class="line"><span class="comment"># create polynomial function</span></span><br><span class="line">bsBasis &lt;- bs(training$age,df=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># fit the outcome on the three polynomial terms</span></span><br><span class="line">lm1 &lt;- lm(wage ~ bsBasis,data=training)</span><br><span class="line"><span class="comment"># plot all age vs wage data</span></span><br><span class="line">plot(training$age,training$wage,pch=<span class="number">19</span>,cex=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># plot the fitted polynomial function</span></span><br><span class="line">points(training$age,predict(lm1,newdata=training),col=<span class="string">"red"</span>,pch=<span class="number">19</span>,cex=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-31-1.png" style="display: block; margin: auto;"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict on test values</span></span><br><span class="line">head(predict(bsBasis,age=testing$age))</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">##              <span class="number">1</span>          <span class="number">2</span>           <span class="number">3</span></span><br><span class="line">## [<span class="number">1</span>,] <span class="number">0.0000000</span> <span class="number">0.00000000</span> <span class="number">0.000000000</span></span><br><span class="line">## [<span class="number">2</span>,] <span class="number">0.2368501</span> <span class="number">0.02537679</span> <span class="number">0.000906314</span></span><br><span class="line">## [<span class="number">3</span>,] <span class="number">0.4163380</span> <span class="number">0.32117502</span> <span class="number">0.082587862</span></span><br><span class="line">## [<span class="number">4</span>,] <span class="number">0.3063341</span> <span class="number">0.42415495</span> <span class="number">0.195763821</span></span><br><span class="line">## [<span class="number">5</span>,] <span class="number">0.3776308</span> <span class="number">0.09063140</span> <span class="number">0.007250512</span></span><br><span class="line">## [<span class="number">6</span>,] <span class="number">0.4403553</span> <span class="number">0.25969672</span> <span class="number">0.051051492</span></span><br></pre></td></tr></table></figure>
<h3 id="multicore-parallel-processing">Multicore Parallel Processing</h3>
<ul>
<li>many of the algorithms in the <code>caret</code> package are computationally intensive</li>
<li>since most of the modern machines have multiple cores on their CPUs, it is often wise to enable <strong><em>multicore parallel processing</em></strong> to expedite the computations</li>
<li><code>doMC</code> package is recommended to be used for <code>caret</code> computations (<a href="http://topepo.github.io/caret/parallel.html" target="_blank" rel="noopener">reference</a>)
<ul>
<li><code>doMC::registerDoMC(cores=4)</code> = registers 4 cores for R to utilize</li>
<li>the number of cores you should specify depends on the CPU on your computer (system information usually contains the number of cores)
<ul>
<li>it’s also possible to find the number of cores by directly searching for your CPU model number on the Internet</li>
</ul></li>
<li><em><strong>Note</strong>: once registered, you should see in your task manager/activity monitor that 4 “R Session” appear when you run your code </em></li>
</ul></li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="preprocessing-with-principal-component-analysis-pca">Preprocessing with Principal Component Analysis (PCA)</h2>
<ul>
<li>constructing a prediction model may not require every predictor</li>
<li>ideally we want to capture the <strong><em>most variation</em></strong> with the <strong><em>least</em></strong> amount of variables
<ul>
<li>weighted combination of predictors may improve fit</li>
<li>combination needs to capture the <em>most information</em></li>
</ul></li>
<li>PCA is suited to do this and will help reduce number of predictors as well as reduce noise (due to averaging)
<ul>
<li>statistical goal = find new set of multivariate variables that are <em>uncorrelated</em> and explain as much variance as possible</li>
<li>data compression goal = find the best matrix created with fewer variables that explains the original data</li>
<li>PCA is most useful for linear-type models (GLM, LDA)</li>
<li>generally more difficult to interpret the predictors (complex weighted sums of variables)</li>
<li><em><strong>Note</strong>: outliers are can be detrimental to PCA as they may represent a lot of variation in data </em>
<ul>
<li>exploratory analysis (plots/tables) should be used to identify problems with the predictors</li>
<li>transformations with log/BoxCox may be helpful</li>
</ul></li>
</ul></li>
</ul>
<h3 id="prcomp-function"><code>prcomp</code> Function</h3>
<ul>
<li><code>pr&lt;-prcomp(data)</code> = performs PCA on all variables and returns a <code>prcomp</code> object that contains information about standard deviations and rotations
<ul>
<li><code>pr$rotations</code> = returns eigenvectors for the linear combinations of all variables (coefficients that variables are multiplied by to come up with the principal components) <span class="math inline">\(\rightarrow\)</span> how the principal components are created</li>
<li>often times, it is useful to take the <code>log</code> transformation of the variables and adding 1 before performing PCA
<ul>
<li>helps to reduce skewness or strange distribution in data</li>
<li>log(0) = - infinity, so we add 1 to account for zero values</li>
<li>makes data more Gaussian</li>
</ul></li>
<li><code>plot(pr)</code> = plots the percent variation explained by the first 10 principal components (PC)
<ul>
<li>can be used to find the PCs that represent the most variation</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load  spam data</span></span><br><span class="line">data(spam)</span><br><span class="line"><span class="comment"># perform PCA on dataset</span></span><br><span class="line">prComp &lt;- prcomp(log10(spam[,-<span class="number">58</span>]+<span class="number">1</span>))</span><br><span class="line"><span class="comment"># print out the eigenvector/rotations first 5 rows and PCs</span></span><br><span class="line">head(prComp$rotation[, <span class="number">1</span>:<span class="number">5</span>], <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">##                 <span class="selector-tag">PC1</span>           <span class="selector-tag">PC2</span>         <span class="selector-tag">PC3</span>         <span class="selector-tag">PC4</span>          <span class="selector-tag">PC5</span></span><br><span class="line">## <span class="selector-tag">make</span>    0<span class="selector-class">.019370409</span>  0<span class="selector-class">.0427855959</span> <span class="selector-tag">-0</span><span class="selector-class">.01631961</span>  0<span class="selector-class">.02798232</span> <span class="selector-tag">-0</span><span class="selector-class">.014903314</span></span><br><span class="line">## <span class="selector-tag">address</span> 0<span class="selector-class">.010827343</span>  0<span class="selector-class">.0408943785</span>  0<span class="selector-class">.07074906</span> <span class="selector-tag">-0</span><span class="selector-class">.01407049</span>  0<span class="selector-class">.037237531</span></span><br><span class="line">## <span class="selector-tag">all</span>     0<span class="selector-class">.040923168</span>  0<span class="selector-class">.0825569578</span> <span class="selector-tag">-0</span><span class="selector-class">.03603222</span>  0<span class="selector-class">.04563653</span>  0<span class="selector-class">.001222215</span></span><br><span class="line">## <span class="selector-tag">num3d</span>   0<span class="selector-class">.006486834</span> <span class="selector-tag">-0</span><span class="selector-class">.0001333549</span>  0<span class="selector-class">.01234374</span> <span class="selector-tag">-0</span><span class="selector-class">.01005991</span> <span class="selector-tag">-0</span><span class="selector-class">.001282330</span></span><br><span class="line">## <span class="selector-tag">our</span>     0<span class="selector-class">.036963221</span>  0<span class="selector-class">.0941456085</span> <span class="selector-tag">-0</span><span class="selector-class">.01871090</span>  0<span class="selector-class">.05098463</span> <span class="selector-tag">-0</span><span class="selector-class">.010582039</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create new variable that marks spam as 2 and nospam as 1</span></span><br><span class="line">typeColor &lt;- ((spam$type==<span class="string">"spam"</span>)*<span class="number">1</span> + <span class="number">1</span>)</span><br><span class="line"><span class="comment"># plot the first two principal components</span></span><br><span class="line">plot(prComp$x[,<span class="number">1</span>],prComp$x[,<span class="number">2</span>],col=typeColor,xlab=<span class="string">"PC1"</span>,ylab=<span class="string">"PC2"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-32-1.png" style="display: block; margin: auto;"></p>
<h3 id="caret-package"><code>caret</code> Package</h3>
<ul>
<li><code>pp&lt;-preProcess(log10(training[,-58]+1),method=&quot;pca&quot;,pcaComp=2,thresh=0.8))</code> = perform PCA with <code>preProcess</code> function and returns the number of principal components that can capture the majority of the variation
<ul>
<li>creates a <code>preProcess</code> object that can be applied using <code>predict</code> function</li>
<li><code>pcaComp=2</code> = specifies the number of principal components to compute (2 in this case)</li>
<li><code>thresh=0.8</code> = threshold for variation captured by principal components
<ul>
<li><code>thresh=0.95</code> = default value, which returns the number of principal components that are needed to capture 95% of the variation in data</li>
</ul></li>
</ul></li>
<li><code>predict(pp, training)</code> = computes new variables for the PCs (2 in this case) for the training data set
<ul>
<li>the results from <code>predict</code> can then be used as data for the prediction model</li>
<li><em><strong>Note</strong>: the same PCA must be performed on the test set </em></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create train and test sets</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=spam$type,p=<span class="number">0.75</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- spam[inTrain,]</span><br><span class="line">testing &lt;- spam[-inTrain,]</span><br><span class="line"><span class="comment"># create preprocess object</span></span><br><span class="line">preProc &lt;- preProcess(log10(training[,-<span class="number">58</span>]+<span class="number">1</span>),method=<span class="string">"pca"</span>,pcaComp=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># calculate PCs for training data</span></span><br><span class="line">trainPC &lt;- predict(preProc,log10(training[,-<span class="number">58</span>]+<span class="number">1</span>))</span><br><span class="line"><span class="comment"># run model on outcome and principle components</span></span><br><span class="line">modelFit &lt;- train(y=training$type,x=trainPC,method=<span class="string">"glm"</span>)</span><br><span class="line"><span class="comment"># calculate PCs for test data</span></span><br><span class="line">testPC &lt;- predict(preProc,log10(testing[,-<span class="number">58</span>]+<span class="number">1</span>))</span><br><span class="line"><span class="comment"># compare results</span></span><br><span class="line">confusionMatrix(testing$type,predict(modelFit,testPC))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Confusion Matrix and Statistics</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#           Reference</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Prediction nonspam spam</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    nonspam     647   50</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    spam         66  387</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                           </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                Accuracy : 0.8991          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                  95% CI : (0.8803, 0.9159)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     No Information Rate : 0.62            </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     P-Value [Acc &gt; NIR] : &lt;2e-16          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                           </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                   Kappa : 0.7874          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  Mcnemar's Test P-Value : 0.1637          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                           </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#             Sensitivity : 0.9074          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#             Specificity : 0.8856          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          Pos Pred Value : 0.9283          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          Neg Pred Value : 0.8543          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#              Prevalence : 0.6200          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          Detection Rate : 0.5626          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    Detection Prevalence : 0.6061          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#       Balanced Accuracy : 0.8965          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                           </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#        'Positive' Class : nonspam         </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>alternatively, PCA can be directly performed with the <code>train</code> method
<ul>
<li><code>train(outcome ~ ., method=&quot;glm&quot;, preProcess=&quot;pca&quot;, data=training)</code> = performs PCA first on the training set and then runs the specified model
<ul>
<li>effectively the same procedures as above (<code>preProcess</code> <span class="math inline">\(\rightarrow\)</span> <code>predict</code>)</li>
</ul></li>
</ul></li>
<li><em><strong>Note</strong>: in both cases, the PCs were able to achieve 90+% accuracy </em></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># construct model</span></span><br><span class="line"><span class="comment">#modelFit &lt;- train(training$type ~ .,method="glm",preProcess="pca",data=training)</span></span><br><span class="line">modelFit &lt;- train(y=training$type,x=training[,-<span class="number">58</span>],method=<span class="string">"glm"</span>,preProcess=<span class="string">"pca"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print results of model</span></span><br><span class="line">confusionMatrix(testing$type,predict(modelFit,testing))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Confusion Matrix and Statistics</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#           Reference</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Prediction nonspam spam</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    nonspam     664   33</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    spam         46  407</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                           </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                Accuracy : 0.9313          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                  95% CI : (0.9151, 0.9452)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     No Information Rate : 0.6174          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     P-Value [Acc &gt; NIR] : &lt;2e-16          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                           </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                   Kappa : 0.8554          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  Mcnemar's Test P-Value : 0.177           </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                           </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#             Sensitivity : 0.9352          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#             Specificity : 0.9250          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          Pos Pred Value : 0.9527          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          Neg Pred Value : 0.8985          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#              Prevalence : 0.6174          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          Detection Rate : 0.5774          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    Detection Prevalence : 0.6061          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#       Balanced Accuracy : 0.9301          </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                                           </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#        'Positive' Class : nonspam         </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#</span></span></span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="predicting-with-regression">Predicting with Regression</h2>
<ul>
<li><strong>prediction with regression</strong> = fitting regression model (line) to data <span class="math inline">\(\rightarrow\)</span> multiplying each variable by coefficients to predict outcome</li>
<li>useful when the relationship between the variables can be modeled as linear</li>
<li>the model is easy to implement and the coefficients are easy to interpret</li>
<li>if the relationships are non-linear, the regression model may produce poor results/accuracy
<ul>
<li><em><strong>Note</strong>: linear regressions are generally used in combination with other models </em></li>
</ul></li>
<li><strong>model</strong> <span class="math display">\[Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots + \beta_p X_{pi} + e_i \]</span>
<ul>
<li>where <span class="math inline">\(\beta_0\)</span> is the intercept (when all variables are 0)</li>
<li><span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span> are the coefficients</li>
<li><span class="math inline">\(X_{1i}, X_{2i}, \ldots, X_{pi}\)</span> are the variables/covariates</li>
<li><span class="math inline">\(e_i\)</span> is the error</li>
<li><span class="math inline">\(Y_i\)</span> is the outcome</li>
</ul></li>
<li><strong>prediction</strong> <span class="math display">\[\hat Y_i = \hat \beta_0 + \hat \beta_1 X_{1i} + \hat \beta_2 X_{2i} + \ldots + \hat \beta_p X_{pi}\]</span>
<ul>
<li>where <span class="math inline">\(\hat \beta_0\)</span> is the estimated intercept (when all variables are 0)</li>
<li><span class="math inline">\(\hat \beta_1, \hat \beta_2, \ldots, \hat \beta_p\)</span> are the estimated coefficients</li>
<li><span class="math inline">\(X_{1i}, X_{2i}, \ldots, X_{pi}\)</span> are the variables/covariates</li>
<li><span class="math inline">\(\hat Y_i\)</span> is the <strong><em>predicted outcome</em></strong></li>
</ul></li>
</ul>
<h3 id="r-commands-and-examples">R Commands and Examples</h3>
<ul>
<li><code>lm&lt;-lm(y ~ x, data=train)</code> = runs a linear model of outcome y on predictor x <span class="math inline">\(\rightarrow\)</span> univariate regression
<ul>
<li><code>summary(lm)</code> = returns summary of the linear regression model, which will include coefficients, standard errors, <span class="math inline">\(t\)</span> statistics, and p values</li>
<li><code>lm(y ~ x1+x2+x3, data=train)</code> = run linear model of outcome y on predictors x1, x2, and x3</li>
<li><code>lm(y ~ ., data=train</code> = run linear model of outcome y on all predictors</li>
</ul></li>
<li><code>predict(lm, newdata=df)</code> = use the constructed linear model to predict outcomes (<span class="math inline">\(\hat Y_i\)</span>) for the new values
<ul>
<li><code>newdata</code> data frame must have the same variables (factors must have the same levels) as the training data</li>
<li><code>newdata=test</code> = predict outcomes for the test set based on linear regression model from the training</li>
<li><em><strong>Note</strong>: the regression line will not be a perfect fit on the test set since it was constructed on the training set </em></li>
</ul></li>
<li>RSME can be calculated to measure the accuracy of the linear model
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(RSME_{test}\)</span>, which estimates the out-of-sample error, is almost always <strong>GREATER</strong> than <span class="math inline">\(RSME_{train}\)</span> </em></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">data(faithful)</span><br><span class="line"><span class="comment"># create train and test sets</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=faithful$waiting, p=<span class="number">0.5</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">trainFaith &lt;- faithful[inTrain,]; testFaith &lt;- faithful[-inTrain,]</span><br><span class="line"><span class="comment"># build linear model</span></span><br><span class="line">lm1 &lt;- lm(eruptions ~ waiting,data=trainFaith)</span><br><span class="line"><span class="comment"># print summary of linear model</span></span><br><span class="line">summary(lm1)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Call:</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># lm(formula = eruptions ~ waiting, data = trainFaith)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Residuals:</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#      Min       1Q   Median       3Q      Max </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># -1.18990 -0.33724  0.06439  0.32056  1.22913 </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Coefficients:</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#              Estimate Std. Error t value Pr(&gt;|t|)    </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># (Intercept) -2.048931   0.219126   -9.35 2.49e-16 ***</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># waiting      0.077458   0.003032   25.55  &lt; 2e-16 ***</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># ---</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Residual standard error: 0.4801 on 135 degrees of freedom</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Multiple R-squared:  0.8286,	Adjusted R-squared:  0.8274 </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># F-statistic: 652.8 on 1 and 135 DF,  p-value: &lt; 2.2e-16</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict eruptions for new waiting time</span></span><br><span class="line">newdata &lt;- data.frame(waiting=<span class="number">80</span>)</span><br><span class="line">predict(lm1,newdata)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#        1 </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 4.147697</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create 1 x 2 panel plot</span></span><br><span class="line">par(mfrow=c(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"><span class="comment"># plot train data with the regression line</span></span><br><span class="line">plot(trainFaith$waiting,trainFaith$eruptions,pch=<span class="number">19</span>,col=<span class="string">"blue"</span>,xlab=<span class="string">"Waiting"</span>,</span><br><span class="line">	ylab=<span class="string">"Duration"</span>, main = <span class="string">"Train"</span>)</span><br><span class="line">lines(trainFaith$waiting,predict(lm1),lwd=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># plot test data with the regression line</span></span><br><span class="line">plot(testFaith$waiting,testFaith$eruptions,pch=<span class="number">19</span>,col=<span class="string">"blue"</span>,xlab=<span class="string">"Waiting"</span>,</span><br><span class="line">	ylab=<span class="string">"Duration"</span>, main = <span class="string">"Test"</span>)</span><br><span class="line">lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-35-1.png" style="display: block; margin: auto;"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Calculate RMSE on training and test sets</span></span><br><span class="line">c(trainRMSE = sqrt(sum((lm1$fitted-trainFaith$eruptions)^<span class="number">2</span>)),</span><br><span class="line">	testRMSE = sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^<span class="number">2</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># trainRMSE  testRMSE </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  5.578530  6.013711</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>pi&lt;-predict(lm, newdata=test, interval=&quot;prediction&quot;)</code> = returns 3 columns for <code>fit</code> (predicted value, same as before), <code>lwr</code> (lower bound of prediction interval), and <code>upr</code> (upper bound of prediction interval)
<ul>
<li><code>matlines(x, pi, type=&quot;l&quot;)</code> = plots three lines, one for the linear fit and two for upper/lower prediction interval bounds</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># calculate prediction interval</span></span><br><span class="line">pred1 &lt;- predict(lm1,newdata=testFaith,interval=<span class="string">"prediction"</span>)</span><br><span class="line"><span class="comment"># plot data points (eruptions, waiting)</span></span><br><span class="line">plot(testFaith$waiting,testFaith$eruptions,pch=<span class="number">19</span>,col=<span class="string">"blue"</span>)</span><br><span class="line"><span class="comment"># plot fit line and prediction interval</span></span><br><span class="line">matlines(testFaith$waiting,pred1,type=<span class="string">"l"</span>,,col=c(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>),lty = c(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>), lwd=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-36-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><code>lm &lt;- train(y ~ x, method=&quot;lm&quot;, data=train)</code> = run linear model on the training data <span class="math inline">\(\rightarrow\)</span> identical to <code>lm</code> function
<ul>
<li><code>summary(lm$finalModel)</code> = returns summary of the linear regression model, which will include coefficients, standard errors, <span class="math inline">\(t\)</span> statistics, and p values <span class="math inline">\(\rightarrow\)</span> identical to <code>summary(lm)</code> for a <code>lm</code> object</li>
<li><code>train(y ~ ., method=&quot;lm&quot;, data=train)</code> = run linear model on all predictors in training data
<ul>
<li>multiple predictors (dummy/indicator variables) are created for factor variables</li>
</ul></li>
<li><code>plot(lm$finalModel)</code> = construct 4 diagnostic plots for evaluating the model
<ul>
<li><em><strong>Note</strong>: more information on these plots can be found at <code>?plot.lm</code> </em></li>
<li><strong><em>Residuals vs Fitted</em></strong></li>
<li><strong><em>Normal Q-Q</em></strong></li>
<li><strong><em>Scale-Location</em></strong></li>
<li><strong><em>Residuals vs Leverage</em></strong></li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create train and test sets</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=Wage$wage,p=<span class="number">0.7</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- Wage[inTrain,]; testing &lt;- Wage[-inTrain,]</span><br><span class="line"><span class="comment"># fit linear model for age jobclass and education</span></span><br><span class="line">modFit&lt;- train(wage ~ age + jobclass + education,method = <span class="string">"lm"</span>,data=training)</span><br><span class="line"><span class="comment"># store final model</span></span><br><span class="line">finMod &lt;- modFit$finalModel</span><br><span class="line"><span class="comment"># set up 2 x 2 panel plot</span></span><br><span class="line">par(mfrow = c(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># construct diagnostic plots for model</span></span><br><span class="line">plot(finMod,pch=<span class="number">19</span>,cex=<span class="number">0.5</span>,col=<span class="string">"#00000010"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-37-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>plotting residuals by fitted values and coloring with a variable not used in the model helps spot a trend in that variable.</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot fitted values by residuals </span></span><br><span class="line">qplot(finMod$fitted, finMod$residuals, color=race, data=training)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-38-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>plotting residuals by index (ie; row numbers) can be helpful in showing missing variables
<ul>
<li><code>plot(finMod$residuals)</code> = plot the residuals against index (row number)</li>
<li>if there’s a trend/pattern in the residuals, it is highly likely that another variable (such as age/time) should be included.
<ul>
<li>residuals should not have relationship to index</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot residual by index</span></span><br><span class="line">plot(finMod$residuals,pch=<span class="number">19</span>,cex=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-39-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>here the residuals increase linearly with the index, and the highest residuals are concentrated in the higher indexes, so there must be a missing variable</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="prediction-with-trees">Prediction with Trees</h2>
<ul>
<li><strong>prediction with trees</strong> = iteratively split variables into groups (effectively constructing decision trees) <span class="math inline">\(\rightarrow\)</span> produces nonlinear model
<ul>
<li>the classification tree uses interactions between variables <span class="math inline">\(\rightarrow\)</span> the ultimate groups/leafs may depend on many variables</li>
</ul></li>
<li>the result (tree) is easy to interpret, and generally performs better predictions than regression models when the relationships are <strong><em>non-linear</em></strong></li>
<li>transformations less important <span class="math inline">\(\rightarrow\)</span> monotone transformations (order unchanged, such as <span class="math inline">\(\log\)</span>) will produce same splits</li>
<li>trees can be used for regression problems as well and use RMSE as <em>measure of impurity</em></li>
<li>however, without proper cross-validation, the model can be <strong><em>over-fitted</em></strong> (especially with large number of variables) and results may be variable from one run to the next
<ul>
<li>it is also harder to estimate the uncertainty of the model</li>
</ul></li>
<li><code>party</code>, <code>rpart</code>, <code>tree</code> packages can all build trees</li>
</ul>
<h3 id="process">Process</h3>
<ol type="1">
<li>start with all variables in one group</li>
<li>find the variable that best splits the outcomes into two groups</li>
<li>divide data into two groups (<em>leaves</em>) based on the split performed (<em>node</em>)</li>
<li>within each split, find variables to split the groups again</li>
<li>continue this process until all groups are sufficiently small/homogeneous/“pure”</li>
</ol>
<h3 id="measures-of-impurity-reference">Measures of Impurity (<a href="http://en.wikipedia.org/wiki/Decision_tree_learning" target="_blank" rel="noopener">Reference</a>)</h3>
<p><span class="math display">\[\hat{p}_{mk} = \frac{\sum_{i}^m \mathbb{1}(y_i = k)}{N_m}\]</span></p>
<ul>
<li><span class="math inline">\(\hat p_mk\)</span> is the probability of the objects in group <span class="math inline">\(m\)</span> to take on the classification <span class="math inline">\(k\)</span></li>
<li><p><span class="math inline">\(N_m\)</span> is the size of the group</p></li>
<li><strong>Misclassification Error</strong> <span class="math display">\[ 1 - \hat{p}_{m~k(m)}\]</span> where <span class="math inline">\(k(m)\)</span> is the most common classification/group
<ul>
<li>0 = perfect purity</li>
<li>0.5 = no purity
<ul>
<li><em><strong>Note</strong>: it is not 1 here because when <span class="math inline">\(\hat{p}_{m~k(m)} &lt; 0.5\)</span> or there’s not predominant classification for the objects, it means the group should be further subdivided until there’s a majority </em></li>
</ul></li>
</ul></li>
<li><strong>Gini Index</strong><span class="math display">\[ \sum_{k \neq k&#39;} \hat{p}_{mk} \times \hat{p}_{mk&#39;} = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}) = 1 - \sum_{k=1}^K p_{mk}^2\]</span>
<ul>
<li>0 = perfect purity</li>
<li>0.5 = no purity</li>
</ul></li>
<li><strong>Deviance</strong> <span class="math display">\[ -\sum_{k=1}^K \hat{p}_{mk} \log_e\hat{p}_{mk} \]</span>
<ul>
<li>0 = perfect purity</li>
<li>1 = no purity</li>
</ul></li>
<li><strong>Information Gai</strong> <span class="math display">\[ -\sum_{k=1}^K \hat{p}_{mk} \log_2\hat{p}_{mk} \]</span>
<ul>
<li>0 = perfect purity</li>
<li>1 = no purity</li>
</ul></li>
<li><p><strong><em>example</em></strong></p></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set margin and seed</span></span><br><span class="line">par(mar=c(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>), mfrow = c(<span class="number">1</span>, <span class="number">2</span>)); set.seed(<span class="number">1234</span>);</span><br><span class="line"><span class="comment"># simulate data</span></span><br><span class="line">x = rep(<span class="number">1</span>:<span class="number">4</span>,each=<span class="number">4</span>); y = rep(<span class="number">1</span>:<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment"># plot first scenario</span></span><br><span class="line">plot(x,y,xaxt=<span class="string">"n"</span>,yaxt=<span class="string">"n"</span>,cex=<span class="number">3</span>,col=c(rep(<span class="string">"blue"</span>,<span class="number">15</span>),rep(<span class="string">"red"</span>,<span class="number">1</span>)),pch=<span class="number">19</span>)</span><br><span class="line"><span class="comment"># plot second scenario</span></span><br><span class="line">plot(x,y,xaxt=<span class="string">"n"</span>,yaxt=<span class="string">"n"</span>,cex=<span class="number">3</span>,col=c(rep(<span class="string">"blue"</span>,<span class="number">8</span>),rep(<span class="string">"red"</span>,<span class="number">8</span>)),pch=<span class="number">19</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-40-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>left graph
<ul>
<li><strong>Misclassification:</strong> <span class="math inline">\(\frac{1}{16} = 0.06\)</span></li>
<li><strong>Gini:</strong> <span class="math inline">\(1 - [(\frac{1}{16})^2 + (\frac{15}{16})^2] = 0.12\)</span></li>
<li><strong>Information:</strong><span class="math inline">\(-[\frac{1}{16} \times \log_2 (\frac{1}{16})+ \frac{15}{16} \times \log_2(\frac{1}{16})] = 0.34\)</span></li>
</ul></li>
<li>right graph
<ul>
<li><strong>Misclassification:</strong> <span class="math inline">\(\frac{8}{16} = 0.5\)</span></li>
<li><strong>Gini:</strong> <span class="math inline">\(1 - [(\frac{8}{16})^2 + (\frac{8}{16})^2] = 0.5\)</span></li>
<li><strong>Information:</strong><span class="math inline">\(-[\frac{8}{16} \times \log_2 (\frac{8}{16})+ \frac{8}{16} \times \log_2(\frac{8}{16})] = 1\)</span></li>
</ul></li>
</ul>
<h3 id="constructing-trees-with-caret-package">Constructing Trees with <code>caret</code> Package</h3>
<ul>
<li><code>tree&lt;-train(y ~ ., data=train, method=&quot;rpart&quot;)</code> = constructs trees based on the outcome and predictors
<ul>
<li>produces an <code>rpart</code> object, which can be used to <code>predict</code> new/test values</li>
<li><code>print(tree$finalModel)</code> = returns text summary of all nodes/splits in the tree constructed</li>
</ul></li>
<li><code>plot(tree$finalModel, uniform=TRUE)</code> = plots the classification tree with all nodes/splits
<ul>
<li>[<code>rattle</code> package] <code>fancyRpartPlot(tree$finalModel)</code> = produces more readable, better formatted classification tree diagrams</li>
<li>each split will have the condition/node in bold and the splits/leafs on the left and right sides following the “yes” or “no” indicators
<ul>
<li>“yes” <span class="math inline">\(\rightarrow\)</span> go left</li>
<li>“no” <span class="math inline">\(\rightarrow\)</span> go right</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load iris data set</span></span><br><span class="line">data(iris)</span><br><span class="line"><span class="comment"># create test/train data sets</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=iris$Species,p=<span class="number">0.7</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- iris[inTrain,]</span><br><span class="line">testing &lt;- iris[-inTrain,]</span><br><span class="line"><span class="comment"># fit classification tree as a model</span></span><br><span class="line">modFit &lt;- train(Species ~ .,method=<span class="string">"rpart"</span>,data=training)</span><br><span class="line"><span class="comment"># print the classification tree</span></span><br><span class="line">print(modFit$finalModel)</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">## n= <span class="number">105</span> </span><br><span class="line">## </span><br><span class="line">## node), split, n, loss, yval, (yprob)</span><br><span class="line">##       * denotes terminal node</span><br><span class="line">## </span><br><span class="line">## <span class="number">1</span>) root <span class="number">105</span> <span class="number">70</span> setosa (<span class="number">0.33333333</span> <span class="number">0.33333333</span> <span class="number">0.33333333</span>)  </span><br><span class="line">##   <span class="number">2</span>) Petal.Length&lt; <span class="number">2.45</span> <span class="number">35</span>  <span class="number">0</span> setosa (<span class="number">1.00000000</span> <span class="number">0.00000000</span> <span class="number">0.00000000</span>) *</span><br><span class="line">##   <span class="number">3</span>) Petal.Length&gt;=<span class="number">2.45</span> <span class="number">70</span> <span class="number">35</span> versicolor (<span class="number">0.00000000</span> <span class="number">0.50000000</span> <span class="number">0.50000000</span>)  </span><br><span class="line">##     <span class="number">6</span>) Petal.Width&lt; <span class="number">1.65</span> <span class="number">34</span>  <span class="number">1</span> versicolor (<span class="number">0.00000000</span> <span class="number">0.97058824</span> <span class="number">0.02941176</span>) *</span><br><span class="line">##     <span class="number">7</span>) Petal.Width&gt;=<span class="number">1.65</span> <span class="number">36</span>  <span class="number">2</span> virginica (<span class="number">0.00000000</span> <span class="number">0.05555556</span> <span class="number">0.94444444</span>) *</span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the classification tree</span></span><br><span class="line">rattle::fancyRpartPlot(modFit$finalModel)</span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">## Warning: Bad <span class="string">'data'</span> field <span class="keyword">in</span> model <span class="string">'call'</span>.</span><br><span class="line">## To silence this warning:</span><br><span class="line">##     Call prp <span class="keyword">with</span> roundint=FALSE,</span><br><span class="line">##     or rebuild the rpart model <span class="keyword">with</span> model=TRUE.</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-41-1.png" style="display: block; margin: auto;"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict on test values</span></span><br><span class="line">predict(modFit,newdata=testing)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  [1] setosa     setosa     setosa     setosa     setosa     setosa    </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  [7] setosa     setosa     setosa     setosa     setosa     setosa    </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [13] setosa     setosa     setosa     versicolor versicolor versicolor</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [19] versicolor versicolor versicolor versicolor versicolor versicolor</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [25] versicolor versicolor versicolor versicolor versicolor versicolor</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [31] virginica  virginica  virginica  virginica  virginica  virginica </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [37] versicolor virginica  virginica  versicolor versicolor virginica </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [43] virginica  virginica  virginica </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Levels: setosa versicolor virginica</span></span></span><br></pre></td></tr></table></figure>
<h2 id="bagging">Bagging</h2>
<ul>
<li><strong>bagging</strong> = bootstrap aggregating
<ul>
<li>resample training data set (with replacement) and recalculate predictions</li>
<li>average the predictions together or majority vote</li>
<li>more information can be found <a href="http://stat.ethz.ch/education/semesters/FS_2008/CompStat/sk-ch8.pdf" target="_blank" rel="noopener">here</a></li>
</ul></li>
<li>averaging multiple complex models have <strong><em>similar bias</em></strong> as each of the models on its own, and <strong><em>reduced variance</em></strong> because of the average</li>
<li><p>most useful for non-linear models</p></li>
<li><strong><em>example</em></strong>
<ul>
<li><code>loess(y ~ x, data=train, span=0.2)</code> = fits a smooth curve to data
<ul>
<li><code>span=0.2</code> = controls how smooth the curve should be</li>
</ul></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line"><span class="keyword">library</span>(ElemStatLearn); data(ozone,package=<span class="string">"ElemStatLearn"</span>)</span><br><span class="line"><span class="comment"># reorder rows based on ozone variable</span></span><br><span class="line">ozone &lt;- ozone[order(ozone$ozone),]</span><br><span class="line"><span class="comment"># create empty matrix</span></span><br><span class="line">ll &lt;- matrix(<span class="literal">NA</span>,nrow=<span class="number">10</span>,ncol=<span class="number">155</span>)</span><br><span class="line"><span class="comment"># iterate 10 times</span></span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">10</span>)&#123;</span><br><span class="line">	<span class="comment"># create sample from data with replacement</span></span><br><span class="line">	ss &lt;- sample(<span class="number">1</span>:dim(ozone)[<span class="number">1</span>],replace=<span class="literal">T</span>)</span><br><span class="line">	<span class="comment"># draw sample from the dataa and reorder rows based on ozone</span></span><br><span class="line">	ozone0 &lt;- ozone[ss,]; ozone0 &lt;- ozone0[order(ozone0$ozone),]</span><br><span class="line">	<span class="comment"># fit loess function through data (similar to spline)</span></span><br><span class="line">	loess0 &lt;- loess(temperature ~ ozone,data=ozone0,span=<span class="number">0.2</span>)</span><br><span class="line">	<span class="comment"># prediction from loess curve for the same values each time</span></span><br><span class="line">	ll[i,] &lt;- predict(loess0,newdata=data.frame(ozone=<span class="number">1</span>:<span class="number">155</span>))</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># plot the data points</span></span><br><span class="line">plot(ozone$ozone,ozone$temperature,pch=<span class="number">19</span>,cex=<span class="number">0.5</span>)</span><br><span class="line"><span class="comment"># plot each prediction model</span></span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">10</span>)&#123;lines(<span class="number">1</span>:<span class="number">155</span>,ll[i,],col=<span class="string">"grey"</span>,lwd=<span class="number">2</span>)&#125;</span><br><span class="line"><span class="comment"># plot the average in red</span></span><br><span class="line">lines(<span class="number">1</span>:<span class="number">155</span>,apply(ll,<span class="number">2</span>,mean),col=<span class="string">"red"</span>,lwd=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-42-1.png" style="display: block; margin: auto;"></p>
<h3 id="bagging-algorithms">Bagging Algorithms</h3>
<ul>
<li>in the <code>caret</code> package, there are three options for the <code>train</code> function to perform bagging
<ul>
<li><code>bagEarth</code> - Bagged MARS (<a href="http://www.inside-r.org/packages/cran/caret/docs/bagEarth" target="_blank" rel="noopener">documentation</a>)</li>
<li><code>treebag</code> - Bagged CART (<a href="http://www.inside-r.org/packages/cran/ipred/docs/bagging" target="_blank" rel="noopener">documentation</a>)</li>
<li><code>bagFDA</code> - Bagged Flexible Discriminant Analysis (<a href="http://www.inside-r.org/packages/cran/caret/docs/bagFDA" target="_blank" rel="noopener">documentation</a>)</li>
</ul></li>
<li>alternatively, custom <code>bag</code> functions can be constructed (<a href="http://www.inside-r.org/packages/cran/caret/docs/nbBag" target="_blank" rel="noopener">documentation</a>)
<ul>
<li><code>bag(predictors, outcome, B=10, bagControl(fit, predict, aggregate))</code> = define and execute custom bagging algorithm
<ul>
<li><code>B=10</code> = iterations/resampling to perform</li>
<li><code>bagControl()</code> = controls for how the bagging should be executed
<ul>
<li><code>fit=ctreeBag$fit</code> = the model ran on each resampling of data</li>
<li><code>predict=ctreeBag$predict</code> = how predictions should be calculated from each model</li>
<li><code>aggregate=ctreeBag$aggregate</code> = how the prediction models should be combined/averaged</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load relevant package and data</span></span><br><span class="line"><span class="keyword">library</span>(party); data(ozone,package=<span class="string">"ElemStatLearn"</span>)</span><br><span class="line"><span class="comment"># reorder rows based on ozone variable</span></span><br><span class="line">ozone &lt;- ozone[order(ozone$ozone),]</span><br><span class="line"><span class="comment"># extract predictors</span></span><br><span class="line">predictors &lt;- data.frame(ozone=ozone$ozone)</span><br><span class="line"><span class="comment"># extract outcome</span></span><br><span class="line">temperature &lt;- ozone$temperature</span><br><span class="line"><span class="comment"># run bagging algorithm</span></span><br><span class="line">treebag &lt;- bag(predictors, temperature, B = <span class="number">10</span>,</span><br><span class="line">				<span class="comment"># custom bagging function</span></span><br><span class="line">                bagControl = bagControl(fit = ctreeBag$fit,</span><br><span class="line">                                        predict = ctreeBag$pred,</span><br><span class="line">                                        aggregate = ctreeBag$aggregate))</span><br><span class="line"><span class="comment"># plot data points</span></span><br><span class="line">plot(ozone$ozone,temperature,col=<span class="string">'lightgrey'</span>,pch=<span class="number">19</span>)</span><br><span class="line"><span class="comment"># plot the first fit</span></span><br><span class="line">points(ozone$ozone,predict(treebag$fits[[<span class="number">1</span>]]$fit,predictors),pch=<span class="number">19</span>,col=<span class="string">"red"</span>)</span><br><span class="line"><span class="comment"># plot the aggregated predictions</span></span><br><span class="line">points(ozone$ozone,predict(treebag,predictors),pch=<span class="number">19</span>,col=<span class="string">"blue"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-43-1.png" style="display: block; margin: auto;"></p>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="random-forest">Random Forest</h2>
<ul>
<li><strong>random forest</strong> = extension of bagging on classification/regression trees
<ul>
<li>one of the most used/accurate algorithms along with boosting</li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-44-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><strong>process</strong>
<ul>
<li>bootstrap samples from training data (with replacement)</li>
<li>split and bootstrap variables</li>
<li>grow trees (repeat split/bootstrap) and vote/average final trees</li>
</ul></li>
<li><strong>drawbacks</strong>
<ul>
<li>algorithm can be slow (process large number of trees)</li>
<li>hard to interpret (large numbers of splits and nodes)</li>
<li>over-fitting (difficult to know which tree is causing over-fitting)</li>
<li><em><strong>Note</strong>: it is extremely important to use cross validation when running random forest algorithms </em></li>
</ul></li>
</ul>
<h3 id="r-commands-and-examples-1">R Commands and Examples</h3>
<ul>
<li><code>rf&lt;-train(outcome ~ ., data=train, method=&quot;rf&quot;, prox=TRUE, ntree=500)</code> = runs random forest algorithm on the training data against all predictors
<ul>
<li><em><strong>Note</strong>: random forest algorithm automatically bootstrap by default, but it is still important to have train/test/validation split to verify the accuracy of the model </em></li>
<li><code>prox=TRUE</code> = the proximity measures between observations should be calculated (used in functions such as <code>classCenter()</code> to find center of groups)
<ul>
<li><code>rf$finalModel$prox</code> = returns matrix of proximities</li>
</ul></li>
<li><code>ntree=500</code> = specify number of trees that should be constructed</li>
<li><code>do.trace=TRUE</code> = prints logs as the trees are being built <span class="math inline">\(\rightarrow\)</span> useful by indicating progress to user</li>
<li><em><strong>Note</strong>: <code>randomForest()</code> function can be used to perform random forest algorithm (syntax is the same as <code>train</code>) and is much faster </em></li>
</ul></li>
<li><code>getTree(rf$finalModel, k=2)</code> = return specific tree from random forest model</li>
<li><code>classCenters(predictors, outcome, proximity, nNbr)</code> = return computes the cluster centers using the <code>nNbr</code> nearest neighbors of the observations
<ul>
<li><code>prox = rf$finalModel$prox</code> = proximity matrix from the random forest model</li>
<li><code>nNbr</code> = number of nearest neighbors that should be used to compute cluster centers</li>
</ul></li>
<li><code>predict(rf, test)</code> = apply the random forest model to test data set
<ul>
<li><code>confusionMatrix(predictions, actualOutcome)</code> = tabulates the predictions of the model against the truths
<ul>
<li><em><strong>Note</strong>: this is generally done for the validation data set using the model built from training </em></li>
</ul></li>
</ul></li>
<li><strong><em>example</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(randomForest)</span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">data(iris)</span><br><span class="line"><span class="comment"># create train/test data sets</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=iris$Species,p=<span class="number">0.7</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- iris[inTrain,]</span><br><span class="line">testing &lt;- iris[-inTrain,]</span><br><span class="line"><span class="comment"># apply random forest</span></span><br><span class="line">modFit &lt;- train(Species~ .,data=training,method=<span class="string">"rf"</span>,prox=<span class="literal">TRUE</span>)</span><br><span class="line"><span class="comment"># return the second tree (first 6 rows)</span></span><br><span class="line">head(getTree(modFit$finalModel,k=<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">##   left daughter right daughter split var split point status prediction</span><br><span class="line">## <span class="number">1</span>             <span class="number">2</span>              <span class="number">3</span>         <span class="number">3</span>        <span class="number">2.45</span>      <span class="number">1</span>          <span class="number">0</span></span><br><span class="line">## <span class="number">2</span>             <span class="number">0</span>              <span class="number">0</span>         <span class="number">0</span>        <span class="number">0.00</span>     <span class="number">-1</span>          <span class="number">1</span></span><br><span class="line">## <span class="number">3</span>             <span class="number">4</span>              <span class="number">5</span>         <span class="number">3</span>        <span class="number">4.75</span>      <span class="number">1</span>          <span class="number">0</span></span><br><span class="line">## <span class="number">4</span>             <span class="number">6</span>              <span class="number">7</span>         <span class="number">4</span>        <span class="number">1.65</span>      <span class="number">1</span>          <span class="number">0</span></span><br><span class="line">## <span class="number">5</span>             <span class="number">8</span>              <span class="number">9</span>         <span class="number">3</span>        <span class="number">5.30</span>      <span class="number">1</span>          <span class="number">0</span></span><br><span class="line">## <span class="number">6</span>             <span class="number">0</span>              <span class="number">0</span>         <span class="number">0</span>        <span class="number">0.00</span>     <span class="number">-1</span>          <span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute cluster centers</span></span><br><span class="line">irisP &lt;- classCenter(training[,c(<span class="number">3</span>,<span class="number">4</span>)], training$Species, modFit$finalModel$prox)</span><br><span class="line"><span class="comment"># convert irisP to data frame and add Species column</span></span><br><span class="line">irisP &lt;- as.data.frame(irisP); irisP$Species &lt;- rownames(irisP)</span><br><span class="line"><span class="comment"># plot data points</span></span><br><span class="line">p &lt;- qplot(Petal.Width, Petal.Length, col=Species,data=training)</span><br><span class="line"><span class="comment"># add the cluster centers</span></span><br><span class="line">p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=<span class="number">5</span>,shape=<span class="number">4</span>,data=irisP)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-45-1.png" style="display: block; margin: auto;"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># predict outcome for test data set using the random forest model</span></span><br><span class="line">pred &lt;- predict(modFit,testing)</span><br><span class="line"><span class="comment"># logic value for whether or not the rf algorithm predicted correctly</span></span><br><span class="line">testing$predRight &lt;- pred==testing$Species</span><br><span class="line"><span class="comment"># tabulate results</span></span><br><span class="line">table(pred,testing$Species)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#             </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># pred         setosa versicolor virginica</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   setosa         15          0         0</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   versicolor      0         15         3</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   virginica       0          0        12</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot data points with the incorrect classification highlighted</span></span><br><span class="line">qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main=<span class="string">"newdata Predictions"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-45-2.png" style="display: block; margin: auto;"></p>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="boosting">Boosting</h2>
<ul>
<li><strong>boosting</strong> = one of the most widely used and accurate prediction models, along with random forest</li>
<li>boosting can be done with any set of classifiers, and a well-known approach is <a href="http://en.wikipedia.org/wiki/Gradient_boosting" target="_blank" rel="noopener">gradient boosting</a></li>
<li><p>more detail tutorial can be found <a href="http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf" target="_blank" rel="noopener">here</a></p></li>
<li><strong>process</strong>: take a group of weak predictors <span class="math inline">\(\rightarrow\)</span> weight them and add them up <span class="math inline">\(\rightarrow\)</span> result in a stronger predictor
<ul>
<li>start with a set of classifiers <span class="math inline">\(h_1, \ldots, h_k\)</span>
<ul>
<li>examples: all possible trees, all possible regression models, all possible cutoffs (divide data into different parts)</li>
</ul></li>
<li>calculate a weighted sum of classifiers as the prediction value <span class="math display">\[f(x) = \sum_i \alpha_i h_i(x)\]</span> where <span class="math inline">\(\alpha_i\)</span> = coefficient/weight and <span class="math inline">\(h_i(x)\)</span> = value of classifier
<ul>
<li>goal = minimize error (on training set)</li>
<li>select one <span class="math inline">\(h\)</span> at each step (iterative)</li>
<li>calculate weights based on errors</li>
<li>up-weight missed classifications and select next <span class="math inline">\(h\)</span></li>
</ul></li>
</ul></li>
<li><p><strong><em>example</em></strong></p></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-46-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>we start with space with <strong>blue +</strong> and <strong>red -</strong> and the goal is to classify all the object correctly</li>
<li>only straight lines will be used for classification</li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-47-1.png" style="display: block; margin: auto;"> <img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-48-1.png" style="display: block; margin: auto;"> <img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-49-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>from the above, we can see that a group of weak predictors (lines in this case), can be combined and weighed to become a much stronger predictor</li>
</ul>
<h3 id="r-commands-and-examples-2">R Commands and Examples</h3>
<ul>
<li><code>gbm &lt;- train(outcome ~ variables, method=&quot;gbm&quot;, data=train, verbose=F)</code> = run boosting model on the given data
<ul>
<li>options for <code>method</code> for boosting
<ul>
<li><a href="http://cran.r-project.org/web/packages/gbm/index.html" target="_blank" rel="noopener"><code>gbm</code></a> - boosting with trees</li>
<li><a href="http://cran.r-project.org/web/packages/mboost/index.html" target="_blank" rel="noopener"><code>mboost</code></a> - model based boosting</li>
<li><a href="http://cran.r-project.org/web/packages/ada/index.html" target="_blank" rel="noopener"><code>ada</code></a> - statistical boosting based on <a href="http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aos/1016218223" target="_blank" rel="noopener">additive logistic regression</a></li>
<li><a href="http://cran.r-project.org/web/packages/GAMBoost/index.html" target="_blank" rel="noopener"><code>gamBoost</code></a> for boosting generalized additive models</li>
<li><em><strong>Note</strong>: differences between packages include the choice of basic classification functions and combination rules </em></li>
</ul></li>
</ul></li>
<li><p><code>predict</code> function can be used to apply the model to test data, similar to the rest of the algorithms in <code>caret</code> package</p></li>
<li><p><strong><em>example</em></strong></p></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ISLR)</span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">data(Wage)</span><br><span class="line"><span class="comment"># remove log wage variable (we are trying to predict wage)</span></span><br><span class="line">Wage &lt;- subset(Wage,select=-c(logwage))</span><br><span class="line"><span class="comment"># create train/test data sets</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=Wage$wage,p=<span class="number">0.7</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- Wage[inTrain,]; testing &lt;- Wage[-inTrain,]</span><br><span class="line"><span class="comment"># run the gbm model</span></span><br><span class="line">modFit &lt;- train(wage ~ ., method=<span class="string">"gbm"</span>,data=training,verbose=<span class="literal">FALSE</span>)</span><br><span class="line"><span class="comment"># print model summary</span></span><br><span class="line">print(modFit)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Stochastic Gradient Boosting </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2102 samples</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    9 predictor</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># No pre-processing</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Resampling: Bootstrapped (25 reps) </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Summary of sample sizes: 2102, 2102, 2102, 2102, 2102, 2102, ... </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Resampling results across tuning parameters:</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   interaction.depth  n.trees  RMSE      Rsquared   MAE     </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   1                   50      35.49016  0.3114599  24.35923</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   1                  100      34.95818  0.3222808  23.95843</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   1                  150      34.90950  0.3236008  23.97720</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   2                   50      34.91530  0.3248413  23.87687</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   2                  100      34.79451  0.3280679  23.83395</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   2                  150      34.90113  0.3251693  23.94022</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   3                   50      34.79871  0.3280576  23.76077</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   3                  100      34.97846  0.3223291  23.92947</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   3                  150      35.19116  0.3163043  24.10556</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Tuning parameter 'shrinkage' was held constant at a value of 0.1</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Tuning parameter 'n.minobsinnode' was held constant at a value of 10</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># RMSE was used to select the optimal model using the smallest value.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># The final values used for the model were n.trees = 100,</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  interaction.depth = 2, shrinkage = 0.1 and n.minobsinnode = 10.</span></span></span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="model-based-prediction">Model Based Prediction</h2>
<ul>
<li><strong>model based prediction</strong> = assumes the data follow a probabilistic model/distribution and use <em>Bayes’ theorem</em> to identify optimal classifiers/variables
<ul>
<li>can potentially take advantage of structure of the data</li>
<li>could help reduce computational complexity (reduce variables)</li>
<li>can be reasonably accurate on real problems</li>
</ul></li>
<li>this approach does make <em><strong>additional assumptions</strong></em> about the data, which can lead to model failure/reduced accuracy if they are too far off</li>
<li><strong><em>goal</em></strong> = build parameter-based model (based on probabilities) for conditional distribution <span class="math inline">\(P(Y = k~|~X = x)\)</span>, or the probability of the outcome <span class="math inline">\(Y\)</span> is equal to a particular value <span class="math inline">\(k\)</span> given a specific set of predictor variables <span class="math inline">\(x\)</span>
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(X\)</span> is the data for the model (observations for all predictor variables), which is also known as the <strong>design matrix</strong> </em></li>
</ul></li>
<li><strong>typical approach/process</strong>
<ol type="1">
<li>start with the quantity <span class="math inline">\(P(Y = k~|~X = x)\)</span></li>
<li>apply <em>Bayes’ Theorem</em> such that <span class="math display">\[ P(Y = k ~|~ X=x) = \frac{P(X=x~|~Y=k)P(Y=k)}{\sum_{\ell=1}^K P(X=x ~|~Y = \ell) P(Y=\ell)}\]</span> where the denominator is simply the sum of probabilities for the predictor variables are the set specified in <span class="math inline">\(x\)</span> for all outcomes of <span class="math inline">\(Y\)</span></li>
<li>assume the term <span class="math inline">\(P(X=x~|~Y=k)\)</span> in the numerator follows a parameter-based probability distribution, or <span class="math inline">\(f_k(x)\)</span>
<ul>
<li>common choice = <strong><em>Gaussian distribution</em></strong> <span class="math display">\[f_k(x) = \frac{1}{\sigma_k \sqrt{2 \pi}}e^{-\frac{(x-\mu_k)^2}{2\sigma_k^2}}\]</span></li>
</ul></li>
<li>assume the probability for the outcome <span class="math inline">\(Y\)</span> to take on value of <span class="math inline">\(k\)</span>, or <span class="math inline">\(P(Y=k)\)</span>, is determined from the data to be some known quantity <span class="math inline">\(\pi_k\)</span>
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(P(Y=k)\)</span> is known as the <a href="http://en.wikipedia.org/wiki/Prior_probability" target="_blank" rel="noopener"><strong>prior probability</strong></a> </em></li>
</ul></li>
<li>so the quantity <span class="math inline">\(P(Y = k~|~X = x)\)</span> can be rewritten as <span class="math display">\[P(Y = k ~|~ X=x) = \frac{f_k(x) \pi_k}{\sum_{\ell = 1}^K f_{\ell}(x) \pi_{\ell}}\]</span></li>
<li>estimate the parameters (<span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(\sigma_k^2\)</span>) for the function <span class="math inline">\(f_k(x)\)</span> from the data</li>
<li>calculate <span class="math inline">\(P(Y = k~|~X = x)\)</span> using the parameters</li>
<li>the outcome <span class="math inline">\(Y\)</span> is where the value of <span class="math inline">\(P(Y = k ~|~ X = x)\)</span> is the highest</li>
</ol></li>
<li>prediction models that leverage this approach
<ul>
<li><a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis" target="_blank" rel="noopener"><strong><em>linear discriminant analysis</em></strong></a> = assumes <span class="math inline">\(f_k(x)\)</span> is multivariate Gaussian distribution with <strong>same</strong> covariance for each predictor variables
<ul>
<li>effectively drawing lines through “covariate space”</li>
</ul></li>
<li><a href="http://en.wikipedia.org/wiki/Quadratic_classifier" target="_blank" rel="noopener"><strong><em>quadratic discriminant analysis</em></strong></a> = assumes <span class="math inline">\(f_k(x)\)</span> is multivariate Gaussian distribution with <strong>different</strong> covariance for predictor variables
<ul>
<li>effectively drawing curves through “covariate space”</li>
</ul></li>
<li><a href="http://www.stat.washington.edu/mclust/" target="_blank" rel="noopener"><strong><em>normal mixture modeling</em></strong></a> = assumes more complicated covariance matrix for the predictor variables</li>
<li><a href="http://en.wikipedia.org/wiki/Naive_Bayes_classifier" target="_blank" rel="noopener"><strong><em>naive Bayes</em></strong></a> = assumes independence between predictor variables/features for model building (covariance = 0)
<ul>
<li><em><strong>Note</strong>: this may be an incorrect assumption but it helps to reduce computational complexity and may still produce a useful result </em></li>
</ul></li>
</ul></li>
</ul>
<h3 id="linear-discriminant-analysis">Linear Discriminant Analysis</h3>
<ul>
<li>to compare the probability for outcome <span class="math inline">\(Y = k\)</span> versus probability for outcome <span class="math inline">\(Y = k\)</span>, we can look at the ratio of <span class="math display">\[\frac{P(Y=k~|~X=x)}{P(Y=j~|~X=x)}\]</span></li>
<li>take the <strong>log</strong> of the ratio and apply Bayes’ Theorem, we get <span class="math display">\[\log \frac{P(Y = k~|~X=x)}{P(Y = j~|~X=x)} = \log \frac{f_k(x)}{f_j(x)} + \log \frac{\pi_k}{\pi_j}\]</span> which is effectively the log ratio of probability density functions plus the log ratio of prior probabilities
<ul>
<li><em><strong>Note</strong>: <span class="math inline">\(\log\)</span> = <strong>monotone</strong> transformation, which means taking the <span class="math inline">\(\log\)</span> of a quantity does not affect implication of the ratio since th <span class="math inline">\(\log(ratio)\)</span> is <em>directly correlated</em> with ratio </em></li>
</ul></li>
<li>if we substitute <span class="math inline">\(f_k(x)\)</span> and <span class="math inline">\(f_l(x)\)</span> with Gaussian probability density functions <span class="math display">\[f(x) = \frac{1}{\sigma \sqrt{2 \pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\]</span> so the ratio can be simplified to <span class="math display">\[\log \frac{P(Y = k~|~X=x)}{P(Y = j~|~X=x)} = \log \frac{\pi_k}{\pi_j} - \frac{1}{2}(\mu_k + \mu_j)^T \Sigma^{-1}(\mu_k + \mu_j) + x^T \Sigma^{-1} (\mu_k - \mu_j)\]</span> where <span class="math inline">\(\Sigma^{-1}\)</span> = covariance matrix for the predictor variables, <span class="math inline">\(x^T\)</span> = set of predictor variables, and <span class="math inline">\(\mu_k\)</span> / <span class="math inline">\(\mu_j\)</span> = mean of <span class="math inline">\(k\)</span>, <span class="math inline">\(j\)</span> respectively</li>
<li>as annotated above, the log-ratio is effectively an equation of a line for a set of predictor variables <span class="math inline">\(x\)</span> <span class="math inline">\(\rightarrow\)</span> the first two terms are constants and the last term is in the form of <span class="math inline">\(X \beta\)</span>
<ul>
<li><em><strong>Note</strong>: the lines are also known as <strong>decision boundaries</strong> </em></li>
</ul></li>
<li>therefore, we can classify values based on <strong><em>which side of the line</em></strong> the value is located (<span class="math inline">\(k\)</span> vs <span class="math inline">\(j\)</span>)</li>
<li><strong>discriminant functions</strong> are used to determine value of <span class="math inline">\(k\)</span>, the functions are in the form of <span class="math display">\[\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2}\mu_k \Sigma^{-1}\mu_k + log(\mu_k)\]</span>
<ul>
<li>plugging in the set of predictor variables, <span class="math inline">\(x^T\)</span>, into the discriminant function, we can find the value of <span class="math inline">\(k\)</span> that <strong><em>maximizes</em></strong> the function <span class="math inline">\(\delta_k(x)\)</span></li>
<li>the terms of the discriminant function can be estimated using maximum likelihood</li>
</ul></li>
<li><p>the predicted value for the outcome is therefore <span class="math inline">\(\hat{Y}(x) = argmax_k \delta_k(x)\)</span></p></li>
<li><strong><em>example</em></strong>
<ul>
<li>classify a group of values into 3 groups using 2 variables (<span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> coordinates)</li>
<li>3 lines are draw to split the data into 3 Gaussian distributions
<ul>
<li>each line splits the data into two groups <span class="math inline">\(\rightarrow\)</span> 1 vs 2, 2 vs 3, 1 vs 3</li>
<li>each side of the line represents a region where the probability of one group (1, 2, or 3) is the highest<br>
</li>
</ul></li>
<li>the result is represented in the the following graph</li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-51-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><strong>R Commands</strong>
<ul>
<li><code>lda&lt;-train(outcome ~ predictors, data=training, method=&quot;lda&quot;)</code> = constructs a linear discriminant analysis model on the predictors with the provided training data</li>
<li><code>predict(lda, test)</code> = applies the LDA model to test data and return the prediction results in data frame</li>
<li><strong><em>example: <code>caret</code> package</em></strong></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load data</span></span><br><span class="line">data(iris)</span><br><span class="line"><span class="comment"># create training and test sets</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=iris$Species,p=<span class="number">0.7</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- iris[inTrain,]</span><br><span class="line">testing &lt;- iris[-inTrain,]</span><br><span class="line"><span class="comment"># run the linear discriminant analysis on training data</span></span><br><span class="line">lda &lt;- train(Species ~ .,data=training,method=<span class="string">"lda"</span>)</span><br><span class="line"><span class="comment"># predict test outcomes using LDA model</span></span><br><span class="line">pred.lda &lt;- predict(lda,testing)</span><br><span class="line"><span class="comment"># print results</span></span><br><span class="line">pred.lda</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  [1] setosa     setosa     setosa     setosa     setosa     setosa    </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  [7] setosa     setosa     setosa     setosa     setosa     setosa    </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [13] setosa     setosa     setosa     versicolor versicolor versicolor</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [19] versicolor versicolor versicolor versicolor versicolor versicolor</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [25] versicolor versicolor versicolor versicolor versicolor versicolor</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [31] virginica  virginica  virginica  virginica  virginica  virginica </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [37] virginica  virginica  virginica  versicolor virginica  virginica </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [43] virginica  virginica  virginica </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Levels: setosa versicolor virginica</span></span></span><br></pre></td></tr></table></figure>
<h3 id="naive-bayes">Naive Bayes</h3>
<ul>
<li>for predictors <span class="math inline">\(X_1,\ldots,X_m\)</span>, we want to model <span class="math inline">\(P(Y = k ~|~ X_1,\ldots,X_m)\)</span></li>
<li>by applying <em>Bayes’ Theorem</em>, we get <span class="math display">\[P(Y = k ~|~ X_1,\ldots,X_m) = \frac{\pi_k P(X_1,\ldots,X_m~|~ Y=k)}{\sum_{\ell = 1}^K P(X_1,\ldots,X_m ~|~ Y=k) \pi_{\ell}}\]</span></li>
<li>since the denominator is just a sum (constant), we can rewrite the quantity as <span class="math display">\[P(Y = k ~|~ X_1,\ldots,X_m) \propto \pi_k P(X_1,\ldots,X_m~|~ Y=k)\]</span> or the probability is <strong>proportional to</strong> the numerator
<ul>
<li><em><strong>Note</strong>: maximizing the numerator is the same as maximizing the ratio </em></li>
</ul></li>
<li><span class="math inline">\(\pi_k P(X_1,\ldots,X_m~|~ Y=k)\)</span> can be rewritten as <span class="math display">\[\begin{aligned}
\pi_k P(X_1,\ldots,X_m~|~ Y=k) &amp; = \pi_k P(X_1 ~|~ Y = k)P(X_2,\ldots,X_m ~|~ X_1,Y=k) \\
&amp; = \pi_k P(X_1 ~|~ Y = k) P(X_2 ~|~ X_1, Y=k) P(X_3,\ldots,X_m ~|~ X_1,X_2, Y=k) \\
&amp; = \pi_k P(X_1 ~|~ Y = k) P(X_2 ~|~ X_1, Y=k)\ldots P(X_m~|~X_1\ldots,X_{m-1},Y=k) \\
\end{aligned}\]</span> where each variable has its own probability term that depends on all the terms before it
<ul>
<li>this is effectively indicating that each of the predictors may be dependent on other predictors</li>
</ul></li>
<li>however, if we make the assumption that all predictor variables are <strong>independent</strong> to each other, the quantity can be simplified to <span class="math display">\[ \pi_k P(X_1,\ldots,X_m~|~ Y=k) \approx \pi_k P(X_1 ~|~ Y = k) P(X_2 ~|~ Y = k)\ldots P(X_m ~|~,Y=k)\]</span> which is effectively the product of the prior probability for <span class="math inline">\(k\)</span> and the probability of variables <span class="math inline">\(X_1,\ldots,X_m\)</span> given that <span class="math inline">\(Y = k\)</span>
<ul>
<li><em><strong>Note</strong>: the assumption is naive in that it is unlikely the predictors are completely independent of each other, but this model still produces <strong>useful</strong> results particularly with <strong>large number of binary/categorical variables</strong> </em>
<ul>
<li>text and document classification usually require large quantities of binary and categorical features</li>
</ul></li>
</ul></li>
<li><strong>R Commands</strong>
<ul>
<li><code>nb &lt;- train(outcome ~ predictors, data=training, method=&quot;nb&quot;)</code> = constructs a naive Bayes model on the predictors with the provided training data</li>
<li><code>predict(nb, test)</code> = applies the naive Bayes model to test data and return the prediction results in data frame</li>
<li><strong><em>example: <code>caret</code> package</em></strong></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using the same data from iris, run naive Bayes on training data</span></span><br><span class="line">nb &lt;- train(Species ~ ., data=training,method=<span class="string">"nb"</span>)</span><br><span class="line"><span class="comment"># predict test outcomes using naive Bayes model</span></span><br><span class="line">pred.nb &lt;- predict(nb,testing)</span><br><span class="line"><span class="comment"># print results</span></span><br><span class="line">pred.nb</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  [1] setosa     setosa     setosa     setosa     setosa     setosa    </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#  [7] setosa     setosa     setosa     setosa     setosa     setosa    </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [13] setosa     setosa     setosa     versicolor versicolor versicolor</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [19] versicolor versicolor versicolor versicolor versicolor versicolor</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [25] versicolor versicolor versicolor versicolor versicolor versicolor</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [31] virginica  virginica  virginica  virginica  virginica  virginica </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [37] virginica  virginica  virginica  versicolor versicolor virginica </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># [43] virginica  virginica  virginica </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Levels: setosa versicolor virginica</span></span></span><br></pre></td></tr></table></figure>
<h3 id="compare-results-for-lda-and-naive-bayes">Compare Results for LDA and Naive Bayes</h3>
<ul>
<li>linear discriminant analysis and naive Bayes generally produce similar results for small data sets</li>
<li>for our example data from <code>iris</code> data set, we can compare the prediction the results from the two models</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tabulate the prediction results from LDA and naive Bayes</span></span><br><span class="line">table(pred.lda,pred.nb)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#             pred.nb</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># pred.lda     setosa versicolor virginica</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   setosa         15          0         0</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   versicolor      0         16         0</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   virginica       0          1        13</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create logical variable that returns TRUE for when predictions from the two models match</span></span><br><span class="line">equalPredictions &lt;- (pred.lda==pred.nb)</span><br><span class="line"><span class="comment"># plot the comparison</span></span><br><span class="line">qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-54-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>as we can see from above, only one data point, which is located inbetween the two classes is predicted differently by the two models</li>
</ul>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="model-selection">Model Selection</h2>
<ul>
<li>the general behavior of the errors of the training and test sets are as follows
<ul>
<li>as the number of predictors used increases (or model complexity), the error for the prediction model on <em>training</em> set <strong>always decreases</strong></li>
<li>the error for the prediction model on <em>test</em> set <strong>decreases first and then increases</strong> as number of predictors used approaches the total number of predictors available</li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-55-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>this is expected since as more predictors used, the model is more likely to <em>overfit</em> the training data</li>
<li><strong>goal</strong> in selecting models = <strong><em>avoid overfitting</em></strong> on training data and <strong><em>minimize error</em></strong> on test data</li>
<li><strong><em>approaches</em></strong>
<ul>
<li>split samples</li>
<li>decompose expected prediction error</li>
<li>hard thresholding for high-dimensional data</li>
<li>regularization for regression
<ul>
<li>ridge regression</li>
<li>lasso regression</li>
</ul></li>
</ul></li>
<li><strong><em>problems</em></strong>
<ul>
<li>time/computational complexity limitations</li>
<li>high dimensional</li>
</ul></li>
</ul>
<h3 id="example-training-vs-test-error-for-combination-of-predictors">Example: Training vs Test Error for Combination of Predictors</h3>
<ul>
<li><em><strong>Note</strong>: the code for this example comes from <a href="http://www.cbcb.umd.edu/~hcorrada/PracticalML/src/selection.R" target="_blank" rel="noopener">Hector Corrada Bravo’s Practical Machine Learning Course</a> </em></li>
<li>to demonstrate the behavior of training and test errors, the <code>prostate</code> dataset from <em>Elements of Statistical Learning</em> is used</li>
<li>all combinations of predictors are used to produce prediction models, and Residual Squared Error (RSS) is calculated for all models on both the training and test sets</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ElemStatLearn)</span><br><span class="line"><span class="comment"># load data and set seed</span></span><br><span class="line">data(prostate); set.seed(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># define outcome y and predictors x</span></span><br><span class="line">covnames &lt;- names(prostate[-(<span class="number">9</span>:<span class="number">10</span>)])</span><br><span class="line">y &lt;- prostate$lpsa; x &lt;- prostate[,covnames]</span><br><span class="line"><span class="comment"># create test set predictors and outcomes</span></span><br><span class="line">train.ind &lt;- sample(nrow(prostate), ceiling(nrow(prostate))/<span class="number">2</span>)</span><br><span class="line">y.test &lt;- prostate$lpsa[-train.ind]; x.test &lt;- x[-train.ind,]</span><br><span class="line"><span class="comment"># create training set predictors and outcomes</span></span><br><span class="line">y &lt;- prostate$lpsa[train.ind]; x &lt;- x[train.ind,]</span><br><span class="line"><span class="comment"># p = number of predictors</span></span><br><span class="line">p &lt;- length(covnames)</span><br><span class="line"><span class="comment"># initialize the list of residual sum squares</span></span><br><span class="line">rss &lt;- list()</span><br><span class="line"><span class="comment"># loop through each combination of predictors and build models</span></span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:p) &#123;</span><br><span class="line">    <span class="comment"># compute matrix for p choose i predictors for i = 1...p (creates i x p matrix)</span></span><br><span class="line">    Index &lt;- combn(p,i)</span><br><span class="line">    <span class="comment"># calculate residual sum squares of each combination of predictors</span></span><br><span class="line">    rss[[i]] &lt;- apply(Index, <span class="number">2</span>, <span class="keyword">function</span>(is) &#123;</span><br><span class="line">    	<span class="comment"># take each combination (or column of Index matrix) and create formula for regression</span></span><br><span class="line">        form &lt;- as.formula(paste(<span class="string">"y~"</span>, paste(covnames[is], collapse=<span class="string">"+"</span>), sep=<span class="string">""</span>))</span><br><span class="line">        <span class="comment"># run linear regression with combination of predictors on training data</span></span><br><span class="line">        isfit &lt;- lm(form, data=x)</span><br><span class="line">        <span class="comment"># predict outcome for all training data points</span></span><br><span class="line">        yhat &lt;- predict(isfit)</span><br><span class="line">        <span class="comment"># calculate residual sum squares for predictions on training data</span></span><br><span class="line">        train.rss &lt;- sum((y - yhat)^<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># predict outcome for all test data points</span></span><br><span class="line">        yhat &lt;- predict(isfit, newdata=x.test)</span><br><span class="line">        <span class="comment"># calculate residual sum squares for predictions on test data</span></span><br><span class="line">        test.rss &lt;- sum((y.test - yhat)^<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># store each pair of training and test residual sum squares as a list</span></span><br><span class="line">        c(train.rss, test.rss)</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># set up plot with labels, title, and proper x and y limits</span></span><br><span class="line">plot(<span class="number">1</span>:p, <span class="number">1</span>:p, type=<span class="string">"n"</span>, ylim=range(unlist(rss)), xlim=c(<span class="number">0</span>,p),</span><br><span class="line">	xlab=<span class="string">"Number of Predictors"</span>, ylab=<span class="string">"Residual Sum of Squares"</span>,</span><br><span class="line">	main=<span class="string">"Prostate Cancer Data - Training vs Test RSS"</span>)</span><br><span class="line"><span class="comment"># add data points for training and test residual sum squares</span></span><br><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:p) &#123;</span><br><span class="line">	<span class="comment"># plot training residual sum squares in blue</span></span><br><span class="line">    points(rep(i, ncol(rss[[i]])), rss[[i]][<span class="number">1</span>, ], col=<span class="string">"blue"</span>, cex = <span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># plot test residual sum squares in red</span></span><br><span class="line">    points(rep(i, ncol(rss[[i]])), rss[[i]][<span class="number">2</span>, ], col=<span class="string">"red"</span>, cex = <span class="number">0.5</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># find the minimum training RSS for each combination of predictors</span></span><br><span class="line">minrss &lt;- sapply(rss, <span class="keyword">function</span>(x) min(x[<span class="number">1</span>,]))</span><br><span class="line"><span class="comment"># plot line through the minimum training RSS data points in blue</span></span><br><span class="line">lines((<span class="number">1</span>:p), minrss, col=<span class="string">"blue"</span>, lwd=<span class="number">1.7</span>)</span><br><span class="line"><span class="comment"># find the minimum test RSS for each combination of predictors</span></span><br><span class="line">minrss &lt;- sapply(rss, <span class="keyword">function</span>(x) min(x[<span class="number">2</span>,]))</span><br><span class="line"><span class="comment"># plot line through the minimum test RSS data points in blue</span></span><br><span class="line">lines((<span class="number">1</span>:p), minrss, col=<span class="string">"red"</span>, lwd=<span class="number">1.7</span>)</span><br><span class="line"><span class="comment"># add legend</span></span><br><span class="line">legend(<span class="string">"topright"</span>, c(<span class="string">"Train"</span>, <span class="string">"Test"</span>), col=c(<span class="string">"blue"</span>, <span class="string">"red"</span>), pch=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-56-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>from the above, we can clearly that test RSS error approaches the minimum at around 3 predictors and increases slightly as more predictors are used</li>
</ul>
<h3 id="split-samples">Split Samples</h3>
<ul>
<li>the best method to pick predictors/model is to split the given data into different test sets</li>
<li><strong>process</strong>
<ol type="1">
<li>divide data into training/test/validation sets (60 - 20 - 20 split)</li>
<li>train all competing models on the training data</li>
<li>apply the models on validation data and choose the best performing model</li>
<li>re-split data into training/test/validation sets and repeat steps 1 to 3</li>
<li>apply the overall best performing model on test set to appropriately assess performance on new data</li>
</ol></li>
<li><strong>common problems</strong>
<ul>
<li>limited data = if not enough data is available, it may not be possible to produce a good model fit after splitting the data into 3 sets</li>
<li>computational complexity = modeling with all subsets of models can be extremely taxing in terms of computations, especially when a large number of predictors are available</li>
</ul></li>
</ul>
<h3 id="decompose-expected-prediction-error">Decompose Expected Prediction Error</h3>
<ul>
<li>the outcome <span class="math inline">\(Y_i\)</span> can be modeled by <span class="math display">\[Y_i = f(X_i) + \epsilon_i\]</span> where <span class="math inline">\(\epsilon_i\)</span> = error term</li>
<li>the <strong>expected prediction error</strong> is defined as <span class="math display">\[EPE(\lambda) = E\left[\left(Y - \hat{f}_{\lambda}(X)\right)^2\right]\]</span> where <span class="math inline">\(\lambda\)</span> = specific set of tuning parameters</li>
<li>estimates from the model constructed with training data can be denoted as <span class="math inline">\(\hat{f}_{\lambda}(x^*)\)</span> where <span class="math inline">\(X = x^*\)</span> is the new data point that we would like to predict at</li>
<li>the expected prediction error is as follows <span class="math display">\[\begin{aligned}
E\left[\left(Y - \hat{f}_{\lambda}(x^*)\right)^2\right] &amp; = \sigma^2 + \left(E[\hat{f}_{\lambda}(x^*)] - f(x^*)\right)^2 + E\left[\hat{f}_{\lambda}(x^*) - E[\hat{f}_{\lambda}(x^*)]\right]^2\\
&amp; = \mbox{Irreducible Error} + \mbox{Bias}^2 + \mbox{Variance}\\
\end{aligned} \]</span>
<ul>
<li><strong>goal of prediction model</strong> = minimize overall expected prediction error</li>
<li>irreducible error = noise inherent to the data collection process <span class="math inline">\(\rightarrow\)</span> cannot be reduced</li>
<li>bias/variance = can be traded in order to find optimal model (least error)</li>
</ul></li>
</ul>
<h3 id="hard-thresholding">Hard Thresholding</h3>
<ul>
<li>if there are more predictors than observations (high-dimensional data), linear regressions will only return coefficients for some of the variables because there’s not enough data to estimate the rest of the parameters
<ul>
<li>conceptually, this occurs because the design matrix that the model is based on cannot be inverted</li>
<li><em><strong>Note</strong>: ridge regression can help address this problem </em></li>
</ul></li>
<li><strong>hard thresholding</strong> can help estimate the coefficients/model by taking subsets of predictors and building models</li>
<li><strong>process</strong>
<ul>
<li>model the outcome as <span class="math display">\[Y_i = f(X_i) + \epsilon_i\]</span> where <span class="math inline">\(\epsilon_i\)</span> = error term</li>
<li>assume the prediction estimate has a linear form <span class="math display">\[\hat{f}_{\lambda}(x) = x&#39;\beta\]</span> where only <span class="math inline">\(\lambda\)</span> coefficients for the set of predictors <span class="math inline">\(x\)</span> are <strong><em>nonzero</em></strong></li>
<li>after setting the value of <span class="math inline">\(\lambda\)</span>, compute models using all combinations of <span class="math inline">\(\lambda\)</span> variables to find which variables’ coefficients should be set to be zero</li>
</ul></li>
<li><strong>problem</strong>
<ul>
<li>computationally intensive</li>
</ul></li>
<li><strong><em>example</em></strong>
<ul>
<li>as we can see from the results below, some of the coefficients have values of <code>NA</code></li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load prostate data</span></span><br><span class="line">data(prostate)</span><br><span class="line"><span class="comment"># create subset of observations with 10 variables</span></span><br><span class="line">small = prostate[<span class="number">1</span>:<span class="number">5</span>,]</span><br><span class="line"><span class="comment"># print linear regression</span></span><br><span class="line">lm(lpsa ~ .,data =small)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Call:</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># lm(formula = lpsa ~ ., data = small)</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Coefficients:</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># (Intercept)       lcavol      lweight          age         lbph  </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     9.60615      0.13901     -0.79142      0.09516           NA  </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#         svi          lcp      gleason        pgg45    trainTRUE  </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#          NA           NA     -2.08710           NA           NA</span></span></span><br></pre></td></tr></table></figure>
<h3 id="regularized-regression-concept-resource">Regularized Regression Concept (<a href="http://www.cbcb.umd.edu/~hcorrada/PracticalML/pdf/lectures/selection.pdf" target="_blank" rel="noopener">Resource</a>)</h3>
<ul>
<li><strong>regularized regression</strong> = fit a regression model and adjust for the large coefficients in attempt to help with bias/variance trade-off or model selection
<ul>
<li>when running regressions unconstrained (without specifying any criteria for coefficients), the model may be susceptible to high variance (coefficients explode <span class="math inline">\(\rightarrow\)</span> very large values) if there are variables that are highly correlated</li>
<li>controlling/regularizing coefficients may slightly <strong><em>increase bias</em></strong> (lose a bit of prediction capability) but will <strong><em>reduce variance</em></strong> and improve the prediction error</li>
<li>however, this approach may be very demanding computationally and generally does not perform as well as random forest/boosting</li>
</ul></li>
<li><strong>Penalized Residual Sum of Squares (PRSS)</strong> is calculated by adding a penalty term to the prediction squared error <span class="math display">\[PRSS(\beta) = \sum_{j=1}^n (Y_j - \sum_{i=1}^m \beta_{1i} X_{ij})^2 + P(\lambda; \beta)\]</span>
<ul>
<li>penalty shrinks coefficients if their values become too large</li>
<li>penalty term is generally used to reduce complexity and variance for the model, while respecting the structure of the data/relationship</li>
</ul></li>
<li><strong><em>example: co-linear variables</em></strong>
<ul>
<li>given a linear model, <span class="math display">\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon\]</span> where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are nearly perfectly correlated (co-linear)</li>
<li>the model can then be approximated by this model by omitting <span class="math inline">\(X_2\)</span>, so the model becomes <span class="math display">\[Y = \beta_0 + (\beta_1 + \beta_2)X_1 + \epsilon\]</span></li>
<li>with the above model, we can get a good estimate of <span class="math inline">\(Y\)</span>
<ul>
<li>the estimate of <span class="math inline">\(Y\)</span> will be biased</li>
<li>but the variance of the prediction may be reduced</li>
</ul></li>
</ul></li>
</ul>
<h3 id="regularized-regression---ridge-regression">Regularized Regression - Ridge Regression</h3>
<ul>
<li>the penalized residual sum of squares (PRSS) takes the form of <span class="math display">\[PRSS(\beta_j) = \sum_{i=1}^N \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^p \beta_j^2\]</span></li>
<li>this is equivalent to solving the equation <span class="math display">\[PRSS(\beta_j) = \sum_{i=1}^N \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2\]</span> subject to constraint <span class="math inline">\(\sum_{j=1}^p \beta_j^2 \leq s\)</span> where <span class="math inline">\(s\)</span> is inversely proportional to <span class="math inline">\(\lambda\)</span>
<ul>
<li>if the coefficients <span class="math inline">\(\beta_j\)</span> are large in value, the term <span class="math inline">\(\sum_{j=1}^p \beta_j^2\)</span> will cause the overall PRSS value to increase, leading to worse models</li>
<li>the presence of the term thus requires some of the coefficients to be small</li>
</ul></li>
<li>inclusion of <span class="math inline">\(\lambda\)</span> makes the problem <em>non-singular</em> even if <span class="math inline">\(X^TX\)</span> is not invertible
<ul>
<li>this means that even in cases where there are more predictors than observations, the coefficients of the predictors can still be estimated</li>
</ul></li>
<li><span class="math inline">\(\lambda\)</span> = tuning parameter
<ul>
<li>controls size of coefficients or the amount of regularization</li>
<li>as <span class="math inline">\(\lambda \rightarrow 0\)</span>, the result approaches the least square solution</li>
<li>as <span class="math inline">\(\lambda \rightarrow \infty\)</span>, all of the coefficients receive large penalties and the conditional coefficients <span class="math inline">\(\hat{\beta}_{\lambda=\infty}^{ridge}\)</span> approaches zero collectively</li>
<li><span class="math inline">\(\lambda\)</span> should be carefully chosen through cross-validation/other techniques to find the optimal tradeoff of bias for variance</li>
<li><em><strong>Note</strong>: it is important realize that all coefficients (though they may be shrunk to very small values) will <strong>still be included</strong> in the model when applying ridge regression </em></li>
</ul></li>
<li><strong>R Commands</strong>
<ul>
<li>[<code>MASS</code> package] <code>ridge&lt;-lm.ridge(outcome ~ predictors, data=training, lambda=5)</code> = perform ridge regression with given outcome and predictors using the provided <span class="math inline">\(\lambda\)</span> value
<ul>
<li><em><strong>Note</strong>: the predictors are <strong>centered and scaled first</strong> before the regression is run </em></li>
<li><code>lambda=5</code> = tuning parameter</li>
<li><code>ridge$xm</code> = returns column/predictor mean from the data</li>
<li><code>ridge$scale</code> = returns the scaling performed on the predictors for the ridge regression
<ul>
<li><em><strong>Note</strong>: all the variables are divided by the biased standard deviation <span class="math inline">\(\sum (X_i - \bar X_i) / n\)</span> </em></li>
</ul></li>
<li><code>ridge$coef</code> = returns the conditional coefficients, <span class="math inline">\(\beta_j\)</span> from the ridge regression</li>
<li><code>ridge$ym</code> = return mean of outcome</li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>train(outcome ~ predictors, data=training, method=&quot;ridge&quot;, lambda=5)</code> = perform ridge regression with given outcome and predictors
<ul>
<li><code>preProcess=c(&quot;center&quot;, &quot;scale&quot;)</code> = centers and scales the predictors before the model is built
<ul>
<li><em><strong>Note</strong>: this is generally a good idea for building ridge regressions </em></li>
</ul></li>
<li><code>lambda=5</code> = tuning parameter</li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>train(outcome ~ predictors, data=training, method=&quot;foba&quot;, lambda=5, k=4)</code> = perform ridge regression with variable selection
<ul>
<li><code>lambda=5</code> = tuning parameter</li>
<li><code>k=4</code> = number of variables that should be retained
<ul>
<li>this means that <code>length(predictors)-k</code> variables will be eliminated</li>
</ul></li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>predict(model,test)</code> = use the model to predict on test set <span class="math inline">\(\rightarrow\)</span> similar to all other <code>caret</code> algorithms</li>
</ul></li>
<li><strong><em>example: ridge coefficient paths vs <span class="math inline">\(\lambda\)</span></em></strong>
<ul>
<li>using the same <code>prostate</code> dataset, we will run ridge regressions with different values of <span class="math inline">\(\lambda\)</span> and find the optimum <span class="math inline">\(\lambda\)</span> value that minimizes test RSS</li>
</ul></li>
</ul>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-58-1.png" style="display: block; margin: auto;"><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-58-2.png" style="display: block; margin: auto;"></p>
<h3 id="regularized-regression---lasso-regression">Regularized Regression - LASSO Regression</h3>
<ul>
<li>LASSO (least absolute shrinkage and selection operator) was introduced by Tibshirani (Journal of the Royal Statistical Society 1996)</li>
<li>similar to <strong>ridge</strong>, with slightly different penalty term</li>
<li>the penalized residual sum of squares (PRSS) takes the form of <span class="math display">\[PRSS(\beta_j) = \sum_{i=1}^N \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^p |\beta_j|\]</span></li>
<li>this is equivalent to solving the equation <span class="math display">\[PRSS(\beta_j) = \sum_{i=1}^N \left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2\]</span> subject to constraint <span class="math inline">\(\sum_{j=1}^p |\beta_j| \leq s\)</span> where <span class="math inline">\(s\)</span> is inversely proportional to <span class="math inline">\(\lambda\)</span></li>
<li><span class="math inline">\(\lambda\)</span> = tuning parameter
<ul>
<li>controls size of coefficients or the amount of regularization</li>
<li>large values of <span class="math inline">\(\lambda\)</span> will set some coefficient equal to zero
<ul>
<li><em><strong>Note</strong>: LASSO effectively performs model selection (choose subset of predictors) while shrinking other coefficients, where as Ridge only shrinks the coefficients </em></li>
</ul></li>
</ul></li>
<li><strong>R Commands</strong>
<ul>
<li>[<code>lars</code> package] <code>lasso&lt;-lars(as.matrix(x), y, type=&quot;lasso&quot;, trace=TRUE)</code> = perform lasso regression by adding predictors one at a time (or setting some variables to 0)
<ul>
<li><em><strong>Note</strong>: the predictors are <strong>centered and scaled first</strong> before the regression is run </em></li>
<li><code>as.matrix(x)</code> = the predictors must be in matrix/dataframe format</li>
<li><code>trace=TRUE</code> = prints progress of the lasso regression</li>
<li><code>lasso$lambda</code> = return the <span class="math inline">\(\lambda\)</span>s used for each step of the lasso regression</li>
<li><code>plot(lasso)</code> = prints plot that shows the progression of the coefficients as they are set to zero one by one</li>
<li><code>predit.lars(lasso, test)</code> = use the lasso model to predict on test data
<ul>
<li><em><strong>Note</strong>: more information/documentation can be found in <code>?predit.lars</code> </em></li>
</ul></li>
</ul></li>
<li>[<code>lars</code> package] <code>cv.lars(as.matrix(x), y, K=10, type=&quot;lasso&quot;, trace=TRUE)</code> = computes K-fold cross-validated mean squared prediction error for lasso regression
<ul>
<li>effectively the <code>lars</code> function is run <code>K</code> times with each of the folds to estimate the</li>
<li><code>K=10</code> = create 10-fold cross validation</li>
<li><code>trace=TRUE</code> = prints progress of the lasso regression</li>
</ul></li>
<li>[<code>enet</code> package] <code>lasso&lt;-enet(predictors, outcome, lambda = 0)</code> = perform elastic net regression on given predictors and outcome
<ul>
<li><code>lambda=0</code> = default value for <span class="math inline">\(\lambda\)</span>
<ul>
<li><em><strong>Note</strong>: lasso regression is a special case of elastic net regression, and forcing <code>lambda=0</code> tells the function to fit a lasso regression </em></li>
</ul></li>
<li><code>plot(lasso)</code> = prints plot that shows the progression of the coefficients as they are set to zero one by one</li>
<li><code>predict.ent(lasso, test)</code>= use the lasso model to predict on test data</li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>train(outcome ~ predictors, data=training, method=&quot;lasso&quot;)</code> = perform lasso regression with given outcome and predictors
<ul>
<li><em><strong>Note</strong>: outcome and predictors must be in the same dataframe </em></li>
<li><code>preProcess=c(&quot;center&quot;, &quot;scale&quot;)</code> = centers and scales the predictors before the model is built
<ul>
<li><em><strong>Note</strong>: this is generally a good idea for building lasso regressions </em></li>
</ul></li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>train(outcome~predictors,data=train,method=&quot;relaxo&quot;,lambda=5,phi=0.3)</code> = perform relaxed lasso regression on given predictors and outcome
<ul>
<li><code>lambda=5</code> = tuning parameter</li>
<li><code>phi=0.3</code> = relaxation parameter
<ul>
<li><code>phi=1</code> corresponds to the regular Lasso solutions</li>
<li><code>phi=0</code> computes the OLS estimates on the set of variables selected by the Lasso</li>
</ul></li>
</ul></li>
<li><a href="#caret-package"><code>caret</code> package</a> <code>predict(model,test)</code> = use the model to predict on test set <span class="math inline">\(\rightarrow\)</span> similar to all other <code>caret</code> algorithms</li>
</ul></li>
<li><strong><em>example: <code>lars</code> package</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load lars package</span></span><br><span class="line"><span class="keyword">library</span>(lars)</span><br><span class="line"><span class="comment"># perform lasso regression</span></span><br><span class="line">lasso.fit &lt;- lars(as.matrix(x), y, type=<span class="string">"lasso"</span>, trace=<span class="literal">TRUE</span>)</span><br><span class="line"><span class="comment"># plot lasso regression model</span></span><br><span class="line">plot(lasso.fit, breaks=<span class="literal">FALSE</span>, cex = <span class="number">0.75</span>)</span><br><span class="line"><span class="comment"># add legend</span></span><br><span class="line">legend(<span class="string">"topleft"</span>, covnames, pch=<span class="number">8</span>, lty=<span class="number">1</span>:length(covnames),</span><br><span class="line">	col=<span class="number">1</span>:length(covnames), cex = <span class="number">0.6</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-59-1.png" style="display: block; margin: auto;"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plots the cross validation curve</span></span><br><span class="line">lasso.cv &lt;- cv.lars(as.matrix(x), y, K=<span class="number">10</span>, type=<span class="string">"lasso"</span>, trace=<span class="literal">TRUE</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-59-2.png" style="display: block; margin: auto;"></p>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="combining-predictors">Combining Predictors</h2>
<ul>
<li><strong>combining predictors</strong> = also known as <a href="http://en.wikipedia.org/wiki/Ensemble_learning" target="_blank" rel="noopener"><em>ensembling methods in learning</em></a>, combine classifiers by averaging/voting to improve accuracy (generally)
<ul>
<li>this reduces interpretability and increases computational complexity</li>
<li>boosting/bagging/random forest algorithms all use this idea, except all classifiers averaged are of the same type</li>
</ul></li>
<li>Netflix Competition was won by a team that blended together 107 machine learning algorithms, Heritage Health Prize was also won by a combination of algorithms
<ul>
<li><em><strong>Note</strong>: the winning algorithm for Netflix was not used because it was too computationally complex/intensive, so the trade-off between accuracy and scalability is very important </em></li>
</ul></li>
<li><strong>approach</strong>
<ul>
<li>combine similar classifiers using bagging/boosting/random forest</li>
<li>combine different classifiers using model stacking/ensembling
<ul>
<li>build an odd number of models (we need an odd number for the majority vote to avoid ties)</li>
<li>predict with each model</li>
<li>combine the predictions and predict the final outcome by majority vote</li>
</ul></li>
</ul></li>
<li><strong>process: simple model ensembling</strong>
<ol type="1">
<li><strong><em>build multiple models</em></strong> on the training data set</li>
<li>use the models to <strong><em>predict</em></strong> on the training/test set
<ul>
<li><em>predict on training</em> if the data was divided to only training/test sets</li>
<li><em>predict on test</em> if the data was divided into training/test/validation sets</li>
</ul></li>
<li>combine the prediction results from each model and the true results for the training/test set into a <strong><em>new data frame</em></strong>
<ul>
<li>one column for each model</li>
<li>add the true outcome from the training/test set as a separate column</li>
</ul></li>
<li>train the new data frame with a <strong><em>new model</em></strong> the true outcome as the outcome, and the predictions from various models as predictors</li>
<li>use the combined model fit to predict results on the training/test data
<ul>
<li>calculate the RMSE for all models, including combined fit, to evaluate the accuracy of the different models</li>
<li><em><strong>Note</strong>: the RMSE for the combined fit should generally be <strong>lower</strong> than the the rest of the models </em></li>
</ul></li>
<li>to predict on the final test/validation set, use all the initial models to predict on the data set first to <strong><em>recreate a prediction data frame</em></strong> for the test/validation set like in <strong><em>step 3</em></strong>
<ul>
<li>one column for each model, no truth column this time</li>
</ul></li>
<li><strong><em>apply the combined fit</em></strong> on the combined prediction data frame to get the final resultant predictions</li>
</ol></li>
</ul>
<h3 id="example---majority-vote">Example - Majority Vote</h3>
<ul>
<li>suppose we have 5 independent classifiers/models</li>
<li>each has 70% accuracy</li>
<li><strong>majority vote accuracy (mva)</strong> = probability of the majority of the models achieving 70% at the same time</li>
</ul>
<p><span class="math display">\[\begin{aligned}
\mbox{majority vote accuracy} &amp; = p(3~correct,~2~wrong) + p(4~correct,~1~wrong) \\
&amp;\qquad+ p(5~correct) \\
&amp; = {5 \choose 3} \times(0.7)^3(0.3)^2 + {5 \choose 4} \times(0.7)^4(0.3)^1 - {5 \choose 5} (0.7)^5 \\
&amp; = 10 \times(0.7)^3(0.3)^2 + 5 \times(0.7)^4(0.3)^2 - 1 \times (0.7)^5 \\
&amp; = 83.7% \\
\end{aligned}\]</span></p>
<ul>
<li>with 101 classifiers, the majority vote accuracy becomes 99.9%</li>
</ul>
<h3 id="example---model-ensembling">Example - Model Ensembling</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(parallel)</span><br><span class="line"><span class="keyword">library</span>(doParallel)</span><br><span class="line"><span class="comment"># StartCPU</span></span><br><span class="line">cluster &lt;- makeCluster(detectCores() - <span class="number">1</span>)</span><br><span class="line">registerDoParallel(cluster)</span><br><span class="line"><span class="comment"># set up data</span></span><br><span class="line">inBuild &lt;- createDataPartition(y=Wage$wage,p=<span class="number">0.7</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">validation &lt;- Wage[-inBuild,]; buildData &lt;- Wage[inBuild,]</span><br><span class="line">inTrain &lt;- createDataPartition(y=buildData$wage,p=<span class="number">0.7</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- buildData[inTrain,]; testing &lt;- buildData[-inTrain,]</span><br><span class="line"><span class="comment"># train the data using both glm and random forest models</span></span><br><span class="line">glm.fit &lt;- train(wage ~.,method=<span class="string">"glm"</span>,data=training)</span><br><span class="line">rf.fit &lt;- train(wage ~.,method=<span class="string">"rf"</span>,data=training,</span><br><span class="line">	trControl = trainControl(method=<span class="string">"cv"</span>),number=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># use the models to predict the results on the testing set</span></span><br><span class="line">glm.pred.test &lt;- predict(glm.fit,testing)</span><br><span class="line">rf.pred.test &lt;- predict(rf.fit,testing)</span><br><span class="line"><span class="comment"># combine the prediction results and the true results into new data frame</span></span><br><span class="line">combinedTestData &lt;- data.frame(glm.pred=glm.pred.test,</span><br><span class="line">	rf.pred = rf.pred.test,wage=testing$wage)</span><br><span class="line"><span class="comment"># run a Generalized Additive Model (gam) model on the combined test data</span></span><br><span class="line">comb.fit &lt;- train(wage ~.,method=<span class="string">"gam"</span>,data=combinedTestData)</span><br><span class="line"><span class="comment"># use the resultant model to predict on the test set</span></span><br><span class="line">comb.pred.test &lt;- predict(comb.fit, combinedTestData)</span><br><span class="line"><span class="comment"># use the glm and rf models to predict results on the validation data set</span></span><br><span class="line">glm.pred.val &lt;- predict(glm.fit,validation)</span><br><span class="line">rf.pred.val &lt;- predict(rf.fit,validation)</span><br><span class="line"><span class="comment"># combine the results into data frame for the comb.fit</span></span><br><span class="line">combinedValData &lt;- data.frame(glm.pred=glm.pred.val,rf.pred=glm.pred.val)</span><br><span class="line"><span class="comment"># run the comb.fit on the combined validation data</span></span><br><span class="line">comb.pred.val &lt;- predict(comb.fit,combinedValData)</span><br><span class="line"><span class="comment"># tabulate the results - test data set RMSE Errors</span></span><br><span class="line">rbind(test = c(glm = sqrt(sum((glm.pred.test-testing$wage)^<span class="number">2</span>)),</span><br><span class="line">		rf = sqrt(sum((rf.pred.test-testing$wage)^<span class="number">2</span>)),</span><br><span class="line">		combined = sqrt(sum((comb.pred.test-testing$wage)^<span class="number">2</span>))),</span><br><span class="line">	<span class="comment"># validation data set RMSE Errors</span></span><br><span class="line">	validation = c(sqrt(sum((glm.pred.val-validation$wage)^<span class="number">2</span>)),</span><br><span class="line">		sqrt(sum((rf.pred.val-validation$wage)^<span class="number">2</span>)),</span><br><span class="line">		sqrt(sum((comb.pred.val-validation$wage)^<span class="number">2</span>))))</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                  glm        rf  combined</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># test        858.7074  893.4517  850.0222</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># validation 1061.0891 1088.4277 1056.5144</span></span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># stopCPU</span></span><br><span class="line">stopCluster(cluster)</span><br><span class="line">registerDoSEQ()</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="forecasting">Forecasting</h2>
<ul>
<li><strong>forecasting</strong> = typically used with time series, predict one or more observations into the future
<ul>
<li>data are dependent over time so subsampling/splitting data into training/test is more complicated and must be done very carefully</li>
</ul></li>
<li>specific patterns need to be considered for time series data (<em>time series decomposition</em>)
<ul>
<li><strong>trend</strong> = long term increase/decrease in data</li>
<li><strong>seasonal</strong> = patterns related to time of week/month/year/etc</li>
<li><strong>cyclic</strong> = patterns that rise/fall periodically</li>
</ul></li>
<li><em><strong>Note</strong>: issues that arise from time series are similar to those from spatial data </em>
<ul>
<li>dependency between nearby observations</li>
<li>location specific effects</li>
</ul></li>
<li>all standard predictions models can be used but requires more consideration</li>
<li><p><em><strong>Note</strong>: more detailed tutorial can be found in <a href="https://www.otexts.org/fpp/" target="_blank" rel="noopener">Rob Hyndman’s Forecasting: principles and practice</a> </em></p></li>
<li><strong>considerations for interpreting results</strong>
<ul>
<li>unrelated time series can often seem to be correlated with each other (<em>spurious correlations</em>)</li>
<li>geographic analysis may exhibit similar patterns due to population distribution/concentrations</li>
<li>extrapolations too far into future can be dangerous as they can produce in insensible results</li>
<li>dependencies over time (seasonal effects) should be examined and isolated from the trends</li>
</ul></li>
<li><strong>process</strong>
<ul>
<li>ensure the data is a time series data type</li>
<li>split data into training and test sets
<ul>
<li>both must have consecutive time points</li>
</ul></li>
<li>choose forecast approach (SMA - <code>ma</code> vs EMA - <code>ets</code>, see below) and apply corresponding functions to training data</li>
<li>apply constructed forecast model to test data using <code>forecast</code> function</li>
<li>evaluate accuracy of forecast using <code>accuracy</code> function</li>
</ul></li>
<li><strong>approaches</strong>
<ul>
<li><p><strong><em>simple moving averages</em></strong> = prediction will be made for a time point by averaging together values from a number of prior periods <span class="math display">\[ Y_{t}=\frac{1}{2*k+1}\sum_{j=-k}^k {y_{t+j}}\]</span></p></li>
<li><strong><em>exponential smoothing/exponential moving average</em></strong> = weight time points that are closer to point of prediction than those that are further away <span class="math display">\[\hat{y}_{t+1} = \alpha y_t + (1-\alpha)\hat{y}_{t-1}\]</span>
<ul>
<li><em><strong>Note</strong>: many different methods of exponential smoothing are available, more information can be found <a href="https://www.otexts.org/fpp/7/6" target="_blank" rel="noopener">here</a> </em></li>
</ul></li>
</ul></li>
</ul>
<h3 id="r-commands-and-examples-3">R Commands and Examples</h3>
<ul>
<li><code>quantmod</code> package can be used to pull trading/price information for publicly traded stocks
<ul>
<li><code>getSymbols(&quot;TICKER&quot;, src=&quot;google&quot;, from=date, to=date)</code> = gets the <strong><em>daily</em></strong> high/low/open/close price and volume information for the specified stock ticker
<ul>
<li>returns the data in a data frame under the stock ticker’s name</li>
<li><code>&quot;TICKER&quot;</code> = ticker of the stock you are attempting to pull information for</li>
<li><code>src=&quot;google&quot;</code> = get price/volume information from Google finance
<ul>
<li>default source of information is Yahoo Finance</li>
</ul></li>
<li><code>from</code> and <code>to</code> = from and to dates for the price/volume information
<ul>
<li>both arguments must be specified with <code>date</code> objects</li>
</ul></li>
<li><em><strong>Note</strong>: more information about how to use <code>getSymbols</code> can be found in the documentation <code>?getSymbols</code> </em></li>
</ul></li>
<li><code>to.monthly(GOOG)</code> = converts stock data to monthly time series from daily data
<ul>
<li>the function aggregates the open/close/high/low/volume information for each day into monthly data</li>
<li><code>GOOG</code> = data frame returned from <code>getSymbols</code> function</li>
<li><em><strong>Note</strong>: <code>?to.period</code> contains documentation for converting time series to OHLC (open high low close) series </em></li>
</ul></li>
<li><code>googOpen&lt;-Op(GOOG)</code> = returns the opening price from the stock data frame
<ul>
<li><code>Cl(), Hi(), Lo()</code> = returns the close, high and low price from the stock data frame</li>
</ul></li>
<li><code>ts(googOpen, frequency=12)</code> = convert data to a time series with <code>frequency</code> observations per time unit
<ul>
<li><code>frequency=12</code> = number of observations per unit time (12 in this case because there are 12 months in each year <span class="math inline">\(\rightarrow\)</span> converts data into <strong><em>yearly</em></strong> time series)</li>
</ul></li>
</ul></li>
<li><code>decompose(ts)</code> = decomposes time series into trend, seasonal, and irregular components by using moving averages
<ul>
<li><code>ts</code> = time series object</li>
</ul></li>
<li><code>window(ts, start=1, end=6)</code> = subsets the time series at the specified starting and ending points
<ul>
<li><code>start</code> and <code>end</code> arguments must correspond to the <strong>time unit</strong> rather than the <strong><em>index</em></strong>
<ul>
<li>for instance, if the <code>ts</code> is a yearly series (<code>frequency = 12</code>), <code>start</code>/<code>end</code> should correspond to the row numbers or year (each year has 12 observations corresponding to the months)</li>
<li><code>c(1, 7)</code> can be used to specify the element of a particular year (in this case, July of the first year/row)</li>
<li><em><strong>Note</strong>: you can use 9.5 or any decimal as values for <code>start</code>/<code>end</code>, and the closest element (June of the 9th year in this case) will be used </em></li>
<li><em><strong>Note</strong>: <code>end=9-0.01</code> can be used a short cut to specify “up to 9”, since <code>end = 9</code> will include the first element of the 9th row </em></li>
</ul></li>
</ul></li>
<li><code>forecast</code> package can be used for forecasting time series data
<ul>
<li><code>ma(ts, order=3)</code> = calculates the simple moving average for the order specified
<ul>
<li><code>order=3</code> = order of moving average smoother, effectively the number of values that should be used to calculate the moving average</li>
</ul></li>
<li><code>ets(train, model=&quot;MMM&quot;)</code> = runs exponential smoothing model on training data
<ul>
<li><code>model = &quot;MMM&quot;</code> = method used to create exponential smoothing
<ul>
<li><em><strong>Note</strong>: more information can be found at <code>?ets</code> and the corresponding model chart is <a href="https://www.otexts.org/fpp/7/6" target="_blank" rel="noopener">here</a> </em></li>
</ul></li>
</ul></li>
<li><code>forecast(ts)</code> = performs forecast on specified time series and returns 5 columns: forecast values, high/low 80 confidence intervals bounds, high/low 95 percent interval bounds
<ul>
<li><code>plot(forecast)</code> = plots the forecast object, which includes the training data, forecast values for test periods, as well as the 80 and 95 percent confidence interval regions</li>
</ul></li>
<li><code>accuracy(forecast, testData)</code> = returns the accuracy metrics (RMSE, etc.) for the forecast model</li>
</ul></li>
<li><code>quandl</code> package is also used for finance-related predictions</li>
<li><strong><em>example: decomposed time series</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load quantmod package</span></span><br><span class="line"><span class="keyword">library</span>(quantmod);</span><br><span class="line"><span class="comment"># specify to and from dates</span></span><br><span class="line">from.dat &lt;- as.Date(<span class="string">"01/01/00"</span>, format=<span class="string">"%m/%d/%y"</span>)</span><br><span class="line">to.dat &lt;- as.Date(<span class="string">"3/2/15"</span>, format=<span class="string">"%m/%d/%y"</span>)</span><br><span class="line"><span class="comment"># get data for AAPL from Google Finance for the specified dates</span></span><br><span class="line">getSymbols(<span class="string">"AAPL"</span>, src=<span class="string">"yahoo"</span>, from = from.dat, to = to.dat) <span class="comment"># Google Finance stopped providing data in March, 2018.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">## [<span class="number">1</span>] <span class="string">"AAPL"</span></span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># convert the retrieved daily data to monthly data</span></span><br><span class="line">mAAPL &lt;- to.monthly(AAPL)</span><br><span class="line"><span class="comment"># extract the closing price and convert it to yearly time series (12 observations per year)</span></span><br><span class="line">ts &lt;- ts(Cl(mAAPL), frequency = <span class="number">12</span>)</span><br><span class="line"><span class="comment"># plot the decomposed parts of the time series</span></span><br><span class="line">plot(decompose(ts),xlab=<span class="string">"Years"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-61-1.png" style="display: block; margin: auto;"></p>
<ul>
<li><strong><em>example: forecast</em></strong></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load forecast library</span></span><br><span class="line"><span class="keyword">library</span>(forecast)</span><br><span class="line"><span class="comment"># find the number of rows (years)</span></span><br><span class="line">rows &lt;- ceiling(length(ts)/<span class="number">12</span>)</span><br><span class="line"><span class="comment"># use 90% of the data to create training set</span></span><br><span class="line">ts.train &lt;- window(ts, start = <span class="number">1</span>, end = floor(rows*<span class="number">.9</span>)-<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># use the rest of data to create test set</span></span><br><span class="line">ts.test &lt;- window(ts, start = floor(rows*<span class="number">.9</span>))</span><br><span class="line"><span class="comment"># plot the training set</span></span><br><span class="line">plot(ts.train)</span><br><span class="line"><span class="comment"># add the moving average in red</span></span><br><span class="line">lines(ma(ts.train,order=<span class="number">3</span>),col=<span class="string">"red"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-62-1.png" style="display: block; margin: auto;"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compute the exponential smoothing average</span></span><br><span class="line">ets &lt;- ets(ts.train,model=<span class="string">"MMM"</span>)</span><br><span class="line"><span class="comment"># construct a forecasting model using the exponential smoothing function</span></span><br><span class="line">fcast &lt;- forecast(ets)</span><br><span class="line"><span class="comment"># plot forecast and add actual data in red</span></span><br><span class="line">plot(fcast); lines(ts.test,col=<span class="string">"red"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-62-2.png" style="display: block; margin: auto;"></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print the accuracy of the forecast model</span></span><br><span class="line">accuracy(fcast,ts.test)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                        ME      RMSE       MAE        MPE     MAPE</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Training set  -0.01738163  2.698714  1.460038  -1.763296 10.52864</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Test set     -34.49892700 35.393299 34.498927 -44.319234 44.31923</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#                   MASE       ACF1 Theil's U</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Training set 0.1785449 0.07841581        NA</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># Test set     4.2187985 0.61247460  6.792398</span></span></span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\pagebreak\)</span></p>
<h2 id="unsupervised-prediction">Unsupervised Prediction</h2>
<ul>
<li><strong>supervised classification</strong> = predicting outcome when we know what the different classifications are
<ul>
<li><em>example</em>: predicting the type of flower (setosa, versicolor, or virginica) based on sepal width/length</li>
</ul></li>
<li><strong>unsupervised classification</strong> = predicting outcome when we don’t know what the different classifications are
<ul>
<li><em>example</em>: splitting all data for sepal width/length into different groups (cluster similar data together)</li>
</ul></li>
<li><strong>process</strong>
<ul>
<li>provided that the labels for prediction/outcome are unknown, we first build clusters from observed data
<ul>
<li>creating clusters are not noiseless process, and thus may introduce higher variance/error for data</li>
<li><strong><em>K-means</em></strong> is an example of a clustering approach</li>
</ul></li>
<li>label the clusters
<ul>
<li>interpreting the clusters well (sensible vs non-sensible clusters) is incredibly challenging</li>
</ul></li>
<li>build prediction model with the clusters as the outcome
<ul>
<li>all algorithms can be applied here</li>
</ul></li>
<li>in new data set, we will predict the clusters labels</li>
</ul></li>
<li>unsupervised prediction is effectively a <strong><em>exploratory technique</em></strong>, so the resulting clusters should be carefully interpreted
<ul>
<li>clusters may be highly variable depending on the method through which the data is sample</li>
</ul></li>
<li>generally a good idea to create custom clustering algorithms for given data as it is <strong>crucial</strong> to define the process to identify clusters for interpretability and utility of the model</li>
<li>unsupervised prediction = basic approach to <a href="http://en.wikipedia.org/wiki/Recommender_system" target="_blank" rel="noopener">recommendation engines</a>, in which the tastes of the existing users are clustered and applied to new users</li>
</ul>
<h3 id="r-commands-and-examples-4">R Commands and Examples</h3>
<ul>
<li><code>kmeans(data, centers=3)</code> = can be used to perform clustering from the provided data
<ul>
<li><code>centers=3</code> = controls the number of clusters the algorithm should aim to divide the data into</li>
</ul></li>
<li><code>cl_predict</code> function in <code>clue</code> package provides similar functionality</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(gridExtra);</span><br><span class="line"><span class="comment"># load iris data</span></span><br><span class="line">data(iris)</span><br><span class="line"><span class="comment"># create training and test sets</span></span><br><span class="line">inTrain &lt;- createDataPartition(y=iris$Species,p=<span class="number">0.7</span>, list=<span class="literal">FALSE</span>)</span><br><span class="line">training &lt;- iris[inTrain,]; testing &lt;- iris[-inTrain,]</span><br><span class="line"><span class="comment"># perform k-means clustering for the data without the Species information</span></span><br><span class="line"><span class="comment"># Species = what the true clusters are</span></span><br><span class="line">kMeans1 &lt;- kmeans(subset(training,select=-c(Species)),centers=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># add clusters as new variable to training set</span></span><br><span class="line">training$clusters &lt;- as.factor(kMeans1$cluster)</span><br><span class="line"><span class="comment"># plot clusters vs Species classification</span></span><br><span class="line">p1 &lt;- qplot(Petal.Width,Petal.Length,colour=clusters,data=training) +</span><br><span class="line">	ggtitle(<span class="string">"Clusters Classification"</span>)</span><br><span class="line">p2 &lt;- qplot(Petal.Width,Petal.Length,colour=Species,data=training) +</span><br><span class="line">	ggtitle(<span class="string">"Species Classification (Truth)"</span>)</span><br><span class="line">grid.arrange(p1, p2, ncol = <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p><img src="Practical_Machine_Learning_Course_Notes_files/figure-html/unnamed-chunk-63-1.png" style="display: block; margin: auto;"></p>
<ul>
<li>as we can see, there are three clear groups that emerge from the data
<ul>
<li>this is fairly close to the actual results from Species</li>
<li>we can compare the results from the clustering and Species classification by tabulating the values</li>
</ul></li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tabulate the results from clustering and actual species</span></span><br><span class="line">table(kMeans1$cluster,training$Species)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     setosa versicolor virginica</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   1      0          2        26</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   2      0         33         9</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   3     35          0         0</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>with the clusters determined, the training data can be trained on all predictors with the clusters from k-means as outcome</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># build classification trees using the k-means cluster</span></span><br><span class="line">clustering &lt;- train(clusters ~.,data=subset(training,select=-c(Species)),method=<span class="string">"rpart"</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>we can compare the prediction results on training set vs truth</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tabulate the prediction results on training set vs truth</span></span><br><span class="line">table(predict(clustering,training),training$Species)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     setosa versicolor virginica</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   1      0          0        23</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   2      0         35        12</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   3     35          0         0</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>similarly, we can compare the prediction results on test set vs truth</li>
</ul>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tabulate the prediction results on test set vs truth</span></span><br><span class="line">table(predict(clustering,testing),testing$Species)</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#    </span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#     setosa versicolor virginica</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   1      0          0        11</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   2      0         15         4</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#   3     15          0         0</span></span></span><br></pre></td></tr></table></figure>
<center>
<strong>THE END</strong>
</center>

      
    </div>

    
      


    

    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Xing Su</li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="http://yoursite.com/2018/08/07/Practical-Machine-Learning/" title="Practical Machine Learning">http://yoursite.com/2018/08/07/Practical-Machine-Learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> unless stating additionally.</li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/R/" rel="tag"># R</a>
          
            <a href="/tags/DataScience/" rel="tag"># DataScience</a>
          
            <a href="/tags/R-Markdown/" rel="tag"># R Markdown</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/08/07/Regression-Models/" rel="next" title="Regression Models">
                <i class="fa fa-chevron-left"></i> Regression Models
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/08/07/Developing-Data-Products/" rel="prev" title="Developing Data Products">
                Developing Data Products <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC8zODY0Mi8xNTE3MA=="></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avator.png"
                alt="Autoz" />
            
              <p class="site-author-name" itemprop="name">Autoz</p>
              <p class="site-description motion-element" itemprop="description">Fidelty.</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/autolordz" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:autolordz@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#prediction"><span class="nav-number">1.</span> <span class="nav-text">Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#in-sample-vs-out-of-sample-errors"><span class="nav-number">1.1.</span> <span class="nav-text">In Sample vs Out of Sample Errors</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#prediction-study-design"><span class="nav-number">2.</span> <span class="nav-text">Prediction Study Design</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sample-division-guidelines-for-prediction-study-design"><span class="nav-number">2.1.</span> <span class="nav-text">Sample Division Guidelines for Prediction Study Design</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#picking-the-right-data"><span class="nav-number">2.2.</span> <span class="nav-text">Picking the Right Data</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#types-of-errors"><span class="nav-number">3.</span> <span class="nav-text">Types of Errors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#notable-measurements-for-error-binary-variables"><span class="nav-number">3.1.</span> <span class="nav-text">Notable Measurements for Error – Binary Variables</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#notable-measurements-for-error-continuous-variables"><span class="nav-number">3.2.</span> <span class="nav-text">Notable Measurements for Error – Continuous Variables</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#receiver-operating-characteristic-curves"><span class="nav-number">4.</span> <span class="nav-text">Receiver Operating Characteristic Curves</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cross-validation"><span class="nav-number">5.</span> <span class="nav-text">Cross Validation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#random-subsampling"><span class="nav-number">5.1.</span> <span class="nav-text">Random Subsampling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#k-fold"><span class="nav-number">5.2.</span> <span class="nav-text">K-Fold</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#leave-one-out"><span class="nav-number">5.3.</span> <span class="nav-text">Leave One Out</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#caret-package-tutorial"><span class="nav-number">6.</span> <span class="nav-text">caret Package (tutorial)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#data-slicing"><span class="nav-number">6.1.</span> <span class="nav-text">Data Slicing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#training-options-tutorial"><span class="nav-number">6.2.</span> <span class="nav-text">Training Options (tutorial)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#plotting-predictors-tutorial"><span class="nav-number">6.3.</span> <span class="nav-text">Plotting Predictors (tutorial)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#preprocessing-tutorial"><span class="nav-number">6.4.</span> <span class="nav-text">Preprocessing (tutorial)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#covariate-creationfeature-extraction"><span class="nav-number">7.</span> <span class="nav-text">Covariate Creation/Feature Extraction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#creating-dummy-variables"><span class="nav-number">7.1.</span> <span class="nav-text">Creating Dummy Variables</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#removing-zero-covariates"><span class="nav-number">7.2.</span> <span class="nav-text">Removing Zero Covariates</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#creating-splines-polynomial-functions"><span class="nav-number">7.3.</span> <span class="nav-text">Creating Splines (Polynomial Functions)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#multicore-parallel-processing"><span class="nav-number">7.4.</span> <span class="nav-text">Multicore Parallel Processing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#preprocessing-with-principal-component-analysis-pca"><span class="nav-number">8.</span> <span class="nav-text">Preprocessing with Principal Component Analysis (PCA)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#prcomp-function"><span class="nav-number">8.1.</span> <span class="nav-text">prcomp Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#caret-package"><span class="nav-number">8.2.</span> <span class="nav-text">caret Package</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#predicting-with-regression"><span class="nav-number">9.</span> <span class="nav-text">Predicting with Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#r-commands-and-examples"><span class="nav-number">9.1.</span> <span class="nav-text">R Commands and Examples</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#prediction-with-trees"><span class="nav-number">10.</span> <span class="nav-text">Prediction with Trees</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#process"><span class="nav-number">10.1.</span> <span class="nav-text">Process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#measures-of-impurity-reference"><span class="nav-number">10.2.</span> <span class="nav-text">Measures of Impurity (Reference)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#constructing-trees-with-caret-package"><span class="nav-number">10.3.</span> <span class="nav-text">Constructing Trees with caret Package</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging"><span class="nav-number">11.</span> <span class="nav-text">Bagging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#bagging-algorithms"><span class="nav-number">11.1.</span> <span class="nav-text">Bagging Algorithms</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#random-forest"><span class="nav-number">12.</span> <span class="nav-text">Random Forest</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#r-commands-and-examples-1"><span class="nav-number">12.1.</span> <span class="nav-text">R Commands and Examples</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#boosting"><span class="nav-number">13.</span> <span class="nav-text">Boosting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#r-commands-and-examples-2"><span class="nav-number">13.1.</span> <span class="nav-text">R Commands and Examples</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model-based-prediction"><span class="nav-number">14.</span> <span class="nav-text">Model Based Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#linear-discriminant-analysis"><span class="nav-number">14.1.</span> <span class="nav-text">Linear Discriminant Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#naive-bayes"><span class="nav-number">14.2.</span> <span class="nav-text">Naive Bayes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#compare-results-for-lda-and-naive-bayes"><span class="nav-number">14.3.</span> <span class="nav-text">Compare Results for LDA and Naive Bayes</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#model-selection"><span class="nav-number">15.</span> <span class="nav-text">Model Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#example-training-vs-test-error-for-combination-of-predictors"><span class="nav-number">15.1.</span> <span class="nav-text">Example: Training vs Test Error for Combination of Predictors</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#split-samples"><span class="nav-number">15.2.</span> <span class="nav-text">Split Samples</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decompose-expected-prediction-error"><span class="nav-number">15.3.</span> <span class="nav-text">Decompose Expected Prediction Error</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hard-thresholding"><span class="nav-number">15.4.</span> <span class="nav-text">Hard Thresholding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#regularized-regression-concept-resource"><span class="nav-number">15.5.</span> <span class="nav-text">Regularized Regression Concept (Resource)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#regularized-regression---ridge-regression"><span class="nav-number">15.6.</span> <span class="nav-text">Regularized Regression - Ridge Regression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#regularized-regression---lasso-regression"><span class="nav-number">15.7.</span> <span class="nav-text">Regularized Regression - LASSO Regression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#combining-predictors"><span class="nav-number">16.</span> <span class="nav-text">Combining Predictors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#example---majority-vote"><span class="nav-number">16.1.</span> <span class="nav-text">Example - Majority Vote</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example---model-ensembling"><span class="nav-number">16.2.</span> <span class="nav-text">Example - Model Ensembling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#forecasting"><span class="nav-number">17.</span> <span class="nav-text">Forecasting</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#r-commands-and-examples-3"><span class="nav-number">17.1.</span> <span class="nav-text">R Commands and Examples</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#unsupervised-prediction"><span class="nav-number">18.</span> <span class="nav-text">Unsupervised Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#r-commands-and-examples-4"><span class="nav-number">18.1.</span> <span class="nav-text">R Commands and Examples</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Autoz</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Symbols count total: </span>
    
    <span title="Symbols count total">526k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    
    <span title="Reading time total">7:59</span>
  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.5.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Gemini</a> v6.3.0</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="Total Visitors">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="Total Views">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.3.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=6.3.0"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=6.3.0"></script>


  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('Copy').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('Copied')
          else $(this).text('Copy failed')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('Copy')
        }, 300)
      }).append(e)
    })
  </script>


</body>
</html>
